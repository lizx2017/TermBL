issueID:CARBONDATA-1
type:New Feature
changed files:
texts:CarbonData supports docker for simplifying deployment
CarbonData supports docker for simplifying deployment
issueID:CARBONDATA-10
type:Bug
changed files:
texts:Avoid to much logging of timestamp parsing exception in TimeStampDirectDictionaryGenerator
1. Avoid to much logging of timestamp parsing exception in TimeStampDirectDictionaryGenerator.2. No logging the parsing fail for configured "carbon.cutOffTimestamp" while calculating the cutOffTimeStamp
issueID:CARBONDATA-100
type:Improvement
changed files:processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactDataHandlerColumnar.java
texts:BigInt compression
In Carbon bigint is stored as long. There is no compression done on data.Change is required to do compression on bigint data as we do for double
issueID:CARBONDATA-1000
type:Bug
changed files:
texts:decimal datatype shouldn&#39;t give exception either new precision or scale is greater than or equl to the old precision or scale during alter table

issueID:CARBONDATA-1001
type:Bug
changed files:
texts:Data type change should support int to long conversion

issueID:CARBONDATA-1002
type:Bug
changed files:
texts:Results order does not display same as hive in Carbon data .
Result order does not display same as hive in Carbondata.Steps to reproduces:1: CARBONDATA:a):Create table in CarbonDatacreate table employee (Id int,Name String,Salary int,Designation String,Dept String) STORED BY 'org.apache.carbondata.format';b):Load data in tableLOAD DATA INPATH 'hdfs://localhost:54310/Employee1.csv' into table employee;c): select * from employee;-----------------------------------------------------------------------+    Id     Name    Salary        Designation                   Dept            -----------------------------------------------------------------------+ 1000001   Zoe    8567816   J BUSH & CO             Warehouse/Equipment Agent   900001    Zoe    6380353   J C MALONE ASSOCIATES   Water Services Technician   800001    Zoe    2937793   J C P & L CO            Websphere Consultant        700001    Zoe    9237710   J C PENNEY              Wedding Consultant          600001    Zoe    2663980   J C PENNEY CO           Wedding Coordinator         500001    Zoe    6355842   J C PENNEY CO INC       Wedding Sales Manager       400001    Zoe    3966825   J C PENNEY COMPANY      Weight Loss Consultant      300001    Zoe    7679689   J D CHADNEY MD          Welder                      200001    Zoe    9589193   J DOMANISH ARCHT        Welding Engineer            100001    Zoe    7958183   J F K HIGH SCHOOL       Wheelchair agent            1         Zoe    3640571   J GRAHAM BROWN CANCER   Yachting                   -----------------------------------------------------------------------+2:HIVEa):Create table in CarbonDatacreate table employeeH (Id int,Name String,Salary int,Designation String,Dept String) ROW FORMAT DELIMITED FIELDS TERMINATED BY ",";b):Load data in tableLOAD DATA LOCAL INPATH '/home/vinod/Desktop/AllCSV/Employee1.csv'OVERWRITE INTO TABLE employeeH;c: select * from employeeH;-----------------------------------------------------------------------+    Id     Name    Salary        Designation                   Dept            -----------------------------------------------------------------------+ 1         Zoe    3640571   J GRAHAM BROWN CANCER   Yachting                    100001    Zoe    7958183   J F K HIGH SCHOOL       Wheelchair agent            200001    Zoe    9589193   J DOMANISH ARCHT        Welding Engineer            300001    Zoe    7679689   J D CHADNEY MD          Welder                      400001    Zoe    3966825   J C PENNEY COMPANY      Weight Loss Consultant      500001    Zoe    6355842   J C PENNEY CO INC       Wedding Sales Manager       600001    Zoe    2663980   J C PENNEY CO           Wedding Coordinator         700001    Zoe    9237710   J C PENNEY              Wedding Consultant          800001    Zoe    2937793   J C P & L CO            Websphere Consultant        900001    Zoe    6380353   J C MALONE ASSOCIATES   Water Services Technician   1000001   Zoe    8567816   J BUSH & CO             Warehouse/Equipment Agent  -----------------------------------------------------------------------+
issueID:CARBONDATA-1003
type:Improvement
changed files:
texts:Inaccurate result displays while using covar_pop and  covar_samp aggregate functions in presto integration
Inaccurate result displays while using covar_pop and  covar_samp aggregate functions in presto integration..Steps to reproduce :1. In CarbonData:a) Create table:CREATE TABLE uniqdata (CUST_ID int,CUST_NAME String,ACTIVE_EMUI_VERSION string, DOB timestamp, DOJ timestamp, BIGINT_COLUMN1 bigint,BIGINT_COLUMN2 bigint,DECIMAL_COLUMN1 decimal(30,10), DECIMAL_COLUMN2 decimal(36,10),Double_COLUMN1 double, Double_COLUMN2 double,INTEGER_COLUMN1 int) STORED BY 'org.apache.carbondata.format' TBLPROPERTIES ("TABLE_BLOCKSIZE"= "256 MB");b) Load data : LOAD DATA INPATH 'hdfs://localhost:54310/2000_UniqData.csv' into table uniqdata OPTIONS('DELIMITER'=',' , 'QUOTECHAR'='"','BAD_RECORDS_ACTION'='FORCE','FILEHEADER'='CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1,Double_COLUMN2,INTEGER_COLUMN1');2. In presto  Execute the query: 1.) select covar_pop(BIGINT_COLUMN1,BIGINT_COLUMN1) as a  from (select BIGINT_COLUMN1 from uniqdata order by BIGINT_COLUMN1) tActual result:In CarbonData :"-----------------------+           a           -----------------------+ 6.158207330830757E20  -----------------------+1 row selected (0.86 seconds)"In presto:"     a      ------------- 6.158207E20 (1 row)Query 20170419_063811_00020_khh7w, FINISHED, 1 nodeSplits: 35 total, 35 done (100.00%)0:00 &#91;2.01K rows, 1.97KB&#93; &#91;8.39K rows/s, 8.21KB/s&#93;"2.)select covar_samp(BIGINT_COLUMN1,BIGINT_COLUMN1) as a  from (select BIGINT_COLUMN1 from uniqdata order by BIGINT_COLUMN1) tActual result:In CarbonData:"-----------------------+           a           -----------------------+ 6.161286434496173E20  -----------------------+1 row selected (0.764 seconds)"In presto:"      a      ------------- 6.161286E20 (1 row)Query 20170419_070158_00021_khh7w, FINISHED, 1 nodeSplits: 35 total, 35 done (100.00%)0:00 &#91;2.01K rows, 1.97KB&#93; &#91;7.09K rows/s, 6.94KB/s&#93;"Expected result :it should display the same result as showing in CarbonData.
issueID:CARBONDATA-1004
type:Bug
changed files:
texts:Broadcast join is not happening in spark 2.1
In spark broad cast join should happen on small tables to optimize the query execution and avoid shuffling. But it is not happening in spark 2.1
issueID:CARBONDATA-1005
type:Bug
changed files:processing/src/main/java/org/apache/carbondata/processing/store/writer/v3/CarbonFactDataWriterImplV3.java
processing/src/main/java/org/apache/carbondata/processing/loading/steps/DataConverterProcessorStepImpl.java
processing/src/main/java/org/apache/carbondata/processing/store/writer/v3/BlockletDataHolder.java
processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactDataHandlerColumnar.java
processing/src/main/java/org/apache/carbondata/processing/loading/converter/impl/RowConverterImpl.java
texts:Data load does not load all rows when data size is multiples of page size
Data load does not load all rows when data size is multiples of page size. The page size of data load is 32000 , so if my data size is multiples 32000 like 64000 then it cannot load all data
issueID:CARBONDATA-1006
type:Bug
changed files:
texts:Range flter test case is failing in current master
Range flter test case is failing in current master
issueID:CARBONDATA-1007
type:Bug
changed files:processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/UnsafeSortDataRows.java
core/src/main/java/org/apache/carbondata/core/memory/IntPointerBuffer.java
texts:Current unsafe sort does not keep pointers in memory
Current unsafe sort does not keep pointers in memory. Memory profiler shows considerable amount of java heap taken by row pointers.
issueID:CARBONDATA-1008
type:Sub-task
changed files:
texts:Make Hive table schema compatible with spark sql
Now we need alter table schema after creating table by spark.So can we make it compatible when creating table?
issueID:CARBONDATA-1009
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/scan/expression/conditional/InExpression.java
core/src/main/java/org/apache/carbondata/core/scan/expression/conditional/NotInExpression.java
texts:Select statement is not working with empty string in where clause.
Steps to reproduces:1- Create table in hive:CREATE TABLE uniqdata_h (CUST_ID int,CUST_NAME String,ACTIVE_EMUI_VERSION string, DOB timestamp, DOJ timestamp, BIGINT_COLUMN1 bigint,BIGINT_COLUMN2 bigint,DECIMAL_COLUMN1 decimal(30,10), DECIMAL_COLUMN2 decimal(36,10),Double_COLUMN1 double, Double_COLUMN2 double,INTEGER_COLUMN1 int) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' ;2:Load data in Hive:load data local inpath '/home/manoj/Downloads/2000_UniqData.csv' into table uniqdata_h ;load data local inpath '/home/manoj/Downloads/4000_UniqData.csv' into table uniqdata_h ;load data local inpath '/home/manoj/Downloads/6000_UniqData.csv' into table uniqdata_h ;load data local inpath '/home/manoj/Downloads/7000_UniqData.csv' into table uniqdata_h ;load data local inpath '/home/manoj/3000_1_UniqData.csv' into table uniqdata_h ;3: Execute Selet queries:select CUST_ID,CUST_NAME,DOB,BIGINT_COLUMN1,DECIMAL_COLUMN1,Double_COLUMN1,INTEGER_COLUMN1 from uniqdata_h where CUST_ID in ('10020','10030','10032','10035','10040','10060','',NULL,' ')  and  BIGINT_COLUMN1 in (123372037874,123372037884,123372037886,123372037889,'',NULL,' ') ;Result:--------------------------------------------------------------------------------------------------------------------------+ CUST_ID      CUST_NAME               DOB            BIGINT_COLUMN1       DECIMAL_COLUMN1         Double_COLUMN1     INTEGER_COLUMN1  --------------------------------------------------------------------------------------------------------------------------+ 10020     CUST_NAME_01020   1972-10-17 01:00:03.0   123372037874     12345679921.1234000000   1.12345674897976E10   1021              10030     CUST_NAME_01030   1972-10-27 01:00:03.0   123372037884     12345679931.1234000000   1.12345674897976E10   1031              10032     CUST_NAME_01032   1972-10-29 01:00:03.0   123372037886     12345679933.1234000000   1.12345674897976E10   1033              10035     CUST_NAME_01035   1972-11-01 01:00:03.0   123372037889     12345679936.1234000000   1.12345674897976E10   1036             --------------------------------------------------------------------------------------------------------------------------+........................................................................................................................................Create table in Carbondata..1: create table:CREATE TABLE uniqdata (CUST_ID int,CUST_NAME String,ACTIVE_EMUI_VERSION string, DOB timestamp, DOJ timestamp, BIGINT_COLUMN1 bigint,BIGINT_COLUMN2 bigint,DECIMAL_COLUMN1 decimal(30,10), DECIMAL_COLUMN2 decimal(36,10),Double_COLUMN1 double, Double_COLUMN2 double,INTEGER_COLUMN1 int) STORED BY 'org.apache.carbondata.format' TBLPROPERTIES ("TABLE_BLOCKSIZE"= "256 MB")2:Load data in table:LOAD DATA INPATH 'HDFS_URL/BabuStore/Data/uniqdata/2000_UniqData_tabdelm.csv' into table uniqdata OPTIONS('DELIMITER'='/' , 'QUOTECHAR'='"','BAD_RECORDS_ACTION'='FORCE','FILEHEADER'='CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1,Double_COLUMN2,INTEGER_COLUMN1') ;LOAD DATA INPATH 'HDFS_URL/BabuStore/Data/uniqdata/3000_UniqDatatdelm.csv' into table uniqdata OPTIONS('DELIMITER'='\t' , 'QUOTECHAR'='"','BAD_RECORDS_ACTION'='FORCE','FILEHEADER'='CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1,Double_COLUMN2,INTEGER_COLUMN1') ;LOAD DATA INPATH 'HDFS_URL/BabuStore/Data/uniqdata/4000_UniqDataquotedelm.csv' into table uniqdata OPTIONS('DELIMITER'='"' , 'QUOTECHAR'='"','BAD_RECORDS_ACTION'='FORCE','FILEHEADER'='CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1,Double_COLUMN2,INTEGER_COLUMN1') ;LOAD DATA INPATH 'HDFS_URL/BabuStore/Data/uniqdata/5000_UniqData!delm.csv' into table uniqdata OPTIONS('DELIMITER'='!' , 'QUOTECHAR'='"','BAD_RECORDS_ACTION'='FORCE','FILEHEADER'='CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1,Double_COLUMN2,INTEGER_COLUMN1') ;LOAD DATA INPATH 'HDFS_URL/BabuStore/Data/uniqdata/6000_UniqData|delm.csv' into table uniqdata OPTIONS('DELIMITER'='|' , 'QUOTECHAR'='"','BAD_RECORDS_ACTION'='FORCE','FILEHEADER'='CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1,Double_COLUMN2,INTEGER_COLUMN1') ;LOAD DATA INPATH 'HDFS_URL/BabuStore/Data/uniqdata/7000_UniqDatadelm.csv' into table uniqdata OPTIONS('DELIMITER'='' , 'QUOTECHAR'='"','BAD_RECORDS_ACTION'='FORCE','FILEHEADER'='CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1,Double_COLUMN2,INTEGER_COLUMN1') ;LOAD DATA INPATH 'HDFS_URL/BabuStore/Data/uniqdata/3000_1_UniqData.csv' into table uniqdata OPTIONS('DELIMITER'=',' , 'QUOTECHAR'='"','BAD_RECORDS_ACTION'='FORCE','FILEHEADER'='CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1,Double_COLUMN2,INTEGER_COLUMN1') ;3: Execute Selet queries:select CUST_ID,CUST_NAME,DOB,BIGINT_COLUMN1,DECIMAL_COLUMN1,Double_COLUMN1,INTEGER_COLUMN1 from uniqdata where CUST_ID in ('10020','10030','10032','10035','10040','10060','',NULL,' ')  and  BIGINT_COLUMN1 in (123372037874,123372037884,123372037886,123372037889,'',NULL,' ') ;Result:Error: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 56.0 failed 1 times, most recent failure: Lost task 1.0 in stage 56.0 (TID 216, localhost, executor driver): org.apache.spark.util.TaskCompletionListenerException: java.util.concurrent.ExecutionException: java.lang.NullPointerException at org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:105) at org.apache.spark.scheduler.Task.run(Task.scala:112) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745)Driver stacktrace: (state=,code=0)
issueID:CARBONDATA-1010
type:Bug
changed files:
texts:Data loaded differently in Hive & Carbondata because of that comparision failure occurs on Select * statement
carbondata:CREATE TABLE uniqdata_char (CUST_ID int,CUST_NAME char(30),ACTIVE_EMUI_VERSION char(30), DOB timestamp,DOJ timestamp, BIGINT_COLUMN1 bigint,BIGINT_COLUMN2 bigint,DECIMAL_COLUMN1 decimal(30,10),DECIMAL_COLUMN2 decimal(36,10),Double_COLUMN1 double, Double_COLUMN2 double,INTEGER_COLUMN1 int) STORED BY 'org.apache.carbondata.format'TBLPROPERTIES ("TABLE_BLOCKSIZE"= "256 MB")LOAD DATA INPATH 'hdfs://192.168.2.145:54310/BabuStore/Data/uniqdata/2000_UniqData.csv' into table uniqdata_char OPTIONS('DELIMITER'=',' ,'QUOTECHAR'='"','FILEHEADER'='CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1,Double_COLUMN2,INTEGER_COLUMN1')0: jdbc:hive2://hadoop-master:10000> ;---------+ Result  ---------+---------+No rows selected (3.988 seconds)0: jdbc:hive2://hadoop-master:10000> select CUST_NAME from uniqdata_char;------------------+    CUST_NAME     ------------------+             CUST_NAME_00000   CUST_NAME_00000   CUST_NAME_00001   CUST_NAME_00002   CUST_NAME_00003   CUST_NAME_00004   CUST_NAME_00005   CUST_NAME_00006   CUST_NAME_00007   CUST_NAME_00008   CUST_NAME_00009   CUST_NAME_00010  Hive: CREATE TABLE uniqdata_char (CUST_ID int,CUST_NAME char(30),ACTIVE_EMUI_VERSION char(30), DOB timestamp,DOJ timestamp, BIGINT_COLUMN1 bigint,BIGINT_COLUMN2 bigint,DECIMAL_COLUMN1 decimal(30,10),DECIMAL_COLUMN2 decimal(36,10),Double_COLUMN1 double, Double_COLUMN2 double,INTEGER_COLUMN1 int)ROW FORMAT DELIMITED FIELDS TERMINATED BY ','---------+ result  ---------+---------+No rows selected (0.093 seconds)0: jdbc:hive2://hadoop-master:10000> LOAD DATA LOCAL INPATH  '/opt/Carbon/CarbonData/TestData/Data/uniqdata/2000_UniqData.csv' into table uniqdata_char ;---------+ Result  ---------+---------+No rows selected (0.228 seconds)0: jdbc:hive2://hadoop-master:10000> select CUST_NAME from uniqdata_char ;---------------------------------+            CUST_NAME            ---------------------------------+   CUST_NAME_00000                            CUST_NAME_00000                  CUST_NAME_00001                  CUST_NAME_00002                  CUST_NAME_00003                  CUST_NAME_00004                  CUST_NAME_00005                  CUST_NAME_00006                  CUST_NAME_00007                  CUST_NAME_00008                  CUST_NAME_00009                  CUST_NAME_00010                 There is a data mismatch in select query. In Carbondata all the empty space is displayed first where as the same is not performed in hive. Because odf that comparison failures occurs while executing query in automation framework.
issueID:CARBONDATA-1011
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/scan/collector/impl/RestructureBasedVectorResultCollector.java
texts:select * doesn&#39;t work after adding column of date type
Select * from <tablename> doesn't work , after a new column of date type(with default value) has been added to the table.Steps to reproduce : CREATE TABLE uniqdata2 (CUST_ID int,CUST_NAME String,ACTIVE_EMUI_VERSION string, DOB timestamp, DOJ timestamp, BIGINT_COLUMN1 bigint,BIGINT_COLUMN2 bigint,DECIMAL_COLUMN1 decimal(30,10), DECIMAL_COLUMN2 decimal(36,10),Double_COLUMN1 double, Double_COLUMN2 double,INTEGER_COLUMN1 int) STORED BY 'org.apache.carbondata.format' TBLPROPERTIES ("TABLE_BLOCKSIZE"= "256 MB");LOAD DATA INPATH 'hdfs://localhost:54310/Files/3000_UniqData.csv' into table uniqdata2 OPTIONS('DELIMITER'=',' , 'QUOTECHAR'='"','BAD_RECORDS_ACTION'='FORCE','FILEHEADER'='CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1,Double_COLUMN2,INTEGER_COLUMN1');Query before alter : select * from uniqdata2;                           //Works finealter table uniqdata2 add columns(date1 date) TBLPROPERTIES('DEFAULT.VALUE.date1'= '2017-01-01');Query after alter : select * from uniqdata2;Expected Output - Display all the data of the table.Actual Output - Error: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 201.0 failed 4 times, most recent failure: Lost task 0.3 in stage 201.0 (TID 402, 192.168.1.7, executor 0): java.lang.ClassCastException: java.lang.Integer cannot be cast to java.lang.Long at org.apache.carbondata.core.scan.collector.impl.RestructureBasedVectorResultCollector.fillDirectDictionaryData(RestructureBasedVectorResultCollector.java:167) at org.apache.carbondata.core.scan.collector.impl.RestructureBasedVectorResultCollector.fillDataForNonExistingDimensions(RestructureBasedVectorResultCollector.java:130) at org.apache.carbondata.core.scan.collector.impl.RestructureBasedVectorResultCollector.collectVectorBatch(RestructureBasedVectorResultCollector.java:112) at org.apache.carbondata.core.scan.processor.impl.DataBlockIteratorImpl.processNextBatch(DataBlockIteratorImpl.java:65) at org.apache.carbondata.core.scan.result.iterator.VectorDetailQueryResultIterator.processNextBatch(VectorDetailQueryResultIterator.java:46) at org.apache.carbondata.spark.vectorreader.VectorizedCarbonRecordReader.nextBatch(VectorizedCarbonRecordReader.java:251) at org.apache.carbondata.spark.vectorreader.VectorizedCarbonRecordReader.nextKeyValue(VectorizedCarbonRecordReader.java:141) at org.apache.carbondata.spark.rdd.CarbonScanRDD$$anon$1.hasNext(CarbonScanRDD.scala:221) at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.scan_nextBatch$(Unknown Source) at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source) at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43) at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377) at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:231) at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:225) at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:826) at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:826) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) at org.apache.spark.rdd.RDD.iterator(RDD.scala:287) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87) at org.apache.spark.scheduler.Task.run(Task.scala:99) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745)Driver stacktrace: (state=,code=0)
issueID:CARBONDATA-1013
type:Bug
changed files:
texts:Unexpected characters displays in results while using join query.
Unexpected characters displays in result-set while using join query.Steps to reproduce:1) Create tables:a) In carbondata:table 1 : create table Comp_TABLE_ONE_JOIN (customer_uid String,customer_id int, gender String, first_name String, middle_name String, last_name String,customer_address String, country String) STORED BY 'org.apache.carbondata.format' TBLPROPERTIES('DICTIONARY_EXCLUDE'='customer_uid')table 2: create table Comp_TABLE_TWO_JOIN (customer_payment_id String,customer_id int,payment_amount Decimal(15,5), payment_mode String,payment_details String) STORED BY 'org.apache.carbondata.format' TBLPROPERTIES('DICTIONARY_EXCLUDE'='customer_payment_id')b) In hive:table 1: create table Comp_TABLE_ONE_JOIN_h (customer_uid String,customer_id int, gender String, first_name String, middle_name String, last_name String,customer_address String, country String)ROW FORMAT DELIMITED FIELDS TERMINATED BY ",";table 2:create table Comp_TABLE_TWO_JOIN_h (customer_payment_id String,customer_id int,payment_amount Decimal(15,5), payment_mode String,payment_details String) ROW FORMAT DELIMITED FIELDS TERMINATED BY ",";2) Load Data :a) In Carbondata:table 1 : LOAD DATA INPATH 'HDFS_URL/BabuStore/Data/customer_C1.csv' INTO table Comp_TABLE_ONE_JOIN options ('DELIMITER'=',','QUOTECHAR'='"','BAD_RECORDS_ACTION'='FORCE','FILEHEADER'='customer_uid,customer_id,gender,first_name,middle_name,last_name,customer_address,country')table 2:LOAD DATA INPATH 'HDFS_URL/BabuStore/Data/payment_C1.csv' INTO table Comp_TABLE_TWO_JOIN options ('DELIMITER'=',','QUOTECHAR'='"','BAD_RECORDS_ACTION'='FORCE','FILEHEADER'='customer_payment_id ,customer_id,payment_amount ,payment_mode, payment_details')b) In hive:table 1: LOAD DATA LOCAL INPATH '/home/knoldus/Desktop/csv/TestData2/Data/customer_C1.csv' INTO table Comp_TABLE_ONE_JOIN_h;table 2: LOAD DATA LOCAL INPATH '/home/knoldus/Desktop/csv/TestData2/Data/payment_C1.csv' INTO table Comp_TABLE_TWO_JOIN_h;3)Execute query:select * from Comp_TABLE_ONE_JOIN join Comp_TABLE_TWO_JOIN on Comp_TABLE_ONE_JOIN.customer_id=Comp_TABLE_TWO_JOIN.customer_id limit  2;Actual Result:a) In Carbondata:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ customer_uid   customer_id   gender   first_name   middle_name   last_name   customer_address    country     customer_payment_id   customer_id   payment_amount   payment_mode         payment_details       --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ UID31a31       31            female   fname31      mname31       lname31     address 31         country31   Cust_payment_ID31a31   31            193288.72000     p     UID31a31       31            female   fname31      mname31       lname31     address 31         country31   Cust_payment_ID31a31   31            193288.72000     p    --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+2 rows selected (0.499 seconds)b) In Hive---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- customer_uid   customer_id   gender   first_name   middle_name   last_name   customer_address    country   customer_payment_id   customer_id   payment_amount   payment_mode           payment_details         ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ UID1a1         1             female   fname1       mname1        lname1      address 1          country1   Cust_payment_ID1a1    1             6235.12000       debit          details of the payment for : 1   UID1a1         1             female   fname1       mname1        lname1      address 1          country1   Cust_payment_ID1a1    1             6235.12000       debit          details of the payment for : 1  ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+2 rows selected (0.224 seconds)Expected result: unwanted characters should not be displayed.
issueID:CARBONDATA-1015
type:Sub-task
changed files:processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/SortParameters.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/merger/UnsafeIntermediateFileMerger.java
processing/src/main/java/org/apache/carbondata/processing/loading/steps/DataWriterProcessorStepImpl.java
core/src/main/java/org/apache/carbondata/core/util/CarbonProperties.java
processing/src/main/java/org/apache/carbondata/processing/datatypes/StructDataType.java
core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
core/src/main/java/org/apache/carbondata/core/datastore/exception/CarbonDataWriterException.java
core/src/main/java/org/apache/carbondata/core/datastore/block/SegmentProperties.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/holder/UnsafeSortTempFileChunkHolder.java
processing/src/main/java/org/apache/carbondata/processing/loading/DataLoadProcessBuilder.java
processing/src/main/java/org/apache/carbondata/processing/loading/CarbonDataLoadConfiguration.java
processing/src/main/java/org/apache/carbondata/processing/merger/CompactionResultSortProcessor.java
core/src/main/java/org/apache/carbondata/core/util/AbstractDataFileFooterConverter.java
processing/src/main/java/org/apache/carbondata/processing/merger/RowResultMergerProcessor.java
core/src/main/java/org/apache/carbondata/core/memory/IntPointerBuffer.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/EncodedData.java
core/src/main/java/org/apache/carbondata/core/util/ByteUtil.java
processing/src/main/java/org/apache/carbondata/processing/store/writer/AbstractFactDataWriter.java
core/src/main/java/org/apache/carbondata/core/scan/executor/impl/AbstractQueryExecutor.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/column/ColumnSchema.java
core/src/main/java/org/apache/carbondata/core/datastore/row/CarbonRow.java
processing/src/main/java/org/apache/carbondata/processing/loading/steps/InputProcessorStepImpl.java
core/src/main/java/org/apache/carbondata/core/metadata/ValueEncoderMeta.java
processing/src/main/java/org/apache/carbondata/processing/loading/steps/SortProcessorStepImpl.java
core/src/main/java/org/apache/carbondata/core/datastore/ColumnType.java
processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactDataHandlerModel.java
processing/src/main/java/org/apache/carbondata/processing/loading/steps/DataConverterProcessorStepImpl.java
processing/src/main/java/org/apache/carbondata/processing/util/CarbonDataProcessorUtil.java
core/src/main/java/org/apache/carbondata/core/util/CarbonMetadataUtil.java
processing/src/main/java/org/apache/carbondata/processing/loading/row/CarbonRowBatch.java
processing/src/main/java/org/apache/carbondata/processing/loading/converter/FieldConverter.java
processing/src/main/java/org/apache/carbondata/processing/loading/converter/impl/MeasureFieldConverterImpl.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/merger/UnsafeSingleThreadFinalSortFilesMerger.java
processing/src/main/java/org/apache/carbondata/processing/loading/converter/impl/FieldEncoderFactory.java
core/src/main/java/org/apache/carbondata/core/datastore/page/statistics/SimpleStatsResult.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/SortDataRows.java
processing/src/main/java/org/apache/carbondata/processing/loading/steps/CarbonRowDataWriterProcessorStepImpl.java
core/src/main/java/org/apache/carbondata/core/util/DataTypeUtil.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/SingleThreadFinalSortFilesMerger.java
core/src/main/java/org/apache/carbondata/core/metadata/datatype/DataType.java
core/src/main/java/org/apache/carbondata/core/scan/executor/util/RestructureUtil.java
processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactDataHandlerColumnar.java
processing/src/main/java/org/apache/carbondata/processing/datatypes/GenericDataType.java
core/src/main/java/org/apache/carbondata/core/scan/executor/util/QueryUtil.java
core/src/main/java/org/apache/carbondata/core/util/NonDictionaryUtil.java
processing/src/main/java/org/apache/carbondata/processing/store/writer/v3/CarbonFactDataWriterImplV3.java
core/src/main/java/org/apache/carbondata/core/datastore/page/ColumnPage.java
core/src/main/java/org/apache/carbondata/core/datastore/TableSpec.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/impl/ParallelReadMergeSorterImpl.java
core/src/main/java/org/apache/carbondata/core/datastore/page/statistics/TablePageStatistics.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/impl/ParallelReadMergeSorterWithBucketingImpl.java
processing/src/main/java/org/apache/carbondata/processing/store/TablePage.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/SortTempFileChunkHolder.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/reader/measure/v3/MeasureChunkReaderV3.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/UnsafeSortDataRows.java
processing/src/main/java/org/apache/carbondata/processing/loading/converter/impl/NonDictionaryFieldConverterImpl.java
core/src/main/java/org/apache/carbondata/core/datastore/columnar/BlockIndexerStorageForNoInvertedIndexForShort.java
processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactHandler.java
core/src/main/java/org/apache/carbondata/core/datastore/page/key/TablePageKey.java
processing/src/main/java/org/apache/carbondata/processing/store/writer/CarbonFactDataWriter.java
processing/src/main/java/org/apache/carbondata/processing/datatypes/PrimitiveDataType.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/UnsafeCarbonRowPage.java
processing/src/main/java/org/apache/carbondata/processing/loading/AbstractDataLoadProcessorStep.java
core/src/main/java/org/apache/carbondata/core/metadata/CarbonMetadata.java
processing/src/main/java/org/apache/carbondata/processing/loading/converter/RowConverter.java
processing/src/main/java/org/apache/carbondata/processing/loading/converter/impl/RowConverterImpl.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/column/CarbonImplicitDimension.java
processing/src/main/java/org/apache/carbondata/processing/loading/converter/impl/DirectDictionaryFieldConverterImpl.java
processing/src/main/java/org/apache/carbondata/processing/datatypes/ArrayDataType.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
core/src/main/java/org/apache/carbondata/core/datastore/page/ComplexColumnPage.java
processing/src/main/java/org/apache/carbondata/processing/loading/row/CarbonSortBatch.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/column/CarbonDimension.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/ColumnPageCodec.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/impl/UnsafeParallelReadMergeSorterImpl.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/IntermediateFileMerger.java
processing/src/main/java/org/apache/carbondata/processing/loading/converter/impl/ComplexFieldConverterImpl.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/impl/UnsafeParallelReadMergeSorterWithBucketingImpl.java
texts:Refactor write step to use ColumnarPage

issueID:CARBONDATA-1017
type:Sub-task
changed files:core/src/main/java/org/apache/carbondata/core/datastore/page/LazyColumnPage.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/DeltaIntegralCodec.java
core/src/main/java/org/apache/carbondata/core/datastore/page/ColumnPageValueConverter.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/AdaptiveIntegralCodec.java
core/src/main/java/org/apache/carbondata/core/metadata/datatype/DataType.java
processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactDataHandlerColumnar.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
processing/src/main/java/org/apache/carbondata/processing/store/writer/v3/CarbonFactDataWriterImplV3.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/adaptive/AdaptiveCodec.java
core/src/main/java/org/apache/carbondata/core/scan/result/BlockletScannedResult.java
core/src/main/java/org/apache/carbondata/core/datastore/page/ColumnPage.java
core/src/main/java/org/apache/carbondata/core/datastore/page/statistics/ColumnPageStatsCollector.java
core/src/main/java/org/apache/carbondata/core/datastore/page/statistics/TablePageStatistics.java
processing/src/main/java/org/apache/carbondata/processing/store/TablePage.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/reader/measure/AbstractMeasureChunkReader.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/reader/measure/v3/MeasureChunkReaderV3.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/UnsafeSortDataRows.java
core/src/main/java/org/apache/carbondata/core/memory/IntPointerBuffer.java
processing/src/main/java/org/apache/carbondata/processing/store/writer/CarbonFactDataWriter.java
core/src/main/java/org/apache/carbondata/core/scan/collector/impl/AbstractScannedResultCollector.java
core/src/main/java/org/apache/carbondata/core/memory/MemoryException.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/UnsafeCarbonRowPage.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/column/ColumnSchema.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/DirectCompressCodec.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/column/CarbonImplicitDimension.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/sort/UnsafeIntSortDataFormat.java
core/src/main/java/org/apache/carbondata/core/datastore/page/ComplexColumnPage.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/EncodingFactory.java
core/src/main/java/org/apache/carbondata/core/memory/UnsafeMemoryManager.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/ColumnPageCodec.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/impl/UnsafeParallelReadMergeSorterImpl.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/DefaultEncodingFactory.java
core/src/main/java/org/apache/carbondata/core/scan/result/vector/MeasureDataVectorProcessor.java
core/src/main/java/org/apache/carbondata/core/scan/expression/conditional/NotEqualsExpression.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/impl/UnsafeParallelReadMergeSorterWithBucketingImpl.java
texts:Add interface for column encoding and compression
Open up interface for:1. Make current encoding and compression decoupled, so that they can be controlled separately.2. Let user choose encoding and compression method by config or table option.
issueID:CARBONDATA-1018
type:Sub-task
changed files:core/src/main/java/org/apache/carbondata/core/datastore/page/LazyColumnPage.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/impl/MeasureRawColumnChunk.java
core/src/main/java/org/apache/carbondata/core/datastore/page/UnsafeVarLengthColumnPage.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/DeltaIntegralCodec.java
core/src/main/java/org/apache/carbondata/core/datastore/page/VarLengthColumnPageBase.java
core/src/main/java/org/apache/carbondata/core/memory/IntPointerBuffer.java
core/src/main/java/org/apache/carbondata/core/util/ByteUtil.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/AdaptiveIntegralCodec.java
core/src/main/java/org/apache/carbondata/core/metadata/datatype/DataType.java
core/src/main/java/org/apache/carbondata/core/scan/scanner/impl/BlockletFilterScanner.java
processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactDataHandlerColumnar.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/DirectCompressCodec.java
core/src/main/java/org/apache/carbondata/core/datastore/page/UnsafeFixLengthColumnPage.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/reader/MeasureColumnChunkReader.java
core/src/main/java/org/apache/carbondata/core/datastore/page/SafeVarLengthColumnPage.java
processing/src/main/java/org/apache/carbondata/processing/store/writer/v3/CarbonFactDataWriterImplV3.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/adaptive/AdaptiveCodec.java
core/src/main/java/org/apache/carbondata/core/scan/result/BlockletScannedResult.java
core/src/main/java/org/apache/carbondata/core/scan/result/impl/FilterQueryScannedResult.java
core/src/main/java/org/apache/carbondata/core/datastore/page/ColumnPage.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
core/src/main/java/org/apache/carbondata/core/datastore/page/SafeFixLengthColumnPage.java
core/src/main/java/org/apache/carbondata/core/memory/UnsafeMemoryManager.java
processing/src/main/java/org/apache/carbondata/processing/store/TablePage.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/ColumnPageCodec.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/reader/measure/v3/MeasureChunkReaderV3.java
texts:Make ColumnPage use Unsafe
Implement Unsafe ColumnPage to save data, which can be stored either offheap or onheap by configuration.This can help reducing memory requirement in data load write step
issueID:CARBONDATA-1019
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelRangeGrtThanFiterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelRangeGrtrThanEquaToFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelRangeLessThanFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelRangeLessThanEqualFilterExecuterImpl.java
texts:Like Filter Pushdown
Like Filter Pushdown in carbon layer to convert into Range Filter.
issueID:CARBONDATA-102
type:Improvement
changed files:
texts:Exclude the Spark and hadoop from CarbonData assembly jar by default and reduce the jar file size
Currently CarbonData assembly jar is huge and it is about 200MB size because it includes Spark, Scala and Hadoop dependency jars.So we should not include Hadoop, Scala and Spark dependencies in CarbonData jar by default as it is going to be deployed in Spark cluster.If user wish to include them we should give the option in maven build to include all dependencies.Like default build like below will have only CarbonData and its dependencies apart from Spark, Scala and Hadoop dependencies. mvn clean -DskipTests packageBelow build includes all dependencies like Spark, Scala and Hadoopmvn clean -DskipTests -Pinclude-all package
issueID:CARBONDATA-1020
type:Task
changed files:
texts:Remove incubator info as top level project

issueID:CARBONDATA-1022
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/datastore/impl/FileFactory.java
texts:Get errors while do "select" query in Spark-shell
Spark1.6's spark-shell, error info as below : scala> cc.sql("select * from connectdemo1").show17/05/04 23:06:36 ERROR SegmentUpdateStatusManager: pool-178-thread-1 Invalid tuple id /carbondata/store/default/connectdemo1/Fact/0/0/0-0_batchno0-0-1493910126468/0
issueID:CARBONDATA-1023
type:Bug
changed files:
texts:Able to do load from dataframe with byte data type in carbon table
i am able to load data with bytetype from dataframe in a carbon tableas far as documentation says byte type is not supported val rdd1 = sqlContext.sparkContext.parallelize(      Row("byte",1234.toByte) :: Nil)val schema1 = StructType(      StructField("string",StringType, nullable = false) ::      StructField("byte",ByteType, nullable = false) :: Nil)   val dataFrame = sqlContext.createDataFrame(rdd1, schema1) dataFrame.write      .format("carbondata")      .option("tableName", "carbon1")      .option("compress", "true")      .mode(SaveMode.Overwrite)      .save()      sql("select * from carbon1").show()--------+stringbyte--------+  byte -46--------+if we try to create a table with byte type it shows error  sql(      "CREATE TABLE restructure (empno byte, empname String, designation String, doj Timestamp, " +      "workgroupcategory int, workgroupcategoryname String, deptno int, deptname String, " +      "projectcode int, projectjoindate Timestamp, projectenddate Timestamp,attendance int," +      "utilization int,salary int) STORED BY 'org.apache.carbondata.format'")result org.apache.carbondata.spark.exception.MalformedCarbonCommandException: Unsupported data type: StructField(empno,ByteType,true).getType
issueID:CARBONDATA-1024
type:Improvement
changed files:
texts:Support reading Float Data Type In Hive From Carbon Table
currently float  data type is not supported in hive,can we support it?
issueID:CARBONDATA-1027
type:Bug
changed files:
texts:insert into/data load failing for numeric dictionary included column having null value
insert into/data load failing for numeric dictionary included column having null value
issueID:CARBONDATA-1028
type:Bug
changed files:
texts:Null displays while using limit with order by clause in sort_column
Null displays while using limit with order by clause in sort_columnSteps to reproduced:1) Create table:CREATE TABLE sorttable3 (empno int, empname String, designation String, doj Timestamp, workgroupcategory int, workgroupcategoryname String, deptno int, deptname String, projectcode int, projectjoindate Timestamp, projectenddate Timestamp,attendance int,utilization int,salary int) STORED BY 'org.apache.carbondata.format' tblproperties('sort_columns'='doj')2) Load data:LOAD DATA local inpath 'hdfs://localhost:54310/newdata.csv' INTO TABLE sorttable3 OPTIONS('DELIMITER'= ',', 'QUOTECHAR'= '\"');3)Execute Query:select doj from sorttable3 order by doj limit 10;Actual Result:-------+  doj  -------+ NULL   NULL   NULL   NULL   NULL   NULL   NULL   NULL   NULL   NULL  -------+Expected Result:it should display ordered doj as per limit.
issueID:CARBONDATA-103
type:Bug
changed files:
texts:Rename CreateCube to CreateTable to correct the audit log of create table commnad

issueID:CARBONDATA-1031
type:Bug
changed files:
texts:spark-sql can&#39;t read the carbon table
I create a carbon table by spark-shellAnd then I use this command  "spark-sql --jars carbon*.jar" to start spark-sql cli.When the first time I execute this "select * from temp.test-schema", spark will throw exception. After I execute another command, It will be ok.17/05/06 21:43:12 ERROR [org.apache.spark.sql.hive.thriftserver.SparkSQLDriver(91) -- main]: Failed in [select * from temp.test_schema]java.lang.AssertionError: assertion failed: No plan for Relation[id#10,name#11,scale#12,country#13,salary#14] CarbonDatasourceHadoopRelation(org.apache.spark.sql.SparkSession@42d9ea3b,[Ljava.lang.String;@70a0e9c6,Map(path -> hdfs:////user/hadoop/carbon/store/temp/test_schema, serialization.format -> 1, dbname -> temp, tablepath -> hdfs:////user/hadoop/carbon/store/temp/test_schema, tablename -> test_schema),None,ArrayBuffer())        at scala.Predef$.assert(Predef.scala:170)        at org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:92)        at org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$2$$anonfun$apply$2.apply(QueryPlanner.scala:77)        at org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$2$$anonfun$apply$2.apply(QueryPlanner.scala:74)        at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)        at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)        at scala.collection.Iterator$class.foreach(Iterator.scala:893)        at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)        at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157)        at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1336)        at org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$2.apply(QueryPlanner.scala:74)        at org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$2.apply(QueryPlanner.scala:66)        at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)        at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)        at org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:92)        at org.apache.spark.sql.execution.QueryExecution.sparkPlan$lzycompute(QueryExecution.scala:84)        at org.apache.spark.sql.execution.QueryExecution.sparkPlan(QueryExecution.scala:80)        at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:89)        at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:89)        at org.apache.spark.sql.execution.QueryExecution.hiveResultString(QueryExecution.scala:119)        at org.apache.spark.sql.hive.thriftserver.SparkSQLDriver.run(SparkSQLDriver.scala:67)        at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.processCmd(SparkSQLCLIDriver.scala:335)        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:376)        at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver$.main(SparkSQLCLIDriver.scala:247)        at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.main(SparkSQLCLIDriver.scala)        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)        at java.lang.reflect.Method.invoke(Method.java:606)        at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:742)        at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:186)        at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:211)        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126)        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
issueID:CARBONDATA-1032
type:Bug
changed files:
texts:NumberFormatException and NegativeArraySizeException for select with in clause filter limit for unsafe true configuration
Carbon .properties are configured as below:carbon.allowed.compaction.days = 2carbon.enable.auto.load.merge = falsecarbon.compaction.level.threshold = 3,2carbon.timestamp.format = yyyy-MM-ddcarbon.badRecords.location = /tmp/carboncarbon.numberof.preserve.segments = 2carbon.sort.file.buffer.size = 20max.query.execution.time = 60carbon.number.of.cores.while.loading = 8carbon.storelocation =hdfs://hacluster/opt/CarbonStoreenable.data.loading.statistics = trueenable.unsafe.sort = trueoffheap.sort.chunk.size.inmb = 128sort.inmemory.size.inmb = 30720carbon.enable.vector.reader=trueenable.unsafe.in.query.processing=trueenable.query.statistics=truecarbon.blockletgroup.size.in.mb=128high.cardinality.identify.enable=TRUEhigh.cardinality.threshold=10000high.cardinality.value=1000high.cardinality.row.count.percentage=40carbon.data.file.version=2carbon.major.compaction.size=2carbon.enable.auto.load.merge=FALSEcarbon.numberof.preserve.segments=1carbon.allowed.compaction.days=1User creates table, loads 1535088 records data and executes the select with in clause filter limit. Actual Result :NumberFormatException and NegativeArraySizeException for select with in clause filter limit for unsafe true configuration.0: jdbc:hive2://172.168.100.199:23040> select * from flow_carbon_test4 where opp_bk in ('1491999999158','1491999999116','1491999999022','1491999999031')  and dt>='20140101' and dt <= '20160101' order by bal asc limit 1000;Error: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 2109.0 failed 4 times, most recent failure: Lost task 1.3 in stage 2109.0 (TID 75120, linux-49, executor 2): java.lang.NegativeArraySizeException        at org.apache.carbondata.core.datastore.chunk.store.impl.unsafe.UnsafeBigDecimalMeasureChunkStore.getBigDecimal(UnsafeBigDecimalMeasureChunkStore.java:132)        at org.apache.carbondata.core.datastore.compression.decimal.CompressByteArray.getBigDecimalValue(CompressByteArray.java:94)        at org.apache.carbondata.core.datastore.dataholder.CarbonReadDataHolder.getReadableBigDecimalValueByIndex(CarbonReadDataHolder.java:38)        at org.apache.carbondata.core.scan.result.vector.MeasureDataVectorProcessor$DecimalMeasureVectorFiller.fillMeasureVectorForFilter(MeasureDataVectorProcessor.java:253)        at org.apache.carbondata.core.scan.result.impl.FilterQueryScannedResult.fillColumnarMeasureBatch(FilterQueryScannedResult.java:119)        at org.apache.carbondata.core.scan.collector.impl.DictionaryBasedVectorResultCollector.scanAndFillResult(DictionaryBasedVectorResultCollector.java:145)        at org.apache.carbondata.core.scan.collector.impl.DictionaryBasedVectorResultCollector.collectVectorBatch(DictionaryBasedVectorResultCollector.java:137)        at org.apache.carbondata.core.scan.processor.impl.DataBlockIteratorImpl.processNextBatch(DataBlockIteratorImpl.java:65)        at org.apache.carbondata.core.scan.result.iterator.VectorDetailQueryResultIterator.processNextBatch(VectorDetailQueryResultIterator.java:46)        at org.apache.carbondata.spark.vectorreader.VectorizedCarbonRecordReader.nextBatch(VectorizedCarbonRecordReader.java:251)        at org.apache.carbondata.spark.vectorreader.VectorizedCarbonRecordReader.nextKeyValue(VectorizedCarbonRecordReader.java:141)        at org.apache.carbondata.spark.rdd.CarbonScanRDD$$anon$1.hasNext(CarbonScanRDD.scala:221)        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.scan_nextBatch$(Unknown Source)        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)        at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)        at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)        at scala.collection.convert.Wrappers$IteratorWrapper.hasNext(Wrappers.scala:30)        at org.spark_project.guava.collect.Ordering.leastOf(Ordering.java:628)        at org.apache.spark.util.collection.Utils$.takeOrdered(Utils.scala:37)        at org.apache.spark.sql.execution.TakeOrderedAndProjectExec$$anonfun$5.apply(limit.scala:148)        at org.apache.spark.sql.execution.TakeOrderedAndProjectExec$$anonfun$5.apply(limit.scala:147)        at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:796)        at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:796)        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)        at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)        at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)        at org.apache.spark.scheduler.Task.run(Task.scala:99)        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)        at java.lang.Thread.run(Thread.java:745)Driver stacktrace: (state=,code=0)0: jdbc:hive2://172.168.100.199:23040> select  *  from flow_carbon_test4 where  cus_ac  like '622262135067246539%'  and (txn_dte>='20150101' and txn_dte<='20160101') and txn_bk IN ('00000000000', '00000000001','00000000002') OR own_bk IN ('00000000424','00000001383','00000001942','00000001262') limit 1000;Error: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 131.0 failed 4 times, most recent failure: Lost task 0.3 in stage 131.0 (TID 240, linux-51, executor 1): java.lang.NumberFormatException: Zero length BigInteger        at java.math.BigInteger.<init>(BigInteger.java:293)        at org.apache.carbondata.core.util.DataTypeUtil.byteToBigDecimal(DataTypeUtil.java:189)        at org.apache.carbondata.core.datastore.chunk.store.impl.unsafe.UnsafeBigDecimalMeasureChunkStore.getBigDecimal(UnsafeBigDecimalMeasureChunkStore.java:136)        at org.apache.carbondata.core.datastore.compression.decimal.CompressByteArray.getBigDecimalValue(CompressByteArray.java:94)        at org.apache.carbondata.core.datastore.dataholder.CarbonReadDataHolder.getReadableBigDecimalValueByIndex(CarbonReadDataHolder.java:38)        at org.apache.carbondata.core.scan.collector.impl.AbstractScannedResultCollector.getMeasureData(AbstractScannedResultCollector.java:104)        at org.apache.carbondata.core.scan.collector.impl.AbstractScannedResultCollector.fillMeasureData(AbstractScannedResultCollector.java:78)        at org.apache.carbondata.core.scan.collector.impl.DictionaryBasedResultCollector.fillMeasureData(DictionaryBasedResultCollector.java:158)        at org.apache.carbondata.core.scan.collector.impl.DictionaryBasedResultCollector.collectData(DictionaryBasedResultCollector.java:115)        at org.apache.carbondata.core.scan.processor.impl.DataBlockIteratorImpl.next(DataBlockIteratorImpl.java:51)        at org.apache.carbondata.core.scan.processor.impl.DataBlockIteratorImpl.next(DataBlockIteratorImpl.java:32)        at org.apache.carbondata.core.scan.result.iterator.DetailQueryResultIterator.getBatchResult(DetailQueryResultIterator.java:50)        at org.apache.carbondata.core.scan.result.iterator.DetailQueryResultIterator.next(DetailQueryResultIterator.java:41)        at org.apache.carbondata.core.scan.result.iterator.DetailQueryResultIterator.next(DetailQueryResultIterator.java:31)        at org.apache.carbondata.core.scan.result.iterator.ChunkRowIterator.<init>(ChunkRowIterator.java:41)        at org.apache.carbondata.hadoop.CarbonRecordReader.initialize(CarbonRecordReader.java:78)        at org.apache.carbondata.spark.rdd.CarbonScanRDD.compute(CarbonScanRDD.scala:204)        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)        at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)        at org.apache.spark.sql.CarbonDecoderRDD.compute(CarbonDictionaryDecoder.scala:538)        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)        at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)        at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)        at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)        at org.apache.spark.scheduler.Task.run(Task.scala:99)        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)        at java.lang.Thread.run(Thread.java:745)Driver stacktrace: (state=,code=0)Expected Result : select with in clause filter limit for unsafe true configuration should execute successfully displaying correct result set without exception.
issueID:CARBONDATA-1033
type:Bug
changed files:
texts:using column with array<string> type bucket table is created but exception thrown when select performed
User tries to create a bucket table with array<string>.Table is successful as shown below.0: jdbc:hive2://172.168.100.199:23040> CREATE TABLE uniqData_t4(ID Int, date Timestamp, country String,name String, phonetype String, serialname String, salary Int,mobile array<string>)USING org.apache.spark.sql.CarbonSource OPTIONS("bucketnumber"="1", "bucketcolumns"="name","tableName"="uniqData_t4");---------+ Result  ---------+---------+No rows selected (0.061 seconds)User executes select query on bucket table with column type having array<string>.Actual Issue :When user performs select query on bucket table with column type having array<string> the UncheckedExecutionException is thrown.0: jdbc:hive2://172.168.100.199:23040> select count from uniqData_t4;Error: org.spark_project.guava.util.concurrent.UncheckedExecutionException: java.lang.Exception: Do not have default and uniqdata_t4 (state=,code=0)0: jdbc:hive2://172.168.100.199:23040> select * from uniqData_t4;Error: org.spark_project.guava.util.concurrent.UncheckedExecutionException: java.lang.Exception: Do not have default and uniqdata_t4 (state=,code=0)Expected : The bucket table creation with array<string> type should not be created. If its created then the select query should return correct result set without throwing exception.
issueID:CARBONDATA-1034
type:Bug
changed files:processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/SortDataRows.java
integration/spark2/src/main/java/org/apache/carbondata/spark/readsupport/SparkRowReadSupportImpl.java
core/src/main/java/org/apache/carbondata/core/scan/collector/impl/AbstractScannedResultCollector.java
core/src/main/java/org/apache/carbondata/core/util/DataTypeUtil.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/UnsafeCarbonRowPage.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/holder/UnsafeSortTempFileChunkHolder.java
core/src/main/java/org/apache/carbondata/core/scan/expression/ExpressionResult.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/SortTempFileChunkHolder.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/scan/collector/impl/RestructureBasedDictionaryResultCollector.java
texts:FilterUnsupportedException thrown for select from table where = filter for int column has negative of value larger than int max range
In Beeline user creates a table and loads data in the table.User executes select from table where = filter for int column has negative of value larger than int max range.0: jdbc:hive2://172.168.100.199:23040> CREATE table mycube21 (column1 STRING, column2 STRING,column3 INT, column4 INT,column5 INT, column6 INT) stored by 'org.apache.carbondata.format' TBLPROPERTIES("columnproperties.column1.shared_column"="shared.column1","columnproperties.column2.shared_column"="shared.column2");---------+ Result  ---------+---------+No rows selected (0.059 seconds)0: jdbc:hive2://172.168.100.199:23040> LOAD DATA INPATH 'hdfs://hacluster/chetan/file.csv' INTO TABLE mycube21 OPTIONS('DELIMITER'=',','QUOTECHAR'='"','BAD_RECORDS_ACTION'='FORCE','FILEHEADER'='');---------+ Result  ---------+---------+No rows selected (1.198 seconds)0: jdbc:hive2://172.168.100.199:23040> select * from mycube21 where column4=-9223372036854775808;Actual Result : FilterUnsupportedException thrown for select from table where = filter for int column has negative of value larger than int max range.0: jdbc:hive2://172.168.100.199:23040> select * from mycube21 where column4=-9223372036854775808;Error: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 936.0 failed 4 times, most recent failure: Lost task 0.3 in stage 936.0 (TID 42603, linux-53, executor 1): org.apache.spark.util.TaskCompletionListenerException: java.util.concurrent.ExecutionException: org.apache.carbondata.core.scan.expression.exception.FilterUnsupportedException: java.lang.Long cannot be cast to java.lang.Integer        at org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:105)        at org.apache.spark.scheduler.Task.run(Task.scala:112)        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)        at java.lang.Thread.run(Thread.java:745)Driver stacktrace: (state=,code=0)0: jdbc:hive2://172.168.100.199:23040> select c1_int from test_boundary where c1_int in (2.147483647E9,2345.0,1234.0);Error: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 787.0 failed 4 times, most recent failure: Lost task 0.3 in stage 787.0 (TID 9758, linux-49, executor 2): org.apache.spark.util.TaskCompletionListenerException: java.util.concurrent.ExecutionException: org.apache.carbondata.core.scan.expression.exception.FilterUnsupportedException: java.lang.Long cannot be cast to java.lang.Integerat org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:105)at org.apache.spark.scheduler.Task.run(Task.scala:112)at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)at java.lang.Thread.run(Thread.java:745)Also the issue is happening with the below queries.select c1_int from test_boundary where c1_int not in (2.147483647E9,2345.0,1234.0);select c1_int+0.100 from Test_Boundary where c1_int < 2.147483647E9 ;select * from (select c1_int from Test_Boundary where c1_int between -2.147483648E9 and 2.147483647E9) e ;Expected Result : Exception should not be thrown. Only error message should be displayed.
issueID:CARBONDATA-1037
type:Bug
changed files:
texts:Select query is not returning any data when we query on New Table after Alter Table rename operation
create database Priyal;Use Priyal;Create TableCREATE TABLE uniqdata111785 (CUST_ID int,CUST_NAME String,ACTIVE_EMUI_VERSION string, DOB timestamp, DOJ timestamp, BIGINT_COLUMN1 bigint,BIGINT_COLUMN2 bigint,DECIMAL_COLUMN1 decimal(30,10), DECIMAL_COLUMN2 decimal(36,10),Double_COLUMN1 double, Double_COLUMN2 double,INTEGER_COLUMN1 int) STORED BY 'org.apache.carbondata.format' TBLPROPERTIES('DICTIONARY_INCLUDE'='INTEGER_COLUMN1,CUST_ID');Load Data into TableLOAD DATA INPATH 'hdfs://hacluster/user/Priyal/2000_UniqData.csv' into table uniqdata111785 OPTIONS('DELIMITER'=',' , 'QUOTECHAR'='"','BAD_RECORDS_LOGGER_ENABLE'='TRUE', 'BAD_RECORDS_ACTION'='FORCE','FILEHEADER'='CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1,Double_COLUMN2,INTEGER_COLUMN1');LOAD DATA INPATH 'hdfs://hacluster/user/Priyal/2000_UniqData.csv' into table uniqdata111785 OPTIONS('DELIMITER'=',' , 'QUOTECHAR'='"','BAD_RECORDS_LOGGER_ENABLE'='TRUE', 'BAD_RECORDS_ACTION'='FORCE','FILEHEADER'='CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1,Double_COLUMN2,INTEGER_COLUMN1');Select query on Old Table0: jdbc:hive2://172.168.100.199:23040> select * from uniqdata111785 limit 10;------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- CUST_ID      CUST_NAME         ACTIVE_EMUI_VERSION               DOB                     DOJ            BIGINT_COLUMN1   BIGINT_COLUMN2       DECIMAL_COLUMN1          DECIMAL_COLUMN2         Double_COLUMN1        Double_COLUMN2      INTEGER_COLUMN1  ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- 10000     CUST_NAME_01000   ACTIVE_EMUI_VERSION_01000   1972-09-27 00:00:00.0   1972-09-27 00:00:00.0   123372037854     -223372035854    12345679901.1234000000   22345679901.1234000000   1.12345674897976E10   -1.12345674897976E10   1001              10001     CUST_NAME_01001   ACTIVE_EMUI_VERSION_01001   1972-09-28 00:00:00.0   1972-09-28 00:00:00.0   123372037855     -223372035853    12345679902.1234000000   22345679902.1234000000   1.12345674897976E10   -1.12345674897976E10   1002              10002     CUST_NAME_01002   ACTIVE_EMUI_VERSION_01002   1972-09-29 00:00:00.0   1972-09-29 00:00:00.0   123372037856     -223372035852    12345679903.1234000000   22345679903.1234000000   1.12345674897976E10   -1.12345674897976E10   1003              10003     CUST_NAME_01003   ACTIVE_EMUI_VERSION_01003   1972-09-30 00:00:00.0   1972-09-30 00:00:00.0   123372037857     -223372035851    12345679904.1234000000   22345679904.1234000000   1.12345674897976E10   -1.12345674897976E10   1004              10004     CUST_NAME_01004   ACTIVE_EMUI_VERSION_01004   1972-10-01 00:00:00.0   1972-10-01 00:00:00.0   123372037858     -223372035850    12345679905.1234000000   22345679905.1234000000   1.12345674897976E10   -1.12345674897976E10   1005              10005     CUST_NAME_01005   ACTIVE_EMUI_VERSION_01005   1972-10-02 00:00:00.0   1972-10-02 00:00:00.0   123372037859     -223372035849    12345679906.1234000000   22345679906.1234000000   1.12345674897976E10   -1.12345674897976E10   1006              10006     CUST_NAME_01006   ACTIVE_EMUI_VERSION_01006   1972-10-03 00:00:00.0   1972-10-03 00:00:00.0   123372037860     -223372035848    12345679907.1234000000   22345679907.1234000000   1.12345674897976E10   -1.12345674897976E10   1007              10007     CUST_NAME_01007   ACTIVE_EMUI_VERSION_01007   1972-10-04 00:00:00.0   1972-10-04 00:00:00.0   123372037861     -223372035847    12345679908.1234000000   22345679908.1234000000   1.12345674897976E10   -1.12345674897976E10   1008              10008     CUST_NAME_01008   ACTIVE_EMUI_VERSION_01008   1972-10-05 00:00:00.0   1972-10-05 00:00:00.0   123372037862     -223372035846    12345679909.1234000000   22345679909.1234000000   1.12345674897976E10   -1.12345674897976E10   1009              10009     CUST_NAME_01009   ACTIVE_EMUI_VERSION_01009   1972-10-06 00:00:00.0   1972-10-06 00:00:00.0   123372037863     -223372035845    12345679910.1234000000   22345679910.1234000000   1.12345674897976E10   -1.12345674897976E10   1010             ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------Alter Table namealter table Priyal.uniqdata111785 RENAME TO  uniqdata1117856;Select Query after Alter Table rename operation.0: jdbc:hive2://172.168.100.199:23040> select * from priyal.uniqdata1117856 limit 10;------------------------------------------------------------------------------------------------------------------------------------------------------------ CUST_ID   CUST_NAME   ACTIVE_EMUI_VERSION   DOB   DOJ   BIGINT_COLUMN1   BIGINT_COLUMN2   DECIMAL_COLUMN1   DECIMAL_COLUMN2   Double_COLUMN1   Double_COLUMN2   INTEGER_COLUMN1  ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------Show tables;0: jdbc:hive2://172.168.100.199:23040> show tables;---------------------------------------+ database      tableName      isTemporary  ---------------------------------------+ priyal     uniqdata1117856   false        ---------------------------------------+Expected Output: Select Query should return proper result set.
issueID:CARBONDATA-1038
type:Bug
changed files:
texts:DICTIONARY_EXCLUDE is not working for string datatype when used with Spark Datasource DDL
DICTIONARY_EXCLUDE is throwing exception for string datatype when used Spark Datasource DDLSteps to reproduce:CREATE TABLE IF NOT EXISTS uniq_product( productNumber Int, productName String, storeCity String, storeProvince String, productCategory String, productBatch String, saleQuantity Int, revenue Int) USING org.apache.spark.sql.CarbonSource OPTIONS('COLUMN_GROUPS'='(productCategory)', 'DICTIONARY_EXCLUDE'='productName', 'DICTIONARY_INCLUDE'='productNumber', 'NO_INVERTED_INDEX'='productBatch','bucketnumber'='1', 'bucketcolumns'='productNumber','tableName'='uniq_product');Expected Output :Table should get created.Actual Output :Error: org.apache.carbondata.spark.exception.MalformedCarbonCommandException: DICTIONARY_EXCLUDE is unsupported for stringtype data type column: productname (state=,code=0)
issueID:CARBONDATA-104
type:New Feature
changed files:
texts:To support varchar datatype

issueID:CARBONDATA-1042
type:Bug
changed files:
texts:Delete Opertation Failed in automation
Steps to Reproduce :Create Table:CREATE TABLE uniqdata (CUST_ID int,CUST_NAME String,ACTIVE_EMUI_VERSION string, DOB timestamp, DOJ timestamp, BIGINT_COLUMN1 bigint,BIGINT_COLUMN2 bigint,DECIMAL_COLUMN1 decimal(30,10), DECIMAL_COLUMN2 decimal(36,10),Double_COLUMN1 double, Double_COLUMN2 double,INTEGER_COLUMN1 int) STORED BY 'org.apache.carbondata.format'Load Data: LOAD DATA INPATH 'HDFS_URL/BabuStore/Data/uniqdata/2000_UniqData.csv' into table uniqdata OPTIONS('DELIMITER'=',' , 'QUOTECHAR'='"','BAD_RECORDS_ACTION'='FORCE','FILEHEADER'='CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1,Double_COLUMN2,INTEGER_COLUMN1')Delete Query :delete from uniqdata where doj='1970-01-15 02:00:03' and dob='1970-01-15 01:00:03' or INTEGER_COLUMN1=15Result In Automation : Delete_193,FAIL,Delete data operation is failed. Job aborted due to stage failure: Task 0 in stage 503.0 failed 4 times, most recent failure: Lost task 0.3 in stage 503.0 (TID 768, hadoop-master): java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.ArrayIndexOutOfBoundsException: 0 at org.apache.carbondata.core.scan.processor.AbstractDataBlockIterator.updateScanner(AbstractDataBlockIterator.java:136) at org.apache.carbondata.core.scan.processor.impl.DataBlockIteratorImpl.next(DataBlockIteratorImpl.java:50) at org.apache.carbondata.core.scan.processor.impl.DataBlockIteratorImpl.next(DataBlockIteratorImpl.java:32) at org.apache.carbondata.core.scan.result.iterator.DetailQueryResultIterator.getBatchResult(DetailQueryResultIterator.java:50) at org.apache.carbondata.core.scan.result.iterator.DetailQueryResultIterator.next(DetailQueryResultIterator.java:41) at org.apache.carbondata.core.scan.result.iterator.DetailQueryResultIterator.next(DetailQueryResultIterator.java:31) at org.apache.carbondata.core.scan.result.iterator.ChunkRowIterator.<init>(ChunkRowIterator.java:41) at org.apache.carbondata.hadoop.CarbonRecordReader.initialize(CarbonRecordReader.java:79) at org.apache.carbondata.spark.rdd.CarbonScanRDD.compute(CarbonScanRDD.scala:204) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306) at org.apache.spark.rdd.RDD.iterator(RDD.scala:270) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306) at org.apache.spark.rdd.RDD.iterator(RDD.scala:270) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306) at org.apache.spark.rdd.RDD.iterator(RDD.scala:270) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306) at org.apache.spark.rdd.RDD.iterator(RDD.scala:270) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306) at org.apache.spark.rdd.RDD.iterator(RDD.scala:270) at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73) at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41) at org.apache.spark.scheduler.Task.run(Task.scala:89) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745)Caused by: java.util.concurrent.ExecutionException: java.lang.ArrayIndexOutOfBoundsException: 0 at java.util.concurrent.FutureTask.report(FutureTask.java:122) at java.util.concurrent.FutureTask.get(FutureTask.java:192) at org.apache.carbondata.core.scan.processor.AbstractDataBlockIterator.getNextScannedResult(AbstractDataBlockIterator.java:146) at org.apache.carbondata.core.scan.processor.AbstractDataBlockIterator.updateScanner(AbstractDataBlockIterator.java:124) ... 29 moreCaused by: java.lang.ArrayIndexOutOfBoundsException: 0 at org.apache.carbondata.core.util.BitSetGroup.getBitSet(BitSetGroup.java:40) at org.apache.carbondata.core.util.BitSetGroup.or(BitSetGroup.java:68) at org.apache.carbondata.core.scan.filter.executer.OrFilterExecuterImpl.applyFilter(OrFilterExecuterImpl.java:40) at org.apache.carbondata.core.scan.scanner.impl.FilterScanner.fillScannedResult(FilterScanner.java:147) at org.apache.carbondata.core.scan.scanner.impl.FilterScanner.scanBlocklet(FilterScanner.java:92) at org.apache.carbondata.core.scan.processor.AbstractDataBlockIterator$1.call(AbstractDataBlockIterator.java:189) at org.apache.carbondata.core.scan.processor.AbstractDataBlockIterator$1.call(AbstractDataBlockIterator.java:176) at java.util.concurrent.FutureTask.run(FutureTask.java:266) ... 3 moreDriver stacktrace:But when we executed the same query manually, it is working fine.
issueID:CARBONDATA-1044
type:Improvement
changed files:
texts:Rename incubator-carbondata to carbondata in docs and readme

issueID:CARBONDATA-1045
type:Bug
changed files:
texts:Mismatch in message display with insert and load operation on failure due to bad records in update operation
When bad records action is set to fail and any IUD operation… is executed and it fails due to bad records error message is not displayed correctly because of which user is not clear with the cause of update operation failure. Whereas in the same case in other operations like data load and insert into, if there is any failure due to bad record proper error message is displayed to the user for failure due to bad record.Steps to reproduce---------------------------create table update_with_bad_record(item int, name String) stored by 'carbondata'LOAD DATA LOCAL INPATH '<data_file_path>' into table update_with_bad_recordupdate update_with_bad_record set (item)=(3.45)dummy data-------------------item,name2,Apple
issueID:CARBONDATA-1046
type:Bug
changed files:
texts:Single_pass_loading is throwing an error in Spark1.6 in automation
Steps to Reproduce :Create Table :CREATE TABLE uniqdata_INCLUDEDICTIONARY (CUST_ID int,CUST_NAME String,ACTIVE_EMUI_VERSION string, DOB timestamp, DOJ timestamp, BIGINT_COLUMN1 bigint,BIGINT_COLUMN2 bigint,DECIMAL_COLUMN1 decimal(30,10), DECIMAL_COLUMN2 decimal(36,10),Double_COLUMN1 double, Double_COLUMN2 double,INTEGER_COLUMN1 int) STORED BY 'org.apache.carbondata.format' TBLPROPERTIES('DICTIONARY_INCLUDE'='CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1,Double_COLUMN2,INTEGER_COLUMN1');---------+ Result  ---------+---------+No rows selected (1.709 seconds)Load Query :LOAD DATA INPATH 'hdfs://hadoop-master:54310/BabuStore/Data/uniqdata/7000_UniqData.csv' into table uniqdata_INCLUDEDICTIONARY OPTIONS('DELIMITER'=',' , 'QUOTECHAR'='"','BAD_RECORDS_LOGGER_ENABLE'='TRUE', 'BAD_RECORDS_ACTION'='FORCE','FILEHEADER'='CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1,Double_COLUMN2,INTEGER_COLUMN1','SINGLE_Pass'='true');Stack Trace :INFO  11-05 13:54:45,047 - Running query 'LOAD DATA INPATH 'hdfs://hadoop-master:54310/BabuStore/Data/uniqdata/7000_UniqData.csv' into table uniqdata_INCLUDEDICTIONARY OPTIONS('DELIMITER'=',' , 'QUOTECHAR'='"','BAD_RECORDS_LOGGER_ENABLE'='TRUE', 'BAD_RECORDS_ACTION'='FORCE','FILEHEADER'='CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1,Double_COLUMN2,INTEGER_COLUMN1','SINGLE_Pass'='true')' with 44e92bcb-f9e1-4b2e-835e-a82eae525fe4INFO  11-05 13:54:45,047 - pool-31-thread-3 Query &#91;LOAD DATA INPATH &#39;HDFS://HADOOP-MASTER:54310/BABUSTORE/DATA/UNIQDATA/7000_UNIQDATA.CSV&#39; INTO TABLE UNIQDATA_INCLUDEDICTIONARY OPTIONS(&#39;DELIMITER&#39;=&#39;,&#39; , &#39;QUOTECHAR&#39;=&#39;"&#39;,&#39;BAD_RECORDS_LOGGER_ENABLE&#39;=&#39;TRUE&#39;, &#39;BAD_RECORDS_ACTION&#39;=&#39;FORCE&#39;,&#39;FILEHEADER&#39;=&#39;CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,DOUBLE_COLUMN1,DOUBLE_COLUMN2,INTEGER_COLUMN1&#39;,&#39;SINGLE_PASS&#39;=&#39;TRUE&#39;)&#93;INFO  11-05 13:54:45,065 - pool-31-thread-3 HDFS lock path:hdfs://192.168.2.145:54310/opt/olapcontent/default/uniqdata_includedictionary/meta.lockINFO  11-05 13:54:45,097 - Successfully able to get the table metadata file lockINFO  11-05 13:54:45,099 - pool-31-thread-3 Initiating Direct Load for the Table : (default.uniqdata_includedictionary)AUDIT 11-05 13:54:45,100 - &#91;hadoop-master&#93;&#91;hduser&#93;&#91;Thread-150&#93;Data load request has been received for table default.uniqdata_includedictionaryAUDIT 11-05 13:54:45,100 - &#91;hadoop-master&#93;&#91;hduser&#93;&#91;Thread-150&#93;Data is loading with New Data Flow for table default.uniqdata_includedictionaryERROR 11-05 13:54:45,104 - Dictionary server Dictionary Server Start Failedjava.net.BindException: Address already in use at sun.nio.ch.Net.bind0(Native Method) at sun.nio.ch.Net.bind(Net.java:433) at sun.nio.ch.Net.bind(Net.java:425) at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:223) at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74) at io.netty.channel.socket.nio.NioServerSocketChannel.doBind(NioServerSocketChannel.java:125) at io.netty.channel.AbstractChannel$AbstractUnsafe.bind(AbstractChannel.java:485) at io.netty.channel.DefaultChannelPipeline$HeadContext.bind(DefaultChannelPipeline.java:1089) at io.netty.channel.AbstractChannelHandlerContext.invokeBind(AbstractChannelHandlerContext.java:430) at io.netty.channel.AbstractChannelHandlerContext.bind(AbstractChannelHandlerContext.java:415) at io.netty.channel.DefaultChannelPipeline.bind(DefaultChannelPipeline.java:903) at io.netty.channel.AbstractChannel.bind(AbstractChannel.java:198) at io.netty.bootstrap.AbstractBootstrap$2.run(AbstractBootstrap.java:348) at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:357) at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:357) at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111) at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:137) at java.lang.Thread.run(Thread.java:745)INFO  11-05 13:54:45,131 - pool-31-thread-3 &#91;Block Distribution&#93;INFO  11-05 13:54:45,131 - pool-31-thread-3 totalInputSpaceConsumed: 1505367 , defaultParallelism: 8INFO  11-05 13:54:45,131 - pool-31-thread-3 mapreduce.input.fileinputformat.split.maxsize: 16777216INFO  11-05 13:54:45,132 - Total input paths to process : 1INFO  11-05 13:54:45,133 - pool-31-thread-3 Executors configured : 1INFO  11-05 13:54:45,133 - pool-31-thread-3 Requesting total executors: 1INFO  11-05 13:54:45,136 - pool-31-thread-3 Total Time taken to ensure the required executors : 1INFO  11-05 13:54:45,136 - pool-31-thread-3 Time elapsed to allocate the required executors: 0INFO  11-05 13:54:45,136 - pool-31-thread-3 Total Time taken in block allocation: 3INFO  11-05 13:54:45,136 - pool-31-thread-3 Total no of blocks: 1, No.of Nodes: 1INFO  11-05 13:54:45,136 - pool-31-thread-3 #Node: hadoop-master no.of.blocks: 1INFO  11-05 13:54:45,225 - Block broadcast_2 stored as values in memory (estimated size 122.2 MB, free 227.2 MB)INFO  11-05 13:54:45,231 - Block broadcast_2_piece0 stored as bytes in memory (estimated size 19.4 KB, free 227.3 MB)INFO  11-05 13:54:45,231 - Added broadcast_2_piece0 in memory on 192.168.2.145:39344 (size: 19.4 KB, free: 1983.0 MB)INFO  11-05 13:54:45,232 - Created broadcast 2 from broadcast at NewCarbonDataLoadRDD.scala:108INFO  11-05 13:54:45,238 - Starting job: collect at CarbonDataRDDFactory.scala:643INFO  11-05 13:54:45,238 - Got job 1 (collect at CarbonDataRDDFactory.scala:643) with 1 output partitionsINFO  11-05 13:54:45,238 - Final stage: ResultStage 1 (collect at CarbonDataRDDFactory.scala:643)INFO  11-05 13:54:45,238 - Parents of final stage: List()INFO  11-05 13:54:45,238 - Missing parents: List()INFO  11-05 13:54:45,239 - Submitting ResultStage 1 (NewCarbonDataLoadRDD&#91;3&#93; at RDD at NewCarbonDataLoadRDD.scala:90), which has no missing parentsINFO  11-05 13:54:45,239 - Preferred Location for split : hadoop-masterINFO  11-05 13:54:45,240 - Block broadcast_3 stored as values in memory (estimated size 11.9 KB, free 227.3 MB)INFO  11-05 13:54:45,240 - Block broadcast_3_piece0 stored as bytes in memory (estimated size 5.8 KB, free 227.3 MB)INFO  11-05 13:54:45,241 - Added broadcast_3_piece0 in memory on 192.168.2.145:39344 (size: 5.8 KB, free: 1983.0 MB)INFO  11-05 13:54:45,241 - Created broadcast 3 from broadcast at DAGScheduler.scala:1006INFO  11-05 13:54:45,241 - Submitting 1 missing tasks from ResultStage 1 (NewCarbonDataLoadRDD&#91;3&#93; at RDD at NewCarbonDataLoadRDD.scala:90)INFO  11-05 13:54:45,241 - Adding task set 1.0 with 1 tasksINFO  11-05 13:54:45,242 - Starting task 0.0 in stage 1.0 (TID 2, hadoop-master, partition 0,NODE_LOCAL, 2536 bytes)INFO  11-05 13:54:45,256 - Added broadcast_3_piece0 in memory on hadoop-master:45786 (size: 5.8 KB, free: 7.0 GB)INFO  11-05 13:54:45,271 - Added broadcast_2_piece0 in memory on hadoop-master:45786 (size: 19.4 KB, free: 7.0 GB)AUDIT 11-05 13:54:45,290 - &#91;hadoop-master&#93;&#91;hduser&#93;&#91;Thread-138&#93;Connected io.netty.channel.DefaultChannelHandlerContext@54cef80cAUDIT 11-05 13:54:46,309 - &#91;hadoop-master&#93;&#91;hduser&#93;&#91;Thread-139&#93;Connected io.netty.channel.DefaultChannelHandlerContext@295ea64bERROR 11-05 13:54:47,339 - nioEventLoopGroup-4-6 java.lang.IndexOutOfBoundsException: readerIndex(67) + length(25) exceeds writerIndex(80): UnpooledUnsafeDirectByteBuf(ridx: 67, widx: 80, cap: 80) at io.netty.buffer.AbstractByteBuf.checkReadableBytes(AbstractByteBuf.java:1166) at io.netty.buffer.AbstractByteBuf.readBytes(AbstractByteBuf.java:676) at io.netty.buffer.AbstractByteBuf.readBytes(AbstractByteBuf.java:684) at org.apache.carbondata.core.dictionary.generator.key.DictionaryMessage.readData(DictionaryMessage.java:70) at org.apache.carbondata.core.dictionary.server.DictionaryServerHandler.channelRead(DictionaryServerHandler.java:59) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308) at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294) at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:846) at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131) at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:511) at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468) at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382) at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354) at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111) at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:137) at java.lang.Thread.run(Thread.java:745)ERROR 11-05 13:54:47,339 - nioEventLoopGroup-4-6 exceptionCaughtjava.lang.IndexOutOfBoundsException: readerIndex(67) + length(25) exceeds writerIndex(80): UnpooledUnsafeDirectByteBuf(ridx: 67, widx: 80, cap: 80) at io.netty.buffer.AbstractByteBuf.checkReadableBytes(AbstractByteBuf.java:1166) at io.netty.buffer.AbstractByteBuf.readBytes(AbstractByteBuf.java:676) at io.netty.buffer.AbstractByteBuf.readBytes(AbstractByteBuf.java:684) at org.apache.carbondata.core.dictionary.generator.key.DictionaryMessage.readData(DictionaryMessage.java:70) at org.apache.carbondata.core.dictionary.server.DictionaryServerHandler.channelRead(DictionaryServerHandler.java:59) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308) at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294) at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:846) at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131) at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:511) at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468) at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382) at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354) at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111) at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:137) at java.lang.Thread.run(Thread.java:745)WARN  11-05 13:55:59,151 - Lost task 0.1 in stage 0.0 (TID 1, hadoop-master): org.apache.carbondata.processing.newflow.exception.CarbonDataLoadingException:  at org.apache.carbondata.processing.newflow.sort.impl.ParallelReadMergeSorterImpl.checkError(ParallelReadMergeSorterImpl.java:164) at org.apache.carbondata.processing.newflow.sort.impl.ParallelReadMergeSorterImpl.sort(ParallelReadMergeSorterImpl.java:117) at org.apache.carbondata.processing.newflow.steps.SortProcessorStepImpl.execute(SortProcessorStepImpl.java:82) at org.apache.carbondata.processing.newflow.steps.DataWriterProcessorStepImpl.execute(DataWriterProcessorStepImpl.java:92) at org.apache.carbondata.processing.newflow.DataLoadExecutor.execute(DataLoadExecutor.java:48) at org.apache.carbondata.spark.rdd.NewCarbonDataLoadRDD$$anon$1.<init>(NewCarbonDataLoadRDD.scala:166) at org.apache.carbondata.spark.rdd.NewCarbonDataLoadRDD.compute(NewCarbonDataLoadRDD.scala:142) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306) at org.apache.spark.rdd.RDD.iterator(RDD.scala:270) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66) at org.apache.spark.scheduler.Task.run(Task.scala:89) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745)Caused by: java.lang.RuntimeException: java.lang.RuntimeException: Request timed out for key : DictionaryKey{ columnName='active_emui_version', data='ACTIVE_EMUI_VERSION_20007', dictionaryValue=-1, type=DICT_GENERATION} at org.apache.carbondata.core.dictionary.client.DictionaryClientHandler.getDictionary(DictionaryClientHandler.java:99) at org.apache.carbondata.core.dictionary.client.DictionaryClient.getDictionary(DictionaryClient.java:74) at org.apache.carbondata.processing.newflow.dictionary.DictionaryServerClientDictionary.getOrGenerateKey(DictionaryServerClientDictionary.java:57) at org.apache.carbondata.processing.newflow.dictionary.DictionaryServerClientDictionary.getOrGenerateKey(DictionaryServerClientDictionary.java:32) at org.apache.carbondata.processing.newflow.converter.impl.DictionaryFieldConverterImpl.convert(DictionaryFieldConverterImpl.java:115) at org.apache.carbondata.processing.newflow.converter.impl.RowConverterImpl.convert(RowConverterImpl.java:155) at org.apache.carbondata.processing.newflow.steps.DataConverterProcessorStepImpl.processRowBatch(DataConverterProcessorStepImpl.java:105) at org.apache.carbondata.processing.newflow.steps.DataConverterProcessorStepImpl$1.next(DataConverterProcessorStepImpl.java:91) at org.apache.carbondata.processing.newflow.steps.DataConverterProcessorStepImpl$1.next(DataConverterProcessorStepImpl.java:80) at org.apache.carbondata.processing.newflow.sort.impl.ParallelReadMergeSorterImpl$SortIteratorThread.call(ParallelReadMergeSorterImpl.java:227) at org.apache.carbondata.processing.newflow.sort.impl.ParallelReadMergeSorterImpl$SortIteratorThread.call(ParallelReadMergeSorterImpl.java:201) at java.util.concurrent.FutureTask.run(FutureTask.java:266) ... 3 moreCaused by: java.lang.RuntimeException: Request timed out for key : DictionaryKey{ columnName='active_emui_version', data='ACTIVE_EMUI_VERSION_20007', dictionaryValue=-1, type=DICT_GENERATION} at org.apache.carbondata.core.dictionary.client.DictionaryClientHandler.getDictionary(DictionaryClientHandler.java:94) ... 14 moreINFO  11-05 13:55:59,152 - Starting task 0.2 in stage 0.0 (TID 3, hadoop-master, partition 0,NODE_LOCAL, 2536 bytes)AUDIT 11-05 13:55:59,162 - &#91;hadoop-master&#93;&#91;hduser&#93;&#91;Thread-140&#93;Connected io.netty.channel.DefaultChannelHandlerContext@6d6b8cf6AUDIT 11-05 13:56:00,178 - &#91;hadoop-master&#93;&#91;hduser&#93;&#91;Thread-141&#93;Connected io.netty.channel.DefaultChannelHandlerContext@1f7c81aaERROR 11-05 13:56:01,212 - nioEventLoopGroup-4-8 java.lang.IndexOutOfBoundsException: readerIndex(67) + length(25) exceeds writerIndex(80): UnpooledUnsafeDirectByteBuf(ridx: 67, widx: 80, cap: 80) at io.netty.buffer.AbstractByteBuf.checkReadableBytes(AbstractByteBuf.java:1166) at io.netty.buffer.AbstractByteBuf.readBytes(AbstractByteBuf.java:676) at io.netty.buffer.AbstractByteBuf.readBytes(AbstractByteBuf.java:684) at org.apache.carbondata.core.dictionary.generator.key.DictionaryMessage.readData(DictionaryMessage.java:70) at org.apache.carbondata.core.dictionary.server.DictionaryServerHandler.channelRead(DictionaryServerHandler.java:59) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308) at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294) at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:846) at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131) at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:511) at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468) at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382) at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354) at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111) at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:137) at java.lang.Thread.run(Thread.java:745)ERROR 11-05 13:56:01,213 - nioEventLoopGroup-4-8 exceptionCaughtjava.lang.IndexOutOfBoundsException: readerIndex(67) + length(25) exceeds writerIndex(80): UnpooledUnsafeDirectByteBuf(ridx: 67, widx: 80, cap: 80) at io.netty.buffer.AbstractByteBuf.checkReadableBytes(AbstractByteBuf.java:1166) at io.netty.buffer.AbstractByteBuf.readBytes(AbstractByteBuf.java:676) at io.netty.buffer.AbstractByteBuf.readBytes(AbstractByteBuf.java:684) at org.apache.carbondata.core.dictionary.generator.key.DictionaryMessage.readData(DictionaryMessage.java:70) at org.apache.carbondata.core.dictionary.server.DictionaryServerHandler.channelRead(DictionaryServerHandler.java:59) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308) at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294) at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:846) at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131) at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:511) at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468) at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382) at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354) at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111) at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:137) at java.lang.Thread.run(Thread.java:745)WARN  11-05 13:56:31,750 - Lost task 0.0 in stage 1.0 (TID 2, hadoop-master): org.apache.carbondata.processing.newflow.exception.CarbonDataLoadingException:  at org.apache.carbondata.processing.newflow.sort.impl.ParallelReadMergeSorterImpl.checkError(ParallelReadMergeSorterImpl.java:164) at org.apache.carbondata.processing.newflow.sort.impl.ParallelReadMergeSorterImpl.sort(ParallelReadMergeSorterImpl.java:117) at org.apache.carbondata.processing.newflow.steps.SortProcessorStepImpl.execute(SortProcessorStepImpl.java:82) at org.apache.carbondata.processing.newflow.steps.DataWriterProcessorStepImpl.execute(DataWriterProcessorStepImpl.java:92) at org.apache.carbondata.processing.newflow.DataLoadExecutor.execute(DataLoadExecutor.java:48) at org.apache.carbondata.spark.rdd.NewCarbonDataLoadRDD$$anon$1.<init>(NewCarbonDataLoadRDD.scala:166) at org.apache.carbondata.spark.rdd.NewCarbonDataLoadRDD.compute(NewCarbonDataLoadRDD.scala:142) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306) at org.apache.spark.rdd.RDD.iterator(RDD.scala:270) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66) at org.apache.spark.scheduler.Task.run(Task.scala:89) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745)Caused by: java.lang.RuntimeException: java.lang.RuntimeException: Request timed out for key : DictionaryKey{ columnName='active_emui_version', data='ACTIVE_EMUI_VERSION_20007', dictionaryValue=-1, type=DICT_GENERATION} at org.apache.carbondata.core.dictionary.client.DictionaryClientHandler.getDictionary(DictionaryClientHandler.java:99) at org.apache.carbondata.core.dictionary.client.DictionaryClient.getDictionary(DictionaryClient.java:74) at org.apache.carbondata.processing.newflow.dictionary.DictionaryServerClientDictionary.getOrGenerateKey(DictionaryServerClientDictionary.java:57) at org.apache.carbondata.processing.newflow.dictionary.DictionaryServerClientDictionary.getOrGenerateKey(DictionaryServerClientDictionary.java:32) at org.apache.carbondata.processing.newflow.converter.impl.DictionaryFieldConverterImpl.convert(DictionaryFieldConverterImpl.java:115) at org.apache.carbondata.processing.newflow.converter.impl.RowConverterImpl.convert(RowConverterImpl.java:155) at org.apache.carbondata.processing.newflow.steps.DataConverterProcessorStepImpl.processRowBatch(DataConverterProcessorStepImpl.java:105) at org.apache.carbondata.processing.newflow.steps.DataConverterProcessorStepImpl$1.next(DataConverterProcessorStepImpl.java:91) at org.apache.carbondata.processing.newflow.steps.DataConverterProcessorStepImpl$1.next(DataConverterProcessorStepImpl.java:80) at org.apache.carbondata.processing.newflow.sort.impl.ParallelReadMergeSorterImpl$SortIteratorThread.call(ParallelReadMergeSorterImpl.java:227) at org.apache.carbondata.processing.newflow.sort.impl.ParallelReadMergeSorterImpl$SortIteratorThread.call(ParallelReadMergeSorterImpl.java:201) at java.util.concurrent.FutureTask.run(FutureTask.java:266) ... 3 moreCaused by: java.lang.RuntimeException: Request timed out for key : DictionaryKey{ columnName='active_emui_version', data='ACTIVE_EMUI_VERSION_20007', dictionaryValue=-1, type=DICT_GENERATION} at org.apache.carbondata.core.dictionary.client.DictionaryClientHandler.getDictionary(DictionaryClientHandler.java:94) ... 14 moreINFO  11-05 13:56:31,750 - Starting task 0.1 in stage 1.0 (TID 4, hadoop-master, partition 0,NODE_LOCAL, 2536 bytes)AUDIT 11-05 13:56:31,762 - &#91;hadoop-master&#93;&#91;hduser&#93;&#91;Thread-134&#93;Connected io.netty.channel.DefaultChannelHandlerContext@1e498704AUDIT 11-05 13:56:32,777 - &#91;hadoop-master&#93;&#91;hduser&#93;&#91;Thread-135&#93;Connected io.netty.channel.DefaultChannelHandlerContext@56018951ERROR 11-05 13:56:33,800 - nioEventLoopGroup-4-2 java.lang.IndexOutOfBoundsException: readerIndex(67) + length(25) exceeds writerIndex(80): UnpooledUnsafeDirectByteBuf(ridx: 67, widx: 80, cap: 80) at io.netty.buffer.AbstractByteBuf.checkReadableBytes(AbstractByteBuf.java:1166) at io.netty.buffer.AbstractByteBuf.readBytes(AbstractByteBuf.java:676) at io.netty.buffer.AbstractByteBuf.readBytes(AbstractByteBuf.java:684) at org.apache.carbondata.core.dictionary.generator.key.DictionaryMessage.readData(DictionaryMessage.java:70) at org.apache.carbondata.core.dictionary.server.DictionaryServerHandler.channelRead(DictionaryServerHandler.java:59) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308) at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294) at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:846) at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131) at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:511) at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468) at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382) at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354) at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111) at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:137) at java.lang.Thread.run(Thread.java:745)ERROR 11-05 13:56:33,800 - nioEventLoopGroup-4-2 exceptionCaughtjava.lang.IndexOutOfBoundsException: readerIndex(67) + length(25) exceeds writerIndex(80): UnpooledUnsafeDirectByteBuf(ridx: 67, widx: 80, cap: 80) at io.netty.buffer.AbstractByteBuf.checkReadableBytes(AbstractByteBuf.java:1166) at io.netty.buffer.AbstractByteBuf.readBytes(AbstractByteBuf.java:676) at io.netty.buffer.AbstractByteBuf.readBytes(AbstractByteBuf.java:684) at org.apache.carbondata.core.dictionary.generator.key.DictionaryMessage.readData(DictionaryMessage.java:70) at org.apache.carbondata.core.dictionary.server.DictionaryServerHandler.channelRead(DictionaryServerHandler.java:59) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308) at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294) at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:846) at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131) at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:511) at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468) at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382) at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354) at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111) at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:137) at java.lang.Thread.run(Thread.java:745)
issueID:CARBONDATA-1047
type:Improvement
changed files:processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/SortParameters.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/SortScopeOptions.java
processing/src/main/java/org/apache/carbondata/processing/util/CarbonDataProcessorUtil.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
core/src/main/java/org/apache/carbondata/hadoop/CarbonInputSplit.java
processing/src/main/java/org/apache/carbondata/processing/loading/DataLoadProcessBuilder.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/SorterFactory.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/UnsafeSortDataRows.java
processing/src/main/java/org/apache/carbondata/processing/loading/model/CarbonLoadModel.java
texts:Add load options to perform batch sort and add more testcases
Add load options to perform batch sort and add more testcases.Add options like below to the load command for batch sort.LOAD DATA LOCAL INPATH '$filePath' into table carbon_load1 OPTIONS('batch_sort'='true', 'batch_sort_size_inmb'='1')
issueID:CARBONDATA-1048
type:Improvement
changed files:
texts:Update Hive Guide
Decimal data type raises exception while selecting the data from the table in hive with steps given in hive guide1) In Spark Shell :a) Create Table -import org.apache.spark.sql.SparkSessionimport org.apache.spark.sql.CarbonSession._val carbon = SparkSession.builder().config(sc.getConf).getOrCreateCarbonSession("hdfs://localhost:54310/opt/data")scala> carbon.sql(""" create table testHive1(id int,name string,scale decimal(10,0),country string,salary double) stored by'carbondata' """).showb) Load Data - scala> carbon.sql(""" load data inpath 'hdfs://localhost:54310/Files/testHive1.csv' into table testHive1 """ ).show2) In Hive :a) Add Jars - add jar /home/neha/incubator-carbondata/assembly/target/scala-2.11/carbondata_2.11-1.1.0-incubating-SNAPSHOT-shade-hadoop2.7.2.jar;add jar /opt/spark-2.1.0-bin-hadoop2.7/jars/spark-catalyst_2.11-2.1.0.jar;add jar /home/neha/incubator-carbondata/integration/hive/carbondata-hive-1.1.0-incubating-SNAPSHOT.jar;b) Create Table -create table testHive1(id int,name string,scale decimal(10,0),country              string,salary double);c) Alter location - hive> alter table testHive1 set LOCATION 'hdfs://localhost:54310/opt/data/default/testhive1' ;d) Set Properties - set hive.mapred.supports.subdirectories=true;set mapreduce.input.fileinputformat.input.dir.recursive=true;d) Alter FileFormat -alter table testHive1 set FILEFORMATINPUTFORMAT "org.apache.carbondata.hive.MapredCarbonInputFormat"OUTPUTFORMAT "org.apache.carbondata.hive.MapredCarbonOutputFormat"SERDE "org.apache.carbondata.hive.CarbonHiveSerDe";hive> ADD JAR /home/hduser/spark-2.1.0-bin-hadoop2.7/jars/spark-catalyst_2.11-2.1.0.jar;Added &#91;/home/hduser/spark-2.1.0-bin-hadoop2.7/jars/spark-catalyst_2.11-2.1.0.jar&#93; to class pathAdded resources: &#91;/home/hduser/spark-2.1.0-bin-hadoop2.7/jars/spark-catalyst_2.11-2.1.0.jar&#93;f) Execute Queries - select * from testHive1;3) Query :hive> select * from testHive1;Exception in thread "&#91;main&#93;&#91;partitionID:hive25;queryID:4537623368167&#93;" java.lang.NoClassDefFoundError: scala/math/Ordered at java.lang.ClassLoader.defineClass1(Native Method) at java.lang.ClassLoader.defineClass(ClassLoader.java:763) at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)when i add the scala-library and scala-reflect jar it works finehive> ADD JAR /home/knoldus/Videos/scala-library-2.11.1.jar;Added &#91;/home/knoldus/Videos/scala-library-2.11.1.jar&#93; to class pathAdded resources: &#91;/home/knoldus/Videos/scala-library-2.11.1.jar&#93;hive> ADD JAR /home/knoldus/Videos/scala-reflect-2.11.1.jar;Added &#91;/home/knoldus/Videos/scala-reflect-2.11.1.jar&#93; to class pathAdded resources: &#91;/home/knoldus/Videos/scala-reflect-2.11.1.jar&#93;fired the query againhive> select * from testHive1;OK2 runlin 2 china 33000.21 yuhai 2 china 33000.1so its better to mention about adding these jar in hive
issueID:CARBONDATA-1049
type:Bug
changed files:processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/SortDataRows.java
core/src/main/java/org/apache/carbondata/core/mutate/DeleteDeltaBlockDetails.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/merger/UnsafeIntermediateMerger.java
core/src/main/java/org/apache/carbondata/core/util/DataTypeUtil.java
processing/src/main/java/org/apache/carbondata/processing/store/writer/AbstractFactDataWriter.java
processing/src/main/java/org/apache/carbondata/processing/loading/converter/impl/RowConverterImpl.java
processing/src/main/java/org/apache/carbondata/processing/loading/converter/impl/DirectDictionaryFieldConverterImpl.java
processing/src/main/java/org/apache/carbondata/processing/util/CarbonDataProcessorUtil.java
core/src/main/java/org/apache/carbondata/core/keygenerator/directdictionary/timestamp/TimeStampDirectDictionaryGenerator.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/SortIntermediateFileMerger.java
core/src/main/java/org/apache/carbondata/core/mutate/SegmentUpdateDetails.java
core/src/main/java/org/apache/carbondata/core/cache/CarbonLRUCache.java
core/src/main/java/org/apache/carbondata/core/keygenerator/directdictionary/timestamp/DateDirectDictionaryGenerator.java
core/src/main/java/org/apache/carbondata/core/scan/filter/FilterExpressionProcessor.java
core/src/main/java/org/apache/carbondata/core/scan/filter/FilterUtil.java
processing/src/main/java/org/apache/carbondata/processing/loading/converter/impl/MeasureFieldConverterImpl.java
processing/src/main/java/org/apache/carbondata/processing/loading/converter/BadRecordLogHolder.java
processing/src/main/java/org/apache/carbondata/processing/loading/converter/impl/NonDictionaryFieldConverterImpl.java
texts:avoid logging data into log file

issueID:CARBONDATA-105
type:Bug
changed files:
texts:Correct precalculation of dictionary file existence
In case of concurrent data loading,pre calculation of existence of dictionary file will not have proper result.
issueID:CARBONDATA-1050
type:Bug
changed files:
texts:int and short measures should not be considered as long.

issueID:CARBONDATA-1053
type:Improvement
changed files:integration/hive/src/main/java/org/apache/carbondata/hive/CarbonObjectInspector.java
integration/hive/src/main/java/org/apache/carbondata/hive/CarbonHiveRecordReader.java
integration/hive/src/main/java/org/apache/carbondata/hive/CarbonHiveSerDe.java
texts:Support Reading Char Data Type in Hive From CarbonTable

issueID:CARBONDATA-1054
type:Bug
changed files:
texts:Data Load failure in sort_column table.
Error displays to user while load the Data in table.Step to Reproduce:1: Create table:CREATE TABLE sorttable4_offheap_inmemory (empno int, empname String, designation String, doj Timestamp, workgroupcategory int, workgroupcategoryname String, deptno int, deptname String, projectcode int, projectjoindate Timestamp, projectenddate Timestamp,attendance int,utilization int,salary int) STORED BY 'org.apache.carbondata.format' tblproperties('sort_columns'='workgroupcategory, empname');2:Load Data in table:LOAD DATA local inpath 'hdfs://localhost:54310/newdata.csv' INTO TABLE sorttable4_offheap_inmemory OPTIONS('DELIMITER'= ',', 'QUOTECHAR'= '\"');3:Error on Beeline:0: jdbc:hive2://localhost:10000> LOAD DATA local inpath 'hdfs://localhost:54310/newdata.csv' INTO TABLE sorttable4_offheap_inmemory OPTIONS('DELIMITER'= ',', 'QUOTECHAR'= '\"');Error: java.lang.Exception: DataLoad failure: Problem while shutdown the server (state=,code=0)4: ThriftServer logs:17/05/15 18:07:07 INFO thriftserver.SparkExecuteStatementOperation: Running query 'LOAD DATA local inpath 'hdfs://localhost:54310/newdata.csv' INTO TABLE sorttable4_offheap_inmemory OPTIONS('DELIMITER'= ',', 'QUOTECHAR'= '\"')' with f01a8190-d2c8-40e1-b932-af695cf0129c17/05/15 18:07:07 INFO parser.CarbonSparkSqlParser: Parsing command: LOAD DATA local inpath 'hdfs://localhost:54310/newdata.csv' INTO TABLE sorttable4_offheap_inmemory OPTIONS('DELIMITER'= ',', 'QUOTECHAR'= '\"')17/05/15 18:07:07 INFO optimizer.CarbonLateDecodeRule: pool-24-thread-10 Skip CarbonOptimizer17/05/15 18:07:07 INFO locks.HdfsFileLock: pool-24-thread-10 HDFS lock path:hdfs://localhost:54310/opt/carbonStore/vinod/sorttable4_offheap_inmemory/meta.lock17/05/15 18:07:07 INFO command.LoadTable: pool-24-thread-10 Successfully able to get the table metadata file lock17/05/15 18:07:07 INFO command.LoadTable: pool-24-thread-10 Initiating Direct Load for the Table : (vinod.sorttable4_offheap_inmemory)17/05/15 18:07:07 INFO util.GlobalDictionaryUtil$: pool-24-thread-10 Generate global dictionary from source data files!17/05/15 18:07:07 INFO memory.MemoryStore: Block broadcast_5 stored as values in memory (estimated size 238.9 KB, free 2.4 GB)17/05/15 18:07:07 INFO memory.MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 23.4 KB, free 2.4 GB)17/05/15 18:07:07 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.2.179:46491 (size: 23.4 KB, free: 2.5 GB)17/05/15 18:07:07 INFO spark.SparkContext: Created broadcast 5 from NewHadoopRDD at GlobalDictionaryUtil.scala:38117/05/15 18:07:07 INFO optimizer.CarbonLateDecodeRule: pool-24-thread-10 Skip CarbonOptimizer17/05/15 18:07:07 INFO spark.SparkContext: Starting job: collect at GlobalDictionaryUtil.scala:74617/05/15 18:07:07 INFO input.FileInputFormat: Total input paths to process : 117/05/15 18:07:07 INFO scheduler.DAGScheduler: Registering RDD 26 (RDD at CarbonGlobalDictionaryRDD.scala:274)17/05/15 18:07:07 INFO scheduler.DAGScheduler: Got job 2 (collect at GlobalDictionaryUtil.scala:746) with 4 output partitions17/05/15 18:07:07 INFO scheduler.DAGScheduler: Final stage: ResultStage 4 (collect at GlobalDictionaryUtil.scala:746)17/05/15 18:07:07 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)17/05/15 18:07:07 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 3)17/05/15 18:07:07 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 3 (CarbonBlockDistinctValuesCombineRDD&#91;26&#93; at RDD at CarbonGlobalDictionaryRDD.scala:274), which has no missing parents17/05/15 18:07:07 INFO memory.MemoryStore: Block broadcast_6 stored as values in memory (estimated size 24.1 KB, free 2.4 GB)17/05/15 18:07:07 INFO memory.MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 10.0 KB, free 2.4 GB)17/05/15 18:07:07 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on 192.168.2.179:46491 (size: 10.0 KB, free: 2.5 GB)17/05/15 18:07:07 INFO spark.SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:99617/05/15 18:07:07 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 3 (CarbonBlockDistinctValuesCombineRDD&#91;26&#93; at RDD at CarbonGlobalDictionaryRDD.scala:274)17/05/15 18:07:07 INFO scheduler.TaskSchedulerImpl: Adding task set 3.0 with 1 tasks17/05/15 18:07:07 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 3.0 (TID 6, localhost, executor driver, partition 0, ANY, 6384 bytes)17/05/15 18:07:07 INFO executor.Executor: Running task 0.0 in stage 3.0 (TID 6)17/05/15 18:07:07 INFO rdd.NewHadoopRDD: Input split: hdfs://localhost:54310/newdata.csv:0+166325017/05/15 18:07:07 INFO executor.Executor: Finished task 0.0 in stage 3.0 (TID 6). 1643 bytes result sent to driver17/05/15 18:07:07 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 3.0 (TID 6) in 136 ms on localhost (executor driver) (1/1)17/05/15 18:07:07 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 17/05/15 18:07:07 INFO scheduler.DAGScheduler: ShuffleMapStage 3 (RDD at CarbonGlobalDictionaryRDD.scala:274) finished in 0.136 s17/05/15 18:07:07 INFO scheduler.DAGScheduler: looking for newly runnable stages17/05/15 18:07:07 INFO scheduler.DAGScheduler: running: Set()17/05/15 18:07:07 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 4)17/05/15 18:07:07 INFO scheduler.DAGScheduler: failed: Set()17/05/15 18:07:07 INFO scheduler.DAGScheduler: Submitting ResultStage 4 (CarbonGlobalDictionaryGenerateRDD&#91;28&#93; at RDD at CarbonGlobalDictionaryRDD.scala:335), which has no missing parents17/05/15 18:07:07 INFO memory.MemoryStore: Block broadcast_7 stored as values in memory (estimated size 6.8 KB, free 2.4 GB)17/05/15 18:07:07 INFO memory.MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 3.7 KB, free 2.4 GB)17/05/15 18:07:07 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on 192.168.2.179:46491 (size: 3.7 KB, free: 2.5 GB)17/05/15 18:07:07 INFO spark.SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:99617/05/15 18:07:07 INFO scheduler.DAGScheduler: Submitting 4 missing tasks from ResultStage 4 (CarbonGlobalDictionaryGenerateRDD&#91;28&#93; at RDD at CarbonGlobalDictionaryRDD.scala:335)17/05/15 18:07:07 INFO scheduler.TaskSchedulerImpl: Adding task set 4.0 with 4 tasks17/05/15 18:07:07 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 4.0 (TID 7, localhost, executor driver, partition 0, ANY, 6121 bytes)17/05/15 18:07:07 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 4.0 (TID 8, localhost, executor driver, partition 1, ANY, 6121 bytes)17/05/15 18:07:07 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 4.0 (TID 9, localhost, executor driver, partition 2, ANY, 6121 bytes)17/05/15 18:07:07 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 4.0 (TID 10, localhost, executor driver, partition 3, ANY, 6121 bytes)17/05/15 18:07:07 INFO executor.Executor: Running task 0.0 in stage 4.0 (TID 7)17/05/15 18:07:07 INFO executor.Executor: Running task 1.0 in stage 4.0 (TID 8)17/05/15 18:07:07 INFO executor.Executor: Running task 2.0 in stage 4.0 (TID 9)17/05/15 18:07:07 INFO locks.HdfsFileLock: Executor task launch worker-5 HDFS lock path:hdfs://localhost:54310/opt/carbonStore/vinod/sorttable4_offheap_inmemory/e8c4c615-25f3-4f9f-92e8-760a8f36118b.lock17/05/15 18:07:07 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks17/05/15 18:07:07 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms17/05/15 18:07:07 INFO locks.HdfsFileLock: Executor task launch worker-4 HDFS lock path:hdfs://localhost:54310/opt/carbonStore/vinod/sorttable4_offheap_inmemory/da3e9354-8951-4e1b-bb46-57a4cfd59c2e.lock17/05/15 18:07:07 INFO executor.Executor: Running task 3.0 in stage 4.0 (TID 10)17/05/15 18:07:07 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks17/05/15 18:07:07 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms17/05/15 18:07:07 INFO locks.HdfsFileLock: Executor task launch worker-7 HDFS lock path:hdfs://localhost:54310/opt/carbonStore/vinod/sorttable4_offheap_inmemory/1afdd308-aaf4-4e7d-8eee-b12c0f2ef76b.lock17/05/15 18:07:07 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks17/05/15 18:07:07 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms17/05/15 18:07:07 INFO locks.HdfsFileLock: Executor task launch worker-6 HDFS lock path:hdfs://localhost:54310/opt/carbonStore/vinod/sorttable4_offheap_inmemory/2f15d866-ece2-4639-a656-14beb6172c3a.lock17/05/15 18:07:07 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks17/05/15 18:07:07 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms17/05/15 18:07:08 INFO rdd.CarbonGlobalDictionaryGenerateRDD: Successfully able to get the dictionary lock for designation17/05/15 18:07:08 INFO rdd.CarbonGlobalDictionaryGenerateRDD: Executor task launch worker-5  columnName: designation columnId: e8c4c615-25f3-4f9f-92e8-760a8f36118b new distinct values count: 0 combine lists: 2 create dictionary cache: 2 sort list, distinct and write: 3 write sort info: 017/05/15 18:07:08 INFO rdd.CarbonGlobalDictionaryGenerateRDD: Successfully able to get the dictionary lock for empname17/05/15 18:07:08 INFO rdd.CarbonGlobalDictionaryGenerateRDD: Executor task launch worker-4  columnName: empname columnId: da3e9354-8951-4e1b-bb46-57a4cfd59c2e new distinct values count: 0 combine lists: 4 create dictionary cache: 2 sort list, distinct and write: 1 write sort info: 017/05/15 18:07:08 INFO rdd.CarbonGlobalDictionaryGenerateRDD: Successfully able to get the dictionary lock for deptname17/05/15 18:07:08 INFO rdd.CarbonGlobalDictionaryGenerateRDD: Successfully able to get the dictionary lock for workgroupcategoryname17/05/15 18:07:08 INFO rdd.CarbonGlobalDictionaryGenerateRDD: Executor task launch worker-6  columnName: workgroupcategoryname columnId: 2f15d866-ece2-4639-a656-14beb6172c3a new distinct values count: 0 combine lists: 1 create dictionary cache: 1 sort list, distinct and write: 1 write sort info: 017/05/15 18:07:08 INFO rdd.CarbonGlobalDictionaryGenerateRDD: Executor task launch worker-7  columnName: deptname columnId: 1afdd308-aaf4-4e7d-8eee-b12c0f2ef76b new distinct values count: 0 combine lists: 2 create dictionary cache: 4 sort list, distinct and write: 2 write sort info: 017/05/15 18:07:08 INFO locks.HdfsFileLock: Executor task launch worker-5 Deleted the lock file hdfs://localhost:54310/opt/carbonStore/vinod/sorttable4_offheap_inmemory/e8c4c615-25f3-4f9f-92e8-760a8f36118b.lock17/05/15 18:07:08 INFO rdd.CarbonGlobalDictionaryGenerateRDD: Dictionary designation Unlocked Successfully.17/05/15 18:07:08 INFO executor.Executor: Finished task 1.0 in stage 4.0 (TID 8). 1678 bytes result sent to driver17/05/15 18:07:08 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 4.0 (TID 8) in 110 ms on localhost (executor driver) (1/4)17/05/15 18:07:08 INFO locks.HdfsFileLock: Executor task launch worker-4 Deleted the lock file hdfs://localhost:54310/opt/carbonStore/vinod/sorttable4_offheap_inmemory/da3e9354-8951-4e1b-bb46-57a4cfd59c2e.lock17/05/15 18:07:08 INFO rdd.CarbonGlobalDictionaryGenerateRDD: Dictionary empname Unlocked Successfully.17/05/15 18:07:08 INFO executor.Executor: Finished task 0.0 in stage 4.0 (TID 7). 1678 bytes result sent to driver17/05/15 18:07:08 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 4.0 (TID 7) in 122 ms on localhost (executor driver) (2/4)17/05/15 18:07:08 INFO locks.HdfsFileLock: Executor task launch worker-6 Deleted the lock file hdfs://localhost:54310/opt/carbonStore/vinod/sorttable4_offheap_inmemory/2f15d866-ece2-4639-a656-14beb6172c3a.lock17/05/15 18:07:08 INFO rdd.CarbonGlobalDictionaryGenerateRDD: Dictionary workgroupcategoryname Unlocked Successfully.17/05/15 18:07:08 INFO locks.HdfsFileLock: Executor task launch worker-7 Deleted the lock file hdfs://localhost:54310/opt/carbonStore/vinod/sorttable4_offheap_inmemory/1afdd308-aaf4-4e7d-8eee-b12c0f2ef76b.lock17/05/15 18:07:08 INFO rdd.CarbonGlobalDictionaryGenerateRDD: Dictionary deptname Unlocked Successfully.17/05/15 18:07:08 INFO executor.Executor: Finished task 2.0 in stage 4.0 (TID 9). 1678 bytes result sent to driver17/05/15 18:07:08 INFO executor.Executor: Finished task 3.0 in stage 4.0 (TID 10). 1678 bytes result sent to driver17/05/15 18:07:08 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 4.0 (TID 9) in 131 ms on localhost (executor driver) (3/4)17/05/15 18:07:08 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 4.0 (TID 10) in 130 ms on localhost (executor driver) (4/4)17/05/15 18:07:08 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 17/05/15 18:07:08 INFO scheduler.DAGScheduler: ResultStage 4 (collect at GlobalDictionaryUtil.scala:746) finished in 0.128 s17/05/15 18:07:08 INFO scheduler.DAGScheduler: Job 2 finished: collect at GlobalDictionaryUtil.scala:746, took 0.289833 s17/05/15 18:07:08 INFO util.GlobalDictionaryUtil$: pool-24-thread-10 generate global dictionary successfully17/05/15 18:07:08 AUDIT rdd.CarbonDataRDDFactory$: &#91;vinod-Vostro-3559&#93;&#91;anonymous&#93;&#91;Thread-179&#93;Data load request has been received for table vinod.sorttable4_offheap_inmemory17/05/15 18:07:08 INFO util.CommonUtil$: pool-24-thread-10 &#91;Block Distribution&#93;17/05/15 18:07:08 INFO util.CommonUtil$: pool-24-thread-10 totalInputSpaceConsumed: 1663250 , defaultParallelism: 417/05/15 18:07:08 INFO util.CommonUtil$: pool-24-thread-10 mapreduce.input.fileinputformat.split.maxsize: 1677721617/05/15 18:07:08 INFO input.FileInputFormat: Total input paths to process : 117/05/15 18:07:08 INFO hive.DistributionUtil$: pool-24-thread-10 Executors configured : 117/05/15 18:07:08 INFO hive.DistributionUtil$: pool-24-thread-10 Total Time taken to ensure the required executors : 117/05/15 18:07:08 INFO hive.DistributionUtil$: pool-24-thread-10 Time elapsed to allocate the required executors: 017/05/15 18:07:08 INFO rdd.CarbonDataRDDFactory$: pool-24-thread-10 Total Time taken in block allocation: 117/05/15 18:07:08 INFO rdd.CarbonDataRDDFactory$: pool-24-thread-10 Total no of blocks: 1, No.of Nodes: 117/05/15 18:07:08 INFO rdd.CarbonDataRDDFactory$: pool-24-thread-10 #Node: vinod-Vostro-3559 no.of.blocks: 117/05/15 18:07:08 INFO memory.MemoryStore: Block broadcast_8 stored as values in memory (estimated size 58.0 MB, free 2.4 GB)17/05/15 18:07:08 INFO memory.MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 23.3 KB, free 2.4 GB)17/05/15 18:07:08 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on 192.168.2.179:46491 (size: 23.3 KB, free: 2.5 GB)17/05/15 18:07:08 INFO spark.SparkContext: Created broadcast 8 from broadcast at NewCarbonDataLoadRDD.scala:18517/05/15 18:07:08 INFO spark.SparkContext: Starting job: collect at CarbonDataRDDFactory.scala:63017/05/15 18:07:08 INFO scheduler.DAGScheduler: Got job 3 (collect at CarbonDataRDDFactory.scala:630) with 1 output partitions17/05/15 18:07:08 INFO scheduler.DAGScheduler: Final stage: ResultStage 5 (collect at CarbonDataRDDFactory.scala:630)17/05/15 18:07:08 INFO scheduler.DAGScheduler: Parents of final stage: List()17/05/15 18:07:08 INFO scheduler.DAGScheduler: Missing parents: List()17/05/15 18:07:08 INFO scheduler.DAGScheduler: Submitting ResultStage 5 (NewCarbonDataLoadRDD&#91;29&#93; at RDD at NewCarbonDataLoadRDD.scala:174), which has no missing parents17/05/15 18:07:08 INFO rdd.NewCarbonDataLoadRDD: Preferred Location for split : vinod-Vostro-355917/05/15 18:07:08 INFO memory.MemoryStore: Block broadcast_9 stored as values in memory (estimated size 12.0 KB, free 2.4 GB)17/05/15 18:07:08 INFO memory.MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 6.1 KB, free 2.4 GB)17/05/15 18:07:08 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on 192.168.2.179:46491 (size: 6.1 KB, free: 2.5 GB)17/05/15 18:07:08 INFO spark.SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:99617/05/15 18:07:08 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (NewCarbonDataLoadRDD&#91;29&#93; at RDD at NewCarbonDataLoadRDD.scala:174)17/05/15 18:07:08 INFO scheduler.TaskSchedulerImpl: Adding task set 5.0 with 1 tasks17/05/15 18:07:08 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 5.0 (TID 11, localhost, executor driver, partition 0, ANY, 6650 bytes)17/05/15 18:07:08 INFO executor.Executor: Running task 0.0 in stage 5.0 (TID 11)17/05/15 18:07:08 INFO rdd.NewCarbonDataLoadRDD: Input split: vinod-Vostro-355917/05/15 18:07:08 INFO rdd.NewCarbonDataLoadRDD: The Block Count in this node :117/05/15 18:07:08 INFO newflow.AbstractDataLoadProcessorStep: Thread-105 Rows processed in step Input Processor : 017/05/15 18:07:08 INFO newflow.AbstractDataLoadProcessorStep: Thread-107 Rows processed in step Sort Processor : 017/05/15 18:07:08 INFO newflow.AbstractDataLoadProcessorStep: Thread-108 Rows processed in step Data Writer : 017/05/15 18:07:08 INFO newflow.AbstractDataLoadProcessorStep: Thread-106 Rows processed in step Data Converter : 017/05/15 18:07:08 INFO sortdata.SortParameters: &#91;Executor task launch worker-7&#93;&#91;partitionID:vinod_sorttable4_offheap_inmemory_2552c8a8-99df-4024-a1fd-fc0c86e06d67&#93; Sort size for table: 50000017/05/15 18:07:08 INFO sortdata.SortParameters: &#91;Executor task launch worker-7&#93;&#91;partitionID:vinod_sorttable4_offheap_inmemory_2552c8a8-99df-4024-a1fd-fc0c86e06d67&#93; Number of intermediate file to be merged: 2017/05/15 18:07:08 INFO sortdata.SortParameters: &#91;Executor task launch worker-7&#93;&#91;partitionID:vinod_sorttable4_offheap_inmemory_2552c8a8-99df-4024-a1fd-fc0c86e06d67&#93; File Buffer Size: 104857617/05/15 18:07:08 INFO sortdata.SortParameters: &#91;Executor task launch worker-7&#93;&#91;partitionID:vinod_sorttable4_offheap_inmemory_2552c8a8-99df-4024-a1fd-fc0c86e06d67&#93; temp file location/tmp/26818412296318/0/vinod/sorttable4_offheap_inmemory/Fact/Part0/Segment_0/0/sortrowtmp17/05/15 18:07:08 INFO newflow.DataLoadExecutor: &#91;Executor task launch worker-7&#93;&#91;partitionID:vinod_sorttable4_offheap_inmemory_2552c8a8-99df-4024-a1fd-fc0c86e06d67&#93; Data Loading is started for table sorttable4_offheap_inmemory17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : AL   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Medical Science Liaison to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-07-17 14:32:06 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-06-12 07:11:38 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : PR   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Project Support to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-12-09 01:15:30 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-08-05 10:36:36 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : AL   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Associate Producer to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-08-06 01:35:31 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-08-23 15:29:07 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : CT   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Scientist to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-10-16 21:59:44 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-01-21 10:00:12 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : NY   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Solution Architect to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-08-23 04:03:21 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-04-25 02:54:53 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : AL   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Technology Manager to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-06-22 13:48:07 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-12-07 07:22:53 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : PA   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Clinical Specialist to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-12-12 09:42:46 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-03-28 14:58:20 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : NY   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Line Manager to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-03-29 07:54:51 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-06-09 03:39:52 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : PA   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Program Coordinator to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-10-03 06:08:13 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-12-06 18:55:51 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : AL   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Technology Manager to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-07-19 22:23:37 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-04-09 04:39:45 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : CT   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Clinical Specialist to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-09-22 02:16:29 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-08-22 15:45:01 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : TN   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Solutions Designer to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-03-18 07:47:32 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-01-10 08:09:15 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : NC   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Thought Leader Liaison to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-09-19 09:00:48 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-09-08 00:01:58 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : CA   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Senior Manager to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-08-23 06:39:19 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-06-15 08:25:39 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : MA   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Medical Science Liaison to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-10-05 16:23:33 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-01-25 04:01:21 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : AZ   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Project Support to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-05-29 10:59:55 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-04-14 18:03:54 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : NY   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Associate Producer to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-09-22 07:22:56 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-07-17 09:19:04 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : TX   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Scientist to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-03-10 23:47:07 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-07-25 03:00:22 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : NY   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Solution Architect to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-07-06 07:24:23 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-02-19 21:27:54 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : OR   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Technology Manager to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-03-19 04:38:24 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-08-28 11:34:46 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : PA   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Clinical Specialist to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-11-10 04:22:49 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-05-26 10:10:19 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : FL   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Line Manager to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-05-31 19:37:32 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-08-08 16:39:02 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : TX   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Program Coordinator to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-08-11 12:56:30 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-07-17 16:09:08 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : TX   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Technology Manager to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-09-18 02:29:08 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-02-25 11:04:51 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : CA   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Clinical Specialist to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-08-15 06:46:43 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-07-27 19:40:58 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : TX   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Solutions Designer to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-02-28 05:06:07 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-10-19 01:57:03 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : OH   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Thought Leader Liaison to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-10-31 08:17:00 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-11-30 21:12:27 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : VA   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Senior Manager to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-12-11 08:33:56 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-11-01 17:48:21 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : FL   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Location Manager to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-02-26 23:34:39 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-02-29 09:13:30 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : CA   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Market Planning Manager to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-01-15 12:32:55 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-07-28 23:07:05 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : MO   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Producer to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-11-01 14:19:05 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-09-28 10:02:44 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : NY   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Senior Analyst to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-04-30 23:28:50 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-05-19 22:35:51 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : GA   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Senior Manager to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-06-21 04:27:32 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-10-24 18:08:48 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : MA   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Associate Producer to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-10-15 05:12:14 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-06-09 13:06:35 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : PR   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Scientist to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-09-24 03:19:41 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-09-19 14:30:35 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : PR   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Solution Architect to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-11-28 02:25:18 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-08-09 19:01:20 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : CT   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Technology Manager to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-07-07 20:59:59 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-04-04 14:01:14 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : ME   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Clinical Specialist to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-11-30 04:57:01 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-08-07 16:45:30 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : GA   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Line Manager to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-04-28 22:23:07 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-02-06 23:04:16 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : AL   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Program Coordinator to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-02-17 04:00:31 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-03-12 17:43:33 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : AL   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Technology Manager to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-02-15 09:16:48 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-11-11 08:17:44 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : KY   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Clinical Specialist to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-12-22 00:55:09 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-11-03 22:22:41 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : PR   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Solutions Designer to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-03-09 01:50:32 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-07-10 17:03:41 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : AL   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Thought Leader Liaison to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-03-21 18:38:50 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-08-28 16:14:38 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : NY   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Senior Manager to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-04-30 06:03:31 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-01-02 22:37:24 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : NC   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Clinical Coordinator to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-01-25 19:39:46 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-10-06 07:47:42 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : MA   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Location Manager to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-12-28 02:20:10 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-06-25 16:52:22 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : NY   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Market Planning Manager to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-08-19 01:07:26 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-01-13 05:56:17 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : PA   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Producer to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-02-10 16:38:13 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-05-17 13:30:48 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : RI   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Senior Analyst to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-04-05 03:41:51 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-09-24 11:28:05 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : NJ   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Senior Manager to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-08-31 02:06:01 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-05-25 14:27:04 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : MI   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Associate Producer to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-10-04 19:37:55 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-05-21 10:56:39 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : AL   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Scientist to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-08-17 08:42:46 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-08-20 21:09:02 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : AL   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Solution Architect to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-06-01 22:21:32 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-07-12 13:38:30 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : NC   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Technology Manager to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-02-07 17:17:10 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-02-05 06:07:56 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : CT   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Clinical Specialist to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-07-02 06:14:36 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-07-19 04:38:22 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : VA   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Line Manager to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-01-05 20:03:02 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-10-06 06:05:36 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : MA   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Program Coordinator to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-08-02 03:16:24 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-10-14 09:46:15 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : NY   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Technology Manager to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-08-14 16:56:17 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-11-01 01:25:27 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : NY   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Clinical Specialist to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-04-15 22:32:54 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-09-02 17:32:25 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : WA   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Solutions Designer to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-03-02 06:35:18 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-06-01 20:42:02 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : IL   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Thought Leader Liaison to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-12-17 18:57:46 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-09-17 04:36:10 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : WA   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Senior Manager to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-11-09 12:21:15 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-12-23 16:19:46 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : MA   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Clinical Coordinator to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-10-27 08:11:30 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-12-26 04:15:55 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : IA   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Clinical Specialist to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-01-07 13:40:19 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-04-22 10:05:56 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : CT   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Line Manager to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-01-06 11:49:42 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-02-02 19:32:59 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : PR   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Program Coordinator to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-03-22 17:48:50 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-08-27 13:48:52 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : NH   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Technology Manager to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-07-15 14:52:55 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-09-10 01:38:36 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : CT   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Clinical Specialist to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-03-21 03:09:57 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-02-21 05:54:38 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : TX   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Solutions Designer to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-12-13 11:32:46 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-08-07 07:27:41 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : MS   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Thought Leader Liaison to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-10-02 05:34:05 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-11-28 07:59:50 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : IN   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Senior Manager to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-06-19 15:34:27 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-05-28 14:53:49 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : NJ   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Clinical Coordinator to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-09-17 09:44:58 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-04-24 18:48:47 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : AL   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Location Manager to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-02-05 22:55:54 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-09-24 03:11:26 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : AL   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Market Planning Manager to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-02-05 13:19:51 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-08-15 04:53:21 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : NC   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Producer to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-07-17 05:41:12 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-11-19 15:43:40 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : MA   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Senior Analyst to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-03-11 23:55:17 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-04-03 11:57:54 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : NJ   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Senior Manager to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-07-01 01:31:20 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-12-29 23:15:20 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : NJ   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Consultant to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-06-07 21:45:48 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-06-24 11:22:00 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : NY   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Information Delivery to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-04-19 09:31:16 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-02-11 02:11:46 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : MS   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Location Scout to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-05-14 08:40:24 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-09-16 03:57:00 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : MD   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Producer to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-06-05 09:59:00 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-08-05 07:36:43 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : VA   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Report and Visualization Developer to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-06-25 16:45:06 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-12-23 13:32:00 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : NJ   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Clinical Specialist to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-01-07 10:03:01 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-04-28 09:03:13 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : SC   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Line Manager to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-04-28 16:46:14 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-12-03 10:13:24 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : AL   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Program Coordinator to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-07-25 10:14:31 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-01-09 20:38:00 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : AL   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Technology Manager to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-09-21 19:51:31 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-01-11 21:29:12 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : AL   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Clinical Specialist to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-12-14 18:07:48 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-02-19 17:19:34 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : UT   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Solutions Designer to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-10-31 07:10:08 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-08-02 18:26:56 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : AZ   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Thought Leader Liaison to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-10-17 02:57:13 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-11-17 14:51:03 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : PA   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Senior Manager to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-03-14 15:41:42 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-06-15 11:12:36 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : MI   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Clinical Coordinator to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-10-26 19:27:32 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-10-02 18:08:45 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : OH   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Location Manager to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-04-11 20:31:53 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-11-01 21:27:23 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : AL   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Market Planning Manager to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-07-13 14:23:40 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-01-23 17:31:59 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : PR   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Producer to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-10-25 20:24:13 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-11-17 12:40:31 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : OH   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Senior Analyst to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-10-31 21:49:36 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-04-14 08:07:27 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : VA   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Senior Manager to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-09-12 17:23:32 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-08-06 11:53:44 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : OH   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Consultant to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-08-08 04:30:04 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-01-02 10:26:41 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : GA   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Information Delivery to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-09-09 00:21:25 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-01-05 16:37:38 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : IL   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Location Scout to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-12-29 11:46:52 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-07-08 09:09:15 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : IN   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Producer to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-01-30 19:00:26 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-12-17 06:19:45 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : CA   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Report and Visualization Developer to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-09-03 20:08:33 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-06-28 05:35:55 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : WA   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Leader to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-04-08 17:41:10 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-08-01 08:37:44 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : NY   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Manager to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-11-19 19:21:44 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-02-26 22:21:43 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : TX   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Research Associate to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-05-17 21:47:38 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-04-10 19:07:33 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : TN   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Clinical Specialist to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-09-20 21:45:36 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-04-13 06:10:04 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : PR   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Solutions Designer to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-05-01 23:30:24 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-11-10 16:58:54 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : MA   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Thought Leader Liaison to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-11-04 06:07:08 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-05-06 06:22:42 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : AL   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Senior Manager to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-03-05 10:37:38 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-10-20 10:51:04 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : CT   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Clinical Coordinator to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-11-19 05:23:12 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-02-26 06:09:08 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : TN   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Location Manager to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-08-10 05:46:00 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-05-14 18:53:32 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : NJ   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Market Planning Manager to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-10-01 20:19:40 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-06-08 19:20:19 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : NY   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Producer to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-03-10 13:32:50 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-10-03 20:51:51 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : NH   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Senior Analyst to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-07-07 00:45:34 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-02-15 02:48:44 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : AL   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Senior Manager to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-10-07 18:51:47 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-02-21 07:32:29 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : NY   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Consultant to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-06-03 18:11:37 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-01-09 14:24:57 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : MD   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Information Delivery to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-05-11 18:43:06 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-11-17 04:05:15 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : AL   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Location Scout to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-05-16 19:14:40 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-08-01 21:10:00 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : PA   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Producer to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-02-12 08:22:17 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-02-29 02:28:43 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : NY   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Report and Visualization Developer to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-05-04 23:13:11 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-06-14 20:38:53 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : NC   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : First Officer to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-06-29 02:15:05 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-11-26 11:57:02 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : NC   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Leader to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-02-16 23:44:19 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-09-03 18:16:26 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : NC   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Manager to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-03-06 19:57:44 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-08-09 20:22:18 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : TX   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Research Associate to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-03-08 15:19:03 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-09-26 06:48:24 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : FL   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Clinical Specialist to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-03-05 20:30:57 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-04-14 12:33:57 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : AL   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Solutions Designer to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-10-27 07:28:37 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-03-26 19:02:25 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : AL   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Thought Leader Liaison to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-04-05 11:48:59 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-10-01 20:51:21 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : NJ   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Senior Manager to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-02-28 23:03:44 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-02-26 22:17:05 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : ND   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Clinical Coordinator to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-01-16 12:38:38 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-03-11 09:57:44 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : FL   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Location Manager to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-10-23 13:52:41 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-11-14 00:39:15 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : IN   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Market Planning Manager to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-02-21 12:48:25 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-09-18 02:00:22 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : AL   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Producer to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-03-14 17:31:08 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-07-04 10:51:26 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : ND   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Senior Analyst to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-04-01 17:30:39 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-11-30 01:57:17 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : PA   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Senior Manager to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-01-18 07:42:15 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-11-18 06:58:39 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : NJ   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Consultant to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-08-30 23:42:26 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-01-27 17:22:43 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : TX   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Information Delivery to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-12-18 14:30:51 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-08-27 05:12:48 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : WA   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Location Scout to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-11-10 00:12:53 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-05-16 19:29:22 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : MA   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Producer to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-09-19 00:15:31 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-07-14 01:50:08 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : NY   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Report and Visualization Developer to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-04-16 07:22:29 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-06-12 15:21:48 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : CA   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : First Officer to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-05-30 08:06:41 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-10-29 07:48:53 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : PR   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Senior Manager to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-11-22 07:46:20 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-01-25 02:37:59 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : CT   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Clinical Coordinator to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-06-01 20:05:04 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-05-18 23:50:37 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : VT   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Location Manager to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-10-22 06:47:08 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-11-19 12:14:09 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : AL   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Market Planning Manager to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-08-02 05:51:29 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-04-10 01:13:20 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : NJ   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Producer to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-05-24 05:47:50 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-01-18 21:19:13 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : CT   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Senior Analyst to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-10-13 16:28:53 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-08-15 15:24:23 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : TX   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Senior Manager to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-12-17 22:45:10 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-09-27 02:28:00 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : MA   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Consultant to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-06-28 01:47:28 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-08-19 09:10:51 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : NY   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Information Delivery to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-08-03 12:48:22 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-03-14 13:32:28 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : TN   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Location Scout to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-07-28 12:47:27 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-11-30 05:04:34 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : NJ   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Producer to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-06-03 08:21:30 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-11-23 17:35:34 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : ME   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Report and Visualization Developer to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-03-04 07:09:10 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-08-22 19:21:18 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : MA   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : First Officer to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-06-27 08:12:27 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-05-25 13:50:28 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : AL   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Leader to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-08-17 23:54:08 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-11-12 11:29:25 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : NC   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Manager to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-01-15 07:36:58 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-07-14 19:33:26 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : AL   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Research Associate to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-03-09 20:44:25 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-08-14 02:19:28 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : IL   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Manager to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-12-05 18:53:36 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-10-29 11:42:31 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : NJ   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Strategy Manager to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-09-01 22:22:11 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-02-27 03:50:28 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : AL   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Senior Manager to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-03-24 00:09:12 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-01-05 07:09:24 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : NJ   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Clinical Coordinator to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-09-21 09:49:49 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-07-03 00:49:37 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : ND   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Location Manager to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-11-06 21:14:29 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-12-15 05:38:19 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : WA   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Market Planning Manager to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-04-17 06:56:27 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-11-29 18:37:13 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : AL   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Producer to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-10-26 21:50:05 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-08-04 17:56:45 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : CA   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Senior Analyst to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-09-30 01:27:17 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-09-12 22:22:30 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : NJ   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Senior Manager to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-12-20 23:39:24 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-02-28 18:46:12 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : PA   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Consultant to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-05-04 09:46:21 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-03-27 06:35:03 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : MO   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Information Delivery to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-12-01 16:15:42 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-06-25 23:45:37 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : AL   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Location Scout to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-01-16 18:54:08 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-11-04 09:01:25 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : NJ   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Producer to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-04-06 06:05:47 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-05-16 08:09:05 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : IA   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Report and Visualization Developer to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-02-09 07:12:02 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-07-09 16:20:25 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : OR   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : First Officer to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-05-20 17:41:30 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-02-13 08:51:30 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : DC   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Leader to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-12-06 08:07:56 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-06-16 23:50:58 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : MO   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Manager to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-10-02 22:17:22 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-08-20 09:40:40 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : NY   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Research Associate to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-04-05 06:55:36 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-04-27 22:16:14 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : UT   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Manager to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-06-05 15:06:24 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-02-25 06:24:19 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : CA   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Strategy Manager to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-07-24 15:41:09 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-05-14 17:08:48 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : MI   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Manager to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-01-30 21:43:55 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-06-05 13:49:58 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : OH   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Clinical Coordinator to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-07-18 11:09:29 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-10-20 09:24:40 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : IL   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Location Manager to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-03-01 00:38:25 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-11-27 15:43:44 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : AL   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Market Planning Manager to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-06-23 14:39:59 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-06-02 21:33:17 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : PR   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Producer to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-06-15 06:26:51 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-06-25 11:03:48 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : PR   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Senior Analyst to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-12-20 21:37:56 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-03-30 15:51:07 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : PR   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Senior Manager to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-01-31 07:42:53 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-09-04 05:53:37 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : ME   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Consultant to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-04-17 14:18:09 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-11-13 03:29:44 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : MA   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Information Delivery to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-10-24 18:13:42 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-05-29 20:47:20 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : MA   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Location Scout to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-11-21 17:15:47 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-05-22 13:03:28 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : ME   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Producer to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-01-04 15:10:06 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-12-14 11:17:04 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : NY   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Report and Visualization Developer to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-03-14 19:32:03 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-10-22 22:49:07 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : NJ   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : First Officer to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-01-01 11:53:36 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-06-06 20:41:32 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : CT   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Leader to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-12-17 15:08:13 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-03-25 16:41:10 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : MA   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Manager to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-06-04 21:53:26 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-04-13 09:32:27 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : KY   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Research Associate to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-02-08 02:46:22 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-11-28 22:53:38 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : NC   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Manager to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-07-16 20:52:04 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-08-20 10:33:53 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : MN   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Strategy Manager to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-08-02 06:22:24 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-04-13 06:55:01 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : NJ   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Manager to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-07-10 15:58:49 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-01-21 09:27:48 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : IA   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Clinical Coordinator to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-08-27 04:12:50 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-07-04 00:29:12 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : OH   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Location Manager to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-08-30 21:17:43 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-03-30 12:30:37 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : WA   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Market Planning Manager to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-11-20 09:09:33 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-01-22 07:39:53 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : AL   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Producer to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-07-27 12:34:10 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-02-04 10:33:33 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : NJ   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Senior Analyst to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-12-30 08:36:58 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-09-30 09:57:40 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : AL   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Senior Manager to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-08-23 08:10:15 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-03-03 12:56:49 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : KS   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Consultant to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-12-15 10:44:58 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-06-14 23:57:49 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : OH   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Information Delivery to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-12-18 08:43:49 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-02-17 15:00:25 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : CO   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Location Scout to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-06-16 23:36:04 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-06-15 08:06:10 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : AL   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Producer to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-12-25 16:40:48 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-03-01 14:12:34 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : MA   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Report and Visualization Developer to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-11-25 06:49:49 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-10-28 04:06:43 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : NJ   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : First Officer to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-08-03 06:59:17 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-12-26 09:06:28 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : LA   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Leader to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-06-21 20:13:01 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-10-19 19:05:43 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : GA   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Manager to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-02-11 22:19:00 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-03-20 16:29:29 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : TX   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Research Associate to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-08-02 13:10:25 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-04-06 11:45:53 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : ND   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Manager to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-02-05 20:16:35 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-07-19 12:08:41 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : NY   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Strategy Manager to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-12-11 01:10:25 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-05-11 07:28:22 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : WA   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Consultant to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-01-17 20:52:40 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-02-19 03:30:27 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : CT   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Information Delivery to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-11-19 00:30:42 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-04-29 08:21:42 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : MA   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Location Scout to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-01-06 10:00:49 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-07-15 09:51:10 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : PR   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Producer to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-11-10 12:29:12 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-04-10 05:48:26 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : PR   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Report and Visualization Developer to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-03-17 09:33:53 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-05-06 07:20:25 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : CA   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : First Officer to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-04-30 05:57:41 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-11-20 08:21:36 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : CA   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Leader to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-07-12 23:44:34 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-07-03 13:16:10 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : PR   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Manager to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-06-28 17:33:34 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-12-04 10:41:08 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : CT   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Research Associate to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-05-05 01:40:47 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-09-19 04:24:07 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : MO   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Manager to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-05-11 02:09:07 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-08-06 13:00:06 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : TX   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Strategy Manager to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-08-07 00:30:09 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-10-29 04:30:35 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : NJ   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Manager to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-06-29 12:12:03 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-01-26 22:35:46 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : GA   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Director to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-01-19 19:03:56 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-11-12 06:52:15 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : NC   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Human Resources Partner to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-03-06 20:31:39 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-03-26 22:59:38 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : AL   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Production Assistant to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-09-17 11:43:42 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-03-11 14:29:08 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : PA   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Safety Scientist to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-09-02 23:54:59 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-04-01 11:18:42 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : PA   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Consultant to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-05-12 15:58:46 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-01-14 06:04:27 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : MI   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Information Delivery to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-03-16 19:09:28 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-06-25 20:50:32 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : FL   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Location Scout to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-05-21 15:24:48 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-07-12 09:18:57 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : AL   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Producer to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-01-02 19:18:49 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-09-17 20:49:19 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : WA   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Report and Visualization Developer to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-07-07 18:04:53 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-10-14 13:46:35 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : NJ   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : First Officer to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-01-18 03:24:07 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-09-19 04:50:33 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : IL   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Leader to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-01-07 17:17:06 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-12-30 17:52:30 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : PA   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Manager to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-04-13 17:24:01 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-08-30 17:43:29 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : KS   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Research Associate to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-12-17 05:10:27 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-10-14 03:50:05 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : PA   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Manager to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-10-24 19:10:58 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-08-06 18:26:15 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : IA   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Strategy Manager to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-04-29 15:08:15 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-12-22 06:02:48 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : ND   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Manager to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-10-24 12:34:33 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-01-17 23:32:53 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : MO   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Director to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-02-05 16:06:05 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-10-11 11:34:17 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : TN   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Human Resources Partner to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-04-18 16:50:24 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-10-08 16:32:19 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : LA   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Production Assistant to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-02-06 18:52:26 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-03-28 22:14:42 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : KS   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Safety Scientist to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-08-25 21:03:07 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-06-20 08:40:49 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : TX   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Group Manager to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-04-22 11:42:45 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-08-19 05:48:09 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : MA   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : First Officer to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-04-17 17:10:51 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-03-30 17:38:15 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : FL   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Leader to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-06-09 23:33:59 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-01-11 03:40:00 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : AL   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Manager to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-06-14 06:41:57 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-02-11 23:31:03 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : PR   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Research Associate to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-07-24 03:28:28 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-09-28 19:51:08 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : PR   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Manager to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-04-10 17:25:25 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-12-20 18:09:54 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : NY   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Strategy Manager to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-12-12 19:51:17 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-05-22 12:05:03 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : NC   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Manager to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-03-18 08:35:19 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-04-28 20:35:23 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : RI   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Director to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-08-09 20:49:34 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-10-30 12:30:35 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : AL   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Human Resources Partner to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-08-13 12:15:04 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-06-14 20:33:39 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : KY   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Production Assistant to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-02-13 04:47:09 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-10-07 03:12:08 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : AL   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Safety Scientist to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-08-13 11:11:43 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-07-09 23:07:04 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : PR   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Cabin Stewardess to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-03-03 01:13:22 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-07-19 12:24:47 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : NJ   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Group Manager to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-06-13 14:40:57 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-06-08 07:50:27 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : AL   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : First Officer to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-05-16 09:44:11 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-09-01 22:31:48 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : ND   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Leader to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-07-11 20:18:46 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-06-06 13:17:51 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : CA   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Manager to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-07-30 04:31:01 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-01-26 00:14:14 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : AL   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Research Associate to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-12-13 05:05:25 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-07-23 00:15:41 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : IL   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Manager to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-11-01 16:13:03 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-03-11 00:13:37 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : NJ   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Strategy Manager to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-10-26 21:29:21 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-06-28 12:49:42 to Numeric type value. Value considered as null.17/05/15 18:07:08 INFO storage.BlockManagerInfo: Removed broadcast_6_piece0 on 192.168.2.179:46491 in memory (size: 10.0 KB, free: 2.5 GB)17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : WI   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Manager to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-01-21 11:42:08 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-11-09 16:12:56 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : ND   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Director to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-03-26 14:48:14 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-10-28 03:11:34 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : FL   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Human Resources Partner to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-04-07 02:02:57 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-01-23 21:41:57 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : NJ   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Production Assistant to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-08-02 18:03:01 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-12-21 08:15:33 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : NY   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Safety Scientist to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-11-04 22:49:51 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-08-25 05:41:50 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : NY   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Cabin Stewardess to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-11-24 11:41:21 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-09-14 00:32:33 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : MA   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Manager to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-11-11 08:14:33 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-03-28 23:40:43 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : PR   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Strategy Manager to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-04-03 17:16:09 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-11-07 04:51:51 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : PR   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Manager to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-10-11 04:54:03 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-07-21 08:45:31 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : MD   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Director to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-06-07 17:33:50 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-06-01 12:08:07 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : FL   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Human Resources Partner to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-06-01 13:22:06 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-08-26 13:30:53 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : PA   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Production Assistant to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-07-24 12:02:28 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-07-17 14:09:46 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : KY   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Safety Scientist to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-04-08 06:31:58 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-11-28 11:54:43 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : WV   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Cabin Stewardess to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-10-15 15:43:28 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-08-25 18:52:38 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : PR   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Group Manager to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-09-14 21:34:44 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-12-13 13:49:48 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : VA   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Manager to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-09-21 14:49:37 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-10-24 10:09:36 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : FL   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Strategy Manager to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-07-10 11:15:34 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-05-03 07:19:20 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : PA   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Manager to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-08-23 11:07:28 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-11-03 17:34:15 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : IL   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Director to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-02-22 04:52:24 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-01-01 22:06:52 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : LA   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Human Resources Partner to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-02-20 04:18:29 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-04-27 17:11:26 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : AZ   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Production Assistant to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-02-10 18:24:07 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-05-31 09:24:27 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : PA   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Safety Scientist to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-11-08 12:06:27 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-07-30 11:37:47 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : TX   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Cabin Stewardess to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-05-26 08:59:49 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-12-14 23:28:52 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : VT   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Group Manager to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-09-01 12:00:24 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-08-16 05:46:06 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : NJ   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Scientist to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-06-29 17:31:31 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-07-27 20:28:15 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : NJ   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Solution Architect to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-10-07 03:43:13 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-06-04 20:28:43 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : GA   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Technology Manager to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-11-30 22:16:03 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-08-24 09:01:57 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : AL   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Director to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-11-06 06:36:22 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-04-03 22:38:15 to Numeric type value. Value considered as null.17/05/15 18:07:08 INFO storage.BlockManagerInfo: Removed broadcast_7_piece0 on 192.168.2.179:46491 in memory (size: 3.7 KB, free: 2.5 GB)17/05/15 18:07:08 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on 192.168.2.179:46491 in memory (size: 23.4 KB, free: 2.5 GB)17/05/15 18:07:08 INFO spark.ContextCleaner: Cleaned accumulator 21817/05/15 18:07:08 INFO spark.ContextCleaner: Cleaned accumulator 21917/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : MO   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Human Resources Partner to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-09-30 23:34:09 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-06-29 22:23:23 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : PR   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Production Assistant to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-06-23 13:10:20 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-05-19 04:35:04 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : PR   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Safety Scientist to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-02-13 06:14:09 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-09-09 11:35:02 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : CT   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Cabin Stewardess to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-04-25 23:53:28 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-12-12 16:34:42 to Numeric type value. Value considered as null.17/05/15 18:07:08 INFO spark.ContextCleaner: Cleaned shuffle 117/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : KY   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Group Manager to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-03-30 04:11:42 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-05-05 14:15:43 to Numeric type value. Value considered as null.17/05/15 18:07:08 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on 192.168.2.179:46491 in memory (size: 23.1 KB, free: 2.5 GB)17/05/15 18:07:08 INFO storage.BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.2.179:46491 in memory (size: 3.7 KB, free: 2.5 GB)17/05/15 18:07:08 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.2.179:46491 in memory (size: 10.1 KB, free: 2.5 GB)17/05/15 18:07:08 INFO spark.ContextCleaner: Cleaned shuffle 017/05/15 18:07:08 INFO spark.ContextCleaner: Cleaned accumulator 117/05/15 18:07:08 INFO spark.ContextCleaner: Cleaned accumulator 017/05/15 18:07:08 INFO storage.BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.2.179:46491 in memory (size: 23.4 KB, free: 2.5 GB)17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : AL   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Medical Science Liaison to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-02-03 17:24:13 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-09-17 02:49:09 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : PA   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Project Support to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-10-08 06:33:00 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-08-23 20:20:22 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : MO   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Associate Producer to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-08-20 08:05:50 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-09-06 22:48:44 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : AR   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Scientist to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-09-09 23:46:59 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-01-30 11:11:13 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : UT   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Solution Architect to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-07-06 11:23:20 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-03-05 19:38:49 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : PR   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Technology Manager to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-11-17 16:29:12 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-06-24 10:13:54 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : AL   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Director to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-08-10 15:46:36 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-11-09 15:30:44 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : PA   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Human Resources Partner to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-03-17 20:59:32 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-11-11 02:50:28 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : AL   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Production Assistant to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-09-29 08:41:03 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-12-29 12:24:40 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : AL   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Safety Scientist to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-12-21 19:01:22 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-08-30 05:51:01 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : AZ   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Cabin Stewardess to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-05-16 20:44:43 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-08-29 11:37:58 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : NJ   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Group Manager to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-01-23 10:32:48 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-03-24 07:43:01 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : MO   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Medical Science Liaison to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-10-09 09:15:13 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-01-09 00:21:23 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : PR   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Project Support to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-06-24 10:34:39 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-08-18 10:36:28 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : PA   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Associate Producer to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-09-06 21:57:59 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-09-19 09:51:35 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : CT   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Manager to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-04-26 06:44:59 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-12-03 05:48:17 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : CT   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Director to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-01-19 18:14:55 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-05-04 21:40:57 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : PR   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Human Resources Partner to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-06-17 15:20:46 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-12-16 12:40:01 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : KY   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Production Assistant to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-11-24 10:11:56 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-04-20 00:39:05 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : NJ   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Safety Scientist to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-02-05 12:44:21 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-11-03 07:58:50 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : OH   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Cabin Stewardess to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-08-19 19:19:44 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-02-22 09:16:10 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : NY   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Group Manager to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-06-09 04:03:19 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-09-30 09:38:51 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : NJ   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Medical Science Liaison to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-06-30 03:27:38 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-08-05 00:21:31 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : TN   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Project Support to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-12-23 02:17:26 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-07-09 10:16:05 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : NJ   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Manager to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-09-26 11:22:49 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-05-27 01:24:37 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : PR   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Director to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-05-12 19:36:03 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-12-09 08:04:19 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : ND   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Human Resources Partner to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-02-08 04:31:11 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-08-03 01:23:29 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : AL   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Production Assistant to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-03-09 14:22:50 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-07-23 14:22:53 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : AZ   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Safety Scientist to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-06-10 16:37:02 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-11-12 21:51:00 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : MO   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Cabin Stewardess to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-10-10 12:17:29 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-09-30 06:41:29 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : IN   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Group Manager to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-01-04 12:40:18 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-04-09 08:15:16 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : AL   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Medical Science Liaison to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-01-31 09:01:54 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-07-20 02:22:32 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : CA   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Project Support to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-12-15 05:32:02 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-08-22 23:25:59 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : IL   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Line Manager to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-11-07 18:31:02 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-02-28 13:41:08 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : WA   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Program Coordinator to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-05-14 15:30:38 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-06-11 14:45:29 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : NJ   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Technology Manager to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-11-21 15:08:07 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-10-03 01:28:20 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : TN   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Cabin Stewardess to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-12-16 04:59:06 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-12-03 00:57:26 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : KY   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Group Manager to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-12-16 06:30:07 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-07-19 15:01:48 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : CT   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Medical Science Liaison to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-06-11 21:52:40 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-09-09 14:15:26 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : CT   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Project Support to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-02-17 14:30:35 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-09-07 05:38:13 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : VA   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Associate Producer to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-04-25 09:56:11 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-10-22 08:00:40 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : RI   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Scientist to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-03-04 07:35:00 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-01-18 20:16:14 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : MI   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Solution Architect to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-04-12 00:59:17 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-05-15 00:45:13 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : NC   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Technology Manager to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-03-18 12:50:59 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-07-28 12:12:18 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : TX   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Clinical Specialist to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-08-10 14:50:43 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-07-17 19:01:27 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : SC   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Line Manager to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-09-30 01:47:01 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-07-24 07:00:00 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : AR   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Program Coordinator to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-08-31 15:38:00 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-01-10 12:36:48 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : PR   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Technology Manager to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-11-15 16:57:50 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-11-15 05:27:40 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : AL   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Cabin Stewardess to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-04-12 09:50:17 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-07-07 22:00:27 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : PA   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Group Manager to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-05-26 00:00:40 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-08-17 05:12:18 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : MI   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Medical Science Liaison to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-04-15 06:47:00 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-07-28 04:25:22 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : OH   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Project Support to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-05-06 13:55:35 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-08-17 11:26:47 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : IA   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Associate Producer to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-10-31 11:42:15 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-04-29 23:53:41 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : IL   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Scientist to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-10-07 11:24:37 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-03-25 21:15:22 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : AL   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Solution Architect to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-09-02 03:19:39 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-02-17 01:51:01 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : TX   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Technology Manager to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-02-12 22:55:18 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-06-20 09:49:05 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : PA   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Clinical Specialist to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-05-18 11:31:39 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-12-10 06:38:16 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : PA   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Medical Science Liaison to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-10-21 19:06:22 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-11-08 13:54:50 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : MA   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Project Support to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-11-28 22:12:15 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-06-05 21:45:33 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : PA   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Associate Producer to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-07-11 08:07:00 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-02-18 01:50:55 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : ME   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Scientist to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-04-22 10:50:36 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-09-28 03:23:54 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : PR   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Solution Architect to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-08-03 10:22:56 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-05-06 21:32:20 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : AL   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Technology Manager to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-11-09 03:52:48 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-01-27 02:27:34 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : NJ   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Clinical Specialist to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-05-01 06:01:09 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-11-19 22:28:47 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : AL   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Line Manager to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-12-28 02:16:57 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-06-15 01:39:03 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : AL   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Program Coordinator to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-06-19 19:09:26 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-09-03 14:22:07 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : SC   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Technology Manager to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-04-07 03:54:04 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-05-13 11:46:48 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : TX   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Clinical Specialist to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-01-10 02:09:55 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-04-02 22:53:05 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : CA   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Solutions Designer to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-02-10 00:04:12 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-03-06 12:46:18 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : AL   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Thought Leader Liaison to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-11-10 17:21:21 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-12-24 11:00:21 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : NJ   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Medical Science Liaison to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-11-01 05:43:52 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-04-25 00:25:05 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : AL   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Project Support to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-11-23 02:56:56 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-10-21 01:51:34 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : PR   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Associate Producer to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-10-08 10:34:02 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-10-20 17:30:44 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : FL   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Scientist to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-09-12 13:14:03 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-09-13 18:30:40 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : IL   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Solution Architect to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-09-25 09:41:06 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-08-09 21:37:59 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : ND   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Technology Manager to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-06-26 09:22:13 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-06-26 21:05:35 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : PA   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Clinical Specialist to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-07-22 14:54:41 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-03-11 02:16:51 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : TX   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Line Manager to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-08-12 09:49:21 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-04-19 17:38:37 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : CT   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Program Coordinator to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-11-15 00:23:08 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-01-29 20:10:45 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : KY   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Technology Manager to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-05-06 19:20:55 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-02-22 17:55:08 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : IL   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Clinical Specialist to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-05-18 08:46:15 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-09-17 21:18:10 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : NJ   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Solutions Designer to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-01-17 15:33:58 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-12-30 00:29:04 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : PR   to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : Thought Leader Liaison to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2015-03-04 23:12:01 to Numeric type value. Value considered as null.17/05/15 18:07:08 WARN impl.MeasureFieldConverterImpl: pool-33-thread-1 Cant not convert : 2016-08-11 17:17:52 to Numeric type value. Value considered as null.17/05/15 18:07:08 INFO sortdata.SortDataRows: &#91;Executor task launch worker-7&#93;&#91;partitionID:vinod_sorttable4_offheap_inmemory_2552c8a8-99df-4024-a1fd-fc0c86e06d67&#93; File based sorting will be used17/05/15 18:07:08 INFO newflow.AbstractDataLoadProcessorStep: &#91;Executor task launch worker-7&#93;&#91;partitionID:vinod_sorttable4_offheap_inmemory_2552c8a8-99df-4024-a1fd-fc0c86e06d67&#93; Total rows processed in step Data Writer: 017/05/15 18:07:08 INFO newflow.AbstractDataLoadProcessorStep: &#91;Executor task launch worker-7&#93;&#91;partitionID:vinod_sorttable4_offheap_inmemory_2552c8a8-99df-4024-a1fd-fc0c86e06d67&#93; Total rows processed in step Sort Processor: 1000017/05/15 18:07:08 INFO newflow.AbstractDataLoadProcessorStep: &#91;Executor task launch worker-7&#93;&#91;partitionID:vinod_sorttable4_offheap_inmemory_2552c8a8-99df-4024-a1fd-fc0c86e06d67&#93; Total rows processed in step Data Converter: 1000017/05/15 18:07:08 INFO newflow.AbstractDataLoadProcessorStep: &#91;Executor task launch worker-7&#93;&#91;partitionID:vinod_sorttable4_offheap_inmemory_2552c8a8-99df-4024-a1fd-fc0c86e06d67&#93; Total rows processed in step Input Processor: 1000017/05/15 18:07:08 INFO rdd.NewCarbonDataLoadRDD: DataLoad failureorg.apache.carbondata.processing.newflow.exception.CarbonDataLoadingException: Problem while shutdown the server  at org.apache.carbondata.processing.newflow.sort.impl.ParallelReadMergeSorterImpl.sort(ParallelReadMergeSorterImpl.java:114) at org.apache.carbondata.processing.newflow.steps.SortProcessorStepImpl.execute(SortProcessorStepImpl.java:61) at org.apache.carbondata.processing.newflow.steps.DataWriterProcessorStepImpl.execute(DataWriterProcessorStepImpl.java:82) at org.apache.carbondata.processing.newflow.DataLoadExecutor.execute(DataLoadExecutor.java:48) at org.apache.carbondata.spark.rdd.NewCarbonDataLoadRDD$$anon$1.<init>(NewCarbonDataLoadRDD.scala:241) at org.apache.carbondata.spark.rdd.NewCarbonDataLoadRDD.compute(NewCarbonDataLoadRDD.scala:219) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) at org.apache.spark.rdd.RDD.iterator(RDD.scala:287) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87) at org.apache.spark.scheduler.Task.run(Task.scala:99) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745)Caused by: java.lang.ClassCastException: java.lang.String cannot be cast to [B at org.apache.carbondata.processing.sortandgroupby.sortdata.NewRowComparator.compare(NewRowComparator.java:49) at org.apache.carbondata.processing.sortandgroupby.sortdata.NewRowComparator.compare(NewRowComparator.java:24) at java.util.TimSort.binarySort(TimSort.java:296) at java.util.TimSort.sort(TimSort.java:239) at java.util.Arrays.sort(Arrays.java:1438) at org.apache.carbondata.processing.sortandgroupby.sortdata.SortDataRows.startSorting(SortDataRows.java:197) at org.apache.carbondata.processing.newflow.sort.impl.ParallelReadMergeSorterImpl.processRowToNextStep(ParallelReadMergeSorterImpl.java:170) at org.apache.carbondata.processing.newflow.sort.impl.ParallelReadMergeSorterImpl.sort(ParallelReadMergeSorterImpl.java:111) ... 13 more17/05/15 18:07:08 ERROR rdd.NewCarbonDataLoadRDD: &#91;Executor task launch worker-7&#93;&#91;partitionID:vinod_sorttable4_offheap_inmemory_2552c8a8-99df-4024-a1fd-fc0c86e06d67&#93; org.apache.carbondata.processing.newflow.exception.CarbonDataLoadingException: Problem while shutdown the server  at org.apache.carbondata.processing.newflow.sort.impl.ParallelReadMergeSorterImpl.sort(ParallelReadMergeSorterImpl.java:114) at org.apache.carbondata.processing.newflow.steps.SortProcessorStepImpl.execute(SortProcessorStepImpl.java:61) at org.apache.carbondata.processing.newflow.steps.DataWriterProcessorStepImpl.execute(DataWriterProcessorStepImpl.java:82) at org.apache.carbondata.processing.newflow.DataLoadExecutor.execute(DataLoadExecutor.java:48) at org.apache.carbondata.spark.rdd.NewCarbonDataLoadRDD$$anon$1.<init>(NewCarbonDataLoadRDD.scala:241) at org.apache.carbondata.spark.rdd.NewCarbonDataLoadRDD.compute(NewCarbonDataLoadRDD.scala:219) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) at org.apache.spark.rdd.RDD.iterator(RDD.scala:287) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87) at org.apache.spark.scheduler.Task.run(Task.scala:99) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745)Caused by: java.lang.ClassCastException: java.lang.String cannot be cast to [B at org.apache.carbondata.processing.sortandgroupby.sortdata.NewRowComparator.compare(NewRowComparator.java:49) at org.apache.carbondata.processing.sortandgroupby.sortdata.NewRowComparator.compare(NewRowComparator.java:24) at java.util.TimSort.binarySort(TimSort.java:296) at java.util.TimSort.sort(TimSort.java:239) at java.util.Arrays.sort(Arrays.java:1438) at org.apache.carbondata.processing.sortandgroupby.sortdata.SortDataRows.startSorting(SortDataRows.java:197) at org.apache.carbondata.processing.newflow.sort.impl.ParallelReadMergeSorterImpl.processRowToNextStep(ParallelReadMergeSorterImpl.java:170) at org.apache.carbondata.processing.newflow.sort.impl.ParallelReadMergeSorterImpl.sort(ParallelReadMergeSorterImpl.java:111) ... 13 more17/05/15 18:07:08 ERROR executor.Executor: Exception in task 0.0 in stage 5.0 (TID 11)org.apache.carbondata.processing.newflow.exception.CarbonDataLoadingException: Problem while shutdown the server  at org.apache.carbondata.processing.newflow.sort.impl.ParallelReadMergeSorterImpl.sort(ParallelReadMergeSorterImpl.java:114) at org.apache.carbondata.processing.newflow.steps.SortProcessorStepImpl.execute(SortProcessorStepImpl.java:61) at org.apache.carbondata.processing.newflow.steps.DataWriterProcessorStepImpl.execute(DataWriterProcessorStepImpl.java:82) at org.apache.carbondata.processing.newflow.DataLoadExecutor.execute(DataLoadExecutor.java:48) at org.apache.carbondata.spark.rdd.NewCarbonDataLoadRDD$$anon$1.<init>(NewCarbonDataLoadRDD.scala:241) at org.apache.carbondata.spark.rdd.NewCarbonDataLoadRDD.compute(NewCarbonDataLoadRDD.scala:219) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) at org.apache.spark.rdd.RDD.iterator(RDD.scala:287) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87) at org.apache.spark.scheduler.Task.run(Task.scala:99) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745)Caused by: java.lang.ClassCastException: java.lang.String cannot be cast to [B at org.apache.carbondata.processing.sortandgroupby.sortdata.NewRowComparator.compare(NewRowComparator.java:49) at org.apache.carbondata.processing.sortandgroupby.sortdata.NewRowComparator.compare(NewRowComparator.java:24) at java.util.TimSort.binarySort(TimSort.java:296) at java.util.TimSort.sort(TimSort.java:239) at java.util.Arrays.sort(Arrays.java:1438) at org.apache.carbondata.processing.sortandgroupby.sortdata.SortDataRows.startSorting(SortDataRows.java:197) at org.apache.carbondata.processing.newflow.sort.impl.ParallelReadMergeSorterImpl.processRowToNextStep(ParallelReadMergeSorterImpl.java:170) at org.apache.carbondata.processing.newflow.sort.impl.ParallelReadMergeSorterImpl.sort(ParallelReadMergeSorterImpl.java:111) ... 13 more17/05/15 18:07:08 WARN scheduler.TaskSetManager: Lost task 0.0 in stage 5.0 (TID 11, localhost, executor driver): org.apache.carbondata.processing.newflow.exception.CarbonDataLoadingException: Problem while shutdown the server  at org.apache.carbondata.processing.newflow.sort.impl.ParallelReadMergeSorterImpl.sort(ParallelReadMergeSorterImpl.java:114) at org.apache.carbondata.processing.newflow.steps.SortProcessorStepImpl.execute(SortProcessorStepImpl.java:61) at org.apache.carbondata.processing.newflow.steps.DataWriterProcessorStepImpl.execute(DataWriterProcessorStepImpl.java:82) at org.apache.carbondata.processing.newflow.DataLoadExecutor.execute(DataLoadExecutor.java:48) at org.apache.carbondata.spark.rdd.NewCarbonDataLoadRDD$$anon$1.<init>(NewCarbonDataLoadRDD.scala:241) at org.apache.carbondata.spark.rdd.NewCarbonDataLoadRDD.compute(NewCarbonDataLoadRDD.scala:219) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) at org.apache.spark.rdd.RDD.iterator(RDD.scala:287) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87) at org.apache.spark.scheduler.Task.run(Task.scala:99) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745)Caused by: java.lang.ClassCastException: java.lang.String cannot be cast to [B at org.apache.carbondata.processing.sortandgroupby.sortdata.NewRowComparator.compare(NewRowComparator.java:49) at org.apache.carbondata.processing.sortandgroupby.sortdata.NewRowComparator.compare(NewRowComparator.java:24) at java.util.TimSort.binarySort(TimSort.java:296) at java.util.TimSort.sort(TimSort.java:239) at java.util.Arrays.sort(Arrays.java:1438) at org.apache.carbondata.processing.sortandgroupby.sortdata.SortDataRows.startSorting(SortDataRows.java:197) at org.apache.carbondata.processing.newflow.sort.impl.ParallelReadMergeSorterImpl.processRowToNextStep(ParallelReadMergeSorterImpl.java:170) at org.apache.carbondata.processing.newflow.sort.impl.ParallelReadMergeSorterImpl.sort(ParallelReadMergeSorterImpl.java:111) ... 13 more17/05/15 18:07:08 ERROR scheduler.TaskSetManager: Task 0 in stage 5.0 failed 1 times; aborting job17/05/15 18:07:08 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool 17/05/15 18:07:08 INFO scheduler.TaskSchedulerImpl: Cancelling stage 517/05/15 18:07:08 INFO scheduler.DAGScheduler: ResultStage 5 (collect at CarbonDataRDDFactory.scala:630) failed in 0.659 s due to Job aborted due to stage failure: Task 0 in stage 5.0 failed 1 times, most recent failure: Lost task 0.0 in stage 5.0 (TID 11, localhost, executor driver): org.apache.carbondata.processing.newflow.exception.CarbonDataLoadingException: Problem while shutdown the server  at org.apache.carbondata.processing.newflow.sort.impl.ParallelReadMergeSorterImpl.sort(ParallelReadMergeSorterImpl.java:114) at org.apache.carbondata.processing.newflow.steps.SortProcessorStepImpl.execute(SortProcessorStepImpl.java:61) at org.apache.carbondata.processing.newflow.steps.DataWriterProcessorStepImpl.execute(DataWriterProcessorStepImpl.java:82) at org.apache.carbondata.processing.newflow.DataLoadExecutor.execute(DataLoadExecutor.java:48) at org.apache.carbondata.spark.rdd.NewCarbonDataLoadRDD$$anon$1.<init>(NewCarbonDataLoadRDD.scala:241) at org.apache.carbondata.spark.rdd.NewCarbonDataLoadRDD.compute(NewCarbonDataLoadRDD.scala:219) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) at org.apache.spark.rdd.RDD.iterator(RDD.scala:287) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87) at org.apache.spark.scheduler.Task.run(Task.scala:99) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745)Caused by: java.lang.ClassCastException: java.lang.String cannot be cast to [B at org.apache.carbondata.processing.sortandgroupby.sortdata.NewRowComparator.compare(NewRowComparator.java:49) at org.apache.carbondata.processing.sortandgroupby.sortdata.NewRowComparator.compare(NewRowComparator.java:24) at java.util.TimSort.binarySort(TimSort.java:296) at java.util.TimSort.sort(TimSort.java:239) at java.util.Arrays.sort(Arrays.java:1438) at org.apache.carbondata.processing.sortandgroupby.sortdata.SortDataRows.startSorting(SortDataRows.java:197) at org.apache.carbondata.processing.newflow.sort.impl.ParallelReadMergeSorterImpl.processRowToNextStep(ParallelReadMergeSorterImpl.java:170) at org.apache.carbondata.processing.newflow.sort.impl.ParallelReadMergeSorterImpl.sort(ParallelReadMergeSorterImpl.java:111) ... 13 moreDriver stacktrace:17/05/15 18:07:08 INFO scheduler.DAGScheduler: Job 3 failed: collect at CarbonDataRDDFactory.scala:630, took 0.668890 s17/05/15 18:07:08 INFO rdd.CarbonDataRDDFactory$: pool-24-thread-10 DataLoad failure: Problem while shutdown the server 17/05/15 18:07:08 ERROR rdd.CarbonDataRDDFactory$: pool-24-thread-10 org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 5.0 failed 1 times, most recent failure: Lost task 0.0 in stage 5.0 (TID 11, localhost, executor driver): org.apache.carbondata.processing.newflow.exception.CarbonDataLoadingException: Problem while shutdown the server  at org.apache.carbondata.processing.newflow.sort.impl.ParallelReadMergeSorterImpl.sort(ParallelReadMergeSorterImpl.java:114) at org.apache.carbondata.processing.newflow.steps.SortProcessorStepImpl.execute(SortProcessorStepImpl.java:61) at org.apache.carbondata.processing.newflow.steps.DataWriterProcessorStepImpl.execute(DataWriterProcessorStepImpl.java:82) at org.apache.carbondata.processing.newflow.DataLoadExecutor.execute(DataLoadExecutor.java:48) at org.apache.carbondata.spark.rdd.NewCarbonDataLoadRDD$$anon$1.<init>(NewCarbonDataLoadRDD.scala:241) at org.apache.carbondata.spark.rdd.NewCarbonDataLoadRDD.compute(NewCarbonDataLoadRDD.scala:219) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) at org.apache.spark.rdd.RDD.iterator(RDD.scala:287) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87) at org.apache.spark.scheduler.Task.run(Task.scala:99) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745)Caused by: java.lang.ClassCastException: java.lang.String cannot be cast to [B at org.apache.carbondata.processing.sortandgroupby.sortdata.NewRowComparator.compare(NewRowComparator.java:49) at org.apache.carbondata.processing.sortandgroupby.sortdata.NewRowComparator.compare(NewRowComparator.java:24) at java.util.TimSort.binarySort(TimSort.java:296) at java.util.TimSort.sort(TimSort.java:239) at java.util.Arrays.sort(Arrays.java:1438) at org.apache.carbondata.processing.sortandgroupby.sortdata.SortDataRows.startSorting(SortDataRows.java:197) at org.apache.carbondata.processing.newflow.sort.impl.ParallelReadMergeSorterImpl.processRowToNextStep(ParallelReadMergeSorterImpl.java:170) at org.apache.carbondata.processing.newflow.sort.impl.ParallelReadMergeSorterImpl.sort(ParallelReadMergeSorterImpl.java:111) ... 13 moreDriver stacktrace: at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435) at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423) at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422) at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59) at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48) at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422) at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802) at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802) at scala.Option.foreach(Option.scala:257) at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802) at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650) at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605) at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594) at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48) at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628) at org.apache.spark.SparkContext.runJob(SparkContext.scala:1918) at org.apache.spark.SparkContext.runJob(SparkContext.scala:1931) at org.apache.spark.SparkContext.runJob(SparkContext.scala:1944) at org.apache.spark.SparkContext.runJob(SparkContext.scala:1958) at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:935) at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151) at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112) at org.apache.spark.rdd.RDD.withScope(RDD.scala:362) at org.apache.spark.rdd.RDD.collect(RDD.scala:934) at org.apache.carbondata.spark.rdd.CarbonDataRDDFactory$.loadDataFile$1(CarbonDataRDDFactory.scala:630) at org.apache.carbondata.spark.rdd.CarbonDataRDDFactory$.loadCarbonData(CarbonDataRDDFactory.scala:691) at org.apache.spark.sql.execution.command.LoadTable.run(carbonTableSchema.scala:560) at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58) at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56) at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74) at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114) at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114) at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:135) at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151) at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:132) at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:113) at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:87) at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:87) at org.apache.spark.sql.Dataset.<init>(Dataset.scala:185) at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:64) at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:592) at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:699) at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:220) at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1$$anon$2.run(SparkExecuteStatementOperation.scala:163) at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1$$anon$2.run(SparkExecuteStatementOperation.scala:160) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1698) at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1.run(SparkExecuteStatementOperation.scala:173) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745)Caused by: org.apache.carbondata.processing.newflow.exception.CarbonDataLoadingException: Problem while shutdown the server  at org.apache.carbondata.processing.newflow.sort.impl.ParallelReadMergeSorterImpl.sort(ParallelReadMergeSorterImpl.java:114) at org.apache.carbondata.processing.newflow.steps.SortProcessorStepImpl.execute(SortProcessorStepImpl.java:61) at org.apache.carbondata.processing.newflow.steps.DataWriterProcessorStepImpl.execute(DataWriterProcessorStepImpl.java:82) at org.apache.carbondata.processing.newflow.DataLoadExecutor.execute(DataLoadExecutor.java:48) at org.apache.carbondata.spark.rdd.NewCarbonDataLoadRDD$$anon$1.<init>(NewCarbonDataLoadRDD.scala:241) at org.apache.carbondata.spark.rdd.NewCarbonDataLoadRDD.compute(NewCarbonDataLoadRDD.scala:219) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) at org.apache.spark.rdd.RDD.iterator(RDD.scala:287) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87) at org.apache.spark.scheduler.Task.run(Task.scala:99) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282) ... 3 moreCaused by: java.lang.ClassCastException: java.lang.String cannot be cast to [B at org.apache.carbondata.processing.sortandgroupby.sortdata.NewRowComparator.compare(NewRowComparator.java:49) at org.apache.carbondata.processing.sortandgroupby.sortdata.NewRowComparator.compare(NewRowComparator.java:24) at java.util.TimSort.binarySort(TimSort.java:296) at java.util.TimSort.sort(TimSort.java:239) at java.util.Arrays.sort(Arrays.java:1438) at org.apache.carbondata.processing.sortandgroupby.sortdata.SortDataRows.startSorting(SortDataRows.java:197) at org.apache.carbondata.processing.newflow.sort.impl.ParallelReadMergeSorterImpl.processRowToNextStep(ParallelReadMergeSorterImpl.java:170) at org.apache.carbondata.processing.newflow.sort.impl.ParallelReadMergeSorterImpl.sort(ParallelReadMergeSorterImpl.java:111) ... 13 more17/05/15 18:07:08 INFO rdd.CarbonDataRDDFactory$: pool-24-thread-10 *******starting clean up*********17/05/15 18:07:08 INFO load.CarbonLoaderUtil: pool-34-thread-1 Deleted the local store location/tmp/26818412296318/0 : TIme taken: 017/05/15 18:07:08 INFO rdd.CarbonDataRDDFactory$: pool-24-thread-10 *******clean up done*********17/05/15 18:07:08 AUDIT rdd.CarbonDataRDDFactory$: &#91;vinod-Vostro-3559&#93;&#91;anonymous&#93;&#91;Thread-179&#93;Data load is failed for vinod.sorttable4_offheap_inmemory17/05/15 18:07:08 WARN rdd.CarbonDataRDDFactory$: pool-24-thread-10 Cannot write load metadata file as data load failed17/05/15 18:07:08 ERROR command.LoadTable: pool-24-thread-10 java.lang.Exception: DataLoad failure: Problem while shutdown the server  at org.apache.carbondata.spark.rdd.CarbonDataRDDFactory$.loadCarbonData(CarbonDataRDDFactory.scala:834) at org.apache.spark.sql.execution.command.LoadTable.run(carbonTableSchema.scala:560) at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58) at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56) at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74) at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114) at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114) at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:135) at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151) at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:132) at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:113) at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:87) at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:87) at org.apache.spark.sql.Dataset.<init>(Dataset.scala:185) at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:64) at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:592) at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:699) at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:220) at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1$$anon$2.run(SparkExecuteStatementOperation.scala:163) at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1$$anon$2.run(SparkExecuteStatementOperation.scala:160) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1698) at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1.run(SparkExecuteStatementOperation.scala:173) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745)17/05/15 18:07:08 AUDIT command.LoadTable: &#91;vinod-Vostro-3559&#93;&#91;anonymous&#93;&#91;Thread-179&#93;Dataload failure for vinod.sorttable4_offheap_inmemory. Please check the logs17/05/15 18:07:08 INFO locks.HdfsFileLock: pool-24-thread-10 Deleted the lock file hdfs://localhost:54310/opt/carbonStore/vinod/sorttable4_offheap_inmemory/meta.lock17/05/15 18:07:08 INFO command.LoadTable: pool-24-thread-10 Table MetaData Unlocked Successfully after data load17/05/15 18:07:08 ERROR thriftserver.SparkExecuteStatementOperation: Error executing query, currentState RUNNING, java.lang.Exception: DataLoad failure: Problem while shutdown the server  at org.apache.carbondata.spark.rdd.CarbonDataRDDFactory$.loadCarbonData(CarbonDataRDDFactory.scala:834) at org.apache.spark.sql.execution.command.LoadTable.run(carbonTableSchema.scala:560) at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58) at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56) at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74) at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114) at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114) at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:135) at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151) at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:132) at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:113) at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:87) at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:87) at org.apache.spark.sql.Dataset.<init>(Dataset.scala:185) at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:64) at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:592) at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:699) at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:220) at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1$$anon$2.run(SparkExecuteStatementOperation.scala:163) at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1$$anon$2.run(SparkExecuteStatementOperation.scala:160) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1698) at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1.run(SparkExecuteStatementOperation.scala:173) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745)17/05/15 18:07:08 ERROR thriftserver.SparkExecuteStatementOperation: Error running hive query: org.apache.hive.service.cli.HiveSQLException: java.lang.Exception: DataLoad failure: Problem while shutdown the server  at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:258) at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1$$anon$2.run(SparkExecuteStatementOperation.scala:163) at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1$$anon$2.run(SparkExecuteStatementOperation.scala:160) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1698) at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1.run(SparkExecuteStatementOperation.scala:173) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745)
issueID:CARBONDATA-1055
type:Bug
changed files:
texts:Record count mismatch for Carbon query compared with Parquet for TPCH query 15
User creates a table and loads TPCH data into different tables.User executes all the select queries and compares the record count and performance of the Carbon queries with parquet queries.Actual Issue : Record count mismatch for Carbon query compared with Parquet for TPCH query 15.Carbon record count for TPCH query 15 - 71972Parquet record count for TPCH query 15 - 72343Expected : There should not be record count mismatch for Carbon query compared with Parquet for TPCH query 15.
issueID:CARBONDATA-1056
type:Bug
changed files:
texts:Data_load failure using single_pass true with spark 2.1
Data_load failure using single_pass true with spark 2.1Steps to reproduce:1)Create Table:CREATE TABLE uniq_exclude_sp1 (CUST_ID int,CUST_NAME String,ACTIVE_EMUI_VERSION string, DOB timestamp, DOJ timestamp, BIGINT_COLUMN1 bigint,BIGINT_COLUMN2 bigint,DECIMAL_COLUMN1 decimal(30,10), DECIMAL_COLUMN2 decimal(36,10),Double_COLUMN1 double, Double_COLUMN2 double,INTEGER_COLUMN1 int) STORED BY 'org.apache.carbondata.format' TBLPROPERTIES('DICTIONARY_EXCLUDE'='CUST_NAME,ACTIVE_EMUI_VERSION');2) Load Data:LOAD DATA INPATH 'hdfs://localhost:54310/2000_UniqData.csv' into table uniq_exclude_sp1 OPTIONS('DELIMITER'=',' , 'QUOTECHAR'='"','BAD_RECORDS_ACTION'='FORCE','FILEHEADER'='CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1,Double_COLUMN2,INTEGER_COLUMN1','SINGLE_Pass'='true');3)Result:Actual result on beeline:Error: java.lang.Exception: Dataload failed due to error while writing dictionary file! (state=,code=0)Expected Result: data should be load successfully 4)Thriftserver logs:17/05/16 16:07:20 INFO SparkExecuteStatementOperation: Running query 'LOAD DATA INPATH 'hdfs://localhost:54310/2000_UniqData.csv' into table uniq_exclude_sp1 OPTIONS('DELIMITER'=',' , 'QUOTECHAR'='"','BAD_RECORDS_ACTION'='FORCE','FILEHEADER'='CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1,Double_COLUMN2,INTEGER_COLUMN1','SINGLE_Pass'='true')' with 34eb7e9e-bd49-495c-af68-8f0b5e36b78617/05/16 16:07:20 INFO CarbonSparkSqlParser: Parsing command: LOAD DATA INPATH 'hdfs://localhost:54310/2000_UniqData.csv' into table uniq_exclude_sp1 OPTIONS('DELIMITER'=',' , 'QUOTECHAR'='"','BAD_RECORDS_ACTION'='FORCE','FILEHEADER'='CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1,Double_COLUMN2,INTEGER_COLUMN1','SINGLE_Pass'='true')17/05/16 16:07:20 INFO CarbonLateDecodeRule: pool-23-thread-4 Skip CarbonOptimizer17/05/16 16:07:20 INFO HdfsFileLock: pool-23-thread-4 HDFS lock path:hdfs://localhost:54310/opt/prestocarbonStore/default/uniq_exclude_sp1/meta.lock17/05/16 16:07:20 INFO LoadTable: pool-23-thread-4 Successfully able to get the table metadata file lock17/05/16 16:07:20 INFO LoadTable: pool-23-thread-4 Initiating Direct Load for the Table : (default.uniq_exclude_sp1)17/05/16 16:07:20 AUDIT CarbonDataRDDFactory$: &#91;knoldus&#93;&#91;hduser&#93;&#91;Thread-137&#93;Data load request has been received for table default.uniq_exclude_sp117/05/16 16:07:20 INFO CommonUtil$: pool-23-thread-4 &#91;Block Distribution&#93;17/05/16 16:07:20 INFO CommonUtil$: pool-23-thread-4 totalInputSpaceConsumed: 376223 , defaultParallelism: 417/05/16 16:07:20 INFO CommonUtil$: pool-23-thread-4 mapreduce.input.fileinputformat.split.maxsize: 1677721617/05/16 16:07:20 INFO FileInputFormat: Total input paths to process : 117/05/16 16:07:20 INFO DistributionUtil$: pool-23-thread-4 Executors configured : 117/05/16 16:07:20 INFO DistributionUtil$: pool-23-thread-4 Total Time taken to ensure the required executors : 017/05/16 16:07:20 INFO DistributionUtil$: pool-23-thread-4 Time elapsed to allocate the required executors: 017/05/16 16:07:20 INFO CarbonDataRDDFactory$: pool-23-thread-4 Total Time taken in block allocation: 117/05/16 16:07:20 INFO CarbonDataRDDFactory$: pool-23-thread-4 Total no of blocks: 1, No.of Nodes: 117/05/16 16:07:20 INFO CarbonDataRDDFactory$: pool-23-thread-4 #Node: knoldus no.of.blocks: 117/05/16 16:07:20 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 53.7 MB, free 291.4 MB)17/05/16 16:07:20 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 23.2 KB, free 291.4 MB)17/05/16 16:07:20 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.1.10:42046 (size: 23.2 KB, free: 366.2 MB)17/05/16 16:07:20 INFO SparkContext: Created broadcast 2 from broadcast at NewCarbonDataLoadRDD.scala:18517/05/16 16:07:20 INFO SparkContext: Starting job: collect at CarbonDataRDDFactory.scala:63017/05/16 16:07:20 INFO DAGScheduler: Got job 1 (collect at CarbonDataRDDFactory.scala:630) with 1 output partitions17/05/16 16:07:20 INFO DAGScheduler: Final stage: ResultStage 1 (collect at CarbonDataRDDFactory.scala:630)17/05/16 16:07:20 INFO DAGScheduler: Parents of final stage: List()17/05/16 16:07:20 INFO DAGScheduler: Missing parents: List()17/05/16 16:07:20 INFO DAGScheduler: Submitting ResultStage 1 (NewCarbonDataLoadRDD&#91;4&#93; at RDD at NewCarbonDataLoadRDD.scala:174), which has no missing parents17/05/16 16:07:20 INFO NewCarbonDataLoadRDD: Preferred Location for split : knoldus17/05/16 16:07:20 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 11.8 KB, free 291.4 MB)17/05/16 16:07:20 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 6.0 KB, free 291.4 MB)17/05/16 16:07:20 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.1.10:42046 (size: 6.0 KB, free: 366.2 MB)17/05/16 16:07:20 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:99617/05/16 16:07:20 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (NewCarbonDataLoadRDD&#91;4&#93; at RDD at NewCarbonDataLoadRDD.scala:174)17/05/16 16:07:20 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks17/05/16 16:07:20 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, ANY, 6850 bytes)17/05/16 16:07:20 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)17/05/16 16:07:20 INFO NewCarbonDataLoadRDD: Input split: knoldus17/05/16 16:07:20 INFO NewCarbonDataLoadRDD: The Block Count in this node :117/05/16 16:07:20 INFO AbstractDataLoadProcessorStep: Thread-61 Rows processed in step Input Processor : 017/05/16 16:07:20 INFO AbstractDataLoadProcessorStep: Thread-62 Rows processed in step Data Converter : 017/05/16 16:07:20 INFO AbstractDataLoadProcessorStep: Thread-63 Rows processed in step Sort Processor : 017/05/16 16:07:20 INFO AbstractDataLoadProcessorStep: Thread-64 Rows processed in step Data Writer : 017/05/16 16:07:20 AUDIT DictionaryClient: &#91;knoldus&#93;&#91;hduser&#93;&#91;Thread-149&#93;Starting client on 192.168.1.10 203017/05/16 16:07:20 INFO DictionaryClient: Dictionary client Dictionary client Started, Total time spent : 117/05/16 16:07:20 AUDIT DictionaryClientHandler: &#91;knoldus&#93;&#91;hduser&#93;&#91;Thread-150&#93;Connected client io.netty.channel.DefaultChannelHandlerContext@1d9d500d17/05/16 16:07:20 AUDIT DictionaryServerHandler: &#91;knoldus&#93;&#91;hduser&#93;&#91;Thread-105&#93;Connected io.netty.channel.DefaultChannelHandlerContext@1b9e107317/05/16 16:07:21 INFO SortParameters: &#91;Executor task launch worker-1&#93;&#91;partitionID:default_uniq_exclude_sp1_b1d312f0-b60b-4332-9272-7f05d5758c69&#93; Sort size for table: 50000017/05/16 16:07:21 INFO SortParameters: &#91;Executor task launch worker-1&#93;&#91;partitionID:default_uniq_exclude_sp1_b1d312f0-b60b-4332-9272-7f05d5758c69&#93; Number of intermediate file to be merged: 2017/05/16 16:07:21 INFO SortParameters: &#91;Executor task launch worker-1&#93;&#91;partitionID:default_uniq_exclude_sp1_b1d312f0-b60b-4332-9272-7f05d5758c69&#93; File Buffer Size: 104857617/05/16 16:07:21 INFO SortParameters: &#91;Executor task launch worker-1&#93;&#91;partitionID:default_uniq_exclude_sp1_b1d312f0-b60b-4332-9272-7f05d5758c69&#93; temp file location/tmp/42046473395440/0/default/uniq_exclude_sp1/Fact/Part0/Segment_0/0/sortrowtmp17/05/16 16:07:21 INFO DataLoadExecutor: &#91;Executor task launch worker-1&#93;&#91;partitionID:default_uniq_exclude_sp1_b1d312f0-b60b-4332-9272-7f05d5758c69&#93; Data Loading is started for table uniq_exclude_sp117/05/16 16:07:21 AUDIT DictionaryClient: &#91;knoldus&#93;&#91;hduser&#93;&#91;Thread-149&#93;Starting client on 192.168.1.10 203017/05/16 16:07:21 INFO DictionaryClient: Dictionary client Dictionary client Started, Total time spent : 017/05/16 16:07:21 AUDIT DictionaryClientHandler: &#91;knoldus&#93;&#91;hduser&#93;&#91;Thread-152&#93;Connected client io.netty.channel.DefaultChannelHandlerContext@4b31e77317/05/16 16:07:21 AUDIT DictionaryServerHandler: &#91;knoldus&#93;&#91;hduser&#93;&#91;Thread-105&#93;Connected io.netty.channel.DefaultChannelHandlerContext@7b13e6e917/05/16 16:07:22 INFO SortDataRows: &#91;Executor task launch worker-1&#93;&#91;partitionID:default_uniq_exclude_sp1_b1d312f0-b60b-4332-9272-7f05d5758c69&#93; File based sorting will be used17/05/16 16:07:22 INFO ParallelReadMergeSorterImpl: &#91;Executor task launch worker-1&#93;&#91;partitionID:default_uniq_exclude_sp1_b1d312f0-b60b-4332-9272-7f05d5758c69&#93; Record Processed For table: uniq_exclude_sp117/05/16 16:07:22 INFO SingleThreadFinalSortFilesMerger: &#91;Executor task launch worker-1&#93;&#91;partitionID:default_uniq_exclude_sp1_b1d312f0-b60b-4332-9272-7f05d5758c69&#93; Number of temp file: 117/05/16 16:07:22 INFO SingleThreadFinalSortFilesMerger: &#91;Executor task launch worker-1&#93;&#91;partitionID:default_uniq_exclude_sp1_b1d312f0-b60b-4332-9272-7f05d5758c69&#93; File Buffer Size: 2097152017/05/16 16:07:22 INFO SingleThreadFinalSortFilesMerger: &#91;Executor task launch worker-1&#93;&#91;partitionID:default_uniq_exclude_sp1_b1d312f0-b60b-4332-9272-7f05d5758c69&#93; Started adding first record from each file17/05/16 16:07:22 INFO SingleThreadFinalSortFilesMerger: &#91;Executor task launch worker-1&#93;&#91;partitionID:default_uniq_exclude_sp1_b1d312f0-b60b-4332-9272-7f05d5758c69&#93; Heap Size117/05/16 16:07:22 INFO CarbonFactDataHandlerColumnar: &#91;Executor task launch worker-1&#93;&#91;partitionID:default_uniq_exclude_sp1_b1d312f0-b60b-4332-9272-7f05d5758c69&#93; Initializing writer executors17/05/16 16:07:22 INFO CarbonFactDataHandlerColumnar: &#91;Executor task launch worker-1&#93;&#91;partitionID:default_uniq_exclude_sp1_b1d312f0-b60b-4332-9272-7f05d5758c69&#93; Number of rows per column blocklet 3200017/05/16 16:07:22 INFO AbstractFactDataWriter: &#91;Executor task launch worker-1&#93;&#91;partitionID:default_uniq_exclude_sp1_b1d312f0-b60b-4332-9272-7f05d5758c69&#93; Total file size: 1073741824 and dataBlock Size: 96636764217/05/16 16:07:22 INFO CarbonFactDataHandlerColumnar: pool-43-thread-1 Number Of records processed: 201317/05/16 16:07:22 INFO CarbonFactDataWriterImplV3: pool-44-thread-1 Number of Pages for blocklet is: 1 :Rows Added: 201317/05/16 16:07:22 INFO DataWriterProcessorStepImpl: &#91;Executor task launch worker-1&#93;&#91;partitionID:default_uniq_exclude_sp1_b1d312f0-b60b-4332-9272-7f05d5758c69&#93; Record Processed For table: uniq_exclude_sp117/05/16 16:07:22 INFO DataWriterProcessorStepImpl: &#91;Executor task launch worker-1&#93;&#91;partitionID:default_uniq_exclude_sp1_b1d312f0-b60b-4332-9272-7f05d5758c69&#93; Finished Carbon DataWriterProcessorStepImpl: Read: 2013: Write: 201317/05/16 16:07:22 INFO CarbonFactDataHandlerColumnar: &#91;Executor task launch worker-1&#93;&#91;partitionID:default_uniq_exclude_sp1_b1d312f0-b60b-4332-9272-7f05d5758c69&#93; All blocklets have been finished writing17/05/16 16:07:22 INFO AbstractFactDataWriter: &#91;Executor task launch worker-1&#93;&#91;partitionID:default_uniq_exclude_sp1_b1d312f0-b60b-4332-9272-7f05d5758c69&#93; Copying /tmp/42046473395440/0/default/uniq_exclude_sp1/Fact/Part0/Segment_0/0/part-0-0_batchno0-0-1494931040374.carbondata --> hdfs://localhost:54310/opt/prestocarbonStore/default/uniq_exclude_sp1/Fact/Part0/Segment_017/05/16 16:07:22 INFO AbstractFactDataWriter: &#91;Executor task launch worker-1&#93;&#91;partitionID:default_uniq_exclude_sp1_b1d312f0-b60b-4332-9272-7f05d5758c69&#93; The configured block size is 1024 MB, the actual carbon file size is 89 KB, choose the max value 1024 MB as the block size on HDFS17/05/16 16:07:23 INFO AbstractFactDataWriter: &#91;Executor task launch worker-1&#93;&#91;partitionID:default_uniq_exclude_sp1_b1d312f0-b60b-4332-9272-7f05d5758c69&#93; Total copy time (ms) to copy file /tmp/42046473395440/0/default/uniq_exclude_sp1/Fact/Part0/Segment_0/0/part-0-0_batchno0-0-1494931040374.carbondata is 45817/05/16 16:07:23 INFO AbstractFactDataWriter: &#91;Executor task launch worker-1&#93;&#91;partitionID:default_uniq_exclude_sp1_b1d312f0-b60b-4332-9272-7f05d5758c69&#93; Copying /tmp/42046473395440/0/default/uniq_exclude_sp1/Fact/Part0/Segment_0/0/0_batchno0-0-1494931040374.carbonindex --> hdfs://localhost:54310/opt/prestocarbonStore/default/uniq_exclude_sp1/Fact/Part0/Segment_017/05/16 16:07:23 INFO AbstractFactDataWriter: &#91;Executor task launch worker-1&#93;&#91;partitionID:default_uniq_exclude_sp1_b1d312f0-b60b-4332-9272-7f05d5758c69&#93; The configured block size is 1024 MB, the actual carbon file size is 2 KB, choose the max value 1024 MB as the block size on HDFS17/05/16 16:07:23 INFO AbstractFactDataWriter: &#91;Executor task launch worker-1&#93;&#91;partitionID:default_uniq_exclude_sp1_b1d312f0-b60b-4332-9272-7f05d5758c69&#93; Total copy time (ms) to copy file /tmp/42046473395440/0/default/uniq_exclude_sp1/Fact/Part0/Segment_0/0/0_batchno0-0-1494931040374.carbonindex is 3217/05/16 16:07:23 INFO AbstractDataLoadProcessorStep: &#91;Executor task launch worker-1&#93;&#91;partitionID:default_uniq_exclude_sp1_b1d312f0-b60b-4332-9272-7f05d5758c69&#93; Total rows processed in step Data Writer: 201317/05/16 16:07:23 INFO AbstractDataLoadProcessorStep: &#91;Executor task launch worker-1&#93;&#91;partitionID:default_uniq_exclude_sp1_b1d312f0-b60b-4332-9272-7f05d5758c69&#93; Total rows processed in step Sort Processor: 201317/05/16 16:07:23 INFO AbstractDataLoadProcessorStep: &#91;Executor task launch worker-1&#93;&#91;partitionID:default_uniq_exclude_sp1_b1d312f0-b60b-4332-9272-7f05d5758c69&#93; Total rows processed in step Data Converter: 201317/05/16 16:07:23 INFO AbstractDataLoadProcessorStep: &#91;Executor task launch worker-1&#93;&#91;partitionID:default_uniq_exclude_sp1_b1d312f0-b60b-4332-9272-7f05d5758c69&#93; Total rows processed in step Input Processor: 201317/05/16 16:07:27 INFO DataLoadExecutor: &#91;Executor task launch worker-1&#93;&#91;partitionID:default_uniq_exclude_sp1_b1d312f0-b60b-4332-9272-7f05d5758c69&#93; Data loading is successful for table uniq_exclude_sp117/05/16 16:07:27 INFO CarbonLoaderUtil: pool-47-thread-1 Deleted the local store location/tmp/42046473395440/0 : TIme taken: 117/05/16 16:07:27 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1543 bytes result sent to driver17/05/16 16:07:27 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 6921 ms on localhost (executor driver) (1/1)17/05/16 16:07:27 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 17/05/16 16:07:27 INFO DAGScheduler: ResultStage 1 (collect at CarbonDataRDDFactory.scala:630) finished in 6.920 s17/05/16 16:07:27 INFO DAGScheduler: Job 1 finished: collect at CarbonDataRDDFactory.scala:630, took 6.932695 s17/05/16 16:07:27 INFO CarbonDataRDDFactory$: pool-23-thread-4 *******starting clean up*********17/05/16 16:07:27 ERROR CarbonDataRDDFactory$: pool-23-thread-4 Error while writing dictionary file for default_uniq_exclude_sp117/05/16 16:07:27 ERROR LoadTable: pool-23-thread-4 java.lang.Exception: Dataload failed due to error while writing dictionary file! at org.apache.carbondata.spark.rdd.CarbonDataRDDFactory$.writeDictionary(CarbonDataRDDFactory.scala:981) at org.apache.carbondata.spark.rdd.CarbonDataRDDFactory$.loadCarbonData(CarbonDataRDDFactory.scala:848) at org.apache.spark.sql.execution.command.LoadTable.run(carbonTableSchema.scala:511) at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58) at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56) at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74) at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114) at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114) at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:135) at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151) at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:132) at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:113) at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:87) at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:87) at org.apache.spark.sql.Dataset.<init>(Dataset.scala:185) at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:64) at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:592) at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:699) at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:220) at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1$$anon$2.run(SparkExecuteStatementOperation.scala:163) at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1$$anon$2.run(SparkExecuteStatementOperation.scala:160) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1698) at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1.run(SparkExecuteStatementOperation.scala:173) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745)17/05/16 16:07:27 AUDIT LoadTable: &#91;knoldus&#93;&#91;hduser&#93;&#91;Thread-137&#93;Dataload failure for default.uniq_exclude_sp1. Please check the logs17/05/16 16:07:27 INFO HdfsFileLock: pool-23-thread-4 Deleted the lock file hdfs://localhost:54310/opt/prestocarbonStore/default/uniq_exclude_sp1/meta.lock17/05/16 16:07:27 INFO LoadTable: pool-23-thread-4 Table MetaData Unlocked Successfully after data load17/05/16 16:07:27 ERROR SparkExecuteStatementOperation: Error executing query, currentState RUNNING, java.lang.Exception: Dataload failed due to error while writing dictionary file! at org.apache.carbondata.spark.rdd.CarbonDataRDDFactory$.writeDictionary(CarbonDataRDDFactory.scala:981) at org.apache.carbondata.spark.rdd.CarbonDataRDDFactory$.loadCarbonData(CarbonDataRDDFactory.scala:848) at org.apache.spark.sql.execution.command.LoadTable.run(carbonTableSchema.scala:511) at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58) at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56) at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74) at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114) at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114) at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:135) at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151) at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:132) at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:113) at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:87) at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:87) at org.apache.spark.sql.Dataset.<init>(Dataset.scala:185) at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:64) at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:592) at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:699) at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:220) at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1$$anon$2.run(SparkExecuteStatementOperation.scala:163) at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1$$anon$2.run(SparkExecuteStatementOperation.scala:160) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1698) at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1.run(SparkExecuteStatementOperation.scala:173) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745)17/05/16 16:07:27 ERROR SparkExecuteStatementOperation: Error running hive query: org.apache.hive.service.cli.HiveSQLException: java.lang.Exception: Dataload failed due to error while writing dictionary file! at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:258) at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1$$anon$2.run(SparkExecuteStatementOperation.scala:163) at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1$$anon$2.run(SparkExecuteStatementOperation.scala:160) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1698) at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1.run(SparkExecuteStatementOperation.scala:173) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745)
issueID:CARBONDATA-1057
type:Bug
changed files:
texts:Incorrect result displays while using (>=) and (<=) operators in range filter
Incorrect result displays while using (>=) and (<=) operators in range filterSteps to reproduce:1)Create Table:CREATE TABLE uniqdata (CUST_ID int,CUST_NAME String,ACTIVE_EMUI_VERSION string, DOB timestamp, DOJ timestamp, BIGINT_COLUMN1 bigint,BIGINT_COLUMN2 bigint,DECIMAL_COLUMN1 decimal(30,10), DECIMAL_COLUMN2 decimal(36,10),Double_COLUMN1 double, Double_COLUMN2 double,INTEGER_COLUMN1 int) STORED BY 'org.apache.carbondata.format' TBLPROPERTIES ("TABLE_BLOCKSIZE"= "256 MB")2)Load Data:LOAD DATA INPATH 'hdfs://localhost:54310/2000_UniqData.csv' into table uniqdata OPTIONS('DELIMITER'=',' , 'QUOTECHAR'='"','BAD_RECORDS_ACTION'='FORCE','FILEHEADER'='CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1,Double_COLUMN2,INTEGER_COLUMN1')3)Execute Query:select dob from uniqdata where dob>='1972-12-23' and dob <= '1972-12-31'4) Actual Result:In Carbondata:------+ dob  ------+------+No rows selected (0.259 seconds)In Hive:------------------------+          dob           ------------------------+ 1972-12-23 01:00:03.0   1972-12-24 01:00:03.0   1972-12-25 01:00:03.0   1972-12-26 01:00:03.0   1972-12-27 01:00:03.0   1972-12-28 01:00:03.0   1972-12-29 01:00:03.0   1972-12-30 01:00:03.0  ------------------------+8 rows selected (0.131 seconds)5) Expected result: It should display all dates between the given range
issueID:CARBONDATA-1058
type:Bug
changed files:
texts:When executing select query On boundryvalue an error is displaying "Decimal scale (0) cannot be greater than precision (-28)."
Steps To Reproduce:CARBONDATA :CREATE :0: jdbc:hive2://192.168.2.126:10000> create table Test_Boundary (c1_int int,c2_Bigint Bigint,c3_Decimal Decimal(38,30),c4_double double,c5_string string,c6_Timestamp Timestamp,c7_Datatype_Desc string) STORED BY 'org.apache.carbondata.format'0: jdbc:hive2://192.168.2.126:10000> ;---------+ Result  ---------+---------+No rows selected (0.28 seconds)LOAD :0: jdbc:hive2://192.168.2.126:10000> LOAD DATA INPATH 'hdfs://localhost:54311/BabuStore/TestData/Data/Test_Data1.csv' INTO table Test_Boundary OPTIONS('DELIMITER'=',','QUOTECHAR'='','BAD_RECORDS_ACTION'='FORCE','FILEHEADER'='');---------+ Result  ---------+---------+No rows selected (1.912 seconds)SELECT :0: jdbc:hive2://192.168.2.126:10000> select c3_Decimal from Test_Boundary where floor(c3_Decimal)=0.00 or floor(c3_Decimal) IS NULL0: jdbc:hive2://192.168.2.126:10000> ;Error: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 16.0 failed 1 times, most recent failure: Lost task 0.0 in stage 16.0 (TID 17, localhost): org.apache.spark.sql.AnalysisException: Decimal scale (0) cannot be greater than precision (-28).; at org.apache.spark.sql.types.PrecisionInfo.<init>(DecimalType.scala:32) at org.apache.spark.sql.types.DecimalType.<init>(DecimalType.scala:68) at org.apache.spark.sql.types.DecimalType$.bounded(DecimalType.scala:155) at org.apache.spark.sql.types.Decimal.floor(Decimal.scala:326) at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificPredicate.eval(Unknown Source) at org.apache.spark.sql.catalyst.expressions.codegen.GeneratePredicate$$anonfun$create$2.apply(GeneratePredicate.scala:68) at org.apache.spark.sql.catalyst.expressions.codegen.GeneratePredicate$$anonfun$create$2.apply(GeneratePredicate.scala:68) at org.apache.spark.sql.execution.Filter$$anonfun$2$$anonfun$apply$2.apply(basicOperators.scala:74) at org.apache.spark.sql.execution.Filter$$anonfun$2$$anonfun$apply$2.apply(basicOperators.scala:72) at scala.collection.Iterator$$anon$14.hasNext(Iterator.scala:390) at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327) at scala.collection.Iterator$class.foreach(Iterator.scala:727) at scala.collection.AbstractIterator.foreach(Iterator.scala:1157) at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48) at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103) at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47) at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273) at scala.collection.AbstractIterator.to(Iterator.scala:1157) at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265) at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157) at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252) at scala.collection.AbstractIterator.toArray(Iterator.scala:1157) at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:927) at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:927) at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1858) at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1858) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66) at org.apache.spark.scheduler.Task.run(Task.scala:89) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745)Driver stacktrace: (state=,code=0)HIVE :CREATE:0: jdbc:hive2://192.168.2.126:10000> create table Test_Boundary_h1 (c1_int int,c2_Bigint Bigint,c3_Decimal Decimal(38,30),c4_double double,c5_string string,c6_Timestamp Timestamp,c7_Datatype_Desc string) ROW FORMAT DELIMITED FIELDS TERMINATED BY ','0: jdbc:hive2://192.168.2.126:10000> ;---------+ result  ---------+---------+No rows selected (0.524 seconds)LOAD :0: jdbc:hive2://192.168.2.126:10000> load data local inpath '/opt/Carbon/CarbonData/TestData/Data/Test_Data1_h1.csv' OVERWRITE INTO TABLE Test_Boundary_h10: jdbc:hive2://192.168.2.126:10000> ;---------+ Result  ---------+---------+No rows selected (0.646 seconds)SELECT :0: jdbc:hive2://192.168.2.126:10000> select c3_Decimal from Test_Boundary_h1 where floor(c3_Decimal)=0.0 or floor(c3_Decimal) IS NULL0: jdbc:hive2://192.168.2.126:10000> ;-----------------------------------+            c3_Decimal             -----------------------------------+ 0.123456789009876543211234567890   0.054000000000000000000000000000   0.123456789009876543211234567890   0.000100000000000000000000000000   0.123456789009876543211234567890   NULL                               0.800000000000000000000000000000   0.900000000000000000000000000000   0E-30                              0E-30                              0.123456789009876543211234567890   NULL                               NULL                               NULL                               NULL                               NULL                              -----------------------------------+16 rows selected (0.266 seconds)
issueID:CARBONDATA-1059
type:Bug
changed files:
texts:While executing Select Query getting an exception : "org.apache.hadoop.hive.ql.metadata.HiveException"
CARBONDATA:CREATE:0: jdbc:hive2://192.168.2.126:10000> create table Test_Boundary (c1_int int,c2_Bigint Bigint,c3_Decimal Decimal(38,30),c4_double double,c5_string string,c6_Timestamp Timestamp,c7_Datatype_Desc string) STORED BY 'org.apache.carbondata.format'0: jdbc:hive2://192.168.2.126:10000> ;---------+ Result  ---------+---------+No rows selected (0.28 seconds)LOAD:0: jdbc:hive2://192.168.2.126:10000> LOAD DATA INPATH 'hdfs://localhost:54311/BabuStore/TestData/Data/Test_Data1.csv' INTO table Test_Boundary OPTIONS('DELIMITER'=',','QUOTECHAR'='','BAD_RECORDS_ACTION'='FORCE','FILEHEADER'='');---------+ Result  ---------+---------+No rows selected (1.912 seconds)SELECT :0: jdbc:hive2://192.168.2.126:10000> select min(c1_int),max(c1_int),sum(c1_int),avg(c1_int) , count(c1_int), variance(c1_int) from Test_Boundary where rand(c1_int)=0.6201007799387834 or rand(c1_int)=0.455400227896625930: jdbc:hive2://192.168.2.126:10000> ;Error: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 20.0 failed 1 times, most recent failure: Lost task 0.0 in stage 20.0 (TID 21, localhost): org.apache.hadoop.hive.ql.metadata.HiveException: Unable to execute method public org.apache.hadoop.hive.serde2.io.DoubleWritable org.apache.hadoop.hive.ql.udf.UDFRand.evaluate(org.apache.hadoop.io.LongWritable)  on object org.apache.hadoop.hive.ql.udf.UDFRand@160c41fd of class org.apache.hadoop.hive.ql.udf.UDFRand with arguments {null} of size 1 at org.apache.hadoop.hive.ql.exec.FunctionRegistry.invoke(FunctionRegistry.java:981) at org.apache.spark.sql.hive.HiveSimpleUDF.eval(hiveUDFs.scala:185) at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificPredicate.eval(Unknown Source) at org.apache.spark.sql.catalyst.expressions.codegen.GeneratePredicate$$anonfun$create$2.apply(GeneratePredicate.scala:68) at org.apache.spark.sql.catalyst.expressions.codegen.GeneratePredicate$$anonfun$create$2.apply(GeneratePredicate.scala:68) at org.apache.spark.sql.execution.Filter$$anonfun$2$$anonfun$apply$2.apply(basicOperators.scala:74) at org.apache.spark.sql.execution.Filter$$anonfun$2$$anonfun$apply$2.apply(basicOperators.scala:72) at scala.collection.Iterator$$anon$14.hasNext(Iterator.scala:390) at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327) at org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.processInputs(TungstenAggregationIterator.scala:504) at org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.<init>(TungstenAggregationIterator.scala:686) at org.apache.spark.sql.execution.aggregate.TungstenAggregate$$anonfun$doExecute$1$$anonfun$2.apply(TungstenAggregate.scala:95) at org.apache.spark.sql.execution.aggregate.TungstenAggregate$$anonfun$doExecute$1$$anonfun$2.apply(TungstenAggregate.scala:86) at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710) at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306) at org.apache.spark.rdd.RDD.iterator(RDD.scala:270) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306) at org.apache.spark.rdd.RDD.iterator(RDD.scala:270) at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73) at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41) at org.apache.spark.scheduler.Task.run(Task.scala:89) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745)Caused by: java.lang.reflect.InvocationTargetException at sun.reflect.GeneratedMethodAccessor72.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.hadoop.hive.ql.exec.FunctionRegistry.invoke(FunctionRegistry.java:957) ... 27 moreCaused by: java.lang.NullPointerException at org.apache.hadoop.hive.ql.udf.UDFRand.evaluate(UDFRand.java:57) ... 31 moreDriver stacktrace: (state=,code=0)HIVE:CREATE :0: jdbc:hive2://192.168.2.126:10000> create table Test_Boundary_h1 (c1_int int,c2_Bigint Bigint,c3_Decimal Decimal(38,30),c4_double double,c5_string string,c6_Timestamp Timestamp,c7_Datatype_Desc string) ROW FORMAT DELIMITED FIELDS TERMINATED BY ','0: jdbc:hive2://192.168.2.126:10000> ;---------+ result  ---------+---------+No rows selected (0.524 seconds)LOAD :0: jdbc:hive2://192.168.2.126:10000> load data local inpath '/opt/Carbon/CarbonData/TestData/Data/Test_Data1_h1.csv' OVERWRITE INTO TABLE Test_Boundary_h10: jdbc:hive2://192.168.2.126:10000> ;---------+ Result  ---------+---------+No rows selected (0.646 seconds)SELECT0: jdbc:hive2://192.168.2.126:10000> select min(c1_int),max(c1_int),sum(c1_int),avg(c1_int) , count(c1_int), variance(c1_int) from Test_Boundary_h1 where rand(c1_int)=0.6201007799387834 or rand(c1_int)=0.455400227896625930: jdbc:hive2://192.168.2.126:10000> ;-------------------------------  _c0    _c1    _c2    _c3   _c4    _c5  ------------------------------- NULL   NULL   NULL   NULL   0     NULL  -------------------------------1 row selected (1.015 seconds)
issueID:CARBONDATA-106
type:Improvement
changed files:
texts:Add audit logs for DDL commands
Add audit logs for1.Create table2. Load table
issueID:CARBONDATA-1060
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/stats/QueryStatisticsConstants.java
core/src/main/java/org/apache/carbondata/core/datastore/impl/DFSFileReaderImpl.java
core/src/main/java/org/apache/carbondata/core/stats/QueryStatisticsRecorderImpl.java
core/src/main/java/org/apache/carbondata/core/scan/scanner/impl/BlockletFullScanner.java
core/src/main/java/org/apache/carbondata/core/util/BitSetGroup.java
core/src/main/java/org/apache/carbondata/core/datastore/FileReader.java
core/src/main/java/org/apache/carbondata/core/scan/scanner/impl/BlockletFilterScanner.java
core/src/main/java/org/apache/carbondata/core/scan/result/iterator/AbstractDetailQueryResultIterator.java
core/src/main/java/org/apache/carbondata/core/datastore/impl/FileReaderImpl.java
texts:Query statistics issue in case of multiple blocklet and block
Query statistics issue in case of multiple blocklet and multiple  block
issueID:CARBONDATA-1061
type:Bug
changed files:
texts:If AL_DICTIONARY_PATH is used in load option then by SINGLE_PASS must be used.

issueID:CARBONDATA-1062
type:Bug
changed files:processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/SortParameters.java
processing/src/main/java/org/apache/carbondata/processing/loading/converter/impl/NonDictionaryFieldConverterImpl.java
processing/src/main/java/org/apache/carbondata/processing/merger/CompactionResultSortProcessor.java
texts:Data load fails if a column specified as sort column is of numeric data type
If a numeric data type column is specified as sort column and if it contains non numeric value then data load fails with the below error.ERROR UnsafeBatchParallelReadMergeSorterImpl: pool-20-thread-1 java.lang.ClassCastException: java.lang.String cannot be cast to [B at org.apache.carbondata.processing.newflow.sort.unsafe.UnsafeCarbonRowPage.addRow(UnsafeCarbonRowPage.java:89) at org.apache.carbondata.processing.newflow.sort.unsafe.UnsafeCarbonRowPage.addRow(UnsafeCarbonRowPage.java:74) at org.apache.carbondata.processing.newflow.sort.unsafe.UnsafeSortDataRows.addRowBatch(UnsafeSortDataRows.java:170) at org.apache.carbondata.processing.newflow.sort.impl.UnsafeBatchParallelReadMergeSorterImpl$SortIteratorThread.call(UnsafeBatchParallelReadMergeSorterImpl.java:150) at org.apache.carbondata.processing.newflow.sort.impl.UnsafeBatchParallelReadMergeSorterImpl$SortIteratorThread.call(UnsafeBatchParallelReadMergeSorterImpl.java:117) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745)Steps to reproduce------------------------------CREATE TABLE test_sort_col (id INT, name STRING, age INT) STORED BY 'org.apache.carbondata.format' TBLPROPERTIES('SORT_COLUMNS'='id,age')LOAD DATA local inpath '<CSV_file_path>' INTO TABLE test_sort_colselect * from test_sort_colData-----------id,name,age1,Pallavi,252,Rahul,243,Prabhat,twenty six7,Neha,252,Geetika,223,Sangeeta,26
issueID:CARBONDATA-1063
type:Bug
changed files:processing/src/main/java/org/apache/carbondata/processing/util/CarbonLoaderUtil.java
texts:When multi user perform concurrent operations like show segments NullPointerException is getting thrown
User1 in beeline terminal1 performs below operation1) Create table t12) show segments for t1User2(Does not have privilege to read t1) in beeline terminal2 performs below operation1) show segments for t1 &#8211; Throws permission denied and removes the CarbonTable metadata from CarbonMetadataUser1 in beeline terminal1 performs below operation1) Show segments for t1 - NullPointerException is thrown as the CarbonTable is not fetched from CarbonMetastore, instead fetched from SingleTon Instance CarbonMetadata.This is corrected by fetching the carbonTable from CarbonMetastore of corresponding Session.
issueID:CARBONDATA-1064
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/scan/expression/conditional/NotInExpression.java
texts:Fix NullPointerException in Not InExpression
select * from expression_test where id not in (1,2,'', NULL, ' ')In the above query Right expression value is null in NotInExpression which cased throws NullPointerException
issueID:CARBONDATA-1065
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/util/CarbonSessionInfo.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/SortScopeOptions.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonLoadOptionConstants.java
core/src/main/java/org/apache/carbondata/core/util/CarbonProperties.java
core/src/main/java/org/apache/carbondata/core/util/annotations/CarbonProperty.java
common/src/main/java/org/apache/carbondata/common/constants/LoggerAction.java
core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
processing/src/main/java/org/apache/carbondata/processing/loading/steps/DataConverterProcessorStepImpl.java
processing/src/main/java/org/apache/carbondata/processing/util/CarbonDataProcessorUtil.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonV3DataFormatConstants.java
core/src/main/java/org/apache/carbondata/core/util/SessionParams.java
processing/src/main/java/org/apache/carbondata/processing/loading/DataLoadProcessBuilder.java
core/src/main/java/org/apache/carbondata/core/exception/InvalidConfigurationException.java
core/src/main/java/org/apache/carbondata/core/util/ThreadLocalSessionInfo.java
processing/src/main/java/org/apache/carbondata/processing/loading/model/CarbonLoadModel.java
texts:Implement set command in carbon to update carbon properties dynamically
Currently there is no way to set the carbon properties dynamically, always need to update the carbon.properties file and restart the driver.So it will be easy for users to give option to add/update the properties dynamically through set command.The below sql command updates the carbon properties in driver and as well as in executorset key1=value1
issueID:CARBONDATA-1067
type:Bug
changed files:
texts:can&#39;t update delete successfully using spark2.1+carbon1.1.0
errors show in attached file
issueID:CARBONDATA-1068
type:Bug
changed files:
texts:Error occur while executing select query "local class incompatible: stream classdesc serialVersionUID"
CREATE TABLE :create table Test_Boundary (c1_int int,c2_Bigint Bigint,c3_Decimal Decimal(38,30),c4_double double,c5_string string,c6_Timestamp Timestamp,c7_Datatype_Desc string) STORED BY 'org.apache.carbondata.format'LOAD :LOAD DATA INPATH 'HDFS_URL/BabuStore/Data/Test_Data1.csv' INTO table Test_Boundary OPTIONS('DELIMITER'=',','QUOTECHAR'='','BAD_RECORDS_ACTION'='FORCE','FILEHEADER'='')SELECT:select c4_double,c7_datatype_desc,min(c4_double) from Test_Boundary group by c4_double,c7_datatype_desc having min(c4_double) >1.7976931348623158E308 order by c4_double limit 5boundry_TC_0120,FAIL,Job aborted due to stage failure: Task 0 in stage 12284.0 failed 4 times, most recent failure: Lost task 0.3 in stage 12284.0 (TID 580145, h-slave-1): java.io.InvalidClassException: org.apache.spark.sql.CarbonRelation; local class incompatible: stream classdesc serialVersionUID = 1716814377307478832, local class serialVersionUID = 6286910848280021658 at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:616) at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1630) at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1521) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1781) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353) at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2018) at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1942) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1808) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353) at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2018) at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1942) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1808) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353) at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2018) at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1942) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1808) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353) at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2018) at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1942) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1808) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353) at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2018) at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1942) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1808) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353) at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2018) at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1942) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1808) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353) at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2018) at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1942) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1808) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353) at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2018) at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1942) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1808) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353) at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2018) at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1942) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1808) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353) at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2018) at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1942) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1808) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353) at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2018) at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1942) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1808) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353) at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2018) at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1942) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1808) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353) at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2018) at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1942) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1808) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353) at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2018) at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1942) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1808) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353) at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2018) at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1942) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1808) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353) at java.io.ObjectInputStream.readObject(ObjectInputStream.java:373) at scala.collection.immutable.$colon$colon.readObject(List.scala:362) at sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1058) at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1909) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1808) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353) at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2018) at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1942) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1808) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353) at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2018) at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1942) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1808) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353) at java.io.ObjectInputStream.readObject(ObjectInputStream.java:373) at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:76) at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:115) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61) at org.apache.spark.scheduler.Task.run(Task.scala:89) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745)Driver stacktrace:
issueID:CARBONDATA-107
type:Improvement
changed files:
texts:Remove unnecessary ConverToSafe in spark planner
Query: select ch, sum(c) from (select ch,count(1) as c from t2 group by ch) temp where c > 1 group by chOutput plan looks like:== Physical Plan ==Limit 21 ConvertToSafe  CarbonDictionaryDecoder CarbonDecoderRelation(Map(word#39 -> word#39, ch#40 -> ch#40, value#41 -> value#41),CarbonDatasourceRelation(`default`.`t1`,None)), ExcludeProfile(ArrayBuffer(#103)), CarbonAliasDecoderRelation()   ConvertToSafe    TungstenAggregate(key=ch#40, functions=(sum(c#101L),mode=Final,isDistinct=false), output=ch#40,_c1#102L)     TungstenAggregate(key=ch#40, functions=(sum(c#101L),mode=Partial,isDistinct=false), output=ch#40,currentSum#122L)      Filter (c#101L > FakeCarbonCast(1 as bigint))       CarbonDictionaryDecoder CarbonDecoderRelation(Map(word#39 -> word#39, ch#40 -> ch#40, value#41 -> value#41),CarbonDatasourceRelation(`default`.`t1`,None)), IncludeProfile(ArrayBuffer(#103)), CarbonAliasDecoderRelation()        ConvertToSafe         TungstenAggregate(key=ch#40, functions=&#91;(count(1),mode=Final,isDistinct=false)&#93;, output=ch#40,c#101L)          TungstenExchange hashpartitioning(ch#40)           TungstenAggregate(key=ch#40, functions=&#91;(count(1),mode=Partial,isDistinct=false)&#93;, output=ch#40,currentCount#126L)            Project ch#40             ConvertToSafe              CarbonScan ch#40, (CarbonRelation default, t1, CarbonMetaData(ArrayBuffer(word, ch),ArrayBuffer(value),org.carbondata.core.carbon.metadata.schema.table.CarbonTable@52d54ca2,DictionaryMap(Map(word -> true, ch -> true))), TableMeta(default_t1,/Users/jackylk/code/incubator-carbondata/target/store,org.carbondata.core.carbon.metadata.schema.table.CarbonTable@52d54ca2,Partitioner(org.carbondata.spark.partition.api.impl.SampleDataPartitionerImpl,[Ljava.lang.String;@62a877f4,1,[Ljava.lang.String;@3c180da5)), None), trueThere are unnecessary ConvertToSafe before CarbonDictionaryDecoder.
issueID:CARBONDATA-1070
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/scan/expression/ExpressionResult.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelRangeLessThanFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/scan/expression/conditional/NotInExpression.java
texts:Not In Filter Expression throwing NullPointer Exception
Query containing Not In Filter Expression with null value is  throwing NullPointerException
issueID:CARBONDATA-1073
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/util/AbstractDataFileFooterConverter.java
texts:Support INPUT_FILES
Currently we support INPUT_SEGMENT_NUMBERS to reading from specified segments, but will reading all data for one segment, so add INPUT_FILES to reading from specified files.
issueID:CARBONDATA-1074
type:Sub-task
changed files:processing/src/main/java/org/apache/carbondata/processing/loading/converter/impl/FieldEncoderFactory.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/SortParameters.java
processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactHandler.java
processing/src/main/java/org/apache/carbondata/processing/merger/CompactionResultSortProcessor.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/column/CarbonColumn.java
processing/src/main/java/org/apache/carbondata/processing/merger/RowResultMergerProcessor.java
processing/src/main/java/org/apache/carbondata/processing/loading/steps/CarbonRowDataWriterProcessorStepImpl.java
processing/src/main/java/org/apache/carbondata/processing/loading/steps/DataWriterProcessorStepImpl.java
processing/src/main/java/org/apache/carbondata/processing/store/writer/AbstractFactDataWriter.java
processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactDataHandlerColumnar.java
core/src/main/java/org/apache/carbondata/core/datastore/row/CarbonRow.java
processing/src/main/java/org/apache/carbondata/processing/loading/converter/impl/RowConverterImpl.java
core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
integration/presto/src/main/java/org/apache/carbondata/presto/CarbondataSplitManager.java
processing/src/main/java/org/apache/carbondata/processing/store/writer/v3/CarbonFactDataWriterImplV3.java
core/src/main/java/org/apache/carbondata/core/util/NonDictionaryUtil.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/column/CarbonImplicitDimension.java
processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactDataHandlerModel.java
processing/src/main/java/org/apache/carbondata/processing/util/CarbonDataProcessorUtil.java
core/src/main/java/org/apache/carbondata/core/datastore/page/PrimitiveCodec.java
integration/spark2/src/main/java/org/apache/carbondata/spark/readsupport/SparkRowReadSupportImpl.java
processing/src/main/java/org/apache/carbondata/processing/loading/model/CarbonDataLoadSchema.java
processing/src/main/java/org/apache/carbondata/processing/loading/DataLoadProcessBuilder.java
processing/src/main/java/org/apache/carbondata/processing/store/TablePage.java
processing/src/main/java/org/apache/carbondata/processing/loading/CarbonDataLoadConfiguration.java
processing/src/main/java/org/apache/carbondata/processing/loading/model/CarbonLoadModel.java
core/src/main/java/org/apache/carbondata/core/datastore/columnar/BlockIndexerStorageForShort.java
texts:Add TablePage for data load process
Add TablePage preparing for data load refactory.Unify different steps to use ConvertedRow instead of Object[], steps includes:1. normal sort table2. no sort table3. compaction
issueID:CARBONDATA-1075
type:Bug
changed files:
texts:Close Dictionary Server when application ends
Analysis: The dictionary server which is started when a load is done with "single_pass=true" is never closed which did not allow the carbon application to exit because it is running on the main thread.Solution: Add a listener to the sparkContext which closes the server when application ends
issueID:CARBONDATA-1076
type:Bug
changed files:
texts:Join Issue caused by dictionary and shuffle exchange
We can reproduce this issue as following steps:Step1: create a carbon tablecarbon.sql("CREATE TABLE IF NOT EXISTS carbon_table (col1 int, col2 int, col3 int) STORED by 'carbondata' TBLPROPERTIES('DICTIONARY_INCLUDE'='col1,col2,col3','TABLE_BLOCKSIZE'='4')")Step2: load datacarbon.sql("LOAD DATA LOCAL INPATH '/opt/carbon_table' INTO TABLE carbon_table")data in file carbon_table as follows:col1,col2,col31,2,34,5,67,8,9Step3: do the querycarbon.sql("SELECT c1.col1,c2.col1,c2.col3 FROM (SELECT col1,col2 FROM carbon_table GROUP BY col1,col2) c1 FULL JOIN (SELECT col1,count(col2) as col3 FROM carbon_table GROUP BY col1) c2 ON c1.col1 = c2.col1").show()&#91;expected&#93; Hive table and parquet table get same result as below and it should be correct.col1col1col3   1   1   1   4   4   1   7   7   1&#91;acutally&#93; carbon will get null because wrong match.col1col1col3   1nullnullnull   4   1   4nullnullnull   7   1   7nullnullnull   1   1Root cause analysis:It is because this query has two subquery, and one subquey do the decode after exchange and the other subquery do the decode before exchange, and this may lead to wrong match when execute full join.My idea: Can we move decode before exchange ? Because I am not very familiar with Carbon query, so any idea about this ?Plan as follows:== Physical Plan ==SortMergeJoin col1#3445, col1#3460, FullOuter:- Sort col1#3445 ASC NULLS FIRST, false, 0:  +- Exchange hashpartitioning(col1#3445, 200):     +- CarbonDictionaryDecoder [CarbonDecoderRelation(Map(col1#3445 -> col1#3445, col2#3446 -> col2#3446, col3#3447 -> col3#3447),CarbonDatasourceHadoopRelation [ Database name :tempdev, Table name :carbon_table, Schema :Some(StructType(StructField(col1,IntegerType,true), StructField(col2,IntegerType,true), StructField(col3,IntegerType,true))) ]), CarbonDecoderRelation(Map(col1#3460 -> col1#3460, col2#3461 -> col2#3461, col3#3462 -> col3#3462),CarbonDatasourceHadoopRelation [ Database name :tempdev, Table name :carbon_table, Schema :Some(StructType(StructField(col1,IntegerType,true), StructField(col2,IntegerType,true), StructField(col3,IntegerType,true))) ])], IncludeProfile(ArrayBuffer(col1#3445)), CarbonAliasDecoderRelation(), org.apache.spark.sql.CarbonSession@69e87cbe:        +- HashAggregate(keys=col1#3445, col2#3446, functions=[], output=col1#3445):           +- Exchange hashpartitioning(col1#3445, col2#3446, 200):              +- HashAggregate(keys=col1#3445, col2#3446, functions=[], output=col1#3445, col2#3446):                 +- Scan CarbonDatasourceHadoopRelation [ Database name :tempdev, Table name :carbon_table, Schema :Some(StructType(StructField(col1,IntegerType,true), StructField(col2,IntegerType,true), StructField(col3,IntegerType,true))) ] tempdev.carbon_tablecol1#3445,col2#3446 +- Sort col1#3460 ASC NULLS FIRST, false, 0   +- CarbonDictionaryDecoder [CarbonDecoderRelation(Map(col1#3445 -> col1#3445, col2#3446 -> col2#3446, col3#3447 -> col3#3447),CarbonDatasourceHadoopRelation [ Database name :tempdev, Table name :carbon_table, Schema :Some(StructType(StructField(col1,IntegerType,true), StructField(col2,IntegerType,true), StructField(col3,IntegerType,true))) ]), CarbonDecoderRelation(Map(col1#3460 -> col1#3460, col2#3461 -> col2#3461, col3#3462 -> col3#3462),CarbonDatasourceHadoopRelation [ Database name :tempdev, Table name :carbon_table, Schema :Some(StructType(StructField(col1,IntegerType,true), StructField(col2,IntegerType,true), StructField(col3,IntegerType,true))) ])], IncludeProfile(ArrayBuffer(col1#3460)), CarbonAliasDecoderRelation(), org.apache.spark.sql.CarbonSession@69e87cbe      +- HashAggregate(keys=col1#3460, functions=count(col2#3461), output=col1#3460, col3#3436L)         +- Exchange hashpartitioning(col1#3460, 200)            +- HashAggregate(keys=col1#3460, functions=partial_count(col2#3461), output=col1#3460, count#3472L)               +- CarbonDictionaryDecoder [CarbonDecoderRelation(Map(col1#3445 -> col1#3445, col2#3446 -> col2#3446, col3#3447 -> col3#3447),CarbonDatasourceHadoopRelation [ Database name :tempdev, Table name :carbon_table, Schema :Some(StructType(StructField(col1,IntegerType,true), StructField(col2,IntegerType,true), StructField(col3,IntegerType,true))) ]), CarbonDecoderRelation(Map(col1#3460 -> col1#3460, col2#3461 -> col2#3461, col3#3462 -> col3#3462),CarbonDatasourceHadoopRelation [ Database name :tempdev, Table name :carbon_table, Schema :Some(StructType(StructField(col1,IntegerType,true), StructField(col2,IntegerType,true), StructField(col3,IntegerType,true))) ])], IncludeProfile(ArrayBuffer(col2#3461)), CarbonAliasDecoderRelation(), org.apache.spark.sql.CarbonSession@69e87cbe                  +- Scan CarbonDatasourceHadoopRelation [ Database name :tempdev, Table name :carbon_table, Schema :Some(StructType(StructField(col1,IntegerType,true), StructField(col2,IntegerType,true), StructField(col3,IntegerType,true))) ] tempdev.carbon_tablecol1#3460,col2#3461]
issueID:CARBONDATA-1077
type:Bug
changed files:
texts:ColumnDict  and ALL_DICTIONARY_PATH must be used with SINGLE_PASS=&#39;true&#39;
Add validation that ColumnDict  and ALL_DICTIONARY_PATH must be used with SINGLE_PASS='true'.Problem:If columndict is used without single path, and if the external dictionary file does not have all the values present in the actual raw data to be loaded then load will not have proper data.
issueID:CARBONDATA-1078
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/scan/collector/impl/DictionaryBasedResultCollector.java
texts:Query return incorrect result when selecting complex column before dictionary column in spark 2.1
Query return incorrect result when selecting complex column before dictionary column in spark 2.1
issueID:CARBONDATA-108
type:Improvement
changed files:
texts:Remove unnecessary Project for CarbonScan
For this SQL:select ch, sum(c) from (select ch,count(1) as c from t1 group by ch) temp where c > 1 group by chPhysical plan is:== Physical Plan ==Limit 21 ConvertToSafe  CarbonDictionaryDecoder CarbonDecoderRelation(Map(word#22 -> word#22, ch#23 -> ch#23, value#24L -> value#24L),CarbonDatasourceRelation(`default`.`t1`,None)), ExcludeProfile(ArrayBuffer()), CarbonAliasDecoderRelation()   TungstenAggregate(key=ch#23, functions=(sum(c#18L),mode=Final,isDistinct=false), output=ch#23,_c1#25L)    TungstenAggregate(key=ch#23, functions=(sum(c#18L),mode=Partial,isDistinct=false), output=ch#23,currentSum#48L)     Filter (c#18L > 1)      TungstenAggregate(key=ch#23, functions=&#91;(count(1),mode=Final,isDistinct=false)&#93;, output=ch#23,c#18L)       TungstenExchange hashpartitioning(ch#23)        TungstenAggregate(key=ch#23, functions=&#91;(count(1),mode=Partial,isDistinct=false)&#93;, output=ch#23,currentCount#52L)         Project ch#23          ConvertToSafe           CarbonScan ch#23, (CarbonRelation default, t1, CarbonMetaData(ArrayBuffer(word, ch),ArrayBuffer(value),org.carbondata.core.carbon.metadata.schema.table.CarbonTable@6034ef16,DictionaryMap(Map(word -> true, ch -> true))), TableMeta(default_t1,/Users/jackylk/code/incubator-carbondata/target/store,org.carbondata.core.carbon.metadata.schema.table.CarbonTable@6034ef16,Partitioner(org.carbondata.spark.partition.api.impl.SampleDataPartitionerImpl,[Ljava.lang.String;@450458d7,1,[Ljava.lang.String;@f8a969d)), None), trueThe Project is unnecessary since CarbonScan only scan the requested column
issueID:CARBONDATA-1083
type:Bug
changed files:
texts:Fix Build failure issue on presto branch
Fix the compilation issues on the presto branch related to version mismatch causing build failure on this branch.
issueID:CARBONDATA-1084
type:Improvement
changed files:
texts:Add documentation for V3 Data Format
Benefits to be added in documentation and add commands to set this format and specify that this is the dafault format
issueID:CARBONDATA-1085
type:Improvement
changed files:
texts:add documentation for size based blocklet for V3 data format
Configurable number of pages to improve IO by specifying the property in carbon.properties ( JIRA 766)
issueID:CARBONDATA-1086
type:Improvement
changed files:
texts:Add documentation for batch sort support for data loading
Improves Loading PerformanceCommands to be added ( JIRA 742,JIRA 1047)
issueID:CARBONDATA-1091
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/scan/collector/impl/DictionaryBasedVectorResultCollector.java
core/src/main/java/org/apache/carbondata/core/scan/result/BlockletScannedResult.java
texts:Implicit column tupleId is not returning results if VectorReader is enabled.
If user enables vector reader while querying, implicit column tupleId is not getting filled and throwing exception.eg., carbon.enable.vector.reader = true in Carbon.Propertiesselect getTupleId() as tupleId from carbonTableThis needs to be corrected.
issueID:CARBONDATA-1092
type:Bug
changed files:
texts:alter table add column query should support no_inverted_index

issueID:CARBONDATA-1093
type:Bug
changed files:
texts:User data is getting printed in logs if the server fails to respond to client
java.lang.RuntimeException: Request timed out for key : DictionaryKey{ columnName='p_cap_time', data='2010301', dictionaryValue=-1, type=DICT_GENERATION} at org.apache.carbondata.core.dictionary.client.DictionaryClientHandler.getDictionary(DictionaryClientHandler.java:94) at org.apache.carbondata.core.dictionary.client.DictionaryClient.getDictionary(DictionaryClient.java:81) at org.apache.carbondata.processing.newflow.dictionary.DictionaryServerClientDictionary.getOrGenerateKey(DictionaryServerClientDictionary.java:57) at org.apache.carbondata.processing.newflow.dictionary.DictionaryServerClientDictionary.getOrGenerateKey(DictionaryServerClientDictionary.java:32) at org.apache.carbondata.processing.newflow.converter.impl.DictionaryFieldConverterImpl.convert(DictionaryFieldConverterImpl.java:117) at org.apache.carbondata.processing.newflow.converter.impl.RowConverterImpl.convert(RowConverterImpl.java:155) at org.apache.carbondata.processing.newflow.steps.DataConverterProcessorStepImpl.processRowBatch(DataConverterProcessorStepImpl.java:107) at org.apache.carbondata.processing.newflow.steps.DataConverterProcessorStepImpl$1.next(DataConverterProcessorStepImpl.java:93) at org.apache.carbondata.processing.newflow.steps.DataConverterProcessorStepImpl$1.next(DataConverterProcessorStepImpl.java:81) at org.apache.carbondata.processing.newflow.sort.impl.UnsafeParallelReadMergeSorterImpl$SortIteratorThread.call(UnsafeParallelReadMergeSorterImpl.java:206) at org.apache.carbondata.processing.newflow.sort.impl.UnsafeParallelReadMergeSorterImpl$SortIteratorThread.call(UnsafeParallelReadMergeSorterImpl.java:181) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745)Solution: Remove the data field while printing logs
issueID:CARBONDATA-1094
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelRangeGrtrThanEquaToFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RangeValueFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/IncludeFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelRangeGrtThanFiterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelRangeLessThanFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelRangeLessThanEqualFilterExecuterImpl.java
texts:Wrong results returned by the query in case inverted index is not created on a column
While creating a table if a column is not specified as sort column or included as no inverted index then the column will not be sorted during data load. Unsorted data will have incorrect min/max values and inverted index will not be created for that column.During query, if filter exists for that column it gives incorrect results as binary search cannot be applied on the unsorted data.Commands to reproduce-----------------------------------------CREATE TABLE IF NOT EXISTS index1 (id Int, name String, city String) STORED BY 'org.apache.carbondata.format' TBLPROPERTIES('NO_INVERTED_INDEX'='name,city', 'DICTIONARY_EXCLUDE'='city')LOAD DATA LOCAL INPATH '<csv file path>' into table index1SELECT * FROM index1 WHERE city >= 'Shanghai'--------------- id  name      city--------------- 11 JamesWashington  5  John   Beijing 20 Kevin Singapore 17  Lisa  Hangzhou 12 Maria    Berlin  2  Mark     Paris  9  Mary     Tokyo  6Michel   Chicago 16  Paul  Shanghai 14 Peter    Boston  7Robert   Houston  4  Sara     Tokyo  8 Sunny    Boston---------------
issueID:CARBONDATA-1095
type:Task
changed files:integration/presto/src/main/java/org/apache/carbondata/presto/Types.java
integration/hive/src/main/java/org/apache/carbondata/hive/test/server/HiveEmbeddedServer2.java
integration/hive/src/main/java/org/apache/carbondata/hive/CarbonHiveRecordReader.java
integration/presto/src/main/java/org/apache/carbondata/presto/CarbondataPlugin.java
integration/presto/src/main/java/org/apache/carbondata/presto/impl/CarbonTableReader.java
texts:Fix rebase issues of presto and hive integration
After rebasing presto and hive branch to master , there are some issues need to be fixed, for example : the number of class parameters changed.
issueID:CARBONDATA-1096
type:Bug
changed files:
texts:wrong data show in quering carbontable with timestamp data type from hive
start beeline 1.CREATE TABLE ALLDATATYPETEST(ID INT,NAME STRING,SALARY DECIMAL,MARKS DOUBLE,JOININGDATE DATE,LEAVINGDATE TIMESTAMP) STORED BY 'CARBONDATA' ;2. jdbc:hive2://localhost:10000> LOAD DATA INPATH 'hdfs://localhost:54310/alldatatypetest.csv' into table alldatatypetest;---------+ Result  ---------+---------+No rows selected (8.867 seconds)3: jdbc:hive2://localhost:10000> select * from alldatatypetest;-------------------------------------------------------------- id      name     salary   marks   joiningdate        leavingdate       -------------------------------------------------------------- 1    'ANUBHAV'   200000   100.0   2016-04-14    2016-04-14 15:00:09.0   2    'LIANG'     200000   100.0   2016-04-14    2016-04-14 15:00:09.0  started the hive shell4.hive> CREATE TABLE ALLDATATYPETEST(ID INT,NAME STRING,SALARY DECIMAL,MARKS DOUBLE,JOININGDATE DATE,LEAVINGDATE TIMESTAMP) ROW FORMAT SERDE 'org.apache.carbondata.hive.CarbonHiveSerDe' STORED AS INPUTFORMAT 'org.apache.carbondata.hive.MapredCarbonInputFormat' OUTPUTFORMAT 'org.apache.carbondata.hive.MapredCarbonOutputFormat' TBLPROPERTIES ('spark.sql.sources.provider'='org.apache.spark.sql.CarbonSource');OK5.hive> ALTER TABLE ALLDATATYPETEST SET LOCATION 'hdfs://localhost:54310/opt/carbonStore/default/alldatatypetest';OKTime taken: 0.208 seconds6.hive> SELECT * FROM ALLDATATYPETEST;OK1 'ANUBHAV' 200000 100.0 1970-01-01 48255-05-28 04:00:000.02 'LIANG' 200000 100.0 1970-01-01 48255-05-28 04:00:000.0Time taken: 0.137 seconds, Fetched: 2 row(s)
issueID:CARBONDATA-1097
type:Bug
changed files:processing/src/main/java/org/apache/carbondata/processing/store/TablePage.java
texts:describe formatted query should display no_inverted_index column

issueID:CARBONDATA-1098
type:Sub-task
changed files:core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/EncodedColumnPage.java
core/src/main/java/org/apache/carbondata/core/datastore/page/LazyColumnPage.java
core/src/main/java/org/apache/carbondata/core/datastore/page/statistics/SimpleStatsResult.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/IntermediateFileMerger.java
processing/src/main/java/org/apache/carbondata/processing/loading/steps/DataWriterProcessorStepImpl.java
processing/src/main/java/org/apache/carbondata/processing/store/writer/v3/BlockletDataHolder.java
core/src/main/java/org/apache/carbondata/core/util/DataTypeUtil.java
processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactDataHandlerColumnar.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/compress/DirectCompressCodec.java
core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
core/src/main/java/org/apache/carbondata/core/util/NonDictionaryUtil.java
processing/src/main/java/org/apache/carbondata/processing/store/writer/v3/CarbonFactDataWriterImplV3.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/adaptive/AdaptiveCodec.java
core/src/main/java/org/apache/carbondata/core/datastore/page/ColumnPage.java
core/src/main/java/org/apache/carbondata/core/datastore/TableSpec.java
core/src/main/java/org/apache/carbondata/core/datastore/page/statistics/ColumnPageStatsCollector.java
core/src/main/java/org/apache/carbondata/core/datastore/page/statistics/TablePageStatistics.java
processing/src/main/java/org/apache/carbondata/processing/store/TablePage.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/SortTempFileChunkHolder.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/reader/measure/v3/MeasureChunkReaderV3.java
core/src/main/java/org/apache/carbondata/core/datastore/columnar/BlockIndexerStorageForShort.java
core/src/main/java/org/apache/carbondata/core/datastore/page/EncodedTablePage.java
core/src/main/java/org/apache/carbondata/core/datastore/columnar/BlockIndexerStorageForNoInvertedIndexForShort.java
core/src/main/java/org/apache/carbondata/core/datastore/page/key/TablePageKey.java
processing/src/main/java/org/apache/carbondata/processing/store/writer/CarbonFactDataWriter.java
core/src/main/java/org/apache/carbondata/core/datastore/page/statistics/KeyPageStatsCollector.java
core/src/main/java/org/apache/carbondata/core/scan/collector/impl/AbstractScannedResultCollector.java
processing/src/main/java/org/apache/carbondata/processing/store/writer/AbstractFactDataWriter.java
core/src/main/java/org/apache/carbondata/core/metadata/ValueEncoderMeta.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/adaptive/AdaptiveDeltaIntegralCodec.java
integration/hive/src/main/java/org/apache/carbondata/hive/CarbonHiveSerDe.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
core/src/main/java/org/apache/carbondata/core/util/CarbonMetadataUtil.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/EncodingFactory.java
core/src/main/java/org/apache/carbondata/core/datastore/page/statistics/PrimitivePageStatsCollector.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/ColumnPageCodec.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/adaptive/AdaptiveIntegralCodec.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/DefaultEncodingFactory.java
texts:change statistics to use exact type instead of Object
Currently page statistics uses Object internally, it should be changed to use exact type as schema defines
issueID:CARBONDATA-1099
type:Bug
changed files:
texts:faild to run carbon-spark-shell in spark2 environment
in spark 2.1 version, run carbon-spark-shell in after compilation result in an error because of the following reasons:1. in spark2 integration, the assembly jar lies in scala-2.11 folder, not scala-2.10 in spark1, so the shell script need to be updated.2. in spark2 integration, the repl haven't been implemented, so it need to be added.
issueID:CARBONDATA-11
type:Improvement
changed files:
texts:Support carbon carbon spark shell in carbondata to simplify operations for first time users
Currently in carbon there is no dedicated carbon shell in carbon so for first time users to use carbon is no so easy. This feature simplify operations for first time users and have good experience.
issueID:CARBONDATA-110
type:Bug
changed files:integration/spark-common/src/main/java/org/apache/carbondata/spark/merger/CarbonDataMergerUtil.java
texts:if user deletes the segment already selected for compaction then compaction need to get failed.
if user deletes the segment already selected for compaction then compaction need to get failed.step 1: Start the compaction which selects the segments 0, 1 .step 2: delete the segment 1.so that delete will immediately make the segment 1 as marked as delete. but the compaction is running and it will make the already  deleted segment as compacted. like undo the delete.Behaviour should be , if any segment gets deleted after the trigger of compaction then we need to fail the compaction process.This can be achieved by , check the segments status before updating the compaction status in table status , and if any segment is deleted then need to abort the compaction.
issueID:CARBONDATA-1102
type:Bug
changed files:
texts:Selecting Int type in hive from carbon table is showing class cast exception
in carbon0: jdbc:hive2://localhost:10000> CREATE TABLE ALLDATATYPETEST(ID INT,NAME STRING,SALARY DECIMAL,MARKS DOUBLE,JOININGDATE DATE,LEAVINGDATE TIMESTAMP) STORED BY 'CARBONDATA' ;---------+ Result  ---------+---------+No rows selected (3.702 seconds)0: jdbc:hive2://localhost:10000> LOAD DATA INPATH 'hdfs://localhost:54310/alldatatypetest.csv' into table alldatatypetest;---------+ Result  ---------+---------+No rows selected (7.16 seconds)0: jdbc:hive2://localhost:10000> SELECT * FROM ALLDATATYPETEST;-------------------------------------------------------------- ID      NAME     SALARY   MARKS   JOININGDATE        LEAVINGDATE       -------------------------------------------------------------- 1    'ANUBHAV'   200000   100.0   2016-04-14    2016-04-14 15:00:09.0   2    'LIANG'     200000   100.0   2016-04-14    2016-04-14 15:00:09.0  --------------------------------------------------------------2 rows selected (1.978 seconds)in hivehive> CREATE TABLE ALLDATATYPETEST(ID INT,NAME STRING,SALARY DECIMAL,MARKS DOUBLE,JOININGDATE DATE,LEAVINGDATE TIMESTAMP) ROW FORMAT SERDE 'org.apache.carbondata.hive.CarbonHiveSerDe' STORED AS INPUTFORMAT 'org.apache.carbondata.hive.MapredCarbonInputFormat' OUTPUTFORMAT 'org.apache.carbondata.hive.MapredCarbonOutputFormat' TBLPROPERTIES ('spark.sql.sources.provider'='org.apache.spark.sql.CarbonSource');OKTime taken: 1.934 secondshive> ALTER TABLE ALLDATATYPETEST SET LOCATION 'hdfs://localhost:54310/opt/carbonStore/default/alldatatypetest';OKTime taken: 1.192 secondshive> SELECT * FROM ALLDATATYPETEST;OKFailed with exception java.io.IOException:java.lang.ClassCastException: java.lang.Integer cannot be cast to java.lang.LongTime taken: 0.174 seconds
issueID:CARBONDATA-1103
type:Bug
changed files:
texts:Integer datatype as a long datatype in carbondata on cluster
Integer datatype as a long datatype in carbondata on clusterSteps to reproduce Bug:In CarbonData:Create Table:create table myvmall (imei String,uuid String,MAC String,device_color String,device_shell_color String,device_name String,product_name String,ram String,rom String,cpu_clock String,series String,check_date String,check_year int,check_month int ,check_day int,check_hour int,bom String,inside_name String,packing_date String,packing_year String,packing_month String,packing_day String,packing_hour String,customer_name String,deliveryAreaId String,deliveryCountry String,deliveryProvince String,deliveryCity String,deliveryDistrict String,packing_list_no String,order_no String,Active_check_time String,Active_check_year int,Active_check_month int,Active_check_day int,Active_check_hour int,ActiveAreaId String,ActiveCountry String,ActiveProvince String,Activecity String,ActiveDistrict String,Active_network String,Active_firmware_version String,Active_emui_version String,Active_os_version String,Latest_check_time String,Latest_check_year int,Latest_check_month int,Latest_check_day int,Latest_check_hour int,Latest_areaId String,Latest_country String,Latest_province String,Latest_city String,Latest_district String,Latest_firmware_version String,Latest_emui_version String,Latest_os_version String,Latest_network String,site String,site_desc String,product String,product_desc String) STORED BY 'org.apache.carbondata.format' TBLPROPERTIES ('DICTIONARY_INCLUDE'='check_year,check_month,check_day,check_hour,Active_check_year,Active_check_month,Active_check_day,Active_check_hour,Latest_check_year,Latest_check_month,Latest_check_day') Load Data:LOAD DATA INPATH 'HDFS_URL/BabuStore/Data/100_VMALL_1_Day_DATA_2015-09-15.csv' INTO table myvmall options('DELIMITER'=',', 'QUOTECHAR'='"','BAD_RECORDS_ACTION'='FORCE','FILEHEADER'='imei,uuid,MAC,device_color,device_shell_color,device_name,product_name,ram,rom,cpu_clock,series,check_date,check_year,check_month,check_day,check_hour,bom,inside_name,packing_date,packing_year,packing_month,packing_day,packing_hour,customer_name,deliveryAreaId,deliveryCountry,deliveryProvince,deliveryCity,deliveryDistrict,packing_list_no,order_no,Active_check_time,Active_check_year,Active_check_month,Active_check_day,Active_check_hour,ActiveAreaId,ActiveCountry,ActiveProvince,Activecity,ActiveDistrict,Active_network,Active_firmware_version,Active_emui_version,Active_os_version,Latest_check_time,Latest_check_year,Latest_check_month,Latest_check_day,Latest_check_hour,Latest_areaId,Latest_country,Latest_province,Latest_city,Latest_district,Latest_firmware_version,Latest_emui_version,Latest_os_version,Latest_network,site,site_desc,product,product_desc')description in carbondata:--------------------------------------------+         col_name          data_type   comment  --------------------------------------------+ imei                      string       uuid                      string       mac                       string       device_color              string       device_shell_color        string       device_name               string       product_name              string       ram                       string       rom                       string       cpu_clock                 string       series                    string       check_date                string       check_year                int          check_month               int          check_day                 int          check_hour                int          bom                       string       inside_name               string       packing_date              string       packing_year              string       packing_month             string       packing_day               string       packing_hour              string       customer_name             string       deliveryareaid            string       deliverycountry           string       deliveryprovince          string       deliverycity              string       deliverydistrict          string       packing_list_no           string       order_no                  string       active_check_time         string       active_check_year         int          active_check_month        int          active_check_day          int          active_check_hour         int          activeareaid              string       activecountry             string       activeprovince            string       activecity                string       activedistrict            string       active_network            string       active_firmware_version   string       active_emui_version       string       active_os_version         string       latest_check_time         string       latest_check_year         int          latest_check_month        int          latest_check_day          int          latest_check_hour         bigint       latest_areaid             string       latest_country            string       latest_province           string       latest_city               string       latest_district           string       latest_firmware_version   string       latest_emui_version       string       latest_os_version         string       latest_network            string       site                      string       site_desc                 string       product                   string       product_desc              string      --------------------------------------------+Query Executed:select imei,latest_check_hour from myvmall where myvmall.latest_check_hour  IN (10,14) and myvmall.imei IN ('imeiA009945257','imeiA009945258') and myvmall.check_year IN (2015)Result in CarbonData:-----------------------------------      imei        latest_check_hour  ----------------------------------- imeiA009945257   14                  imeiA009945258   14                 -----------------------------------In hive:Create Table:create table hivevmall (imei String,uuid String,MAC String,device_color String,device_shell_color String,device_name String,product_name String,ram String,rom String,cpu_clock String,series String,check_date String,check_year int,check_month int ,check_day int,check_hour int,bom String,inside_name String,packing_date String,packing_year String,packing_month String,packing_day String,packing_hour String,customer_name String,deliveryAreaId String,deliveryCountry String,deliveryProvince String,deliveryCity String,deliveryDistrict String,packing_list_no String,order_no String,Active_check_time String,Active_check_year int,Active_check_month int,Active_check_day int,Active_check_hour int,ActiveAreaId String,ActiveCountry String,ActiveProvince String,Activecity String,ActiveDistrict String,Active_network String,Active_firmware_version String,Active_emui_version String,Active_os_version String,Latest_check_time String,Latest_check_year int,Latest_check_month int,Latest_check_day int,Latest_check_hour int,Latest_areaId String,Latest_country String,Latest_province String,Latest_city String,Latest_district String,Latest_firmware_version String,Latest_emui_version String,Latest_os_version String,Latest_network String,site String,site_desc String,product String,product_desc String)ROW FORMAT DELIMITED FIELDS TERMINATED BY ','Load data:load data local inpath  '/opt/Carbon/CarbonData/TestData/Data/100_VMALL_1_Day_DATA_2015-09-15.csv' OVERWRITE INTO TABLE hivevmalldescription in hive:--------------------------------------------+         col_name          data_type   comment  --------------------------------------------+ imei                      string      NULL      uuid                      string      NULL      mac                       string      NULL      device_color              string      NULL      device_shell_color        string      NULL      device_name               string      NULL      product_name              string      NULL      ram                       string      NULL      rom                       string      NULL      cpu_clock                 string      NULL      series                    string      NULL      check_date                string      NULL      check_year                int         NULL      check_month               int         NULL      check_day                 int         NULL      check_hour                int         NULL      bom                       string      NULL      inside_name               string      NULL      packing_date              string      NULL      packing_year              string      NULL      packing_month             string      NULL      packing_day               string      NULL      packing_hour              string      NULL      customer_name             string      NULL      deliveryareaid            string      NULL      deliverycountry           string      NULL      deliveryprovince          string      NULL      deliverycity              string      NULL      deliverydistrict          string      NULL      packing_list_no           string      NULL      order_no                  string      NULL      active_check_time         string      NULL      active_check_year         int         NULL      active_check_month        int         NULL      active_check_day          int         NULL      active_check_hour         int         NULL      activeareaid              string      NULL      activecountry             string      NULL      activeprovince            string      NULL      activecity                string      NULL      activedistrict            string      NULL      active_network            string      NULL      active_firmware_version   string      NULL      active_emui_version       string      NULL      active_os_version         string      NULL      latest_check_time         string      NULL      latest_check_year         int         NULL      latest_check_month        int         NULL      latest_check_day          int         NULL      latest_check_hour         int         NULL      latest_areaid             string      NULL      latest_country            string      NULL      latest_province           string      NULL      latest_city               string      NULL      latest_district           string      NULL      latest_firmware_version   string      NULL      latest_emui_version       string      NULL      latest_os_version         string      NULL      latest_network            string      NULL      site                      string      NULL      site_desc                 string      NULL      product                   string      NULL      product_desc              string      NULL     --------------------------------------------+Query Executed:select imei,latest_check_hour from hivevmall where hivevmall.latest_check_hour  IN (10,14) and hivevmall.imei IN ('imeiA009945257','imeiA009945258') and hivevmall.check_year IN (2015)Result In hive:-----------------------------------      imei        latest_check_hour  ----------------------------------- imeiA009945257   14                  imeiA009945258   14                 -----------------------------------Result On Automation:1)In CarbonData:imei StringType,latest_check_hour LongTypeimeiA009945257,14imeiA009945258,142)In hive:imei StringType,latest_check_hour IntegerTypeimeiA009945257,14imeiA009945258,14camparison failur because of datatype( latest_check_hour   int in hive and  latest_check_hour  bigint in carbondata)testCase id:1) Pushup_filter_myvmall_tc013     2) Pushup_filter_myvmall_tc019     3) Pushup_filter_myvmall_tc021       4) Pushup_filter_myvmall_tc022
issueID:CARBONDATA-1104
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/datastore/chunk/store/impl/unsafe/UnsafeVariableLengthDimensionDataChunkStore.java
texts:Query failure while using unsafe for query execution numeric data type column specified as sort column
Steps to reproduce---------------------------------1. Set the parameter enable.unsafe.in.query.processing = true2. CREATE TABLE sorttable1 (empno int, empname String, designation String, doj Timestamp, workgroupcategory int, workgroupcategoryname String, deptno int, deptname String, projectcode int, projectjoindate Timestamp, projectenddate Timestamp,attendance int,utilization int,salary int) STORED BY 'org.apache.carbondata.format' tblproperties('sort_columns'='empno')3. LOAD DATA local inpath '<CSV_path>' INTO TABLE sorttable1 OPTIONS('DELIMITER'= ',', 'QUOTECHAR'= '"')4. select empno from sorttable1Exception thrown--------------------------------17/05/29 08:43:20 ERROR Executor: Exception in task 0.0 in stage 6.0 (TID 12)java.lang.NullPointerException at org.apache.spark.sql.execution.vectorized.ColumnVector.arrayData(ColumnVector.java:858) at org.apache.spark.sql.execution.vectorized.OffHeapColumnVector.putByteArray(OffHeapColumnVector.java:421) at org.apache.spark.sql.execution.vectorized.ColumnVector.putByteArray(ColumnVector.java:569) at org.apache.carbondata.spark.vectorreader.ColumnarVectorWrapper.putBytes(ColumnarVectorWrapper.java:85) at org.apache.carbondata.core.datastore.chunk.store.impl.unsafe.UnsafeVariableLengthDimesionDataChunkStore.fillRow(UnsafeVariableLengthDimesionDataChunkStore.java:167) at org.apache.carbondata.core.datastore.chunk.impl.VariableLengthDimensionDataChunk.fillConvertedChunkData(VariableLengthDimensionDataChunk.java:112) at org.apache.carbondata.core.scan.result.AbstractScannedResult.fillColumnarNoDictionaryBatch(AbstractScannedResult.java:228) at org.apache.carbondata.core.scan.collector.impl.DictionaryBasedVectorResultCollector.scanAndFillResult(DictionaryBasedVectorResultCollector.java:154) at org.apache.carbondata.core.scan.collector.impl.DictionaryBasedVectorResultCollector.collectVectorBatch(DictionaryBasedVectorResultCollector.java:147)
issueID:CARBONDATA-1105
type:Bug
changed files:
texts:Remove the fixed spark.version in submodule for supporting new spark version.
Remove the fixed spark.version in submodule for supporting new spark version.
issueID:CARBONDATA-1107
type:Bug
changed files:processing/src/main/java/org/apache/carbondata/processing/util/CarbonLoaderUtil.java
texts:Multi User load on same table is failing with NullPointerException
User1 in beeline terminal1 performs below operationCreate table t1load data into t1User2(Does not have privilege to read t1) in beeline terminal2 performs below operationload data into t1 &#8211; This command will fail for table folder permission and carbonTable will be removed from CarbonMetadataUser1 in beeline terminal1 performs below operationload data into t1 &#8211; This command will throw NullPointerException for carbonTable.from CarbonMetadata.This is corrected by fetching the CarbonTable from CarbonMetastore of corresponding Session.
issueID:CARBONDATA-1109
type:Bug
changed files:processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactDataHandlerColumnar.java
texts:Page lost in load process when last page is not be consumed at the end
First, we use Producer-Consumer model in the write step, we have n(default value is 2 and can be configured) producers and one consumer. The task of generate last page(less than 32000) is added to thread pool at the end, but can't be guaranteed to be finished and add to BlockletDataHolder at the end. Because we have n tasks running concurrently.Second, we have 2 ways to invoke `writeDataToFile`, one is the size of `DataWriterHolder` reach the size of blocklet and two is the page is the last page.So if the last page is not be consumed at the end, we lost the page which be consumed after last page.
issueID:CARBONDATA-111
type:Bug
changed files:
texts:If  compaction job is killed then need to stop the compaction tasks running.
If the user starts a compaction job. The user kills the compaction job through the Spark UI . here even after the job is killed the task will be still running in the  background. so that the compacted files will still be written to the HDFS.So if the Job is killed in UI , the tasks should also get killed.
issueID:CARBONDATA-1111
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelRangeGrtrThanEquaToFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RangeValueFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelRangeGrtThanFiterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/column/CarbonDimension.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/IncludeFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelRangeLessThanEqualFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/scan/filter/FilterUtil.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelRangeLessThanFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/ExcludeFilterExecuterImpl.java
texts:Improve No dictionary column Include And Exclude filter
Added binary search logic for no dictionary column in include and exclude filters
issueID:CARBONDATA-1113
type:Bug
changed files:
texts:Add validation for partition column feature
Add various validation to partition columns like:-1. Partition columns cannot be drop using alter command2. Range partition info should have values in ascending order
issueID:CARBONDATA-1114
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/exception/ConcurrentOperationException.java
processing/src/main/java/org/apache/carbondata/processing/util/CarbonLoaderUtil.java
integration/spark-common/src/main/java/org/apache/carbondata/spark/exception/ProcessMetaDataException.java
store/sdk/src/main/java/org/apache/carbondata/sdk/file/CSVCarbonWriter.java
processing/src/main/java/org/apache/carbondata/processing/loading/model/CarbonLoadModelBuilder.java
processing/src/main/java/org/apache/carbondata/processing/loading/model/LoadOption.java
processing/src/main/java/org/apache/carbondata/processing/datamap/DataMapWriterListener.java
processing/src/main/java/org/apache/carbondata/processing/merger/CarbonDataMergerUtil.java
processing/src/main/java/org/apache/carbondata/processing/util/CarbonDataProcessorUtil.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonTableInputFormat.java
core/src/main/java/org/apache/carbondata/core/datamap/dev/DataMap.java
core/src/main/java/org/apache/carbondata/core/statusmanager/SegmentStatusManager.java
core/src/main/java/org/apache/carbondata/core/datamap/dev/DataMapFactory.java
datamap/examples/src/minmaxdatamap/main/java/org/apache/carbondata/datamap/examples/MinMaxDataWriter.java
core/src/main/java/org/apache/carbondata/core/locks/LocalFileLock.java
texts:Failed to run tests in windows env
when run build command "mvn clean package -Pspark-2.1 -Dspark.version=2.1.0 -Dmaven.test.failure.ignore=true -Dmaven.test.error.ignore=true" in windows env, there are some failures in tests as below:*MODULE:processing*Failed tests:   LocalFileLockTest.testingLocalFileLockingByAcquiring2Locks:64 nullTests in error:   ZooKeeperLockingTest.testZooKeeperLockingByTryingToAcquire2Locks:98 ? StringIndexOutOfBoundsTests run: 13, Failures: 1, Errors: 1, Skipped: 1*MODULE:core*Failed tests:   AbsoluteTableIdentifierTest.gettablePathTest:89 null  DFSFileHolderImplUnitTest.testDouble:129 Expected: is <7.3083355198552433E18>     but: was <8.0305922754531471E18>  DFSFileHolderImplUnitTest.testReadByteArray:84 Expected: is &#91;<72>&#93;     but: was &#91;<99>&#93;  DFSFileHolderImplUnitTest.testReadByteArrayWithFilePath:90 Expected: is &#91;<108>, <108>&#93;     but: was &#91;<114>, <101>&#93;  DFSFileHolderImplUnitTest.testReadInt:112 Expected: is <1701604463>     but: was <1869767968>  DFSFileHolderImplUnitTest.testReadIntWithFileName:118 Expected: is <1701604463>     but: was <1869767968>  DFSFileHolderImplUnitTest.testReadLong:96 Expected: is <7308335519855243122L>     but: was <8030592275453146721L>  FileFactoryImplUnitTest.testCreateNewFileWithDefaultFileType:85 null  FileFactoryImplUnitTest.testCreateNewLockFileWithDefaultFileType:90 null  FileFactoryImplUnitTest.testCreateNewLockFileWithViewFsFileType:95 null  FileHolderImplUnitTest.testDouble:130 Expected: is <7.3083355198552433E18>     but: was <8.0305922754531471E18>  FileHolderImplUnitTest.testReadByteArray:87 Expected: is &#91;<72>&#93;     but: was &#91;<99>&#93;  FileHolderImplUnitTest.testReadByteArrayWithFilePath:93 Expected: is &#91;<108>, <108>&#93;     but: was &#91;<114>, <101>&#93;  FileHolderImplUnitTest.testReadInt:113Expected: is <1701604463>     but: was <1869767968>  FileHolderImplUnitTest.testReadIntWithFileName:119 Expected: is <1701604463>     but: was <1869767968>  FileHolderImplUnitTest.testReadLong:99 Expected: is <7308335519855243122L>     but: was <8030592275453146721L>  LocalCarbonFileTest.testRenameForce:123 null  LocalCarbonFileTest.testsetLastModifiedTime:139 nullTests run: 841, Failures: 18, Errors: 0, Skipped: 0
issueID:CARBONDATA-1116
type:Bug
changed files:
texts:Not able to connect with Carbonsession while starting carbon spark shell and beeline
Not able to connect with Carbonsession while starting carbon spark shell and beelineSteps to reproduce:1)Start thrift-servera) cd $SPARK-HOME/binb) ./spark-submit --conf spark.sql.hive.thriftServer.singleSession=true --class org.apache.carbondata.spark.thriftserver.CarbonThriftServer /opt/spark/spark-2.1/carbonlib/carbondata_2.11-1.1.0-SNAPSHOT-shade-hadoop2.7.3.jar hdfs://localhost:54310/opt/prestocarbonStore2)Start Beelinea) cd $SPARK-HOME/binb)./beeline3) Connect with carbondata via jdbc!connect jdbc:hive2://localhost:10000Enter username for jdbc:hive2://localhost:10000: hduserEnter password for jdbc:hive2://localhost:10000: ******4) Actual Result:Error: Could not establish connection to jdbc:hive2://localhost:10000: null (state=08S01,code=0)0: jdbc:hive2://localhost:10000 (closed)>5) Expected result : it should connect successfully with carbondata6)console logs:17/06/01 13:03:27 INFO ThriftCLIService: Client protocol version: HIVE_CLI_SERVICE_PROTOCOL_V817/06/01 13:03:27 INFO SessionState: Created local directory: /tmp/addaba65-46c5-4467-a02f-2bbdfd54329a_resources17/06/01 13:03:27 INFO SessionState: Created HDFS directory: /tmp/hive/hduser/addaba65-46c5-4467-a02f-2bbdfd54329a17/06/01 13:03:27 INFO SessionState: Created local directory: /tmp/hduser/addaba65-46c5-4467-a02f-2bbdfd54329a17/06/01 13:03:27 INFO SessionState: Created HDFS directory: /tmp/hive/hduser/addaba65-46c5-4467-a02f-2bbdfd54329a/_tmp_space.db17/06/01 13:03:27 INFO HiveSessionImpl: Operation log session directory is created: /tmp/hduser/operation_logs/addaba65-46c5-4467-a02f-2bbdfd54329a17/06/01 13:03:27 INFO CarbonSparkSqlParser: Parsing command: use defaultException in thread "HiveServer2-Handler-Pool: Thread-84" java.lang.ExceptionInInitializerError at org.apache.spark.sql.hive.CarbonSessionState$$anon$1.<init>(CarbonSessionState.scala:133) at org.apache.spark.sql.hive.CarbonSessionState.analyzer$lzycompute(CarbonSessionState.scala:128) at org.apache.spark.sql.hive.CarbonSessionState.analyzer(CarbonSessionState.scala:127) at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:48) at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:63) at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:592) at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:699) at org.apache.spark.sql.hive.thriftserver.SparkSQLSessionManager.openSession(SparkSQLSessionManager.scala:83) at org.apache.hive.service.cli.CLIService.openSessionWithImpersonation(CLIService.java:202) at org.apache.hive.service.cli.thrift.ThriftCLIService.getSessionHandle(ThriftCLIService.java:351) at org.apache.hive.service.cli.thrift.ThriftCLIService.OpenSession(ThriftCLIService.java:246) at org.apache.hive.service.cli.thrift.TCLIService$Processor$OpenSession.getResult(TCLIService.java:1253) at org.apache.hive.service.cli.thrift.TCLIService$Processor$OpenSession.getResult(TCLIService.java:1238) at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39) at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39) at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:56) at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:286) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745)Caused by: java.lang.NullPointerException at org.apache.spark.sql.hive.CarbonIUDAnalysisRule$.<init>(CarbonAnalysisRules.scala:90) at org.apache.spark.sql.hive.CarbonIUDAnalysisRule$.<clinit>(CarbonAnalysisRules.scala) ... 20 more
issueID:CARBONDATA-1117
type:Sub-task
changed files:
texts:Update SET & RESET command details in online help documentation
Update SET & RESET command details in online help documentation1. update syntax2. Examples with use cases
issueID:CARBONDATA-1118
type:Bug
changed files:
texts:Inset Pushdown in Carbondata.
PushDown Inset Filter in carbon Layer.
issueID:CARBONDATA-1119
type:Bug
changed files:integration/spark-datasource/src/main/scala/org/apache/carbondata/spark/vectorreader/ColumnarVectorWrapper.java
texts:Database drop cascade is not working in Spark 2.1 and alter table not working in vector reader
Database drop cascade is not working in Spark 2.1 And alter table not working when vector reader is enabled.
issueID:CARBONDATA-112
type:Bug
changed files:core/src/main/java/org/carbondata/scan/filter/FilterUtil.java
texts:regexp_replace filter query is failing for carbon table.
In CarbonData, RegExpReplace expression is evaluated at Spark Executor, transient variable in RegExpReplace is null after deserialized in Executor.eg., select imei from carbontable where regexp_replace(imei,'imei','ff') NOT IN ('ff0')RegExpReplace needs to be re-initialized before evaluating at Executor.
issueID:CARBONDATA-1121
type:Bug
changed files:
texts:Restrict Sort Column Addition in Alter Table
Currently Alter table is allowing to create new Sort Column which internally doesn’t make a Sort Column. This Behavior should be restricted. Alter Table should not allow user to create Sort Columns.
issueID:CARBONDATA-1122
type:Bug
changed files:
texts:When user specific operations are performed on multiple terminals, some are failing for missing privileges
When user1 creates a table & user2 wants to alter user1 table, permission missing exception should be displayed.Only Session specific user's hiveClient should be used to perform operations on hive metastore.
issueID:CARBONDATA-1123
type:Improvement
changed files:processing/src/main/java/org/apache/carbondata/processing/store/writer/v3/CarbonFactDataWriterImplV3.java
core/src/main/java/org/apache/carbondata/core/datastore/columnar/BlockIndexerStorageForNoInvertedIndexForShort.java
core/src/main/java/org/apache/carbondata/core/datastore/columnar/ColumnWithRowId.java
core/src/main/java/org/apache/carbondata/core/datastore/columnar/ColumnWithRowIdForHighCard.java
core/src/main/java/org/apache/carbondata/core/datastore/columnar/BlockIndexerStorageForShort.java
texts:Rename interface and variable for RLE encoding
Currently inside RLE encoding implementation, interface and variable is not very readable.
issueID:CARBONDATA-1124
type:Sub-task
changed files:core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/adaptive/AdaptiveCodec.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/DeltaIntegralCodec.java
core/src/main/java/org/apache/carbondata/core/datastore/compression/Compressor.java
core/src/main/java/org/apache/carbondata/core/datastore/page/ColumnPage.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/AdaptiveIntegralCodec.java
core/src/main/java/org/apache/carbondata/core/memory/UnsafeMemoryManager.java
core/src/main/java/org/apache/carbondata/core/datastore/compression/SnappyCompressor.java
processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactDataHandlerColumnar.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/ColumnPageCodec.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/DefaultEncodingFactory.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/compress/DirectCompressCodec.java
core/src/main/java/org/apache/carbondata/core/datastore/page/UnsafeFixLengthColumnPage.java
texts:Use Snappy.rawCompression on unsafe data
After ColumnPage has unsafe implementation, carbon can use Snappy.rawCompress directly on unsafe data
issueID:CARBONDATA-1126
type:Sub-task
changed files:
texts:Change carbon data file definition for encoding override
The encoding metadata should be stored in data file, it requires changes to thrift format definition
issueID:CARBONDATA-113
type:Bug
changed files:
texts:Query issue: select sum(col)+10 for column type decimal(30,10)
create table: CREATE TABLE IF NOT EXISTS t5 (name String, salary decimal(30,10)) STORED BY 'org.apache.carbondata.format'then load dataquery: select sum(salary)+10 from t5query failed, catch exception:org.apache.spark.sql.catalyst.analysis.UnresolvedException: Invalid call to dataType on unresolved object, tree: 'a        at org.apache.spark.sql.catalyst.analysis.UnresolvedAttribute.dataType(unresolved.scala:59)        at org.apache.spark.sql.types.StructType$$anonfun$fromAttributes$1.apply(StructType.scala:335)        at org.apache.spark.sql.types.StructType$$anonfun$fromAttributes$1.apply(StructType.scala:335)        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)        at scala.collection.LinearSeqOptimized$class.foreach(LinearSeqOptimized.scala:60)        at scala.collection.mutable.MutableList.foreach(MutableList.scala:30)        at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)        at scala.collection.AbstractTraversable.map(Traversable.scala:105)        at org.apache.spark.sql.types.StructType$.fromAttributes(StructType.scala:335)        at org.apache.spark.sql.catalyst.plans.QueryPlan.schema$lzycompute(QueryPlan.scala:153)        at org.apache.spark.sql.catalyst.plans.QueryPlan.schema(QueryPlan.scala:153)        at org.apache.spark.sql.execution.SparkPlan$$anonfun$3.apply(SparkPlan.scala:186)        at org.apache.spark.sql.execution.SparkPlan$$anonfun$3.apply(SparkPlan.scala:185)        at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$17.apply(RDD.scala:744)        at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$17.apply(RDD.scala:744)        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:334)        at org.apache.spark.rdd.RDD.iterator(RDD.scala:267)        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:68)        at org.apache.spark.scheduler.Task.run(Task.scala:90)        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:253)        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)        at java.lang.Thread.run(Thread.java:745)
issueID:CARBONDATA-1132
type:Improvement
changed files:
texts:describe formatted query should display SORT_COLUMNS column

issueID:CARBONDATA-1133
type:Bug
changed files:processing/src/main/java/org/apache/carbondata/processing/loading/steps/CarbonRowDataWriterProcessorStepImpl.java
processing/src/main/java/org/apache/carbondata/processing/loading/exception/BadRecordFoundException.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/impl/ThreadStatusObserver.java
processing/src/main/java/org/apache/carbondata/processing/loading/converter/impl/RowConverterImpl.java
texts:Executor lost failure in case of data load failure due to bad records
when we try to do data load with bad records continuously, after some time it is observed that executor is lost due to OOM error and application also gets restarted by yarn after some time. This happens because in case of data load failure due to bad records exception is thrown by the executor and task keeps retrying till the max number of retry attempts are reached. This keeps happening continuously and after some time application is restarted by yarn.
issueID:CARBONDATA-1134
type:Bug
changed files:
texts:Generate redundant folders under integration model when run test cases with mvn command in spark1.6
When run mvn  -Pspark-1.6 -Dspark.version=1.6.3 clean package, it will generate redundant  folders under integration model.
issueID:CARBONDATA-1135
type:Improvement
changed files:
texts:Show partition column information in describe formatted command

issueID:CARBONDATA-1136
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/metadata/schema/table/CarbonTable.java
texts:After compaction, the select query is not showing data
After compaction, the select query is not showing datacreate table part_major_compact(a String, b int) partitioned by (c int) stored by 'carbondata' tblproperties('PARTITION_TYPE'='LIST','LIST_INFO'='1,2')insert into part_major_compact select 'a', 2, 3 from originTable limit 1insert into part_major_compact select 'b', 3, 4 from originTable limit 1insert into part_major_compact select 'c', 4, 5 from originTable limit 1insert into part_major_compact select 'd', 1, 2 from originTable limit 1alter table part_major_compact compact 'major'select * from part_major_compact where c = 4
issueID:CARBONDATA-1137
type:Improvement
changed files:
texts:Documentation for SORT_COLUMNS should be updated in open source doc

issueID:CARBONDATA-1138
type:Bug
changed files:
texts:Exception is expected if SORT_COLUMNS hava duplicate column name

issueID:CARBONDATA-114
type:Bug
changed files:
texts:Decimal Precision and scale getting lost for Complex type columns while describing and querying
While describing/querying the complex type dimension decimal columns system was not able to maintain the scale/precision, always default precision has been taken which needs to be handled.
issueID:CARBONDATA-1144
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/metadata/schema/table/CarbonTable.java
texts:Drop column operation failed in Alter table.
Drop column does not work in Alter table.Steps to reproduce:1: Create a table in Carbon:CREATE TABLE uniqdata (CUST_ID int,CUST_NAME String,ACTIVE_EMUI_VERSION string, DOB timestamp, DOJ timestamp, BIGINT_COLUMN1 bigint,BIGINT_COLUMN2 bigint,DECIMAL_COLUMN1 decimal(30,10), DECIMAL_COLUMN2 decimal(36,10),Double_COLUMN1 double, Double_COLUMN2 double,INTEGER_COLUMN1 int) STORED BY 'org.apache.carbondata.format' TBLPROPERTIES ("TABLE_BLOCKSIZE"= "256 MB");2: Load Data in a table:LOAD DATA INPATH 'hdfs://localhost:54310/2000_UniqData.csv' into table uniqdata OPTIONS('DELIMITER'=',' , 'QUOTECHAR'='"','BAD_RECORDS_ACTION'='FORCE','FILEHEADER'='CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1,Double_COLUMN2,INTEGER_COLUMN1');3: Run the following query: alter table uniqdata drop columns(CUST_NAME);4: Result:0: jdbc:hive2://localhost:10000> alter table uniqdata drop columns(CUST_NAME);Error: java.lang.RuntimeException: Alter table drop column operation failed: null (state=,code=0)Expected Result: Column should be dropped.
issueID:CARBONDATA-1145
type:Bug
changed files:
texts:Single-pass loading not work on partition table

issueID:CARBONDATA-1146
type:Sub-task
changed files:core/src/main/java/org/apache/carbondata/core/scan/collector/impl/RestructureBasedVectorResultCollector.java
core/src/main/java/org/apache/carbondata/core/mutate/TupleIdEnum.java
core/src/main/java/org/apache/carbondata/core/scan/result/BlockletScannedResult.java
core/src/main/java/org/apache/carbondata/core/mutate/DeleteDeltaBlockDetails.java
core/src/main/java/org/apache/carbondata/core/reader/CarbonDeleteFilesDataReader.java
core/src/main/java/org/apache/carbondata/core/scan/collector/impl/DictionaryBasedResultCollector.java
core/src/main/java/org/apache/carbondata/core/mutate/DeleteDeltaBlockletDetails.java
core/src/main/java/org/apache/carbondata/core/scan/collector/impl/DictionaryBasedVectorResultCollector.java
core/src/main/java/org/apache/carbondata/core/scan/collector/impl/RestructureBasedRawResultCollector.java
core/src/main/java/org/apache/carbondata/core/scan/collector/impl/RawBasedResultCollector.java
core/src/main/java/org/apache/carbondata/core/scan/collector/impl/RestructureBasedDictionaryResultCollector.java
texts:V3 format support for delete operation in IUD.
delete operation should handle V3 format. i.e pages of blocklet should be considered in the delete operation.
issueID:CARBONDATA-1149
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/util/DataTypeUtil.java
texts:Fix issue of mismatch type of partition column when specify partition info and range info overlapping values issue

issueID:CARBONDATA-115
type:Bug
changed files:core/src/main/java/org/carbondata/scan/filter/FilterExpressionProcessor.java
texts:Log level updation for better maintainability of logs
The log level has been changed to debug since its been repeatedly getting logged for each block, so if any detail level analysis is requiredThe developer can verify the debug level log to know how much time is taken to generate start and end key.
issueID:CARBONDATA-1150
type:Improvement
changed files:
texts:Update vector reader support in documentation

issueID:CARBONDATA-1151
type:Bug
changed files:
texts:Update useful-tips-on-carbondata.md

issueID:CARBONDATA-1153
type:Bug
changed files:
texts:Can not add column
Sometimes it will throws exception as below. why can't I add column? no one are altering the table... scala> carbon.sql("alter table temp.yuhai_carbon add columns(test1 string)")17/06/11 22:09:13 AUDIT [org.apache.spark.sql.execution.command.AlterTableAddColumns(207) -- main]: [sh-hadoop-datanode-250-104.elenet.me][master][Thread-1]Alter table add columns request has been received for temp.yuhai_carbon17/06/11 22:10:22 ERROR [org.apache.spark.scheduler.TaskSetManager(70) -- task-result-getter-3]: Task 0 in stage 0.0 failed 4 times; aborting job17/06/11 22:10:22 ERROR [org.apache.spark.sql.execution.command.AlterTableAddColumns(141) -- main]: main Alter table add columns failed :Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 3, sh-hadoop-datanode-368.elenet.me, executor 7): java.lang.RuntimeException: Dictionary file test1 is locked for updation. Please try after some time        at scala.sys.package$.error(package.scala:27)        at org.apache.carbondata.spark.util.GlobalDictionaryUtil$.loadDefaultDictionaryValueForNewColumn(GlobalDictionaryUtil.scala:857)        at org.apache.carbondata.spark.rdd.AlterTableAddColumnRDD$$anon$1.<init>(AlterTableAddColumnRDD.scala:83)        at org.apache.carbondata.spark.rdd.AlterTableAddColumnRDD.compute(AlterTableAddColumnRDD.scala:68)        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:331)        at org.apache.spark.rdd.RDD.iterator(RDD.scala:295)        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:88)        at org.apache.spark.scheduler.Task.run(Task.scala:104)        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:351)        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)        at java.lang.Thread.run(Thread.java:745)
issueID:CARBONDATA-1154
type:Bug
changed files:processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactDataHandlerModel.java
integration/presto/src/main/java/org/apache/carbondata/presto/impl/CarbonTableReader.java
texts:Driver Side IUD Performance Optimization
Driver Side IUD Performance Optimization1. Get invalid blocks ony when there is a Update Performed in the Table.2. As UpdateVO is per segment basis no need to call it for each blocks.
issueID:CARBONDATA-1155
type:Bug
changed files:processing/src/main/java/org/apache/carbondata/processing/store/writer/AbstractFactDataWriter.java
texts:DataLoad failure for noDictionarySortColumns with 3Lakh data
CREATE TABLE IF NOT EXISTS flow_carbon_test4(col1            String,col2                  String,col3              String,col4              String,col5              String,col6              String,col7              String,col8         String,col9         String,col10              String,col11              String,col12          String,col13              String,col14               String,col15              String,col16             String,col17             int,col18             int,col19          String,col20          String,col21          String,col22             String,col23             String,col24             String,col25           String,col26              String,col27              String,col28            String,col29             String,col30             String,col31              String,col32              String,col33          String,col34             String,col35             String,col36          String,col37          String,col38         String,col39         String,col40     String,col41                 String,col42              String,col43              String,col44             String,col45             String,col46             String,col47             String,col48             String,col49              String,col50                 decimal(15,2),col51                 decimal(15,2),col52                 String,col53             String,col54         String,col55              String,col56              String,col57              String,col58             String,col59             String,col60     String,col61             String,col62             String,val_dte             String,opp_ac_flg          String,cmb_flg             String,ass_vch_flg         String,col63         String,col64         String,vch_bus_rmk         String,tec_rmk_cde         String,vch_tec_rmk         String,rsv_ara             String,col65     String,col66 String,col67           String,col68            String)STORED BY 'org.apache.carbondata.format'TBLPROPERTIES('DICTIONARY_INCLUDE'='col2,col18,col3,col4,col31,col32,col34,col37,col8,col41,col43,col46,col47,col48,col49,col52,col53,col55,col56,col57,col59,col60,col61,col62,opp_ac_flg,cmb_flg,ass_vch_flg,col63,col64,vch_bus_rmk,tec_rmk_cde,vch_tec_rmk,rsv_ara,col6,col5','DICTIONARY_EXCLUDE'='col15,col16,col19,col20,col21,col22,col23,col24,col10,col25,col26,col11,col27,col14,col1,col28,col29,col30,col33,col35,col36,col38,col39,col40,col9,col42,col44,col45,col54,col58,col13,col12,col7,val_dte,col65,col66,col67,col68','table_blocksize'='1','sort_columns'='col1') LOAD DATA  inpath 'D:/CSVs/20140101_3_3_1.csv' into table flow_carbon_test4 options('DELIMITER'=',', 'QUOTECHAR'='"','FILEHEADER'='col15,col16,col17,col18,col19,col20,col21,col22,col23,col24,col10,col25,col26,col11,col27,col14,col1,col28,col29,col3,col4,col30,col31,col32,col33,col34,col35,col36,col37,col8,col38,col39,col40,col9,col41,col42,col43,col44,col45,col46,col47,col48,col49,col50,col51,col52,col53,col54,col55,col56,col57,col58,col13,col12,col7,col59,col60,col61,col62,val_dte,opp_ac_flg,cmb_flg,ass_vch_flg,col63,col64,vch_bus_rmk,tec_rmk_cde,vch_tec_rmk,rsv_ara,col6,col5,col65,col66,col67,col68,col2','sort_scope'='BATCH_SORT','batch_sort_size_inmb'='64')Error: java.lang.Exception: DataLoad failure: There is an unexpected error: There is an unexpected error while closing data handler (state=,code=0)
issueID:CARBONDATA-1156
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/scan/executor/infos/BlockExecutionInfo.java
core/src/main/java/org/apache/carbondata/core/scan/scanner/impl/BlockletFullScanner.java
core/src/main/java/org/apache/carbondata/core/mutate/CarbonUpdateUtil.java
core/src/main/java/org/apache/carbondata/core/datastore/block/AbstractIndex.java
core/src/main/java/org/apache/carbondata/core/mutate/DeleteDeltaBlockletDetails.java
core/src/main/java/org/apache/carbondata/core/scan/collector/impl/DictionaryBasedResultCollector.java
core/src/main/java/org/apache/carbondata/core/scan/executor/impl/AbstractQueryExecutor.java
core/src/main/java/org/apache/carbondata/core/datastore/block/TableBlockInfo.java
core/src/main/java/org/apache/carbondata/core/scan/scanner/impl/BlockletFilterScanner.java
core/src/main/java/org/apache/carbondata/core/scan/result/iterator/AbstractDetailQueryResultIterator.java
core/src/main/java/org/apache/carbondata/core/scan/collector/impl/RestructureBasedRawResultCollector.java
core/src/main/java/org/apache/carbondata/core/scan/executor/infos/DeleteDeltaInfo.java
core/src/main/java/org/apache/carbondata/core/scan/collector/impl/RawBasedResultCollector.java
core/src/main/java/org/apache/carbondata/core/scan/collector/impl/RestructureBasedDictionaryResultCollector.java
core/src/main/java/org/apache/carbondata/core/mutate/DeleteDeltaVo.java
core/src/main/java/org/apache/carbondata/core/scan/result/BlockletScannedResult.java
core/src/main/java/org/apache/carbondata/core/reader/CarbonDeleteFilesDataReader.java
core/src/main/java/org/apache/carbondata/hadoop/CarbonInputSplit.java
integration/hive/src/main/java/org/apache/carbondata/hive/CarbonHiveInputSplit.java
texts:IUD Performance Improvement And Synchonizaion issue
Delete delta file loading is taking more time as it is read for blocklet level. Now added code to read block level.In current IUD design delete delta files are getting listed for each block in executor level in case of parallel query and iud operation it may give wrong result. Now passing delete delta information from driver to executor
issueID:CARBONDATA-1158
type:Sub-task
changed files:integration/hive/src/main/java/org/apache/carbondata/hive/MapredCarbonInputFormat.java
integration/hive/src/main/java/org/apache/carbondata/hive/CarbonStorageFormatDescriptor.java
integration/hive/src/main/java/org/apache/carbondata/hive/CarbonArrayInspector.java
integration/hive/src/main/java/org/apache/carbondata/hive/CarbonObjectInspector.java
integration/hive/src/main/java/org/apache/carbondata/hive/test/server/HiveEmbeddedServer2.java
integration/hive/src/main/java/org/apache/carbondata/hive/MapredCarbonOutputFormat.java
integration/hive/src/main/java/org/apache/carbondata/hive/CarbonHiveInputSplit.java
integration/hive/src/main/java/org/apache/carbondata/hive/CarbonHiveRecordReader.java
integration/hive/src/main/java/org/apache/carbondata/hive/CarbonHiveSerDe.java
texts:Hive integration code optimization
Hive integration code optimization:1. Remove redundant and unused code.2. Optimize some codea) Convert some internal functions from public to private.b) Fix some code which may generate error.c) Change code as per java code style.
issueID:CARBONDATA-1159
type:Bug
changed files:
texts:Batch sort loading is not proper without synchronization

issueID:CARBONDATA-116
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
core/src/main/java/org/apache/carbondata/core/statusmanager/LoadMetadataDetails.java
integration/spark-common/src/main/java/org/apache/carbondata/spark/merger/CarbonDataMergerUtil.java
texts:major compacted segments are considered for minor also
once a segment is formed by major compaction, that segment can not be considered again for minor compaction.example :If seg 0 and seg 1  is compacted into 0.1 using major compaction.then segment 2 and 3 is loaded and if the minor threshold is 2 . then if i trigger minor compaction , then segment 0.1 should not be considered for minor compaction.; instead 2 and 3 should be considered and be merged into 2.1.
issueID:CARBONDATA-1163
type:Sub-task
changed files:processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactDataHandlerModel.java
processing/src/main/java/org/apache/carbondata/processing/loading/steps/DataConverterProcessorStepImpl.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/SortParameters.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/SortObserver.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/SortScopeOptions.java
processing/src/main/java/org/apache/carbondata/processing/util/CarbonDataProcessorUtil.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
processing/src/main/java/org/apache/carbondata/processing/loading/steps/DataWriterProcessorStepImpl.java
processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactDataHandlerColumnar.java
core/src/main/java/org/apache/carbondata/core/datastore/row/CarbonRow.java
processing/src/main/java/org/apache/carbondata/processing/loading/DataLoadProcessBuilder.java
processing/src/main/java/org/apache/carbondata/processing/loading/model/CarbonLoadModel.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/merger/UnsafeSingleThreadFinalSortFilesMerger.java
texts:Use sortBy operator to load data

issueID:CARBONDATA-1164
type:Improvement
changed files:
texts:Make Column Group feature deprecated
After discussion in community (http://apache-carbondata-dev-mailing-list-archive.1130556.n5.nabble.com/About-ColumnGroup-feature-td14436.html), we conclude that column group feature will be deprecated.
issueID:CARBONDATA-1165
type:Bug
changed files:processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/IntermediateFileMerger.java
texts:Class Cast exception in intermediate file merger when loading data
query: spark.sql("CREATE TABLE ORDERS ( O_ORDERKEY INT ,\n O_CUSTKEY INT ,\n O_ORDERSTATUS STRING ," +              "\n O_TOTALPRICE DECIMAL(15,2) ,\n O_ORDERDATE TIMESTAMP ,\n O_ORDERPRIORITY STRING" +              " , \n O_CLERK STRING , \n O_SHIPPRIORITY INT ,\n O_COMMENT STRING ) STORED BY " +              "'carbondata'")  spark.sql("LOAD DATA INPATH \"hdfs://localhost:54310/user1/orders.csv\" INTO TABLE orders " +              "OPTIONS('DELIMITER'='|' , 'QUOTECHAR'='\"','FILEHEADER'='O_ORDERKEY,O_CUSTKEY," +              "O_ORDERSTATUS,O_TOTALPRICE,O_ORDERDATE,O_ORDERPRIORITY,O_CLERK,O_SHIPPRIORITY," +              "O_COMMENT')")logs:java.lang.ClassCastException: java.lang.Integer cannot be cast to java.lang.Long at org.apache.carbondata.processing.sortandgroupby.sortdata.IntermediateFileMerger.writeDataTofile(IntermediateFileMerger.java:347) at org.apache.carbondata.processing.sortandgroupby.sortdata.IntermediateFileMerger.call(IntermediateFileMerger.java:112) at org.apache.carbondata.processing.sortandgroupby.sortdata.IntermediateFileMerger.call(IntermediateFileMerger.java:37) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745)
issueID:CARBONDATA-1166
type:Bug
changed files:
texts:creating partition on decimal column is failing

issueID:CARBONDATA-1167
type:Bug
changed files:processing/src/main/java/org/apache/carbondata/processing/loading/sort/impl/ParallelReadMergeSorterWithBucketingImpl.java
processing/src/main/java/org/apache/carbondata/processing/merger/CarbonCompactionUtil.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/impl/UnsafeParallelReadMergeSorterWithBucketingImpl.java
texts:Mismatched between class name and logger class name
Mismatched between class name and logger class name are detected:ReverseDictionaryCache.java, HDFSCarbonFileTest.java, LoadMetadataDetailsUnitTest.java, CarbonCompactionUtil.java, LogServiceFactoryTest_UT.javaParallelReadMergeSorterWithBucketingImpl.javaUnsafeParallelReadMergeSorterWithBucketingImpl.java
issueID:CARBONDATA-117
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/scan/executor/infos/BlockExecutionInfo.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
core/src/main/java/org/apache/carbondata/core/util/DataFileFooterConverter.java
core/src/main/java/org/apache/carbondata/core/scan/executor/impl/AbstractQueryExecutor.java
core/src/main/java/org/apache/carbondata/core/datastore/block/TableBlockInfo.java
core/src/main/java/org/apache/carbondata/core/datastore/block/BlockletInfos.java
core/src/main/java/org/apache/carbondata/core/scan/result/iterator/AbstractDetailQueryResultIterator.java
core/src/main/java/org/apache/carbondata/hadoop/CarbonInputSplit.java
hadoop/src/main/java/org/apache/carbondata/hadoop/CarbonRecordReader.java
integration/spark-common/src/main/java/org/apache/carbondata/spark/load/CarbonLoaderUtil.java
texts:BlockLet distribution for optimum resource usage
*Problem: *In present scenario during querying data, the data scan can be done minimum at the block level.Suppose if the system can execute 10 parallel task and number of blocks to be scanned is 3, then only 3 cores could be utilised remaining 7 will be un-utilized.Solution:To optimize the resouce usage the blocks can be further divided into multiple blocks.A block consist of multiple blocklets, these blocklet could be distributed into multiple logical blocks by assigning the  startBlockletNumber and no of BlockLetsTobeScanned.
issueID:CARBONDATA-1170
type:Bug
changed files:
texts:Skip single_pass loading during first load
The user should not be able to load using "single_pass"="true" for the first load instead the load would be restarted with "single_pass"="false"
issueID:CARBONDATA-1171
type:Bug
changed files:
texts:Add support for show partition DDL

issueID:CARBONDATA-1172
type:Bug
changed files:
texts:Batch load fails randomly

issueID:CARBONDATA-1173
type:Sub-task
changed files:
texts:Streaming Ingest: Write path framework implementation
Carbondata with Spark Structured streaming write path framework   Carbondata StreamingOutputWriter, StreamingRecordWriter, metadata writer   classes, etc initial framework for streaming ingest feature
issueID:CARBONDATA-1174
type:Sub-task
changed files:
texts:Streaming Ingest: Write path schema validation/inference
Streaming Ingest: Write path  schema validation / schema inference from existing carbondata table streaming ingest allowed to existing tables only
issueID:CARBONDATA-1175
type:Sub-task
changed files:
texts:Streaming Ingest: Write path data conversion/transformation
Streaming Ingest: Write path Data conversion/Transformation input data is a byte stream in catalyst InternalRow format (row major), which needs to be converted to column format column converter and corresponding iterators needs to be created before invoking carbon layer load path various carbon properties needs to be set, for example SORT_SCOPE (to skip sorting), blocket size, skip global dictionary creation etc
issueID:CARBONDATA-1176
type:Sub-task
changed files:
texts:Streaming Ingest: Write path streaming segment/file creation
Streaming Ingest: Write path : segment /streaming file  Streaming segment creation and streaming file creation resolve conflict with spark structured streaming file names. Spark structured streaming names streaming files with unique batch id to avoid overwriting maintain spark structured streaming recover-ability as streaming file names generated by spark structured streaming are unique and recorded in spark structured streaming metadata.
issueID:CARBONDATA-1177
type:Bug
changed files:
texts:Fixed batch sort synchronization issue

issueID:CARBONDATA-1178
type:Bug
changed files:
texts:Data loading of partitioned table is throwing NPE on badrecords

issueID:CARBONDATA-1179
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/util/ObjectSizeCalculator.java
core/src/main/java/org/apache/carbondata/core/cache/CarbonLRUCache.java
core/src/main/java/org/apache/carbondata/core/cache/dictionary/AbstractColumnDictionaryInfo.java
core/src/main/java/org/apache/carbondata/core/reader/CarbonDictionaryMetadataReader.java
core/src/main/java/org/apache/carbondata/core/cache/dictionary/DictionaryInfo.java
texts:Improve the Object Size calculation for Objects added to LRU cache
Java Object model has a bigger overhead when loading the objects into memory.The current way of estimating the object size by looking at the file size is not correct and gives wrong results.Moreover due to this calculation, we are storing more than the configured size for LRU cache.Improve the ObjectSize calculation by using the spark SizeEstimator utility
issueID:CARBONDATA-118
type:Bug
changed files:processing/src/main/java/org/apache/carbondata/processing/util/CarbonDataProcessorUtil.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/SortDataRows.java
integration/spark-common/src/main/java/org/apache/carbondata/spark/load/CarbonLoaderUtil.java
texts:clean up of temp files in compaction
The temp files which are written in sort step should be cleaned up after the process of compaction.
issueID:CARBONDATA-1180
type:Bug
changed files:
texts:loading data failed for dictionary file id is locked for updation
use Spark 2.1 in yarn-client mode and query from beeline to spark sql thriftserverCREATE TABLE IF NOT EXISTS carbondata_test(id string, name string, city string, age Int) STORED BY 'carbondata';LOAD DATA INPATH 'hdfs:///user/sample-data/sample.csv' INTO TABLE carbondata_test;Data load is failed for following exception.java.lang.RuntimeException: Dictionary file id is locked for updation. Please try after some time +detailsjava.lang.RuntimeException: Dictionary file id is locked for updation. Please try after some time at scala.sys.package$.error(package.scala:27) at org.apache.carbondata.spark.rdd.CarbonGlobalDictionaryGenerateRDD$$anon$1.<init>(CarbonGlobalDictionaryRDD.scala:407) at org.apache.carbondata.spark.rdd.CarbonGlobalDictionaryGenerateRDD.compute(CarbonGlobalDictionaryRDD.scala:345) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) at org.apache.spark.rdd.RDD.iterator(RDD.scala:287) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87) at org.apache.spark.scheduler.Task.run(Task.scala:99) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) at java.lang.Thread.run(Thread.java:745)The 1.2.0 contains the fix in CARBONDATA-614.Any suggestion about this problem? Thanks~
issueID:CARBONDATA-1181
type:Sub-task
changed files:
texts:9. show partitions
show table partition information
issueID:CARBONDATA-1183
type:Bug
changed files:
texts:Update CarbonPartitionTable because partition columns should not be specified in the schema

issueID:CARBONDATA-1186
type:Bug
changed files:
texts:Class Cast Exception In SortStepRowutil when loading data

issueID:CARBONDATA-1187
type:Bug
changed files:
texts:Fix Documentation links pointing to wrong urls in useful-tips-on-carbondata and faq

issueID:CARBONDATA-1188
type:Bug
changed files:
texts:Incorrect data is displayed for double data type
create table Comp_VMALL_DICTIONARY_EXCLUDE (imei string,gamePointId double)  STORED BY 'org.apache.carbondata.format' TBLPROPERTIES('DICTIONARY_EXCLUDE'='imei')LOAD DATA INPATH  '/home/kunal/Downloads/100.csv' INTO table Comp_VMALL_DICTIONARY_EXCLUDE options ('DELIMITER'=',', 'QUOTECHAR'='"', 'BAD_RECORDS_ACTION'='FORCE','FILEHEADER'='imei,gamePointId')select * from Comp_VMALL_DICTIONARY_EXCLUDE
issueID:CARBONDATA-1189
type:Bug
changed files:
texts:Delete with subquery is not working in spark 2.1
Delete with suquery not working in spark 2.1.Example query :delete from  iud_db.dest where c1 IN (select c11 from source2 where c11 = 'b'
issueID:CARBONDATA-119
type:Bug
changed files:
texts:zookeeper lock is not working at executor for dictionary locking
Dictionary generation happens at executor side. Currently zookeeper is initialised at driver side and not at executor side.We need to pass the zookeeper detail to executor to iniialise.We need to pass hdfs temp location also to hdfs in case of hdfs lock.
issueID:CARBONDATA-1191
type:Bug
changed files:
texts:Remove carbon-spark-shell script

issueID:CARBONDATA-1192
type:Bug
changed files:
texts:Unable to Select Data From more than one table in hive
inside spark shellcarbon.sql("DROP TABLE IF EXISTS CUSTOMER ")    carbon.sql("CREATE TABLE CUSTOMER ( C_CUSTKEY INT ,\n C_NAME STRING ,\n C_ADDRESS STRING ,\n " +           "C_NATIONKEY INT ,\n C_PHONE STRING ,\n C_ACCTBAL DECIMAL(15,2) ,\n C_MKTSEGMENT " +           "STRING ,\n C_COMMENT STRING ) STORED BY 'carbondata' ")    carbon.sql("LOAD DATA INPATH \"hdfs://localhost:54310/user1/customer.csv\" INTO TABLE customer " +              "OPTIONS('DELIMITER'='|' , 'QUOTECHAR'='\"' , 'FILEHEADER'='C_CUSTKEY,C_NAME," +              "C_ADDRESS,C_NATIONKEY,C_PHONE,C_ACCTBAL,C_MKTSEGMENT,C_COMMENT')") carbon.sql("DROP TABLE IF EXISTS ORDERS ")carbon.sql("CREATE TABLE ORDERS ( O_ORDERKEY INT ,O_CUSTKEY INT ,O_ORDERSTATUS STRING ,O_TOTALPRICE DECIMAL(15,2) , O_ORDERDATE TIMESTAMP , O_ORDERPRIORITY STRING , O_CLERK STRING , O_SHIPPRIORITY INT , O_COMMENT STRING ) STORED BY 'carbondata' ")    carbon.sql("LOAD DATA INPATH 'hdfs://localhost:54310/user1/orders.csv' INTO TABLE orders " +              "OPTIONS('DELIMITER'='|' , 'QUOTECHAR'='\"','FILEHEADER'='O_ORDERKEY,O_CUSTKEY," +              "O_ORDERSTATUS,O_TOTALPRICE,O_ORDERDATE,O_ORDERPRIORITY,O_CLERK,O_SHIPPRIORITY," +              "O_COMMENT')")read data from hive shellhive> select o_custkey,c_custkey from orders,customer limit 2;Warning: Shuffle Join JOIN&#91;4&#93;[tables = &#91;orders, customer&#93;] in Stage 'Stage-1:MAPRED' is a cross productQuery ID = hduser_20170619125257_d889efa9-261f-436e-9489-fd15d6b76bebTotal jobs = 1Stage-1 is selected by condition resolver.Launching Job 1 out of 1Number of reduce tasks determined at compile time: 1In order to change the average load for a reducer (in bytes):  set hive.exec.reducers.bytes.per.reducer=<number>In order to limit the maximum number of reducers:  set hive.exec.reducers.max=<number>In order to set a constant number of reducers:  set mapreduce.job.reduces=<number>Job running in-process (local Hadoop)2017-06-19 12:53:01,987 Stage-1 map = 0%,  reduce = 0%2017-06-19 12:53:49,113 Stage-1 map = 38%,  reduce = 0%2017-06-19 12:53:51,127 Stage-1 map = 100%,  reduce = 0%Ended Job = job_local1708233203_0001 with errorsError during job, obtaining debugging information...Job Tracking URL: http://localhost:8080/FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.mr.MapRedTaskMapReduce Jobs Launched: Stage-Stage-1:  HDFS Read: 12033731 HDFS Write: 0 FAILTotal MapReduce CPU Time Spent: 0 msec
issueID:CARBONDATA-1194
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/scan/result/BlockletScannedResult.java
texts:Problem in filling/processing multiple implicit columns
If carbon.enable.vector.reader = trueCurrently we are not handling multiple implicit columns .Support Multiple implicit columns.
issueID:CARBONDATA-1195
type:Bug
changed files:
texts:Rectification in configuration-parameters.md

issueID:CARBONDATA-1196
type:Improvement
changed files:processing/src/main/java/org/apache/carbondata/processing/util/CarbonDataProcessorUtil.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
core/src/main/java/org/apache/carbondata/core/util/CarbonProperties.java
processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactDataHandlerColumnar.java
texts:Add 3 Bytes data type support in value compression
If there is an integer column and its max value is less than Math.pow(2, 23), and min value bigger than -Math.pow(2, 23), we can apply value compression to 3 bytes
issueID:CARBONDATA-1197
type:Bug
changed files:
texts:Update related docs which still use incubating such as presto integration
Update related docs which still use incubating.Just update the references links, file name, directory name, etc.
issueID:CARBONDATA-1198
type:Improvement
changed files:
texts:Change Unsafe configuration to dynamic
Currently unsafe column page configuration is a static config, it should support dynamic config
issueID:CARBONDATA-1199
type:Improvement
changed files:
texts:Change Unsafe configuration to dynamic
Currently unsafe column page configuration is a static config, it should support dynamic config
issueID:CARBONDATA-12
type:Bug
changed files:
texts:Carbon data load bad record log file not renamed form inprogress to normal .log

issueID:CARBONDATA-120
type:Bug
changed files:
texts:Explain extended carbon command is failing
Explain carbon commands(expect select statements and hive compatible statements) are throwing parse failed exception.eg., explain show segments for table carbontable
issueID:CARBONDATA-1204
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/scan/scanner/impl/BlockletFullScanner.java
texts:Update operation fail and generate extra records when test with big data

issueID:CARBONDATA-1205
type:Task
changed files:
texts:Use Spark 2.1 as default from 1.2.0 onwards
From 1.2.0, there are many features developing based on Spark 2.1,  this task is to use Spark2.1 as default compilation in parent pom.Discussion session at : https://lists.apache.org/thread.html/5b186b5868de16280ced1b623fae8b5c54933def44398f6a4310ffb3@%3Cdev.carbondata.apache.org%3E
issueID:CARBONDATA-1207
type:Bug
changed files:
texts:Resource leak problem in CarbonDictionaryWriter
If in load during dictionary generation some exception happens during the dictionary writing or dictionary meta data writing, then stream will never be closed.This will load DOS on the incremental load.
issueID:CARBONDATA-1209
type:Sub-task
changed files:
texts:12. Add partitionId in show partition result

issueID:CARBONDATA-121
type:Bug
changed files:integration/spark-common/src/main/java/org/apache/carbondata/spark/merger/CarbonDataMergerUtil.java
texts:Need to check the validity of segments before compaction.
issue1 : Need to check the validity of segments before compaction. issue 2 : Major compaction should stop after compaction of all the segments which are present at the time of triggering the compaction.
issueID:CARBONDATA-1210
type:Bug
changed files:processing/src/main/java/org/apache/carbondata/processing/loading/BadRecordsLogger.java
texts:Exception should be thrown on bad record logger failure to write to log file or csv file.

issueID:CARBONDATA-1211
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/scan/result/BlockletScannedResult.java
texts:Implicit Column Projection
Garbage values coming when projection is being done on implicit column i.e. tupleId. Only occurs when vector reader is enabled.
issueID:CARBONDATA-1212
type:Bug
changed files:processing/src/main/java/org/apache/carbondata/processing/merger/CarbonCompactionExecutor.java
texts:Memory leak in case of compaction when unsafe is true
In case of compaction, queryExecutor object is formed for multiple blocks but the objects are not retained and finish method is called only on the last instance created for query executor. Due to this the memory allocated to precious objects is not released which can lead to out of memory issue.Steps to reproduce:----------------------------------CREATE TABLE IF NOT EXISTS t3 (ID Int, date Date, country String, name String, phonetype String, serialname char(10), salary Int) STORED BY 'carbondata' TBLPROPERTIES('DICTIONARY_EXCLUDE'='name')LOAD DATA LOCAL INPATH 'data.csv' into table t3LOAD DATA LOCAL INPATH 'data.csv' into table t3alter table t3 compact 'major'
issueID:CARBONDATA-1213
type:Bug
changed files:processing/src/main/java/org/apache/carbondata/processing/util/CarbonDataProcessorUtil.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
core/src/main/java/org/apache/carbondata/core/util/CarbonProperties.java
processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactDataHandlerColumnar.java
texts:Removed rowCountPercentage check and fixed IUD data load issue
Problems:1. Row count percentage not required with high cardinality threshold check2. IUD returning incorrect results in case of update on high cardinality columnAnalysis:1. In case a column is identified as high cardinality column still it is not getting converted to no dictionary column because of another parameter check called rowCountPercentage. Default value of rowCountPercentage is 80%. Due to this even though high cardinality column is identified, if it is less than 80% of the total number of rows it will be treated as dictionary column. This can still lead to executor lost failure due to memory constraints.2. RLE on a column is not being set correctly and due to incorrect code design RLE applicable on a column is decided by a different part of code from the one which is actually applying the RLE on a column. Because of this Footer is getting filled with incorrect RLE information and query is failing.
issueID:CARBONDATA-1214
type:Improvement
changed files:
texts:Change the syntax of the Segment Delete by ID and date as per hive syntax.
change the syntax of delete by id and date as per hive syntax.Ex : delete from table carbon where segment.id in (0)delete from table ignoremajor where segment.starttime  before '2099-07-28 11:00:00'
issueID:CARBONDATA-1217
type:Bug
changed files:processing/src/main/java/org/apache/carbondata/processing/loading/BadRecordsLogger.java
core/src/main/java/org/apache/carbondata/core/metadata/CarbonTableIdentifier.java
processing/src/main/java/org/apache/carbondata/processing/loading/DataLoadExecutor.java
texts:Failure in data load when we first load the bad record and then valid record and bad record action is set to Fail
When we load bad record into the table and bad record action is set to "FAIL", then data load will fail. During load a bad record logger static map is maintained which holds the key for bad record. When data load fails due to bad record exception is thrown and key from bad record logger static map is not cleared because of which when valid data is loaded next time data load fails because the key still exists in the map.Steps to reproduce-------------------------------Execute the below commands in sequence in the same session.create table bigtab (val string, bal int) STORED BY 'carbondata'load data  inpath 'bigtabbad.csv' into table bigtab options('DELIMITER'=',','QUOTECHAR'='"','BAD_RECORDS_ACTION'='FAIL','FILEHEADER'='val,bal')load data  inpath 'bigtab.csv' into table bigtab options('DELIMITER'=',','QUOTECHAR'='"','BAD_RECORDS_ACTION'='FAIL','FILEHEADER'='val,bal')
issueID:CARBONDATA-1218
type:Bug
changed files:processing/src/main/java/org/apache/carbondata/processing/util/CarbonBadRecordUtil.java
processing/src/main/java/org/apache/carbondata/processing/loading/steps/DataConverterProcessorStepImpl.java
processing/src/main/java/org/apache/carbondata/processing/util/CarbonDataProcessorUtil.java
processing/src/main/java/org/apache/carbondata/processing/loading/BadRecordsLogger.java
streaming/src/main/java/org/apache/carbondata/streaming/CarbonStreamRecordWriter.java
processing/src/main/java/org/apache/carbondata/processing/loading/BadRecordsLoggerProvider.java
processing/src/main/java/org/apache/carbondata/processing/loading/DataLoadExecutor.java
texts:In case of data-load failure the BadRecordsLogger.badRecordEntry map holding the task Status is not removing the task Entry.
In case of data-load failure the BadRecordsLogger.badRecordEntry map holding the task Status is not removing the task Entry.Because of this the next load is getting  failed even though the data being loaded has no bad records.The map entry must be removed after load completion either success or fail.
issueID:CARBONDATA-1219
type:Bug
changed files:
texts:Documentation - not supported high.cardinality.row.count.percentage
The parameter high.cardinality.row.count.percentage should be removed and all the reference also needs to be removed.
issueID:CARBONDATA-122
type:Improvement
changed files:processing/src/main/java/org/apache/carbondata/processing/csvreaderstep/BlockDetails.java
texts:Provide second and third location preference to spark
Currently as there is grouping of blocks while distribution, only first option of preferred location is given to spark. So Identify the second and third option to schedule.
issueID:CARBONDATA-1220
type:Bug
changed files:
texts:Decimal values are not displayed correctly in presto
I created a table in carbondata having decimal values, when I tried to display all the rows in presto, the scale of the values gets changed.Below are the details of the query:Create table in carbondata:create table decimalOperatorCheck(name String, ids Decimal(10,2)) stored by 'carbondata';Load data:load data inpath 'hdfs://localhost:54311/testFiles/decimaldata.csv' into table decimalOperatorCheck options('delimiter'=',','fileheader'='name,ids');Output in Carbondata:0: jdbc:hive2://localhost:10000> select * from decimaloperatorcheck;------------------  name       ids    ------------------ Alex     123.45     Josh     233.34     Justin   11.90      Ryan     12345.56   name     NULL      ------------------5 rows selected (21.983 seconds)Output in presto:presto:sparkdata> select * from decimaloperatorcheck;  name  |  ids   -------+------- Alex   | 1.23    Josh   | 2.33    Justin | 0.12    Ryan   | 123.46  name   | NULL   (5 rows)
issueID:CARBONDATA-1221
type:Bug
changed files:
texts:DOCUMENTATION - Remove unsupported parameter
The following two parameter is not supported anymore need to be removed from the document:carbon.inmemory.record.sizeno.of.cores.to.load.blocks.in.driver
issueID:CARBONDATA-1222
type:Bug
changed files:
texts:Residual files created from Update are not deleted after clean operation
Spark - sql:1.Create a tablecreate table t_carbn31(item_code string,item_name1 string) stored by 'carbondata'2.Load Datainsert into t_carbn31 select 'a1','Phone';insert into t_carbn31 select 'a2','Router';3.Update the tableupdate t_carbn31 set(item_name1)=('Mobile') where item_code ='a1'update t_carbn31 set(item_name1)=('USB') where item_code ='a2'update t_carbn31 set(item_name1)=('General') where item_code !='a1'4.Run clean files on the tableclean files for table t_carbn31Expected output: clean files should remove the residual carbondata and delete filesActual output : Residual files are not cleaned.
issueID:CARBONDATA-1223
type:Bug
changed files:processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/merger/UnsafeSingleThreadFinalSortFilesMerger.java
texts:Fixing empty file creation in batch sort loading

issueID:CARBONDATA-1224
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/scan/executor/infos/BlockExecutionInfo.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/impl/MeasureRawColumnChunk.java
processing/src/main/java/org/apache/carbondata/processing/merger/CarbonCompactionExecutor.java
core/src/main/java/org/apache/carbondata/core/scan/scanner/impl/BlockletFullScanner.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/reader/dimension/v3/DimensionChunkReaderV3.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/AbstractRawColumnChunk.java
core/src/main/java/org/apache/carbondata/core/scan/scanner/impl/BlockletFilterScanner.java
core/src/main/java/org/apache/carbondata/core/scan/executor/impl/AbstractQueryExecutor.java
core/src/main/java/org/apache/carbondata/core/scan/result/iterator/AbstractDetailQueryResultIterator.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/reader/measure/v3/MeasureChunkPageReaderV3.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockletDataRefNode.java
core/src/main/java/org/apache/carbondata/core/datastore/impl/FileReaderImpl.java
core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/impl/DimensionRawColumnChunk.java
core/src/main/java/org/apache/carbondata/core/datastore/impl/DFSFileReaderImpl.java
core/src/main/java/org/apache/carbondata/core/scan/result/BlockletScannedResult.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
core/src/main/java/org/apache/carbondata/core/datastore/FileReader.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/reader/CarbonDataReaderFactory.java
core/src/main/java/org/apache/carbondata/core/scan/model/QueryModel.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/reader/dimension/v3/DimensionChunkPageReaderV3.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/reader/measure/v3/MeasureChunkReaderV3.java
texts:Going out of memory if more segments are compacted at once in V3 format
In V3 format we read the whole blocklet at once to memory in order save IO time. But it turns out  to be costlier in case of parallel reading of more carbondata files. For example if we need to compact 50 segments then compactor need to open the readers on all the 50 segments to do merge sort. But the memory consumption is too high if each reader reads whole blocklet to the memory and there is high chances of going out of memory.Solution:In this type of scenarios we can introduce new readers for V3 to read the data page by page instead of reading whole blocklet at once to reduce the memory footprint.
issueID:CARBONDATA-1228
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/DefaultEncodingFactory.java
texts:the query result of double is not correct

issueID:CARBONDATA-1229
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/locks/CarbonLockUtil.java
texts:Restrict Drop table if load is in progress
As drop table and dataload can be parallel, if table is dropped and recreated, dataload will write stale data and dictionary files making the table state unstable.So allow dropping of table only if no update/dataload is happening in system.
issueID:CARBONDATA-123
type:Bug
changed files:
texts:Stored by &#39;carbondata&#39; or &#39;org.apache.carbondata.format&#39; shoulb be not case senstive

issueID:CARBONDATA-1231
type:Sub-task
changed files:
texts:Add datamap interfaces for pruning and indexing
Add datamap interfaces for pruning and indexing
issueID:CARBONDATA-1236
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
texts:Support absolute path without scheme in loading

issueID:CARBONDATA-1238
type:Improvement
changed files:integration/hive/src/main/java/org/apache/carbondata/hive/MapredCarbonInputFormat.java
core/src/main/java/org/apache/carbondata/core/scan/executor/infos/BlockExecutionInfo.java
processing/src/main/java/org/apache/carbondata/processing/merger/CarbonCompactionExecutor.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonTableInputFormat.java
core/src/main/java/org/apache/carbondata/core/util/DataTypeConverter.java
core/src/main/java/org/apache/carbondata/core/scan/collector/impl/AbstractScannedResultCollector.java
core/src/main/java/org/apache/carbondata/core/util/DataTypeUtil.java
core/src/main/java/org/apache/carbondata/core/scan/model/QueryModel.java
core/src/main/java/org/apache/carbondata/core/scan/executor/impl/AbstractQueryExecutor.java
integration/hive/src/main/java/org/apache/carbondata/hive/CarbonHiveRecordReader.java
integration/spark-datasource/src/main/scala/org/apache/carbondata/converter/SparkDataTypeConverterImpl.java
core/src/main/java/org/apache/carbondata/core/scan/collector/impl/RawBasedResultCollector.java
core/src/main/java/org/apache/carbondata/core/util/DataTypeConverterImpl.java
texts:Decouple the datatype convert from Spark code in core module
Decouple the datatype convert from Spark code in core module：1.Use decimal(org.apache.spark.sql.types.Decimal.apply()) in Spark engine, use java's decimal in other engines.2.Use org.apache.spark.unsafe.types.UTF8String in Spark engine, use String in other engines.
issueID:CARBONDATA-124
type:Bug
changed files:
texts:Exception thrown while executing drop table command in spark-sql cli
Analysis: Whenever drop table operation is performed, first the cube is deleted from carbon metastore and then command is executed to delete the same table from hive metastore. While deleting from hive metastore the call comes back to carbon metastore catalog where we again check for table existence but table is not found and exception is thrown. Call comes again to carbon because the command is executed as spark sql command and not as hive sql command and we catch the logical plan when drop command is executed as spark sql command.Impact: drop table operation
issueID:CARBONDATA-1241
type:Improvement
changed files:processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactDataHandlerColumnar.java
texts:Single_Pass either should be blocked with Global_Sort

issueID:CARBONDATA-1242
type:Bug
changed files:processing/src/main/java/org/apache/carbondata/processing/util/CarbonLoaderUtil.java
texts:Query block distribution is more time before scheduling tasks to executor.
select * from issue2_2 limit 14Expect Output by Submitter: - Query performance should be equal to executor execution timeActual Output shown currently: - The E2E time is 56.545 seconds,  but the executor time is 0.7 seconds ( tow job: 0.2 seconds + 0.5 seconds)
issueID:CARBONDATA-1244
type:Improvement
changed files:integration/presto/src/main/java/org/apache/carbondata/presto/impl/CarbonLocalInputSplit.java
integration/presto/src/main/java/org/apache/carbondata/presto/impl/CarbonTableCacheModel.java
integration/presto/src/main/java/org/apache/carbondata/presto/impl/CarbonTableReader.java
integration/presto/src/main/java/org/apache/carbondata/presto/CarbondataConnectorFactory.java
texts:Rewrite README.md of presto integration and add/rewrite some comments to presto integration.
Rewrite README.md of presto integration and add some comments to presto integration.Make the README easier for starters to play with. Write more comments for the source code to make it more readable.
issueID:CARBONDATA-1245
type:Bug
changed files:
texts:NullPointerException invoked by CarbonFile.listFiles() function which returns null
In the implementation classes of CarbonFile, the listFiles() function can return null, which incurs NullPointerException when called by CarbonTableReader.updateTableList() function.
issueID:CARBONDATA-1246
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/datastore/filesystem/ViewFSCarbonFile.java
core/src/main/java/org/apache/carbondata/core/datastore/filesystem/LocalCarbonFile.java
core/src/main/java/org/apache/carbondata/core/datastore/filesystem/HDFSCarbonFile.java
core/src/main/java/org/apache/carbondata/core/datastore/filesystem/AlluxioCarbonFile.java
texts:NullPointerException in Presto Integration
In presto integration, CarbonFile.listFiles() function will return null when the specified fileStatus is not a directory or is null. This will incur a NullPointerException when called by CarbonTableReader.updateTableList() function.The listFiles() function should return an empty array or throw a new exception to indicate users, instead of returning a null value.
issueID:CARBONDATA-1247
type:Bug
changed files:
texts:Block pruning not working for date type data type column
1. create table if not exists test_date (id int,first_name String,last_name string,email string,gender string,dob date) stored by 'carbondata'2. LOAD DATA LOCAL INPATH 'D:/data/MOCK_DATA_24.csv' into table test_date3. LOAD DATA LOCAL INPATH 'D:/data/MOCK_DATA_25.csv' into table test_date4. select dob from test_date where dob = '2016-06-24'Actual Result: In the logs it is going to 2 blocks Identified no.of.blocks: 2, no.of.tasks: 2, no.of.nodes: 0, parallelism: 1Expected: It should select only one block
issueID:CARBONDATA-1248
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/datastore/page/LazyColumnPage.java
texts:LazyColumnPage should extend from ColumnPage
LazyColumnPage supports getLong, getDouble only, other function should throw Exceptions
issueID:CARBONDATA-1249
type:Bug
changed files:processing/src/main/java/org/apache/carbondata/processing/util/CarbonDataProcessorUtil.java
core/src/main/java/org/apache/carbondata/core/datastore/row/CarbonRow.java
processing/src/main/java/org/apache/carbondata/processing/loading/converter/impl/RowConverterImpl.java
processing/src/main/java/org/apache/carbondata/processing/loading/steps/InputProcessorStepImpl.java
processing/src/main/java/org/apache/carbondata/processing/loading/model/CarbonLoadModel.java
texts:Wrong order of columns in redirected csv for bad records

issueID:CARBONDATA-125
type:Improvement
changed files:integration/spark-common/src/main/java/org/apache/carbondata/spark/load/CarbonLoaderUtil.java
integration/spark-common/src/main/java/org/apache/carbondata/spark/load/DeleteLoadFolders.java
core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
texts:Remove usage of currentRestructureNumber from the code
Problem: Code contains currentRestructureNumber variable which is not used in the code now. It was used when Carbon had a concept of cube in the previous design.Impact area: Data load flowFix: Remove the usage of currentRestructureNumber variable everywhere in the code
issueID:CARBONDATA-1250
type:Sub-task
changed files:core/src/main/java/org/apache/carbondata/core/scan/filter/FilterProcessor.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/CarbonTable.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/PartitionInfo.java
core/src/main/java/org/apache/carbondata/core/scan/filter/FilterExpressionProcessor.java
core/src/main/java/org/apache/carbondata/core/util/comparator/Comparator.java
core/src/main/java/org/apache/carbondata/core/util/comparator/SerializableComparator.java
processing/src/main/java/org/apache/carbondata/processing/loading/model/CarbonLoadModel.java
core/src/main/java/org/apache/carbondata/core/metadata/converter/ThriftWrapperSchemaConverterImpl.java
texts:13. Change default partition id from Max to 0
Change default partition id from Max to 0
issueID:CARBONDATA-1251
type:Bug
changed files:
texts:Add test cases for IUD feature

issueID:CARBONDATA-1252
type:Sub-task
changed files:
texts:Add BAD_RECORD_PATH option in Load options section in the Carbon Help doc

issueID:CARBONDATA-1253
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/datastore/chunk/store/impl/safe/SafeVariableLengthDimensionDataChunkStore.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/store/impl/unsafe/UnsafeVariableLengthDimensionDataChunkStore.java
core/src/main/java/org/apache/carbondata/core/util/ByteUtil.java
core/src/main/java/org/apache/carbondata/core/util/DataTypeUtil.java
texts:Sort_columns should not support float,double,decimal

issueID:CARBONDATA-1254
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/metadata/schema/table/CarbonTable.java
texts:Post Alter Describe Formatted is listing deleted column in Sort_Colums
0: jdbc:hive2://172.168.100.196:22550/default> create table t3 (id string,country string,population string ) stored by 'carbondata';---------+ Result  ---------+---------+No rows selected (0.474 seconds)0: jdbc:hive2://172.168.100.196:22550/default> desc formatted t3;-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+               col_name                                                     data_type                                                                       comment                                  -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ id                                     string                                                                             DICTIONARY, KEY COLUMN                                                     country                                string                                                                             DICTIONARY, KEY COLUMN                                                     population                             string                                                                             DICTIONARY, KEY COLUMN                                                        ##Detailed Table Information             Database Name:                         default                                                                             Table Name:                            t3                                                                                  CARBON Store Path:                     hdfs://hacluster/user/hive/warehouse/carbon.store                                   Table Block Size :                     1024 MB                                                                                ##Detailed Column property               ADAPTIVE                                 SORT_COLUMNS                           id,country,population                                                                  ##Column Group Information              --------------------------------------------------------------------------------------------------------------0: jdbc:hive2://172.168.100.196:22550/default> alter table t3 drop columns ( country);---------+ Result  ---------+---------+No rows selected (0.652 seconds)0: jdbc:hive2://172.168.100.196:22550/default> desc formatted t3;-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+               col_name                                                     data_type                                                                       comment                                  -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ id                                     string                                                                             DICTIONARY, KEY COLUMN                                                     population                             string                                                                             DICTIONARY, KEY COLUMN                                                        ##Detailed Table Information             Database Name:                         default                                                                             Table Name:                            t3                                                                                  CARBON Store Path:                     hdfs://hacluster/user/hive/warehouse/carbon.store                                   Table Block Size :                     1024 MB                                                                                ##Detailed Column property               ADAPTIVE                                 SORT_COLUMNS                           id,country                                 ##Column Group Information              -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
issueID:CARBONDATA-1255
type:Improvement
changed files:
texts:Remove "COLUMN_GROUPS" feature from documentation
We should remove "COLUMN_GROUPS " feature from documentation as this feature has been removed
issueID:CARBONDATA-1257
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelRangeTypeExecuterFactory.java
core/src/main/java/org/apache/carbondata/core/datastore/page/statistics/KeyPageStatsCollector.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockletDataMap.java
core/src/main/java/org/apache/carbondata/core/util/DataTypeUtil.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RestructureEvaluatorImpl.java
core/src/main/java/org/apache/carbondata/core/metadata/ValueEncoderMeta.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelRangeLessThanEqualFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/ExcludeFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/scan/filter/ColumnFilterInfo.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelRangeGrtrThanEquaToFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/datastore/block/SegmentProperties.java
core/src/main/java/org/apache/carbondata/core/datastore/page/ColumnPage.java
core/src/main/java/org/apache/carbondata/core/datastore/page/statistics/ColumnPageStatsCollector.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/ConditionalFilterResolverImpl.java
core/src/main/java/org/apache/carbondata/core/util/CarbonMetadataUtil.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/EncodingFactory.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/MeasureColumnExecuterFilterInfo.java
core/src/main/java/org/apache/carbondata/core/datastore/page/statistics/PrimitivePageStatsCollector.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/IncludeFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/util/comparator/Comparator.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelRangeGrtThanFiterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/util/comparator/SerializableComparator.java
core/src/main/java/org/apache/carbondata/core/scan/filter/FilterUtil.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/reader/measure/v3/MeasureChunkReaderV3.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelRangeLessThanFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/RowLevelRangeFilterResolverImpl.java
texts:Measure Filter Block Prunning and Filter Evaluation Support
Measure columns currently stores Min and Max values at blocklet level. This helps in Block Prunning in case of Measure Filter Query. The currrent requirement is to a) Enable Block Prunning in case of Measure Filters. b) Handle Measure Data Evaluation in Major Filter Evaluators in Carbon like Include, Exclude, Greater Than , Less Than etc.
issueID:CARBONDATA-1258
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/keygenerator/directdictionary/timestamp/DateDirectDictionaryGenerator.java
texts:CarbonData should not allow loading Date Type values violating the boundary condition ("0001-01-01" through "9999-12-31")
CarbonData should not allow loading Date Type values violating the boundary condition ("0001-01-01" through "9999-12-31")For Date Type the value  X being loaded must be   X >= "0001-01-01" and X <= "9999-12-31".
issueID:CARBONDATA-1259
type:Improvement
changed files:
texts:CompareTest improvement
Improvement:1.check query result details, report error if result is not the same2.add support for comparison with ORC file3.add decimal data type
issueID:CARBONDATA-126
type:Bug
changed files:
texts:Csv FIle stream closing issue
Stream not getting close for last block in data loading
issueID:CARBONDATA-1261
type:Improvement
changed files:
texts:load sql add &#39;header&#39; option
When we load the CSV files without file header and the file header is the same with the table schema,  add 'header'='false' to load data sql, no need to let user provide the file header.
issueID:CARBONDATA-1262
type:Improvement
changed files:
texts:Remove unnecessary LoadConfiguration creation
Remove unnecessary LoadConfiguration creation
issueID:CARBONDATA-1265
type:Bug
changed files:
texts:Fix AllDictionaryExample because it is only supported when single_pass is true

issueID:CARBONDATA-1266
type:Bug
changed files:integration/presto/src/main/java/org/apache/carbondata/presto/impl/CarbonTableReader.java
texts:Select from NonExisting table returns null
Selecting data from non existing table in presto shows error metadata is null instead it should show table doesn't exists.select * from abc;Query 20170705_114255_00000_72hqk failed: metadata is null
issueID:CARBONDATA-1267
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/DeltaIntegralCodec.java
texts:Failure in data loading due to bugs in delta-integer-codec
BRIEF  when encoding column value by DeltaIntegerCodec, SHORT_INT may be a target type, but this value doesnot exist in swith-case branch, so exception will be thrown in some case.    ANALYZE：  This error occurs in particular situation. In my case, there is a measure column called` BEGIN_TIME` and its datatype is `LONG`. When encoding this measure, carbondata use the statistic information `(min: 1497376581, max: 1497423838, decimal: 0 )` and finally decide to use `DeltaIntegerCodec` encoder with source datatype `LONG` and target datatype `SHORT_INT(a internal datatype using three bytes)`. But the encode method in `DeltaIntegerCodec` does not consider this situation, so exception will be thrown. ERRORDetail error message is shown below:java.lang.RuntimeException: internal error: org.apache.carbondata.core.datastore.page.encoding.DeltaIntegerCodec&#91;src type: LONG, target type: SHORT_INT, stats(min: 1497376581, max: 1497423838, decimal: 0 )&#93; at org.apache.carbondata.core.datastore.page.encoding.DeltaIntegerCodec$1.encode(DeltaIntegerCodec.java:147) at org.apache.carbondata.core.datastore.page.SafeFixLengthColumnPage.encode(SafeFixLengthColumnPage.java:324) at org.apache.carbondata.core.datastore.page.encoding.DeltaIntegerCodec.encode(DeltaIntegerCodec.java:71) at org.apache.carbondata.processing.store.TablePageEncoder.encodeAndCompressMeasures(TablePageEncoder.java:75) at org.apache.carbondata.processing.store.TablePageEncoder.encode(TablePageEncoder.java:64) at org.apache.carbondata.processing.store.CarbonFactDataHandlerColumnar.processDataRows(CarbonFactDataHandlerColumnar.java:364) at org.apache.carbondata.processing.store.CarbonFactDataHandlerColumnar.access$500(CarbonFactDataHandlerColumnar.java:65) at org.apache.carbondata.processing.store.CarbonFactDataHandlerColumnar$Producer.call(CarbonFactDataHandlerColumnar.java:749) at org.apache.carbondata.processing.store.CarbonFactDataHandlerColumnar$Producer.call(CarbonFactDataHandlerColumnar.java:726) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:748)
issueID:CARBONDATA-1268
type:Sub-task
changed files:core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/dimension/legacy/ComplexDimensionIndexCodec.java
processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactDataHandlerColumnar.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/dimension/legacy/DictDimensionIndexCodec.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/compress/DirectCompressCodec.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/dimension/legacy/IndexStorageCodec.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/adaptive/AdaptiveCodec.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/dimension/legacy/DirectDictDimensionIndexCodec.java
processing/src/main/java/org/apache/carbondata/processing/util/CarbonDataProcessorUtil.java
core/src/main/java/org/apache/carbondata/core/datastore/page/ColumnPage.java
core/src/main/java/org/apache/carbondata/core/datastore/page/ComplexColumnPage.java
core/src/main/java/org/apache/carbondata/core/datastore/TableSpec.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/EncodingFactory.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/column/CarbonDimension.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/ColumnPageCodec.java
processing/src/main/java/org/apache/carbondata/processing/store/TablePage.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/DefaultEncodingFactory.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/reader/measure/v3/MeasureChunkReaderV3.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/dimension/legacy/HighCardDictDimensionIndexCodec.java
texts:Add encoding selection strategy for columns
For each column, carbon should support encoding strategy to choose the suitable encoding method. This strategy should be extensible, so developer can change its behavior easily.
issueID:CARBONDATA-1269
type:Bug
changed files:
texts:Select query for a table inside an non existing schema shows error: metadata is null
I tried to perform select query for an non existing table inside a non existing database, instead of showing Database error it showed  me  the following error:presto:abc> select * from hbvc;Query 20170706_051345_00008_pqcqv failed: metadata is null
issueID:CARBONDATA-127
type:Bug
changed files:processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/IntermediateFileMerger.java
processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactDataHandlerColumnar.java
texts:Issue while type casting data read from sort temp file to big decimal type
Analysis: Whenever we perform data load operation involving huge data with decimal datatypes, then during intermediate merging pf sort temp file we are trying to typecast byte array to big decimal value after reading from object array. During this operation typecast exception is thrown.Impact area: data load flow with huge data and measures with big decimal datatypeFix: Typescast the object array value to byte array instaed of bigdecimal
issueID:CARBONDATA-1270
type:Sub-task
changed files:
texts:Documentation - Update the segment deletion syntax in documentation
Update the sytax change in the document for:1. DELETE SEGMENT by ID2. DELETE SEGMENT by DATE
issueID:CARBONDATA-1271
type:Sub-task
changed files:integration/hive/src/main/java/org/apache/carbondata/hive/MapredCarbonInputFormat.java
integration/hive/src/main/java/org/apache/carbondata/hive/CarbonArrayInspector.java
core/src/main/java/org/apache/carbondata/core/stats/QueryStatisticsRecorderImpl.java
integration/hive/src/main/java/org/apache/carbondata/hive/test/server/HiveEmbeddedServer2.java
integration/hive/src/main/java/org/apache/carbondata/hive/CarbonHiveInputSplit.java
integration/hive/src/main/java/org/apache/carbondata/hive/CarbonDictionaryDecodeReadSupport.java
integration/hive/src/main/java/org/apache/carbondata/hive/CarbonHiveRecordReader.java
integration/hive/src/main/java/org/apache/carbondata/hive/CarbonHiveSerDe.java
texts:Hive Integration Performance Improvement
Improve the performance of Hive Integration with Carbon Data
issueID:CARBONDATA-1273
type:Improvement
changed files:processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/SortTempFileChunkHolder.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
core/src/main/java/org/apache/carbondata/core/util/CarbonProperties.java
processing/src/main/java/org/apache/carbondata/processing/store/writer/AbstractFactDataWriter.java
texts:String datatype should be no dictionary column by default

issueID:CARBONDATA-1274
type:Task
changed files:
texts:add update and delete examples
add update and delete examples
issueID:CARBONDATA-1276
type:Bug
changed files:
texts:Owner name of delta files created after update/delete records operation in Beeline is spark2x instead of login user who performed delete operation
In spark beeline, user creates a table.User loads data in the table.User deletes/updates records from the table.User executes the hadoop fs-getfacl file system to view the user,group and permissions for the delta file created.User checks the user name/owner name of the delete/update records operation in getfacl command output.
issueID:CARBONDATA-1277
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/datastore/impl/FileFactory.java
core/src/main/java/org/apache/carbondata/core/util/path/HDFSLeaseUtils.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
core/src/main/java/org/apache/carbondata/core/fileoperations/AtomicFileOperationsImpl.java
core/src/main/java/org/apache/carbondata/core/writer/ThriftWriter.java
core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
texts:Dictionary generation failure if there is failure in closing output stream in HDFS
If there is any failure while closing the output stream of dictionary file in HDFS then on next data load, update or insert into operation dictionary generation fails. This is because we open the dictionary file in append mode and when we try to get the output stream for that file HDFS throws an exception that Lease is already acquired by some other client. Exception trace as below.java.io.IOException: Failed to APPEND_FILE /user/hive/warehouse/carbon.store/test/t12/Metadata/010497af-1833-4804-a7ec-849ab7b9bf10.dictmeta for DFSClient_NONMAPREDUCE_-1904161438_159 on 172.168.100.212 because lease recovery is in progress. Try again later.at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.recoverLeaseInternal(FSNamesystem.java:2901)at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.appendFileInternal(FSNamesystem.java:2655)at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.appendFileInt(FSNamesystem.java:2968)at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.appendFile(FSNamesystem.java:2937)at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.append(NameNodeRpcServer.java:776)at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.append(ClientNamenodeProtocolServerSideTranslatorPB.java:458)at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
issueID:CARBONDATA-1278
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/scan/filter/FilterUtil.java
texts:Data Mismatch issue when dictionary column filter values doesn&#39;t exists in dictionary
Data Mismatch issue when dictionary column filter values doesn't exists in dictionary when number of blocklet is more than 1 Select count from table where column2='xxx' &#8211; returning 100 rows.Select count from table where column1='yyy' OR column2='xxx' &#8211; returning 7 rows.Select count from table where column1='yyy' is returning 0 rows yyy does not exists in dictionary file.Query excepted value is 100 as yyy does not exists and its a or condition
issueID:CARBONDATA-1279
type:Bug
changed files:
texts:Push down for some select queries not working as expected in Spark 2.1
User creates a table and loads data into the table.User executes select query with like condition and checks if pushdown is working correctly.
issueID:CARBONDATA-128
type:Bug
changed files:
texts:Add block building statistics
Add detail level block building statistics(BTree building)
issueID:CARBONDATA-1280
type:Bug
changed files:
texts:Solve HiveExample dependency issues and fix CI with spark 1.6
Solve HiveExample dependency issues and fix CI with spark 1.6
issueID:CARBONDATA-1281
type:Improvement
changed files:processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/SortDataRows.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/SortParameters.java
processing/src/main/java/org/apache/carbondata/processing/merger/RowResultMergerProcessor.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/merger/UnsafeIntermediateMerger.java
processing/src/main/java/org/apache/carbondata/processing/loading/steps/CarbonRowDataWriterProcessorStepImpl.java
processing/src/main/java/org/apache/carbondata/processing/loading/steps/DataWriterProcessorStepImpl.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/SingleThreadFinalSortFilesMerger.java
processing/src/main/java/org/apache/carbondata/processing/store/writer/AbstractFactDataWriter.java
processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactDataHandlerColumnar.java
processing/src/main/java/org/apache/carbondata/processing/util/CarbonLoaderUtil.java
processing/src/main/java/org/apache/carbondata/processing/loading/DataLoadExecutor.java
core/src/main/java/org/apache/carbondata/core/util/CarbonProperties.java
processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactDataHandlerModel.java
core/src/main/java/org/apache/carbondata/core/util/path/CarbonTablePath.java
processing/src/main/java/org/apache/carbondata/processing/util/CarbonDataProcessorUtil.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/impl/ParallelReadMergeSorterImpl.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/impl/ParallelReadMergeSorterWithBucketingImpl.java
processing/src/main/java/org/apache/carbondata/processing/loading/DataLoadProcessBuilder.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/impl/UnsafeParallelReadMergeSorterImpl.java
processing/src/main/java/org/apache/carbondata/processing/merger/CompactionResultSortProcessor.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/UnsafeSortDataRows.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/impl/UnsafeParallelReadMergeSorterWithBucketingImpl.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/merger/UnsafeSingleThreadFinalSortFilesMerger.java
texts:Disk hotspot found during data loading
ScenarioCurrently we have done a massive data loading. The input data is about 71GB in CSV format，and have about 88million records. When using carbondata, we do not use any dictionary encoding. Our testing environment has three nodes and each of them have 11 disks as yarn executor directory. We submit the loading command through JDBCServer.The JDBCServer instance have three executors in total, one on each node respectively. The loading takes about 10minutes (+-3min vary from each time).We have observed the nmon information during the loading and find：1. lots of CPU waits in the first half of loading;2. only one single disk has many writes and almost reaches its bottleneck (Avg. 80M/s, Max. 150M/s on SAS Disk)3. the other disks are quite idel AnalyzeWhen do data loading, carbondata read and sort data locally(default scope) and write the temp files to local disk. In my case, there is only one executor in one node, so carbondata write all the temp file to one disk(container directory or yarn local directory), thus resulting into single disk hotspot. ModificationWe should support multiple directory for writing temp files to avoid disk hotspot.Ps: I have improved this in my environment and the result is pretty optimistic: the loading takes about 6minutes (10 minutes before improving).
issueID:CARBONDATA-1282
type:Bug
changed files:
texts:Query with large no of column gives codegeneration issue
When table is having around 2000 columns and query is being done on same table, carbon gives codegeneration issue stating generated code size exceeds 64Kb.
issueID:CARBONDATA-1283
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/datastore/impl/FileFactory.java
core/src/main/java/org/apache/carbondata/core/locks/HdfsFileLock.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
core/src/main/java/org/apache/carbondata/core/util/CarbonProperties.java
core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
texts:Carbon should continue with the default value if wrong value is set for the configurable parameter.
Carbon should continue with the default value if wrong value is set for the configurable parameters.for example for below parameters if wrong value is configured then the executions is failing in the respective scenarios.enable.unsafe.sortcarbon.custom.block.distributioncarbon.enable.vector.readercarbon.lock.typecarbon.csv.read.buffersize.byte
issueID:CARBONDATA-1285
type:Bug
changed files:
texts:Compilation error in HiveEmbeededserver on master branch due to changes in pom.xml of hive

issueID:CARBONDATA-1286
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/metadata/schema/table/TableInfo.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/Writable.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/CarbonTable.java
core/src/main/java/org/apache/carbondata/core/metadata/datatype/DataType.java
core/src/main/java/org/apache/carbondata/core/metadata/CarbonMetadata.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/column/ColumnSchema.java
core/src/main/java/org/apache/carbondata/core/metadata/encoder/Encoding.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/SchemaEvolution.java
processing/src/main/java/org/apache/carbondata/processing/loading/model/CarbonLoadModel.java
core/src/main/java/org/apache/carbondata/core/metadata/converter/ThriftWrapperSchemaConverterImpl.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/WritableUtil.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/TableSchema.java
texts:Change query related RDD to use TableInfo
Currently query related RDD is using CarbonTable which is read from file system, which introduces unnecessary file read. We can pass this schema information from driver by serializing TableInfo.
issueID:CARBONDATA-1287
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/datastore/page/key/TablePageKey.java
texts:Remove unnecessary MDK generation in loading
When updating MDK key in data load write step, there is unnecessary MDK generation. It can be removed to improvement loading performance
issueID:CARBONDATA-1288
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
processing/src/main/java/org/apache/carbondata/processing/loading/CarbonDataLoadConfiguration.java
processing/src/main/java/org/apache/carbondata/processing/loading/DataLoadProcessBuilder.java
processing/src/main/java/org/apache/carbondata/processing/loading/converter/impl/RowConverterImpl.java
processing/src/main/java/org/apache/carbondata/processing/loading/model/CarbonLoadModel.java
texts:Secure Dictionary Server Port
Secure Dictionary Server Port Implementation.During single pass load  Dictionary key creation is done through driver and executor communication through external ports. The communication will happen through TCP communication which is not secure or encrypted.But in case spark turn on Security parameters then carbon dictionary ports also has to follows same authentication and encryption i.e. communicate through SASL Authentication protocol and Digest-MD5 encryption.This PR makes the dictionary server and client communication Secure and encrypted. In case spark turn ON security and authentication through the below parameters then Carbon communication also becomes secure. By default the communication is still non secure. Parameters *spark.authenticate truespark.authenticate.enableSaslEncryption truespark.authenticate.secret*
issueID:CARBONDATA-1289
type:Improvement
changed files:processing/src/main/java/org/apache/carbondata/processing/loading/exception/CarbonDataLoadingException.java
texts:remove a unused method in CarbonDataLoadingException
remove a unused method in CarbonDataLoadingException
issueID:CARBONDATA-129
type:Bug
changed files:
texts:Do null check before adding value to CarbonProperties
CarbonGlobalDictionaryRDD We are adding value to CarbonProperties without check if its null.If added value is null then error will come. Its better to do null check before adding value to CarbonProperties.
issueID:CARBONDATA-1291
type:Bug
changed files:hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonTableInputFormat.java
texts:CarbonData query performace improvement when number of carbon blocks are high
When number of carbon blocks are high query performance is bad
issueID:CARBONDATA-1293
type:Bug
changed files:
texts:update on carbon data failed with carbon.update.persist.enable false

issueID:CARBONDATA-13
type:Improvement
changed files:core/src/main/java/org/carbondata/scan/filter/executer/RowLevelRangeLessThanFiterExecuterImpl.java
core/src/main/java/org/carbondata/scan/filter/executer/RowLevelRangeLessThanEqualFilterExecuterImpl.java
core/src/main/java/org/carbondata/scan/filter/executer/RowLevelRangeTypeExecuterFacory.java
core/src/main/java/org/carbondata/scan/filter/resolver/LogicalFilterResolverImpl.java
core/src/main/java/org/carbondata/scan/filter/resolver/RowLevelRangeFilterResolverImpl.java
core/src/main/java/org/carbondata/scan/filter/executer/RowLevelRangeGrtrThanEquaToFilterExecuterImpl.java
core/src/main/java/org/carbondata/scan/filter/executer/ExcludeFilterExecuterImpl.java
core/src/main/java/org/carbondata/scan/filter/FilterUtil.java
core/src/main/java/org/carbondata/scan/filter/FilterExpressionProcessor.java
core/src/main/java/org/carbondata/scan/filter/executer/IncludeFilterExecuterImpl.java
core/src/main/java/org/carbondata/scan/filter/executer/RowLevelRangeGrtThanFiterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
core/src/main/java/org/carbondata/scan/filter/resolver/ConditionalFilterResolverImpl.java
core/src/main/java/org/carbondata/scan/filter/resolver/FilterResolverIntf.java
texts:Time stamp range filters not able to prune blocks
Time stamp filters which are Direct surrogate key range filters not able to prune blocks.1) specific handling required to handle time stamp filters, while matching min max and btree2) Use binary search within blocklet for >,>=,<,<= cases
issueID:CARBONDATA-130
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
texts:Load data failed when set a wrong kettle home  in local mode
when we install a cluster mode carbon, we use kettle home in executor node, it is ok.But once we run it as local mode, the executor will run on local node(means driver node), load data failed because of the wrong kettle home path (kettle home is exists in executor node, but not in driver node).so we need correct the kettle home when we run carbon in local mode.
issueID:CARBONDATA-1301
type:Improvement
changed files:
texts:change command to update schema and data separately

issueID:CARBONDATA-1304
type:Bug
changed files:
texts:Support IUD with single_pass

issueID:CARBONDATA-1305
type:Bug
changed files:processing/src/main/java/org/apache/carbondata/processing/loading/exception/NoRetryException.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonLoadOptionConstants.java
processing/src/main/java/org/apache/carbondata/processing/loading/DataLoadExecutor.java
texts:On creating the dictinary with large dictionary csv NegativeArraySizeException is thrown
Step to reproduce: 1. create table table1 (a string, b bigint) stored by 'carbondata';2. LOAD DATA inpath 'hdfs://hacluster/user/xyz/datafile_0.csv' into table table1 options('DELIMITER'=',', 'QUOTECHAR'='"','COLUMNDICT'='a:hdfs://hacluster/user/xyz/dict.csv','FILEHEADER'='a,b','SINGLE_PASS'='TRUE');
issueID:CARBONDATA-1306
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/datastore/page/UnsafeFixLengthColumnPage.java
core/src/main/java/org/apache/carbondata/core/metadata/datatype/DataType.java
texts:Carbondata select query crashes when using big data with more than million rows
Carbondata crashes when executing the following queries sequentially of CompareTest .select sum(m1)  from t1select sum(m1), sum(m2) from t1
issueID:CARBONDATA-1307
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/util/ObjectSerializationUtil.java
texts:TableInfo serialization not working in cluster mode

issueID:CARBONDATA-1308
type:Improvement
changed files:integration/hive/src/main/java/org/apache/carbondata/hive/MapredCarbonInputFormat.java
processing/src/main/java/org/apache/carbondata/processing/datatypes/PrimitiveDataType.java
hadoop/src/main/java/org/apache/carbondata/hadoop/readsupport/impl/DictionaryDecodeReadSupport.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/resolverinfo/TrueConditionalResolverImpl.java
core/src/main/java/org/apache/carbondata/core/scan/executor/impl/AbstractQueryExecutor.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/LogicalFilterResolverImpl.java
processing/src/main/java/org/apache/carbondata/processing/util/CarbonLoaderUtil.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/RowLevelFilterResolverImpl.java
integration/hive/src/main/java/org/apache/carbondata/hive/CarbonDictionaryDecodeReadSupport.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/metadata/FilterResolverMetadata.java
core/src/main/java/org/apache/carbondata/core/scan/executor/util/QueryUtil.java
core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/FilterResolverIntf.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/resolverinfo/visitor/DictionaryColumnVisitor.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/resolverinfo/visitor/RangeDictionaryColumnVisitor.java
core/src/main/java/org/apache/carbondata/core/scan/filter/FilterProcessor.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonTableInputFormat.java
hadoop/src/main/java/org/apache/carbondata/hadoop/util/CarbonInputFormatUtil.java
core/src/main/java/org/apache/carbondata/core/scan/filter/FilterExpressionProcessor.java
core/src/main/java/org/apache/carbondata/core/scan/filter/FilterUtil.java
core/src/main/java/org/apache/carbondata/core/cache/dictionary/DictionaryColumnUniqueIdentifier.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/ConditionalFilterResolverImpl.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/RowLevelRangeFilterResolverImpl.java
integration/presto/src/main/java/org/apache/carbondata/presto/impl/CarbonTableReader.java
texts:Added tableProvider to supply carbonTable wherever needed

issueID:CARBONDATA-131
type:Bug
changed files:
texts:when we install a client mode carbon，load data failed
WARN  02-08 18:37:46,529 - Lost task 1.0 in stage 6.0 (TID 20, master): org.carbondata.processing.graphgenerator.GraphGeneratorException: Error While Initializing the Kettel Engine  at org.carbondata.processing.graphgenerator.GraphGenerator.validateAndInitialiseKettelEngine(GraphGenerator.java:309) at org.carbondata.processing.graphgenerator.GraphGenerator.generateGraph(GraphGenerator.java:278) at org.carbondata.spark.load.CarbonLoaderUtil.generateGraph(CarbonLoaderUtil.java:118) at org.carbondata.spark.load.CarbonLoaderUtil.executeGraph(CarbonLoaderUtil.java:173) at org.carbondata.spark.rdd.CarbonDataLoadRDD$$anon$1.<init>(CarbonDataLoadRDD.scala:196) at org.carbondata.spark.rdd.CarbonDataLoadRDD.compute(CarbonDataLoadRDD.scala:155) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306) at org.apache.spark.rdd.RDD.iterator(RDD.scala:270) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66) at org.apache.spark.scheduler.Task.run(Task.scala:89) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) at java.lang.Thread.run(Thread.java:745)Caused by: org.pentaho.di.core.exception.KettleException: Unable to read file '/opt/incubator-carbondata/processing/carbonplugins/.kettle/kettle.properties'/opt/incubator-carbondata/processing/carbonplugins/.kettle/kettle.properties (No such file or directory) at org.pentaho.di.core.util.EnvUtil.readProperties(EnvUtil.java:65) at org.pentaho.di.core.util.EnvUtil.environmentInit(EnvUtil.java:95) at org.carbondata.processing.graphgenerator.GraphGenerator.validateAndInitialiseKettelEngine(GraphGenerator.java:303) ... 13 moreCaused by: java.io.FileNotFoundException: /opt/incubator-carbondata/processing/carbonplugins/.kettle/kettle.properties (No such file or directory) at java.io.FileInputStream.open(Native Method) at java.io.FileInputStream.<init>(FileInputStream.java:146) at java.io.FileInputStream.<init>(FileInputStream.java:101) at org.pentaho.di.core.util.EnvUtil.readProperties(EnvUtil.java:60) ... 15 more
issueID:CARBONDATA-1310
type:Improvement
changed files:
texts:merge test code for AddColumnTestCases, DropColumnTestCases and ChangeDataTypeTestCases
merge test code between 2 packages for AddColumnTestCases, DropColumnTestCases and ChangeDataTypeTestCases
issueID:CARBONDATA-1312
type:Sub-task
changed files:
texts:14. Fix comparator bug

issueID:CARBONDATA-1313
type:Improvement
changed files:processing/src/main/java/org/apache/carbondata/processing/loading/steps/DataWriterProcessorStepImpl.java
core/src/main/java/org/apache/carbondata/core/util/CarbonMetadataUtil.java
texts:Remove unnecessary statistics
Unique Value and Decimal Point is not used, remove them in measure statistics
issueID:CARBONDATA-1316
type:Sub-task
changed files:hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonTableInputFormat.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/PartitionInfo.java
processing/src/main/java/org/apache/carbondata/processing/merger/CarbonDataMergerUtil.java
texts:15. alter table drop partition

issueID:CARBONDATA-1317
type:Bug
changed files:
texts:Multiple dictionary files being created in single_pass
Steps to reproduce:-1. Create table 2. Load 2 times3. Drop table 4. Create table with same name5. Load 2 times
issueID:CARBONDATA-132
type:Improvement
changed files:
texts:Parse some Spark Exception which can be shown to driver side and show them directly.

issueID:CARBONDATA-1323
type:Improvement
changed files:integration/presto/src/main/java/org/apache/carbondata/presto/CarbondataPageSource.java
texts:Presto Performace Improvement at Integration Layer
Presto Performace Improvement
issueID:CARBONDATA-1325
type:Sub-task
changed files:
texts:16. create guidance documents for partition table

issueID:CARBONDATA-1326
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/metadata/schema/table/TableInfo.java
core/src/main/java/org/apache/carbondata/core/datamap/DataMapStoreManager.java
processing/src/main/java/org/apache/carbondata/processing/loading/steps/CarbonRowDataWriterProcessorStepImpl.java
core/src/main/java/org/apache/carbondata/core/scan/expression/RangeExpressionEvaluator.java
processing/src/main/java/org/apache/carbondata/processing/loading/steps/DataWriterProcessorStepImpl.java
core/src/main/java/org/apache/carbondata/core/scan/expression/ExpressionResult.java
core/src/main/java/org/apache/carbondata/core/datastore/block/TableBlockInfo.java
core/src/main/java/org/apache/carbondata/core/util/CarbonProperties.java
core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/bool/BooleanConvert.java
core/src/main/java/org/apache/carbondata/core/metadata/CarbonTableIdentifier.java
core/src/main/java/org/apache/carbondata/core/scan/result/BlockletScannedResult.java
core/src/main/java/org/apache/carbondata/core/datastore/page/ColumnPage.java
core/src/main/java/org/apache/carbondata/core/keygenerator/mdkey/MultiDimKeyVarLengthGenerator.java
processing/src/main/java/org/apache/carbondata/processing/loading/csvinput/CSVInputFormat.java
core/src/main/java/org/apache/carbondata/core/metadata/datatype/DecimalConverterFactory.java
core/src/main/java/org/apache/carbondata/core/reader/CarbonDeleteFilesDataReader.java
core/src/main/java/org/apache/carbondata/core/statusmanager/SegmentStatusManager.java
core/src/main/java/org/apache/carbondata/core/scan/filter/FilterExpressionProcessor.java
core/src/main/java/org/apache/carbondata/core/locks/CarbonLockFactory.java
core/src/main/java/org/apache/carbondata/core/scan/filter/FilterUtil.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/BucketingInfo.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/RowLevelRangeFilterResolverImpl.java
core/src/main/java/org/apache/carbondata/core/datastore/block/BlockInfo.java
core/src/main/java/org/apache/carbondata/core/util/CarbonSessionInfo.java
core/src/main/java/org/apache/carbondata/core/cache/dictionary/ColumnDictionaryInfo.java
core/src/main/java/org/apache/carbondata/core/mutate/DeleteDeltaBlockletDetails.java
processing/src/main/java/org/apache/carbondata/processing/loading/AbstractDataLoadProcessorStep.java
processing/src/main/java/org/apache/carbondata/processing/loading/steps/InputProcessorStepImpl.java
core/src/main/java/org/apache/carbondata/core/keygenerator/mdkey/AbstractKeyGenerator.java
core/src/main/java/org/apache/carbondata/core/metadata/ValueEncoderMeta.java
processing/src/main/java/org/apache/carbondata/processing/loading/steps/SortProcessorStepImpl.java
core/src/main/java/org/apache/carbondata/core/keygenerator/KeyGenerator.java
processing/src/main/java/org/apache/carbondata/processing/loading/steps/DataConverterProcessorStepImpl.java
processing/src/main/java/org/apache/carbondata/processing/merger/CarbonDataMergerUtil.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/PartitionInfo.java
core/src/main/java/org/apache/carbondata/hadoop/CarbonInputSplit.java
texts:Fixed high priority findbug issues
Currently there are lot if find bug issues in the carbondata code. These need to be priortized and fixed. So through this jira all high priority findbug issues are addressed.
issueID:CARBONDATA-1327
type:Task
changed files:
texts:write sort columns example
write sort columns example
issueID:CARBONDATA-1329
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/util/path/CarbonTablePath.java
texts:The first carbonindex file needs to be deleted during clean files operation
The first carbonindex file created while create table operation becomes invalid after the complete update of the table. The file is not deleted during the clean operation.1.User creates a table2.Loads data into table.3.Update the table completely.4.Run clean files.
issueID:CARBONDATA-133
type:Bug
changed files:
texts:remove show load and delete load command

issueID:CARBONDATA-1335
type:Improvement
changed files:
texts:Duplicated & time-consuming method call found in query
ScenarioCurrently we did a concurrent  14 queries on Carbondata. The queries are the same, but on different tables. We have noticed the following scene:+ A single query took about 5s;+ In concurrent scenario, each query took about 15s;By adding checkpoint in the log, we found that there was great latency in starting query jobs in spark. AnalyzeWhen we fire a query, Carbondata firstly do some job in the client side, including parse/analyze plans and prepare filtered blocks and inputSplits. Then Carbondata start to submit query job to spark. We found in the first step, Carbondata took about 7s in current scenario, but it only took about <1s in single scenario.By studying the related code, we found the most time consuming method call was  `CarbonSessionCatalog.lookupRelation`. In side this method, it called `super.lookupRelation` twice, which consumed about 3s each time. SolutionCarbondata only needs to call the `super.lookupRelation` only once, we need to remove the useless duplicated method call.I've tested in my environment and it works well. In concurrent scenario, each query takes about 12s (3s saved for the improvement).
issueID:CARBONDATA-1336
type:Improvement
changed files:
texts:Add issue mailing list
Carbondata's issue related mails have been sent to a new mailing list other than DEV. We need to add the related guidance.
issueID:CARBONDATA-1337
type:Bug
changed files:
texts:Problem while intermediate merging
With PR-1198(CARBONDATA-1281), when loading data, there was error occuring, as following:2017-07-28 13:10:49,443 - ERROR - org.apache.carbondata.common.logging.impl.StandardLogService.logErrorMessage(StandardLogService.java:143) - pool-17-thread-1 -pool-17-thread-1 Problem while intermediate mergingorg.apache.carbondata.processing.sortandgroupby.exception.CarbonSortKeyAndGroupByException: Problem while getting the file        at org.apache.carbondata.processing.sortandgroupby.sortdata.IntermediateFileMerger.initialize(IntermediateFileMerger.java:168)        at org.apache.carbondata.processing.sortandgroupby.sortdata.IntermediateFileMerger.call(IntermediateFileMerger.java:110)        at org.apache.carbondata.processing.sortandgroupby.sortdata.IntermediateFileMerger.call(IntermediateFileMerger.java:37)        at java.util.concurrent.FutureTask.run(FutureTask.java:266)        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)        at java.lang.Thread.run(Thread.java:745)Caused by: java.io.FileNotFoundException: [Ljava.lang.String;@54218780/carbondata_hundred_million_pr11987419814281939480.merge (No such file or directory)        at java.io.FileOutputStream.open0(Native Method)        at java.io.FileOutputStream.open(FileOutputStream.java:270)        at java.io.FileOutputStream.<init>(FileOutputStream.java:213)        at java.io.FileOutputStream.<init>(FileOutputStream.java:162)        at org.apache.carbondata.processing.sortandgroupby.sortdata.IntermediateFileMerger.initialize(IntermediateFileMerger.java:163)        ... 6 moreAccording to xuchuanyin's suggestion, it needs to fix the code of method  SortIntermediateFileMerger.startIntermediateMerging.
issueID:CARBONDATA-1338
type:Bug
changed files:hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonTableInputFormat.java
texts:Spark can not query data when &#39;spark.carbon.hive.schema.store&#39; is true
My step is as blow:  set spark.carbon.hive.schema.store=true in spark-defaults.confspark-shell --jars carbonlib/carbondata_2.11-1.2.0-SNAPSHOT-shade-hadoop2.7.2.jar,carbonlib/carbondata-hive-1.2.0-SNAPSHOT.jarimport org.apache.spark.sql.SparkSession import org.apache.spark.sql.CarbonSession._ val rootPath = "hdfs://mycluster/user/master/carbon" val storeLocation = s"$rootPath/store" val warehouse = s"$rootPath/warehouse" val metastoredb = s"$rootPath/metastore_db" val carbon =SparkSession.builder().enableHiveSupport().getOrCreateCarbonSession(storeLocation, metastoredb) carbon.sql("create table temp.yuhai_carbon(id short, name string, scale decimal, country string, salary double) STORED BY 'carbondata'") carbon.sql("LOAD DATA INPATH 'hdfs://mycluster/user/master/sample.csv&#39; INTO TABLE temp.yuhai_carbon") carbon.sql("select * from temp.yuhai_carbon").show  Exception:  Caused by: java.io.IOException: File does not exist: hdfs://mycluster/user/master/carbon/store/temp/yuhai_carbon/Metadata/schema   at org.apache.carbondata.hadoop.util.SchemaReader.readCarbonTableFromStore(SchemaReader.java:70)   at org.apache.carbondata.hadoop.api.CarbonTableInputFormat.getOrCreateCarbonTable(CarbonTableInputFormat.java:142)   at org.apache.carbondata.hadoop.api.CarbonTableInputFormat.getQueryModel(CarbonTableInputFormat.java:441)   at org.apache.carbondata.spark.rdd.CarbonScanRDD.internalCompute(CarbonScanRDD.scala:191)   at org.apache.carbondata.spark.rdd.CarbonRDD.compute(CarbonRDD.scala:50)   at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:331)   at org.apache.spark.rdd.RDD.iterator(RDD.scala:295)   at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)   at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:331)   at org.apache.spark.rdd.RDD.iterator(RDD.scala:295)   at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)   at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:331)   at org.apache.spark.rdd.RDD.iterator(RDD.scala:295)   at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:88)   at org.apache.spark.scheduler.Task.run(Task.scala:104)   at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:351)   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)   at java.lang.Thread.run(Thread.java:745)
issueID:CARBONDATA-1339
type:Bug
changed files:
texts:CarbonTableInputFormat should use serialized TableInfo

issueID:CARBONDATA-134
type:Bug
changed files:integration/spark-common/src/main/java/org/apache/carbondata/spark/load/CarbonLoaderUtil.java
texts:Temp location of data load is not getting cleared in case of exception in data load
Temp location of data load is not getting cleared in case of exception in data load.
issueID:CARBONDATA-1342
type:Bug
changed files:processing/src/main/java/org/apache/carbondata/processing/loading/BadRecordsLogger.java
processing/src/main/java/org/apache/carbondata/processing/loading/steps/DataConverterProcessorStepImpl.java
core/src/main/java/org/apache/carbondata/core/locks/ZooKeeperLocking.java
core/src/main/java/org/apache/carbondata/core/metadata/AbsoluteTableIdentifier.java
texts:Fixed bugs for spark conf property and debugging in windows
Fixes to be done:1. In spark 2, spark conf once set in spark context cannot be modified with the same context again. Therefore removed setting property in spark conf and directly getting the property from carbon properties.2. Path formation is not proper when we run CarbonSessionExample in windows as File.Separator in the table path formation3. Namenode call is made after completion of each data load for renaming bad folders
issueID:CARBONDATA-1345
type:Bug
changed files:
texts:outdated tablemeta cache cause operation failed in multiple session
Scenario    Steps to reproduce  Start 2 spark-beeline as two different sessions, do the following steps in corresponding session:(SESSION1)1. create table T_Carbn01(Active_status String,Item_type_cd INT,Qty_day_avg INT,Qty_total INT,Sell_price BIGINT,Sell_pricep DOUBLE,Discount_price DOUBLE,Profit DECIMAL(3,2),Item_code String,Item_name String,Outlet_name String,Update_time TIMESTAMP,Create_date String)STORED BY 'org.apache.carbondata.format' TBLPROPERTIES('table_blocksize'='128');2. LOAD DATA INPATH 'hdfs://hacluster/user/Ram/T_Hive1.csv' INTO table T_Carbn01 options ('DELIMITER'=',', 'QUOTECHAR'='\','BAD_RECORDS_LOGGER_ENABLE'='true', 'BAD_RECORDS_ACTION'='REDIRECT', 'FILEHEADER'='Active_status,Item_type_cd,Qty_day_avg,Qty_total,Sell_price,Sell_pricep,Discount_price,Profit,Item_code,Item_name,Outlet_name,Update_time,Create_date');(SESSION2):1. update t_carbn01 set(Active_status) = ('TRUE') where Item_type_cd = 41;(SESSION1):1. Drop table t_carbn01;2. create table T_Carbn01(Active_status String,Item_type_cd INT,Qty_day_avg INT,Qty_total INT,Sell_price BIGINT,Sell_pricep DOUBLE,Discount_price DOUBLE,Profit DECIMAL(3,2),Item_code String,Item_name String,Outlet_name String,Update_time TIMESTAMP,Create_date String)STORED BY 'org.apache.carbondata.format' TBLPROPERTIES('table_blocksize'='128');3. LOAD DATA INPATH 'hdfs://hacluster/user/Ram/T_Hive1.csv' INTO table T_Carbn01 options ('DELIMITER'=',', 'QUOTECHAR'='\','BAD_RECORDS_LOGGER_ENABLE'='true', 'BAD_RECORDS_ACTION'='REDIRECT', 'FILEHEADER'='Active_status,Item_type_cd,Qty_day_avg,Qty_total,Sell_price,Sell_pricep,Discount_price,Profit,Item_code,Item_name,Outlet_name,Update_time,Create_date');(SESSION2):1. update t_carbn01 set(Active_status) = ('TRUE') where Item_type_cd = 41;    Outputs  message are as below:```Error: java.lang.RuntimeException: Update operation failed. Job aborted due to stage failure: Task 0 in stage 14.0 failed 4 times, most recent failure: Lost task 0.3 in stage 14.0 (TID 29, master, executor 2): java.io.IOException: java.io.IOException: Dictionary file does not exist: hdfs://user/hive/warehouse/carbon.store/default/t_carbn01/Metadata/ddfb3bc8-2fea-41fe-a4ff-18588df41aec.dictmeta    at org.apache.carbondata.core.cache.dictionary.ForwardDictionaryCache.getAll(ForwardDictionaryCache.java:146)    at org.apache.spark.sql.DictionaryLoader.loadDictionary(CarbonDictionaryDecoder.scala:686)    at org.apache.spark.sql.DictionaryLoader.getDictionary(CarbonDictionaryDecoder.scala:703)    at org.apache.spark.sql.ForwardDictionaryWrapper.getDictionaryValueForKeyInBytes(CarbonDictionaryDecoder.scala:654)    at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)    at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)    at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:378)    at org.apache.spark.sql.execution.columnar.InMemoryRelation$$anonfun$1$$anon$1.hasNext(InMemoryRelation.scala:132)    at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:215)    at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1041)    at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1032)    at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:972)    at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1032)    at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:715)    at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335)    at org.apache.spark.rdd.RDD.iterator(RDD.scala:286)    at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)```    Input datasample for input data：```TRUE,2,423,3046340,200000000003454300, 121.5,4.99,2.44,SE3423ee,asfdsffdfg,EtryTRWT,2012-01-12 03:14:05.123456729,2012-01-20TRUE,3,453,3003445,200000000000003450, 121.5,4.99,2.44,SE3423ee,asfdsffdfg,ERTEerWT,2012-01-13 03:24:05.123456739,2012-01-20TRUE,4,4350,3044364,200000000000000000, 121.5,4.99,2.44,SE3423ee,asfdsffdfg,ERTtryWT,2012-01-14 23:03:05.123456749,2012-01-20TRUE,114,4520,30000430,200000000004300000, 121.5,4.99,2.44,RE3423ee,asfdsffdfg,4RTETRWT,2012-01-01 23:02:05.123456819,2012-01-20FALSE,123,454,30000040,200000000000000000, 121.5,4.99,2.44,RE3423ee,asfrewerfg,6RTETRWT,2012-01-02 23:04:05.123456829,2012-01-20TRUE,11,4530,3000040,200000000000000000, 121.5,4.99,2.44,SE3423ee,asfdsffder,TRTETRWT,2012-01-03 05:04:05.123456839,2012-01-20TRUE,14,4590,3000400,200000000000000000, 121.5,4.99,2.44,ASD423ee,asfertfdfg,HRTETRWT,2012-01-04 05:06:05.123456849,2012-01-20FALSE,41,4250,00000,200000000000000000, 121.5,4.99,2.44,SAD423ee,asrtsffdfg,HRTETRWT,2012-01-05 05:07:05.123456859,2012-01-20TRUE,13,4510,30400,200000000000000000, 121.5,4.99,2.44,DE3423ee,asfrtffdfg,YHTETRWT,2012-01-06 06:08:05.123456869,2012-01-20```   AnalyzeIn the error message, it says the dictmeta doesnot exist.Actually this file is generated during the first load operation in SESSION1,And the tablemeta is cached in SESSION2 when doing update operation in SESSION2. After DELETE-LOAD operation in SESSION1, old dictionary files has been deleted and new dictionary files are generated in SESSION1. But in SESSION2, when doing update operation, we still use the outdated tablemeta from cache which refers to the dictmeta that were outdated, thus causing the error.To solve this problem, we should refresh the cache for tableMeta when the corresponding data schema has been updated. SolutionRefresh the tablemeta cache when table schema has been changed.Since HiveSessionState.lookupRelation is slow(especially in concurrent query scenario), dont call this method when table schema has not been changed. NotesI've tested the scenario in my environment and it is OK.
issueID:CARBONDATA-1346
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/datamap/DataMapStoreManager.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
core/src/main/java/org/apache/carbondata/core/util/CarbonProperties.java
texts:Develop framework for SDV tests to run in cluster. And add all existing SDV tests to it
Develop framework for SDV tests to run in cluster. And add all existing SDV tests to it
issueID:CARBONDATA-1347
type:Task
changed files:
texts:implement Columnar Reading Of Data for presto Integration

issueID:CARBONDATA-1348
type:Bug
changed files:
texts:Sort_Column should not supported for no dictionary column having numeric data-type and measure column

issueID:CARBONDATA-135
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/datastore/impl/FileFactory.java
core/src/main/java/org/apache/carbondata/core/datastore/impl/DFSFileHolderImpl.java
texts:Multiple hdfs client creation issue
Problem:When opening a input stream we are creating client every time and each time it is taking around 400 ms, In case of detail query we are opening for each blocklet and it is impacting the query performanceSolution:As query execution is a reading operation we can open only one client Impact Area:Query execution and compactionNeed to handle for Below scenario1. All inputstream creation2. All outputstream creation3. All places where we are creating carbon file(HDFS)In this issue we are handling scenario1
issueID:CARBONDATA-1350
type:Bug
changed files:
texts:When &#39;SORT_SCOPE&#39;=&#39;GLOBAL_SORT&#39;, the verification of &#39;single_pass&#39; must be false is invalid.
The value of option 'single_pass' is coverted to low case, but it uses 'single_pass.equals("TRUE")' to vericate, so it is invalid, and leading to load data unsuccessfully.
issueID:CARBONDATA-1351
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/indexstore/UnsafeMemoryDMStore.java
core/src/main/java/org/apache/carbondata/core/util/ThreadLocalTaskInfo.java
texts:When &#39;SORT_SCOPE&#39;=&#39;GLOBAL_SORT&#39; and &#39;enable.unsafe.columnpage&#39;=&#39;true&#39;, &#39;ThreadLocalTaskInfo.getCarbonTaskInfo()&#39; return null
When 'SORT_SCOPE'='GLOBAL_SORT' and 'enable.unsafe.columnpage'='true', it uses native RDD of Spark to load data, the method of 'ThreadLocalTaskInfo.setCarbonTaskInfo(carbonTaskInfo)' in ‘CarbonRDD.compute’ does not be called, so 'ThreadLocalTaskInfo.getCarbonTaskInfo()' will return null in some unsafe related classes, such as: UnsafeFixLengthColumnPage, UnsafeVarLengthColumnPage, UnsafeMemoryDMStore and so on.Solution: Set the CarbonTaskInfo in the method of 'ThreadLocalTaskInfo.getCarbonTaskInfo()' when 'threadLocal.get()' is null.
issueID:CARBONDATA-1352
type:Bug
changed files:
texts:Test case Execute while creating Carbondata jar.
Steps to Reproduce:1: Run the command : mvn -DskipTests -Pspark-2.1 -Dspark.version=2.1.0 clean package2: Check the attached screenshots.Expected Result:1: All the test cases should be skipped while creating a jar.
issueID:CARBONDATA-1353
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelRangeGrtrThanEquaToFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/datastore/page/statistics/PrimitivePageStatsCollector.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/IncludeFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelRangeLessThanEqualFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelRangeGrtThanFiterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelRangeLessThanFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/ExcludeFilterExecuterImpl.java
texts:SDV cluster tests are failing for measure filter feature
SDV cluster tests are failing for measure filter feature.http://144.76.159.231:8080/job/ApacheSDVTests/32/
issueID:CARBONDATA-1354
type:Bug
changed files:
texts:When &#39;SORT_SCOPE&#39;=&#39;GLOBAL_SORT&#39;, &#39;single_pass&#39; can be &#39;true&#39;
now when 'SORT_SCOPE'='GLOBAL_SORT', 'single_pass' can be 'true'
issueID:CARBONDATA-1356
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/datastore/impl/FileFactory.java
processing/src/main/java/org/apache/carbondata/processing/util/CarbonLoaderUtil.java
texts:Invert overwrite should delete files immediately
When user issued insert overwrite command, it should delete the file immediately to avoid stale folders.
issueID:CARBONDATA-1357
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/util/DataTypeConverterImpl.java
texts:byte[] convert to UTF8String bug
public Object convertFromByteToUTF8String(Object data) {    return data.toString();  }toString will get incorrect result like B[******** should use new String
issueID:CARBONDATA-1358
type:Bug
changed files:
texts:Tests are failing in master branch Spark 2.1
Tests are failing in master Spark 2.1 branch http://136.243.101.176:8080/job/ApacheCarbonPRBuilder/3348/console
issueID:CARBONDATA-1359
type:Bug
changed files:integration/hive/src/main/java/org/apache/carbondata/hive/MapredCarbonInputFormat.java
texts:Unable to use carbondata on hive
org.apache.hive.service.cli.HiveSQLException: java.io.IOException: java.lang.NullPointerException at org.apache.hive.service.cli.operation.SQLOperation.getNextRowSet(SQLOperation.java:352) at org.apache.hive.service.cli.operation.OperationManager.getOperationNextRowSet(OperationManager.java:220) at org.apache.hive.service.cli.session.HiveSessionImpl.fetchResults(HiveSessionImpl.java:685) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:497) at org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:78) at org.apache.hive.service.cli.session.HiveSessionProxy.access$000(HiveSessionProxy.java:36) at org.apache.hive.service.cli.session.HiveSessionProxy$1.run(HiveSessionProxy.java:63) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1491) at org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:59) at com.sun.proxy.$Proxy32.fetchResults(Unknown Source) at org.apache.hive.service.cli.CLIService.fetchResults(CLIService.java:454) at org.apache.hive.service.cli.thrift.ThriftCLIService.FetchResults(ThriftCLIService.java:621) at org.apache.hive.service.cli.thrift.TCLIService$Processor$FetchResults.getResult(TCLIService.java:1553) at org.apache.hive.service.cli.thrift.TCLIService$Processor$FetchResults.getResult(TCLIService.java:1538) at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39) at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39) at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:56) at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:285) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745)Caused by: java.io.IOException: java.lang.NullPointerException at org.apache.hadoop.hive.ql.exec.FetchOperator.getNextRow(FetchOperator.java:507) at org.apache.hadoop.hive.ql.exec.FetchOperator.pushRow(FetchOperator.java:414) at org.apache.hadoop.hive.ql.exec.FetchTask.fetch(FetchTask.java:140) at org.apache.hadoop.hive.ql.Driver.getResults(Driver.java:1670) at org.apache.hive.service.cli.operation.SQLOperation.getNextRowSet(SQLOperation.java:347) ... 25 moreCaused by: java.lang.NullPointerException at org.apache.carbondata.core.metadata.AbsoluteTableIdentifier.fromTablePath(AbsoluteTableIdentifier.java:84) at org.apache.carbondata.hive.MapredCarbonInputFormat.populateCarbonTable(MapredCarbonInputFormat.java:105) at org.apache.carbondata.hive.MapredCarbonInputFormat.getCarbonTable(MapredCarbonInputFormat.java:115) at org.apache.carbondata.hive.MapredCarbonInputFormat.getQueryModel(MapredCarbonInputFormat.java:122) at org.apache.carbondata.hive.MapredCarbonInputFormat.getRecordReader(MapredCarbonInputFormat.java:76) at org.apache.hadoop.hive.ql.exec.FetchOperator$FetchInputFormatSplit.getRecordReader(FetchOperator.java:673) at org.apache.hadoop.hive.ql.exec.FetchOperator.getRecordReader(FetchOperator.java:323) at org.apache.hadoop.hive.ql.exec.FetchOperator.getNextRow(FetchOperator.java:445) ... 29 moreException in thread "main" org.apache.hive.service.cli.HiveSQLException: java.io.IOException: java.lang.NullPointerException at org.apache.hive.jdbc.Utils.verifySuccess(Utils.java:256) at org.apache.hive.jdbc.Utils.verifySuccessWithInfo(Utils.java:242) at org.apache.hive.jdbc.HiveQueryResultSet.next(HiveQueryResultSet.java:365) at org.apache.carbondata.hiveexample.HiveExample$.main(HiveExample.scala:108) at org.apache.carbondata.hiveexample.HiveExample.main(HiveExample.scala)Caused by: org.apache.hive.service.cli.HiveSQLException: java.io.IOException: java.lang.NullPointerException at org.apache.hive.service.cli.operation.SQLOperation.getNextRowSet(SQLOperation.java:352) at org.apache.hive.service.cli.operation.OperationManager.getOperationNextRowSet(OperationManager.java:220) at org.apache.hive.service.cli.session.HiveSessionImpl.fetchResults(HiveSessionImpl.java:685) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:497) at org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:78) at org.apache.hive.service.cli.session.HiveSessionProxy.access$000(HiveSessionProxy.java:36) at org.apache.hive.service.cli.session.HiveSessionProxy$1.run(HiveSessionProxy.java:63) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1491) at org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:59) at com.sun.proxy.$Proxy32.fetchResults(Unknown Source) at org.apache.hive.service.cli.CLIService.fetchResults(CLIService.java:454) at org.apache.hive.service.cli.thrift.ThriftCLIService.FetchResults(ThriftCLIService.java:621) at org.apache.hive.service.cli.thrift.TCLIService$Processor$FetchResults.getResult(TCLIService.java:1553) at org.apache.hive.service.cli.thrift.TCLIService$Processor$FetchResults.getResult(TCLIService.java:1538) at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39) at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39) at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:56) at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:285) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745)Caused by: java.io.IOException: java.lang.NullPointerException at org.apache.hadoop.hive.ql.exec.FetchOperator.getNextRow(FetchOperator.java:507) at org.apache.hadoop.hive.ql.exec.FetchOperator.pushRow(FetchOperator.java:414) at org.apache.hadoop.hive.ql.exec.FetchTask.fetch(FetchTask.java:140) at org.apache.hadoop.hive.ql.Driver.getResults(Driver.java:1670) at org.apache.hive.service.cli.operation.SQLOperation.getNextRowSet(SQLOperation.java:347) ... 25 moreCaused by: java.lang.NullPointerException: null at org.apache.carbondata.core.metadata.AbsoluteTableIdentifier.fromTablePath(AbsoluteTableIdentifier.java:84) at org.apache.carbondata.hive.MapredCarbonInputFormat.populateCarbonTable(MapredCarbonInputFormat.java:105) at org.apache.carbondata.hive.MapredCarbonInputFormat.getCarbonTable(MapredCarbonInputFormat.java:115) at org.apache.carbondata.hive.MapredCarbonInputFormat.getQueryModel(MapredCarbonInputFormat.java:122) at org.apache.carbondata.hive.MapredCarbonInputFormat.getRecordReader(MapredCarbonInputFormat.java:76) at org.apache.hadoop.hive.ql.exec.FetchOperator$FetchInputFormatSplit.getRecordReader(FetchOperator.java:673) at org.apache.hadoop.hive.ql.exec.FetchOperator.getRecordReader(FetchOperator.java:323) at org.apache.hadoop.hive.ql.exec.FetchOperator.getNextRow(FetchOperator.java:445) ... 29 more
issueID:CARBONDATA-136
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/scan/result/iterator/AbstractDetailQueryResultIterator.java
texts:Fixed Query data mismatch issue after compaction
Problem: During compaction we are calling query execution, for compaction we are doing merge sort for merge sort all the bucket must be sorted in so as we are calling query execution we can only execute one blocklet at a time, currently we are executing multiple blocklet of difference blocks. Because of this bucket records are not sorted.Solution:Need to execute one blocklet a time during compaction
issueID:CARBONDATA-1361
type:Test
changed files:core/src/main/java/org/apache/carbondata/core/util/SessionParams.java
texts:Reduce the sdv cluster test time

issueID:CARBONDATA-1362
type:Bug
changed files:
texts:ArrayIndexOutOfBoundsException when decoing decimal type
ava.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.lang.ArrayIndexOutOfBoundsException: 0 at org.apache.carbondata.core.scan.processor.AbstractDataBlockIterator.close(AbstractDataBlockIterator.java:231) at org.apache.carbondata.core.scan.result.iterator.AbstractDetailQueryResultIterator.close(AbstractDetailQueryResultIterator.java:306) at org.apache.carbondata.core.scan.executor.impl.AbstractQueryExecutor.finish(AbstractQueryExecutor.java:544) at org.apache.carbondata.spark.vectorreader.VectorizedCarbonRecordReader.close(VectorizedCarbonRecordReader.java:132) at org.apache.carbondata.spark.rdd.CarbonScanRDD$$anon$1$$anonfun$7.apply(CarbonScanRDD.scala:215) at org.apache.carbondata.spark.rdd.CarbonScanRDD$$anon$1$$anonfun$7.apply(CarbonScanRDD.scala:213) at org.apache.spark.TaskContext$$anon$1.onTaskCompletion(TaskContext.scala:123) at org.apache.spark.TaskContextImpl$$anonfun$markTaskCompleted$1.apply(TaskContextImpl.scala:97) at org.apache.spark.TaskContextImpl$$anonfun$markTaskCompleted$1.apply(TaskContextImpl.scala:95) at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59) at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48) at org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:95) at org.apache.spark.scheduler.Task.run(Task.scala:117) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:351) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) at java.lang.Thread.run(Thread.java:745)Caused by: java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.lang.ArrayIndexOutOfBoundsException: 0 at java.util.concurrent.FutureTask.report(FutureTask.java:122) at java.util.concurrent.FutureTask.get(FutureTask.java:188) at org.apache.carbondata.core.scan.processor.AbstractDataBlockIterator.close(AbstractDataBlockIterator.java:226) ... 16 moreCaused by: java.lang.RuntimeException: java.lang.ArrayIndexOutOfBoundsException: 0 at org.apache.carbondata.core.datastore.chunk.impl.MeasureRawColumnChunk.convertToMeasureColDataChunks(MeasureRawColumnChunk.java:62) at org.apache.carbondata.core.scan.scanner.AbstractBlockletScanner.scanBlocklet(AbstractBlockletScanner.java:100) at org.apache.carbondata.core.scan.processor.AbstractDataBlockIterator$1.call(AbstractDataBlockIterator.java:191) at org.apache.carbondata.core.scan.processor.AbstractDataBlockIterator$1.call(AbstractDataBlockIterator.java:178) at java.util.concurrent.FutureTask.run(FutureTask.java:262) ... 3 moreCaused by: java.lang.ArrayIndexOutOfBoundsException: 0 at org.apache.carbondata.core.util.DataTypeUtil.byteToBigDecimal(DataTypeUtil.java:210) at org.apache.carbondata.core.metadata.ColumnPageCodecMeta.deserialize(ColumnPageCodecMeta.java:217) at org.apache.carbondata.core.datastore.chunk.reader.measure.v3.CompressedMeasureChunkFileBasedReaderV3.decodeMeasure(CompressedMeasureChunkFileBasedReaderV3.java:236) at org.apache.carbondata.core.datastore.chunk.reader.measure.v3.CompressedMeasureChunkFileBasedReaderV3.convertToMeasureChunk(CompressedMeasureChunkFileBasedReaderV3.java:219) at org.apache.carbondata.core.datastore.chunk.impl.MeasureRawColumnChunk.convertToMeasureColDataChunks(MeasureRawColumnChunk.java:59)
issueID:CARBONDATA-1363
type:Bug
changed files:processing/src/main/java/org/apache/carbondata/processing/loading/converter/impl/FieldEncoderFactory.java
processing/src/main/java/org/apache/carbondata/processing/store/writer/CarbonFactDataWriter.java
core/src/main/java/org/apache/carbondata/core/datamap/DataMapStoreManager.java
processing/src/main/java/org/apache/carbondata/processing/datatypes/PrimitiveDataType.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockletDataMapFactory.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockletDataMap.java
processing/src/main/java/org/apache/carbondata/processing/store/writer/v3/BlockletDataHolder.java
core/src/main/java/org/apache/carbondata/core/indexstore/BlockletDataMapIndexStore.java
processing/src/main/java/org/apache/carbondata/processing/store/writer/AbstractFactDataWriter.java
processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactDataHandlerColumnar.java
processing/src/main/java/org/apache/carbondata/processing/datatypes/StructDataType.java
processing/src/main/java/org/apache/carbondata/processing/datatypes/GenericDataType.java
core/src/main/java/org/apache/carbondata/core/datamap/DataMapDistributable.java
core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
processing/src/main/java/org/apache/carbondata/processing/store/writer/v3/CarbonFactDataWriterImplV3.java
processing/src/main/java/org/apache/carbondata/processing/datamap/DataMapWriterListener.java
processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactDataHandlerModel.java
core/src/main/java/org/apache/carbondata/core/metadata/CarbonTableIdentifier.java
processing/src/main/java/org/apache/carbondata/processing/util/CarbonDataProcessorUtil.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonTableInputFormat.java
processing/src/main/java/org/apache/carbondata/processing/datatypes/ArrayDataType.java
core/src/main/java/org/apache/carbondata/core/datamap/dev/DataMapWriter.java
core/src/main/java/org/apache/carbondata/core/datamap/dev/DataMap.java
core/src/main/java/org/apache/carbondata/core/datamap/dev/DataMapFactory.java
processing/src/main/java/org/apache/carbondata/processing/store/TablePage.java
core/src/main/java/org/apache/carbondata/core/datamap/TableDataMap.java
core/src/main/java/org/apache/carbondata/core/metadata/AbsoluteTableIdentifier.java
processing/src/main/java/org/apache/carbondata/processing/loading/converter/impl/ComplexFieldConverterImpl.java
core/src/main/java/org/apache/carbondata/core/datamap/DataMapMeta.java
core/src/main/java/org/apache/carbondata/core/datastore/page/EncodedTablePage.java
texts:Add DataMapWriter interface
Open up DataMapWriter interface and invoke it when loading data
issueID:CARBONDATA-1364
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/datamap/DataMapStoreManager.java
core/src/main/java/org/apache/carbondata/core/datamap/DataMapJob.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockletDataMapFactory.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockletDataMap.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockletDataMapDistributable.java
core/src/main/java/org/apache/carbondata/core/datamap/DataMapDistributable.java
core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
core/src/main/java/org/apache/carbondata/core/datamap/DistributableDataMapFormat.java
processing/src/main/java/org/apache/carbondata/processing/store/writer/v3/CarbonFactDataWriterImplV3.java
core/src/main/java/org/apache/carbondata/core/util/DataFileFooterConverterV3.java
processing/src/main/java/org/apache/carbondata/processing/merger/CarbonDataMergerUtil.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonTableInputFormat.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
core/src/main/java/org/apache/carbondata/core/util/CarbonMetadataUtil.java
core/src/main/java/org/apache/carbondata/core/datamap/dev/DataMap.java
core/src/main/java/org/apache/carbondata/core/datamap/dev/DataMapFactory.java
core/src/main/java/org/apache/carbondata/core/datamap/TableDataMap.java
core/src/main/java/org/apache/carbondata/core/indexstore/Blocklet.java
core/src/main/java/org/apache/carbondata/core/util/AbstractDataFileFooterConverter.java
texts:Add the blocklet info to index file and make the datamap distributable with job
Add the blocklet info to index file and make the datamap distributable with job1. Add the blocklet info to the carbonindex file, so datamap not required to read each carbondata file footer to the blocklet information. This makes the datamap loading faster.2. Make the data map distributable and add to the spark job. So datamap pruning could happen distributable and pruned blocklet list would be sent to driver.
issueID:CARBONDATA-1365
type:New Feature
changed files:core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/rle/RLECodec.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/reader/measure/v3/MeasureChunkReaderV3.java
core/src/main/java/org/apache/carbondata/core/datastore/page/ColumnPage.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/ColumnPageCodec.java
texts:Add RLE Codec implementation

issueID:CARBONDATA-1366
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
core/src/main/java/org/apache/carbondata/core/util/CarbonProperties.java
texts:When sort_scope=global_sort, use &#39;StorageLevel.MEMORY_AND_DISK_SER&#39; instead of &#39;StorageLevel.MEMORY_AND_DISK&#39; for &#39;convertRDD&#39; persisting  to improve loading performance
My testing env and configs are as followings:Env:6 executors, 9G mem + 6 cores per executor Configs:SINGLE_PASS=trueSORT_SCOPE=GLOBAL_SORTspark.memory.fraction=0.5if using 'convertRDD.persist(StorageLevel.MEMORY_AND_DISK_SER)' in method 'org.apache.carbondata.spark.load.DataLoadProcessBuilderOnSpark.loadDataUsingGlobalSort', it takes about 7.2 min to load 144136697 lines (10.9 G parquet files), and if using 'convertRDD.persist(StorageLevel.MEMORY_AND_DISK)', it takes about 9.5 min to load 144136697 lines.
issueID:CARBONDATA-1367
type:Bug
changed files:
texts:Fix wrong dependency of carbondata-examples-flink
When compiling Carbondata with profile spark-2.1, `carbondata-examples-flink` will be compiled too. But this module depends on `carbondata-example-spark` and `sparkdata-spark` which will be compiled with profile-1.6, so we need to fix it.
issueID:CARBONDATA-1368
type:Test
changed files:core/src/main/java/org/apache/carbondata/core/locks/HdfsFileLock.java
texts:HDFS lock issue in SDV cluster
HDFS lock issue in SDV clusterAll runs share the same lock so resulting some test fails randomly.
issueID:CARBONDATA-1369
type:Bug
changed files:
texts:timestamp type column in where clause cause empty result
if where clause contains column which is timestamp type, it will return empty result.
issueID:CARBONDATA-137
type:Bug
changed files:
texts:Fixed detail limit query statistics issue
Problem:In case of detail query with limit it total query execution time in statistics is printing negative values as wrong method is getting calledSolution:Need to call correct method
issueID:CARBONDATA-1372
type:Improvement
changed files:
texts:Fix some errors and update the examples in documentation
There are some errors in CarbonData docs, these should be fixed.Some examples in documentation should be updated.
issueID:CARBONDATA-1373
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
core/src/main/java/org/apache/carbondata/core/util/CarbonProperties.java
texts:Enhance update performance in carbondata
ScenarioRecently I have tested the update feature provided in Carbondata and found its poor performance.I had a table containing about 14 million records with about 370 columns(no dictionary columns) and the data files are about 3.8 GB in total. All the data files were in one segment.I performed an update SQL which update a column for all the records and the SQL looked like `UPDATE myTable SET (col1)=(col1+1000) WHERE TRUE`. In my environment, the update job failed with 'executor lost errors'. And I found 'spill data' related messages in the container logs. AnalyzeI've read about the implementation of update-delete in Carbondata in ISSUE#440. The update consists a delete and an insert operation. And the error occurred during the insert operation.After studying the code, I have found that while doing inserting, the updated records are grouped by the `segmentId`, which means all the recoreds in one segment will be processed in only one task, thus will cause task failure when the amount of input data is quite large. SolutionWe should improve the parallelism when doing update for a segment.I append a random key to the `segmentId` to increase the partition number before doing the insertion stage and then remove the suffix when doing the real insertion.I have tested in my example and the job finished in about 13 minutes successfully. The records were updated as expected.
issueID:CARBONDATA-1375
type:Bug
changed files:
texts:clean hive pom
the hive pom contains some unnecessary dependencies
issueID:CARBONDATA-1379
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/keygenerator/directdictionary/timestamp/DateDirectDictionaryGenerator.java
core/src/main/java/org/apache/carbondata/core/scan/expression/ExpressionResult.java
texts:Date range filter with cast not working
Date range filter with cast not working. Query like below cannot work.select doj from directDictionaryTable where doj > cast('2016-03-14' as date)
issueID:CARBONDATA-138
type:Bug
changed files:
texts:Scale up value of Avg aggregation for decimal type keeping sync with hive
when we do avg aggregation for decimal type:hive wiil update the precision of the avg result. e.g: Avg(decimal(30,10)), the result will be cast to decimal(34, 14)so, carbon should keep the sync with hive, also need to improve the precisionby scaling up.
issueID:CARBONDATA-1380
type:Bug
changed files:
texts:Tablestatus file is not updated in case of load failure. Insert Overwrite does not work properly
Tablestatus file is not updated in case of load failure. So Insert Overwrite does not work properly
issueID:CARBONDATA-1386
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/memory/MemoryBlock.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RestructureExcludeFilterExecutorImpl.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/TableInfo.java
core/src/main/java/org/apache/carbondata/core/datamap/DataMapStoreManager.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/comparator/UnsafeRowComparator.java
core/src/main/java/org/apache/carbondata/core/util/CarbonProperties.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/resolverinfo/visitor/NoDictionaryTypeVisitor.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/store/impl/unsafe/UnsafeFixedLengthDimensionDataChunkStore.java
core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
core/src/main/java/org/apache/carbondata/core/datastore/block/TableTaskInfo.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/rle/RLECodec.java
core/src/main/java/org/apache/carbondata/core/memory/CarbonUnsafe.java
core/src/main/java/org/apache/carbondata/core/locks/HdfsFileLock.java
core/src/main/java/org/apache/carbondata/core/memory/IntPointerBuffer.java
core/src/main/java/org/apache/carbondata/core/stats/QueryStatisticsRecorderImpl.java
core/src/main/java/org/apache/carbondata/core/util/ByteUtil.java
core/src/main/java/org/apache/carbondata/core/indexstore/UnsafeMemoryDMStore.java
core/src/main/java/org/apache/carbondata/core/mutate/CarbonUpdateUtil.java
core/src/main/java/org/apache/carbondata/core/memory/UnsafeMemoryAllocator.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockletDataRefNode.java
core/src/main/java/org/apache/carbondata/core/scan/complextypes/ComplexQueryType.java
core/src/main/java/org/apache/carbondata/core/locks/ZooKeeperLocking.java
core/src/main/java/org/apache/carbondata/core/memory/UnsafeSortMemoryManager.java
core/src/main/java/org/apache/carbondata/core/keygenerator/directdictionary/timestamp/TimeStampDirectDictionaryGenerator.java
core/src/main/java/org/apache/carbondata/core/memory/UnsafeMemoryManager.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RestructureIncludeFilterExecutorImpl.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/store/impl/unsafe/UnsafeAbstractDimensionDataChunkStore.java
core/src/main/java/org/apache/carbondata/core/indexstore/row/UnsafeDataMapRow.java
core/src/main/java/org/apache/carbondata/core/keygenerator/directdictionary/timestamp/DateDirectDictionaryGenerator.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/resolverinfo/visitor/RangeDirectDictionaryVisitor.java
core/src/main/java/org/apache/carbondata/core/datamap/TableDataMap.java
core/src/main/java/org/apache/carbondata/core/scan/expression/RangeExpressionEvaluator.java
common/src/main/java/org/apache/carbondata/common/logging/impl/ExtendedRollingFileAppender.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/resolverinfo/MeasureColumnResolvedFilterInfo.java
core/src/main/java/org/apache/carbondata/core/util/DataTypeUtil.java
core/src/main/java/org/apache/carbondata/core/scan/executor/util/RestructureUtil.java
core/src/main/java/org/apache/carbondata/core/scan/expression/conditional/ListExpression.java
core/src/main/java/org/apache/carbondata/core/datastore/block/TableBlockInfo.java
core/src/main/java/org/apache/carbondata/core/scan/result/vector/ColumnVectorInfo.java
core/src/main/java/org/apache/carbondata/core/scan/executor/util/QueryUtil.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/store/impl/unsafe/UnsafeVariableLengthDimensionDataChunkStore.java
core/src/main/java/org/apache/carbondata/core/datastore/impl/FileReaderImpl.java
core/src/main/java/org/apache/carbondata/core/util/DataTypeConverterImpl.java
core/src/main/java/org/apache/carbondata/core/datastore/page/UnsafeFixLengthColumnPage.java
core/src/main/java/org/apache/carbondata/core/scan/filter/optimizer/RangeFilterOptmizer.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/resolverinfo/visitor/RangeDictionaryColumnVisitor.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/comparator/UnsafeRowComparatorForNormalDIms.java
core/src/main/java/org/apache/carbondata/core/datastore/page/ColumnPage.java
core/src/main/java/org/apache/carbondata/core/metadata/datatype/DecimalConverterFactory.java
core/src/main/java/org/apache/carbondata/core/keygenerator/columnar/impl/MultiDimKeyVarLengthVariableSplitGenerator.java
core/src/main/java/org/apache/carbondata/core/scan/filter/FilterExpressionProcessor.java
core/src/main/java/org/apache/carbondata/core/util/comparator/Comparator.java
core/src/main/java/org/apache/carbondata/core/indexstore/Blocklet.java
core/src/main/java/org/apache/carbondata/core/scan/filter/FilterUtil.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/UnsafeSortDataRows.java
core/src/main/java/org/apache/carbondata/core/datastore/columnar/BlockIndexerStorageForShort.java
core/src/main/java/org/apache/carbondata/core/datastore/page/UnsafeVarLengthColumnPage.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockletDataMap.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/impl/FixedLengthDimensionColumnPage.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/UnsafeCarbonRowPage.java
core/src/main/java/org/apache/carbondata/core/scan/collector/impl/RestructureBasedRawResultCollector.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/adaptive/AdaptiveDeltaIntegralCodec.java
core/src/main/java/org/apache/carbondata/core/datastore/impl/FileFactory.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/resolverinfo/visitor/DictionaryColumnVisitor.java
core/src/main/java/org/apache/carbondata/core/keygenerator/columnar/impl/MultiDimKeyVarLengthEquiSplitGenerator.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/resolverinfo/visitor/CustomTypeDictionaryVisitor.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/resolverinfo/visitor/MeasureColumnVisitor.java
core/src/main/java/org/apache/carbondata/core/datastore/page/statistics/PrimitivePageStatsCollector.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/resolverinfo/visitor/RangeNoDictionaryTypeVisitor.java
texts:Fix findbugs issues in carbondata-core module

issueID:CARBONDATA-139
type:Bug
changed files:
texts:Inconsistency during sortindex file reading
Earlier we used to check size of dictionary and then create sortIndex file,to read, by appending size of dictionary e.g columid_offset.sortindexDuring sort index reading phase, read offset from dictionary chunk meta and then get respective file.
issueID:CARBONDATA-1392
type:Bug
changed files:
texts:Fixed bug for fetching the error value of decimal type in presto
The following is based on TPCH test.create statement:create table lineitem (l_orderkey int ,l_partkey int ,l_suppkey int ,l_linenumber int ,l_quantity decimal(15,2) ,l_extendedprice decimal(15,2) ,l_discount decimal(15,2) ,l_tax decimal(15,2) ,l_returnflag string ,l_linestatus string ,l_shipdate date ,l_commitdate date ,l_receiptdate date ,l_shipinstruct string ,l_shipmode string ,l_comment string ) stored by 'carbondata'query sql:select l_quantity,l_extendedprice,l_discount,l_tax from lineitem where l_orderkey=15413986 and l_linenumber=7;The below is correct in spark:l_quantity | l_extendedprice | l_discount | l_tax   11.00     |     16549.17       |    0.07       | 0.04The below is wrong in presto:l_quantity | l_extendedprice | l_discount | l_tax    0.11     |        165.49         |     0.00      | 0.00
issueID:CARBONDATA-1393
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/memory/MemoryBlock.java
core/src/main/java/org/apache/carbondata/core/memory/UnsafeMemoryManager.java
core/src/main/java/org/apache/carbondata/core/memory/HeapMemoryAllocator.java
core/src/main/java/org/apache/carbondata/core/memory/UnsafeMemoryAllocator.java
core/src/main/java/org/apache/carbondata/core/memory/UnsafeSortMemoryManager.java
texts:Throw NullPointerException when UnsafeMemoryManager.freeMemory
UnsafeMemoryManager.freeMemoryAll(long taskId) may run before freeMemory(long taskId, MemoryBlock memoryBlock), taskIdToMemoryBlockMap.get(taskId) will return null and then throw NPE.
issueID:CARBONDATA-1395
type:Bug
changed files:hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonTableInputFormat.java
core/src/main/java/org/apache/carbondata/hadoop/CarbonInputSplit.java
hadoop/src/main/java/org/apache/carbondata/hadoop/util/CarbonInputSplitTaskInfo.java
processing/src/main/java/org/apache/carbondata/processing/util/DeleteLoadFolders.java
texts:Fix Findbugs issues in carbondata-hadoop module

issueID:CARBONDATA-1396
type:Bug
changed files:integration/hive/src/main/java/org/apache/carbondata/hive/MapredCarbonInputFormat.java
integration/hive/src/main/java/org/apache/carbondata/hive/CarbonObjectInspector.java
integration/hive/src/main/java/org/apache/carbondata/hive/test/server/HiveEmbeddedServer2.java
integration/hive/src/main/java/org/apache/carbondata/hive/CarbonHiveInputSplit.java
integration/hive/src/main/java/org/apache/carbondata/hive/CarbonDictionaryDecodeReadSupport.java
integration/hive/src/main/java/org/apache/carbondata/hive/CarbonHiveRecordReader.java
texts:Fix findbugs issues in carbondata-hive module

issueID:CARBONDATA-1397
type:Bug
changed files:integration/presto/src/main/java/org/apache/carbondata/presto/CarbondataPageSource.java
integration/presto/src/main/java/org/apache/carbondata/presto/impl/CarbonTableReader.java
integration/presto/src/main/java/org/apache/carbondata/presto/CarbondataSplitManager.java
texts:Fix findbugs issues in carbondata-presto module

issueID:CARBONDATA-1399
type:Bug
changed files:
texts:Enable findbugs to run by default on every build

issueID:CARBONDATA-14
type:Bug
changed files:
texts:carbon.cutOffTimestamp and carbon.timegranularity is not added in the carbon.properties.template
carbon.cutOffTimestamp and carbon.timegranularity is not added in the carbon.properties.templateExpected:The above property should be available in the carbon.properties.template.Actual:not present in carbon.properties.template
issueID:CARBONDATA-140
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/scan/result/impl/NonFilterQueryScannedResult.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/SchemaReader.java
hadoop/src/main/java/org/apache/carbondata/hadoop/readsupport/CarbonReadSupport.java
core/src/main/java/org/apache/carbondata/core/util/ObjectSerializationUtil.java
hadoop/src/main/java/org/apache/carbondata/hadoop/readsupport/impl/DictionaryDecodeReadSupport.java
core/src/main/java/org/apache/carbondata/core/datastore/block/AbstractIndex.java
hadoop/src/main/java/org/apache/carbondata/hadoop/CarbonProjection.java
hadoop/src/main/java/org/apache/carbondata/hadoop/CarbonRecordReader.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/impl/VariableLengthDimensionDataChunk.java
integration/spark-common/src/main/java/org/apache/carbondata/spark/merger/NodeMultiBlockRelation.java
core/src/main/java/org/apache/carbondata/core/keygenerator/columnar/impl/MultiDimKeyVarLengthVariableSplitGenerator.java
texts:Fix legal
Lot of files doesn't contain the required license header.I will fix that and add a rat plugin profile to check this.
issueID:CARBONDATA-1400
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/datastore/page/LazyColumnPage.java
core/src/main/java/org/apache/carbondata/core/datastore/page/VarLengthColumnPageBase.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/reader/dimension/v3/DimensionChunkReaderV3.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/AbstractRawColumnChunk.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/ColumnPageEncoderMeta.java
processing/src/main/java/org/apache/carbondata/processing/datatypes/StructDataType.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/compress/DirectCompressCodec.java
processing/src/main/java/org/apache/carbondata/processing/datatypes/GenericDataType.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/rle/RLEEncoderMeta.java
core/src/main/java/org/apache/carbondata/core/datastore/page/UnsafeFixLengthColumnPage.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/impl/DimensionRawColumnChunk.java
core/src/main/java/org/apache/carbondata/core/datastore/page/SafeVarLengthColumnPage.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/adaptive/AdaptiveCodec.java
core/src/main/java/org/apache/carbondata/core/datastore/block/SegmentProperties.java
core/src/main/java/org/apache/carbondata/core/datastore/page/ColumnPage.java
core/src/main/java/org/apache/carbondata/core/datastore/TableSpec.java
core/src/main/java/org/apache/carbondata/core/datastore/page/SafeFixLengthColumnPage.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/adaptive/AdaptiveFloatingCodec.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/rle/RLECodec.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/adaptive/AdaptiveIntegralCodec.java
core/src/main/java/org/apache/carbondata/core/scan/complextypes/ArrayQueryType.java
processing/src/main/java/org/apache/carbondata/processing/store/TablePage.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/reader/measure/AbstractMeasureChunkReader.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/reader/measure/v3/MeasureChunkReaderV3.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/impl/MeasureRawColumnChunk.java
core/src/main/java/org/apache/carbondata/core/datastore/page/UnsafeVarLengthColumnPage.java
processing/src/main/java/org/apache/carbondata/processing/datatypes/PrimitiveDataType.java
core/src/main/java/org/apache/carbondata/core/cache/dictionary/ColumnDictionaryInfo.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/ColumnPageEncoder.java
core/src/main/java/org/apache/carbondata/core/scan/complextypes/ComplexQueryType.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/adaptive/AdaptiveDeltaIntegralCodec.java
core/src/main/java/org/apache/carbondata/core/datastore/ColumnType.java
processing/src/main/java/org/apache/carbondata/processing/util/CarbonDataProcessorUtil.java
processing/src/main/java/org/apache/carbondata/processing/datatypes/ArrayDataType.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/store/ColumnPageWrapper.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/EncodingFactory.java
core/src/main/java/org/apache/carbondata/core/datastore/page/statistics/PrimitivePageStatsCollector.java
core/src/main/java/org/apache/carbondata/core/scan/complextypes/StructQueryType.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/DefaultEncodingFactory.java
core/src/main/java/org/apache/carbondata/core/scan/complextypes/PrimitiveQueryType.java
texts:Array column out of bound when writing carbondata file
If there are big array in input csv file, when loading carbondata table, it will throw ArrayIndexOutOfBoundException
issueID:CARBONDATA-1401
type:Bug
changed files:
texts:List Info validate Issue
fix  duplicate issue in list info
issueID:CARBONDATA-1403
type:Bug
changed files:
texts:Compaction log is not correct

issueID:CARBONDATA-1406
type:Bug
changed files:
texts:Fix inconsistent usage of QUOTATION MARK " and LEFT DOUBLE QUOTATION MARK  “ in installation.md file
The inconsistent usage of QUOTATION MARK " and LEFT DOUBLE QUOTATION MARK  “ in installation.md file are creating conversion issue while generating the documentation for the website from the docs.
issueID:CARBONDATA-1407
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/scan/filter/FilterUtil.java
texts:The default end key is not correct for plain-text dimension
Now the default end key is 127 (0xEF),  we should change it to 0xFF
issueID:CARBONDATA-1408
type:Bug
changed files:
texts:Data loading with GlobalSort is failing in long run scenario

issueID:CARBONDATA-1409
type:Improvement
changed files:
texts:data loading quotechar should support empty
now the the default data loading quotechar is a quotes ", so for the following schema and data cannot loading properly. schema:  (id int, name string, address string)data: 1001, "bill01, add01.if load this data to the schema name column will be "bill01, add01" and address column value is null because the default quotechar is ", if support empty quotechar that can loading data properly.
issueID:CARBONDATA-141
type:Bug
changed files:
texts:Polish Maven coordinates and define Apache parent POM
In preparation of the first release, the Maven coordinates should be cleanup, and the main Apache POM should be used.
issueID:CARBONDATA-1410
type:Bug
changed files:processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/SortDataRows.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/merger/UnsafeIntermediateFileMerger.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/merger/UnsafeIntermediateMerger.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/SingleThreadFinalSortFilesMerger.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/SortTempFileChunkHolder.java
processing/src/main/java/org/apache/carbondata/processing/store/writer/AbstractFactDataWriter.java
processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactDataHandlerColumnar.java
processing/src/main/java/org/apache/carbondata/processing/util/CarbonLoaderUtil.java
processing/src/main/java/org/apache/carbondata/processing/loading/converter/impl/RowConverterImpl.java
processing/src/main/java/org/apache/carbondata/processing/loading/steps/InputProcessorStepImpl.java
core/src/main/java/org/apache/carbondata/core/datastore/exception/CarbonDataWriterException.java
processing/src/main/java/org/apache/carbondata/processing/store/writer/v3/CarbonFactDataWriterImplV3.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/impl/ParallelReadMergeSorterImpl.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/SortIntermediateFileMerger.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/holder/UnsafeSortTempFileChunkHolder.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/impl/UnsafeParallelReadMergeSorterImpl.java
core/src/main/java/org/apache/carbondata/core/util/CarbonThreadFactory.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/IntermediateFileMerger.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/UnsafeSortDataRows.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/merger/UnsafeSingleThreadFinalSortFilesMerger.java
texts:Thread leak issue in case of data loading failure
Thread leak issue in case of data loading failure
issueID:CARBONDATA-1411
type:Bug
changed files:
texts:Show Segment command gives Null Pointer Exception after the table is updated
I created a table :CREATE TABLE uniqdata (CUST_ID int,CUST_NAME String,ACTIVE_EMUI_VERSION string, DOB timestamp, DOJ timestamp, BIGINT_COLUMN1 bigint,BIGINT_COLUMN2 bigint,DECIMAL_COLUMN1 decimal(30,10), DECIMAL_COLUMN2 decimal(36,10),Double_COLUMN1 double, Double_COLUMN2 double,INTEGER_COLUMN1 int) STORED BY 'org.apache.carbondata.format' TBLPROPERTIES ("TABLE_BLOCKSIZE"= "256 MB")and loaded the table : LOAD DATA inpath 'hdfs://localhost:54310/data/2000_UniqData.csv' INTO table uniqdata options('DELIMITER'=',', 'FILEHEADER'='CUST_ID, CUST_NAME, ACTIVE_EMUI_VERSION, DOB, DOJ, BIGINT_COLUMN1, BIGINT_COLUMN2, DECIMAL_COLUMN1, DECIMAL_COLUMN2, Double_COLUMN1, Double_COLUMN2, INTEGER_COLUMN1')I loaded the table 7 times using multiple cvs.Then executed the following command :show segments for table uniqdata;and output was :0: jdbc:hive2://localhost:10000> show segments for table uniqdata;------------------------------------------------------------------------------ SegmentSequenceId     Status        Load Start Time            Load End Time       ------------------------------------------------------------------------------ 5                   Success     2017-08-28 13:19:01.779   2017-08-28 13:19:02.527   4                   Success     2017-08-28 13:18:58.597   2017-08-28 13:18:59.339   3                   Compacted   2017-08-28 13:18:48.341   2017-08-28 13:18:49.527   2                   Compacted   2017-08-28 13:18:44.96    2017-08-28 13:18:46.222   1                   Compacted   2017-08-28 13:17:56.818   2017-08-28 13:17:57.988   0.1                 Success     2017-08-28 13:18:49.698   2017-08-28 13:18:50.659   0                   Compacted   2017-08-28 13:17:41.944   2017-08-28 13:17:43.564  ------------------------------------------------------------------------------now after that I executed the following query :update uniqdata set (dob)=('2016-12-31 12:00:00') where cust_name='CUST_NAME_01998';and then again ran : show segments for table uniqdata;And now I am getting : Error: java.lang.NullPointerException (state=,code=0)
issueID:CARBONDATA-1412
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/statusmanager/LoadMetadataDetails.java
texts:delete working incorrectly while using segment.starttime before &#39;<any_date_value>&#39;
Issue exists in the below query :delete from table uniqdata_delete where segment.starttime before 'starttime_of_last_segment_created';It should mark all those segments for delete whose start-time is before the given time and should not delete the segment with the given time.But it is marking the segment for delete which is having the exact start time also.To replicate the issue: CREATE TABLE uniqdata_delete (CUST_ID int,CUST_NAME String,ACTIVE_EMUI_VERSION string, DOB timestamp, DOJ timestamp, BIGINT_COLUMN1 bigint,BIGINT_COLUMN2 bigint,DECIMAL_COLUMN1 decimal(30,10), DECIMAL_COLUMN2 decimal(36,10),Double_COLUMN1 double, Double_COLUMN2 double, INTEGER_COLUMN1 int) STORED BY 'org.apache.carbondata.format' TBLPROPERTIES ("TABLE_BLOCKSIZE"= "256 MB")LOAD DATA INPATH 'hdfs://localhost:54310/user/hduser/input-files/3000_UniqData.csv' into table uniqdata_delete OPTIONS('FILEHEADER'='CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1,Double_COLUMN2,INTEGER_COLUMN1')LOAD DATA INPATH 'hdfs://localhost:54310/user/hduser/input-files/3000_UniqData.csv' into table uniqdata_delete OPTIONS('FILEHEADER'='CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1,Double_COLUMN2,INTEGER_COLUMN1')delete from table uniqdata_delete where segment.starttime before 'starttime_of_last_segment_created';
issueID:CARBONDATA-1413
type:Bug
changed files:
texts:Incorrect result displays after creating a partition table with incorrect range_info
Incorrect result displays after creating a partition table with incorrect range_infoDescription: While creating a partitioned table with string data type column if we provide wrong credentials it should not create partition but in this scenario when I am providing range info as any integer data type instead of string data type, it is creating a partitioned table which is not a correct result, it should display an error saying invalid credentials.Steps to Reproduce1)Create a Partitioned table with incorrect credentialsCREATE TABLE uniqdata_part2_incorrect1 (CUST_ID int,CUST_NAME String,DOB Timestamp,DOJ timestamp, BIGINT_COLUMN1 bigint,BIGINT_COLUMN2 bigint,DECIMAL_COLUMN1 decimal(30,10), DECIMAL_COLUMN2 decimal(36,10),Double_COLUMN1 double, Double_COLUMN2 double,INTEGER_COLUMN1 int) PARTITIONED BY (ACTIVE_EMUI_VERSION string) STORED BY 'org.apache.carbondata.format' TBLPROPERTIES ('PARTITION_TYPE'='RANGE','RANGE_INFO'='1',"TABLE_BLOCKSIZE"= "256 MB");2) Load data into the partitioned tableLOAD DATA INPATH 'hdfs://localhost:54310/uniqdata/2000_UniqData.csv' into table uniqdata_part2_incorrect1 OPTIONS('DELIMITER'=',' , 'QUOTECHAR'='"','BAD_RECORDS_ACTION'='FORCE','FILEHEADER'='CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1,Double_COLUMN2,INTEGER_COLUMN1');3) Execute Query:show partitions uniqdata_part2_incorrect1;4)Result On beeline:-----------------------------------+             partition             -----------------------------------+ 0, active_emui_version = DEFAULT   1, active_emui_version < 1        -----------------------------------+5)Expected Result:The partitioned table should not be created as there were wrong credentials in the create table query(integer at the place of string)
issueID:CARBONDATA-1415
type:Bug
changed files:
texts:Show Segments raises exception for a Partition Table after Updation.
1. Create Partition Table :DROP TABLE IF EXISTS list_partition_table_string; CREATE TABLE list_partition_table_string(shortField SHORT, intField INT, bigintField LONG, doubleField DOUBLE, timestampField TIMESTAMP, decimalField DECIMAL(18,2), dateField DATE, charField CHAR(5), floatField FLOAT, complexData ARRAY<STRING> ) PARTITIONED BY (stringField STRING) STORED BY 'carbondata' TBLPROPERTIES('PARTITION_TYPE'='LIST', 'LIST_INFO'='Asia, America, Europe', 'DICTIONARY_EXCLUDE'='stringfield');2. Load Data :load data inpath 'hdfs://localhost:54310/CSV/list_partition_table.csv' into table list_partition_table_string options('FILEHEADER'='shortfield,intfield,bigintfield,doublefield,stringfield,timestampfield,decimalfield,datefield,charfield,floatfield,complexdata', 'COMPLEX_DELIMITER_LEVEL_1'='$','COMPLEX_DELIMITER_LEVEL_2'='#', 'SINGLE_PASS'='TRUE');3. Update Data :update list_partition_table_string set (stringfield)=('China') where stringfield = 'Japan' ; update list_partition_table_string set (stringfield)=('Japan') where stringfield > 'Europe' ; update list_partition_table_string set (stringfield)=('Asia') where stringfield < 'Europe' ;4. Compaction :ALTER TABLE list_partition_table_string COMPACT 'Minor'; Show segments for table list_partition_table_string;Expected Output: Segments Must be Displayed.Actual Output : Error: java.lang.NullPointerException (state=,code=0)
issueID:CARBONDATA-1417
type:Bug
changed files:
texts:Add Cluster tests for IUD, batch sort and Global sort features

issueID:CARBONDATA-1418
type:Improvement
changed files:integration/presto/src/main/java/org/apache/carbondata/presto/PrestoFilterUtil.java
integration/presto/src/main/java/org/apache/carbondata/presto/impl/CarbonLocalInputSplit.java
integration/presto/src/main/java/org/apache/carbondata/presto/impl/CarbonTableReader.java
texts:Use CarbonTableInputFormat in Presto Integration
Use CarbonTableInputFormat in Presto Integration
issueID:CARBONDATA-142
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/metadata/blocklet/BlockletInfo.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/TableInfo.java
core/src/main/java/org/apache/carbondata/core/datastore/columnar/UnBlockIndexer.java
core/src/main/java/org/apache/carbondata/core/util/DataFileFooterConverter.java
core/src/main/java/org/apache/carbondata/core/scan/collector/impl/RawBasedResultCollector.java
core/src/main/java/org/apache/carbondata/core/scan/model/QueryMeasure.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelRangeGrtrThanEquaToFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/FilterResolverIntf.java
core/src/main/java/org/apache/carbondata/core/datastore/FileHolder.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/SchemaEvolutionEntry.java
core/src/main/java/org/apache/carbondata/core/scan/expression/exception/FilterUnsupportedException.java
integration/spark-common/src/main/java/org/apache/carbondata/spark/merger/CarbonCompactionExecutor.java
core/src/main/java/org/apache/carbondata/core/datastore/block/Distributable.java
core/src/main/java/org/apache/carbondata/core/util/ByteUtil.java
core/src/main/java/org/apache/carbondata/core/datastore/page/KeyColumnPage.java
core/src/main/java/org/apache/carbondata/core/cache/dictionary/ColumnDictionaryInfo.java
integration/spark-common/src/main/java/org/apache/carbondata/spark/merger/CarbonCompactionUtil.java
core/src/main/java/org/apache/carbondata/core/scan/executor/impl/AbstractQueryExecutor.java
core/src/main/java/org/apache/carbondata/core/scan/filter/DimColumnFilterInfo.java
core/src/main/java/org/apache/carbondata/core/scan/expression/LiteralExpression.java
core/src/main/java/org/apache/carbondata/core/datastore/MeasureDataWrapper.java
processing/src/main/java/org/apache/carbondata/processing/util/CarbonDataProcessorUtil.java
core/src/main/java/org/apache/carbondata/core/datastore/IndexKey.java
common/src/main/java/org/apache/carbondata/common/CarbonIterator.java
core/src/main/java/org/apache/carbondata/core/scan/processor/BlockletIterator.java
core/src/main/java/org/apache/carbondata/core/cache/dictionary/DictionaryColumnUniqueIdentifier.java
core/src/main/java/org/apache/carbondata/core/keygenerator/columnar/ColumnarSplitter.java
core/src/main/java/org/apache/carbondata/core/cache/dictionary/Dictionary.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/AndFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/util/DataTypeUtil.java
core/src/main/java/org/apache/carbondata/core/scan/expression/ExpressionResult.java
core/src/main/java/org/apache/carbondata/core/keygenerator/directdictionary/DirectDictionaryGenerator.java
core/src/main/java/org/apache/carbondata/core/cache/dictionary/DictionaryInfo.java
core/src/main/java/org/apache/carbondata/core/datastore/filesystem/AbstractDFSCarbonFile.java
core/src/main/java/org/apache/carbondata/core/datastore/impl/FileHolderImpl.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/reader/MeasureColumnChunkReader.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/SortObserver.java
core/src/main/java/org/apache/carbondata/core/scan/result/iterator/DetailQueryResultIterator.java
integration/spark-common/src/main/java/org/apache/carbondata/spark/exception/MalformedCarbonCommandException.java
processing/src/main/java/org/apache/carbondata/processing/sort/exception/CarbonSortKeyAndGroupByException.java
integration/spark-common/src/main/java/org/apache/carbondata/spark/merger/CarbonDataMergerUtil.java
core/src/main/java/org/apache/carbondata/core/scan/filter/FilterExpressionProcessor.java
core/src/main/java/org/apache/carbondata/core/metadata/converter/SchemaConverter.java
core/src/main/java/org/apache/carbondata/core/scan/expression/conditional/LessThanEqualToExpression.java
core/src/main/java/org/apache/carbondata/core/scan/model/QueryDimension.java
core/src/main/java/org/apache/carbondata/core/scan/collector/impl/AbstractScannedResultCollector.java
core/src/main/java/org/apache/carbondata/core/keygenerator/mdkey/Bits.java
core/src/main/java/org/apache/carbondata/core/scan/executor/exception/QueryExecutionException.java
core/src/main/java/org/apache/carbondata/core/datastore/impl/FileFactory.java
core/src/main/java/org/apache/carbondata/core/datastore/filesystem/LocalCarbonFile.java
core/src/main/java/org/apache/carbondata/core/datastore/compression/Compressor.java
core/src/main/java/org/apache/carbondata/core/scan/expression/conditional/GreaterThanEqualToExpression.java
processing/src/main/java/org/apache/carbondata/processing/surrogatekeysgenerator/csvbased/BadRecordsLogger.java
core/src/main/java/org/apache/carbondata/core/metadata/converter/ThriftWrapperSchemaConverterImpl.java
core/src/main/java/org/apache/carbondata/core/datastore/impl/DFSFileHolderImpl.java
core/src/main/java/org/apache/carbondata/core/stats/QueryStatisticsRecorder.java
core/src/main/java/org/apache/carbondata/core/scan/expression/conditional/EqualToExpression.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/RowLevelFilterResolverImpl.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/column/CarbonMeasure.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/resolverinfo/visitor/NoDictionaryTypeVisitor.java
core/src/main/java/org/apache/carbondata/core/datastore/exception/CarbonDataWriterException.java
core/src/main/java/org/apache/carbondata/core/util/path/CarbonTablePath.java
core/src/main/java/org/apache/carbondata/core/metadata/blocklet/index/BlockletMinMaxIndex.java
core/src/main/java/org/apache/carbondata/core/reader/CarbonFooterReader.java
core/src/main/java/org/apache/carbondata/core/scan/expression/LeafExpression.java
core/src/main/java/org/apache/carbondata/core/datastore/block/TableTaskInfo.java
processing/src/main/java/org/apache/carbondata/processing/store/CarbonDataFileAttributes.java
core/src/main/java/org/apache/carbondata/core/scan/model/QueryModel.java
core/src/main/java/org/apache/carbondata/core/datastore/filesystem/HDFSCarbonFile.java
core/src/main/java/org/apache/carbondata/core/scan/collector/ScannedResultCollector.java
core/src/main/java/org/apache/carbondata/core/scan/expression/conditional/BinaryConditionalExpression.java
core/src/main/java/org/apache/carbondata/core/scan/wrappers/ByteArrayWrapper.java
common/src/main/java/org/apache/carbondata/common/logging/LogService.java
core/src/main/java/org/apache/carbondata/core/metadata/ColumnIdentifier.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/DimColumnExecuterFilterInfo.java
processing/src/main/java/org/apache/carbondata/processing/model/CarbonLoadModel.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/column/ColumnSchema.java
core/src/main/java/org/apache/carbondata/core/keygenerator/mdkey/AbstractKeyGenerator.java
core/src/main/java/org/apache/carbondata/core/metadata/ValueEncoderMeta.java
core/src/main/java/org/apache/carbondata/core/scan/filter/intf/ExpressionType.java
processing/src/main/java/org/apache/carbondata/core/datastore/GenericDataType.java
processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactDataHandlerModel.java
core/src/main/java/org/apache/carbondata/core/stats/QueryStatistic.java
hadoop/src/main/java/org/apache/carbondata/hadoop/CarbonProjection.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/IncludeFilterExecuterImpl.java
integration/spark-common/src/main/java/org/apache/carbondata/spark/merger/NodeMultiBlockRelation.java
core/src/main/java/org/apache/carbondata/core/cache/dictionary/DictionaryByteArrayWrapper.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/SortDataRows.java
core/src/main/java/org/apache/carbondata/core/util/LoadStatistics.java
core/src/main/java/org/apache/carbondata/core/metadata/index/BlockIndexInfo.java
processing/src/main/java/org/carbondata/processing/store/writer/exception/CarbonDataWriterException.java
core/src/main/java/org/carbondata/core/metadata/BlockletInfo.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/resolverinfo/MeasureColumnResolvedFilterInfo.java
core/src/main/java/org/apache/carbondata/core/metadata/datatype/DataType.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/SingleThreadFinalSortFilesMerger.java
core/src/main/java/org/apache/carbondata/core/scan/result/iterator/AbstractDetailQueryResultIterator.java
common/src/main/java/org/apache/carbondata/common/logging/impl/AuditLevel.java
core/src/main/java/org/apache/carbondata/core/scan/executor/util/QueryUtil.java
processing/src/main/java/org/apache/carbondata/processing/datatypes/GenericDataType.java
core/src/main/java/org/apache/carbondata/core/scan/expression/logical/OrExpression.java
core/src/main/java/org/apache/carbondata/core/keygenerator/factory/KeyGeneratorFactory.java
core/src/main/java/org/apache/carbondata/core/datastore/DataRefNode.java
core/src/main/java/org/apache/carbondata/core/util/ObjectSerializationUtil.java
core/src/main/java/org/apache/carbondata/core/keygenerator/mdkey/MultiDimKeyVarLengthGenerator.java
core/src/main/java/org/apache/carbondata/core/reader/CarbonIndexFileReader.java
core/src/main/java/org/apache/carbondata/core/cache/dictionary/ForwardDictionary.java
core/src/main/java/org/apache/carbondata/core/keygenerator/columnar/impl/MultiDimKeyVarLengthVariableSplitGenerator.java
core/src/main/java/org/apache/carbondata/core/scan/filter/FilterUtil.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/column/CarbonColumn.java
core/src/main/java/org/apache/carbondata/core/datastore/block/AbstractIndex.java
core/src/main/java/org/apache/carbondata/core/cache/CacheProvider.java
core/src/main/java/org/apache/carbondata/core/cache/dictionary/AbstractColumnDictionaryInfo.java
core/src/main/java/org/apache/carbondata/core/keygenerator/KeyGenException.java
core/src/main/java/org/apache/carbondata/core/datastore/block/TaskBlockInfo.java
common/src/main/java/org/apache/carbondata/common/logging/impl/StatisticLevel.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/resolverinfo/visitor/DictionaryColumnVisitor.java
core/src/main/java/org/apache/carbondata/core/scan/filter/FilterProcessor.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/resolverinfo/visitor/CustomTypeDictionaryVisitor.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
processing/src/main/java/org/apache/carbondata/processing/datatypes/ArrayDataType.java
core/src/main/java/org/apache/carbondata/core/reader/CarbonDictionaryMetadataReader.java
core/src/main/java/org/apache/carbondata/core/scan/executor/impl/DetailQueryExecutor.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/IntermediateFileMerger.java
core/src/main/java/org/apache/carbondata/core/scan/expression/UnknownExpression.java
core/src/main/java/org/apache/carbondata/core/cache/CacheType.java
core/src/main/java/org/apache/carbondata/core/scan/result/iterator/ChunkRowIterator.java
core/src/main/java/org/apache/carbondata/core/cache/dictionary/DictionaryChunksWrapper.java
integration/spark-common/src/main/java/org/apache/carbondata/spark/util/CarbonQueryUtil.java
core/src/main/java/org/apache/carbondata/core/datastore/filesystem/CarbonFile.java
core/src/main/java/org/apache/carbondata/core/util/CarbonProperties.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/resolverinfo/DimColumnResolvedFilterInfo.java
processing/src/main/java/org/apache/carbondata/processing/datatypes/StructDataType.java
core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
core/src/main/java/org/apache/carbondata/core/service/impl/ColumnUniqueIdGenerator.java
core/src/main/java/org/apache/carbondata/core/scan/filter/intf/RowImpl.java
core/src/main/java/org/apache/carbondata/core/datastore/block/SegmentProperties.java
core/src/main/java/org/apache/carbondata/core/metadata/blocklet/SegmentInfo.java
core/src/main/java/org/apache/carbondata/core/reader/ThriftReader.java
core/src/main/java/org/apache/carbondata/core/datastore/columnar/ColumnarKeyStoreMetadata.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/TableSchema.java
core/src/main/java/org/apache/carbondata/core/service/ColumnUniqueIdService.java
processing/src/main/java/org/carbondata/processing/iterator/CarbonIterator.java
integration/spark-common/src/main/java/org/apache/carbondata/spark/merger/RowResultMerger.java
core/src/main/java/org/apache/carbondata/core/writer/ThriftWriter.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/ExcludeFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/reader/CarbonDictionaryColumnMetaChunk.java
hadoop/src/main/java/org/apache/carbondata/hadoop/readsupport/CarbonReadSupport.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/reader/DimensionColumnChunkReader.java
core/src/main/java/org/apache/carbondata/core/datastore/filesystem/CarbonFileFilter.java
core/src/main/java/org/apache/carbondata/core/scan/expression/logical/BinaryLogicalExpression.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/FilterExecuter.java
core/src/main/java/org/apache/carbondata/core/scan/expression/exception/FilterIllegalMemberException.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/OrFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/metadata/blocklet/DataFileFooter.java
core/src/main/java/org/apache/carbondata/hadoop/CarbonInputSplit.java
core/src/main/java/org/apache/carbondata/core/scan/complextypes/StructQueryType.java
core/src/main/java/org/carbondata/scan/filter/executer/RowLevelRangeGrtThanFiterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/impl/FixedLengthDimensionDataChunk.java
core/src/main/java/org/apache/carbondata/core/metadata/blocklet/index/BlockletBTreeIndex.java
core/src/main/java/org/apache/carbondata/core/scan/result/impl/NonFilterQueryScannedResult.java
core/src/main/java/org/apache/carbondata/core/scan/executor/QueryExecutorFactory.java
core/src/main/java/org/apache/carbondata/core/writer/CarbonIndexFileWriter.java
common/src/main/java/org/apache/carbondata/common/logging/impl/ExtendedRollingFileAppender.java
core/src/main/java/org/apache/carbondata/core/scan/executor/util/RestructureUtil.java
core/src/main/java/org/apache/carbondata/core/scan/expression/conditional/ListExpression.java
core/src/main/java/org/apache/carbondata/core/datastore/block/TableBlockInfo.java
core/src/main/java/org/apache/carbondata/core/scan/expression/conditional/InExpression.java
core/src/main/java/org/apache/carbondata/core/scan/scanner/BlockletScanner.java
core/src/main/java/org/apache/carbondata/core/scan/expression/conditional/ConditionalExpression.java
core/src/main/java/org/apache/carbondata/core/cache/CarbonLRUCache.java
core/src/main/java/org/apache/carbondata/core/metadata/blocklet/datachunk/DataChunk.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/reader/measure/AbstractMeasureChunkReader.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/SchemaEvolution.java
core/src/main/java/org/apache/carbondata/core/scan/filter/GenericQueryType.java
core/src/main/java/org/apache/carbondata/core/util/CarbonLoadStatisticsDummy.java
core/src/main/java/org/apache/carbondata/core/scan/scanner/impl/FilterScanner.java
core/src/main/java/org/apache/carbondata/core/datastore/filesystem/ViewFSCarbonFile.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/RowLevelRangeFilterResolverImpl.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/SortTempFileChunkHolder.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/DimensionColumnDataChunk.java
core/src/main/java/org/apache/carbondata/core/cache/Cacheable.java
processing/src/main/java/org/apache/carbondata/processing/datatypes/PrimitiveDataType.java
core/src/main/java/org/apache/carbondata/core/scan/executor/impl/QueryExecutorProperties.java
core/src/main/java/org/apache/carbondata/core/scan/executor/infos/AggregatorInfo.java
core/src/main/java/org/apache/carbondata/core/scan/collector/impl/DictionaryBasedResultCollector.java
core/src/main/java/org/apache/carbondata/core/metadata/blocklet/index/BlockletIndex.java
core/src/main/java/org/apache/carbondata/core/keygenerator/directdictionary/DirectDictionaryKeyGeneratorFactory.java
core/src/main/java/org/apache/carbondata/core/metadata/CarbonMetadata.java
core/src/main/java/org/apache/carbondata/core/datastore/columnar/ColumnWithIntIndexForHighCard.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/SchemaReader.java
core/src/main/java/org/apache/carbondata/core/scan/scanner/AbstractBlockletScanner.java
core/src/main/java/org/apache/carbondata/core/scan/expression/Expression.java
hadoop/src/main/java/org/apache/carbondata/hadoop/util/CarbonInputFormatUtil.java
core/src/main/java/org/apache/carbondata/core/util/CarbonTimeStatisticsFactory.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/column/CarbonDimension.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelRangeGrtThanFiterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/scan/expression/conditional/NotEqualsExpression.java
integration/spark-common/src/main/java/org/apache/carbondata/spark/merger/NodeBlockRelation.java
core/src/main/java/org/apache/carbondata/core/scan/executor/infos/BlockExecutionInfo.java
processing/src/main/java/org/apache/carbondata/processing/exception/DataLoadingException.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/LogicalFilterResolverImpl.java
core/src/main/java/org/apache/carbondata/core/scan/expression/conditional/LessThanExpression.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelRangeTypeExecuterFacory.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/scan/expression/BinaryExpression.java
processing/src/main/java/org/apache/carbondata/processing/util/NonDictionaryUtil.java
core/src/main/java/org/apache/carbondata/core/datastore/columnar/ColumnarKeyStoreDataHolder.java
core/src/main/java/org/apache/carbondata/core/scan/result/iterator/RawResultIterator.java
core/src/main/java/org/apache/carbondata/core/metadata/CarbonTableIdentifier.java
core/src/main/java/org/apache/carbondata/core/keygenerator/directdictionary/timestamp/TimeStampGranularityTypeValue.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/resolverinfo/visitor/FilterInfoTypeVisitorFactory.java
integration/spark-common/src/main/java/org/apache/carbondata/spark/load/CarbonLoaderUtil.java
common/src/main/java/org/apache/carbondata/common/logging/LogServiceFactory.java
core/src/main/java/org/apache/carbondata/core/scan/result/AbstractScannedResult.java
core/src/main/java/org/apache/carbondata/core/scan/expression/conditional/NotInExpression.java
core/src/main/java/org/apache/carbondata/core/scan/model/QueryColumn.java
processing/src/main/java/org/apache/carbondata/processing/store/writer/AbstractFactDataWriter.java
integration/spark-common/src/main/java/org/apache/carbondata/spark/merger/CompactionType.java
core/src/main/java/org/apache/carbondata/core/scan/complextypes/ComplexQueryType.java
core/src/main/java/org/apache/carbondata/core/keygenerator/KeyGenerator.java
processing/src/main/java/org/apache/carbondata/processing/exception/SliceMergerException.java
core/src/main/java/org/apache/carbondata/core/keygenerator/directdictionary/timestamp/TimeStampDirectDictionaryGenerator.java
core/src/main/java/org/carbondata/scan/filter/executer/RowLevelRangeGrtrThanEquaToFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/util/CarbonMetadataUtil.java
core/src/main/java/org/apache/carbondata/core/scan/executor/QueryExecutor.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/resolverinfo/visitor/ResolvedFilterInfoVisitorIntf.java
core/src/main/java/org/apache/carbondata/core/scan/complextypes/PrimitiveQueryType.java
core/src/main/java/org/apache/carbondata/core/scan/result/BatchResult.java
processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactDataHandlerColumnar.java
processing/src/main/java/org/apache/carbondata/processing/csvreaderstep/BlockDetails.java
core/src/main/java/org/apache/carbondata/core/metadata/encoder/Encoding.java
core/src/main/java/org/apache/carbondata/core/scan/expression/conditional/GreaterThanExpression.java
common/src/main/java/org/apache/carbondata/common/logging/impl/AuditExtendedRollingFileAppender.java
core/src/main/java/org/apache/carbondata/core/scan/filter/intf/FilterExecuterType.java
core/src/main/java/org/apache/carbondata/core/keygenerator/directdictionary/timestamp/TimeStampGranularityConstants.java
core/src/main/java/org/apache/carbondata/core/cache/Cache.java
core/src/main/java/org/apache/carbondata/core/scan/complextypes/ArrayQueryType.java
integration/spark-common/src/main/java/org/apache/carbondata/spark/load/DeleteLoadFolders.java
core/src/main/java/org/apache/carbondata/core/statusmanager/LoadMetadataDetails.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/ConditionalFilterResolverImpl.java
processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactHandler.java
core/src/main/java/org/apache/carbondata/core/scan/expression/ColumnExpression.java
processing/src/main/java/org/apache/carbondata/processing/store/writer/CarbonFactDataWriter.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/CarbonTable.java
core/src/main/java/org/apache/carbondata/core/datastorage/store/compression/CompressorFactory.java
core/src/main/java/org/apache/carbondata/core/scan/filter/intf/RowIntf.java
hadoop/src/main/java/org/apache/carbondata/hadoop/readsupport/impl/DictionaryDecodeReadSupport.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/impl/VariableLengthDimensionDataChunk.java
core/src/main/java/org/apache/carbondata/core/util/CarbonLoadStatisticsImpl.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/metadata/FilterResolverMetadata.java
core/src/main/java/org/apache/carbondata/core/reader/CarbonDictionaryReader.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelRangeLessThanEqualFilterExecuterImpl.java
core/src/main/java/org/carbondata/scan/filter/executer/RowLevelRangeLessThanFiterExecuterImpl.java
core/src/main/java/org/carbondata/scan/filter/executer/RowLevelRangeLessThanEqualFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/keygenerator/columnar/impl/MultiDimKeyVarLengthEquiSplitGenerator.java
processing/src/main/java/org/apache/carbondata/processing/model/CarbonDataLoadSchema.java
core/src/main/java/org/apache/carbondata/core/scan/result/impl/FilterQueryScannedResult.java
core/src/main/java/org/apache/carbondata/core/scan/expression/logical/AndExpression.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelRangeLessThanFiterExecuterImpl.java
hadoop/src/main/java/org/apache/carbondata/hadoop/CarbonRecordReader.java
core/src/main/java/org/apache/carbondata/core/metadata/AbsoluteTableIdentifier.java
core/src/main/java/org/apache/carbondata/core/datastore/exception/IndexBuilderException.java
texts:Rename package org.carbondata to org.apache.carbondata
Per convention, as any other Apache project, CarbonData should use org.apache.carbondata as package name (instead of org.carbondata).
issueID:CARBONDATA-1420
type:Bug
changed files:
texts:Partition Feature doesn&#39;t support a Partition Column of Date Type.
*Create Table :DROP TABLE IF EXISTS list_partition_table_date;CREATE TABLE list_partition_table_date(shortField SHORT, intField INT, bigintField LONG, doubleField DOUBLE, stringField STRING, timestampField TIMESTAMP, decimalField DECIMAL(18,2), charField CHAR(5), floatField FLOAT, complexData ARRAY<STRING> ) PARTITIONED BY (dateField DATE) STORED BY 'carbondata' TBLPROPERTIES('PARTITION_TYPE'='LIST', 'LIST_INFO'='2010/7/23 , 2011/7/23, 2012/7/23');*Output : Error: java.lang.RuntimeException: BaseSqlParser>>>> nullCarbonSqlParser>>>> &#91;1.1&#93; failure: identifier matching regex (?i)ALTER expectedCREATE TABLE list_partition_table_date(shortField SHORT, intField INT, bigintField LONG, doubleField DOUBLE, stringField STRING, timestampField TIMESTAMP, decimalField DECIMAL(18,2), charField CHAR(5), floatField FLOAT, complexData ARRAY<STRING> ) PARTITIONED BY (dateField DATE) STORED BY 'carbondata' TBLPROPERTIES('PARTITION_TYPE'='LIST', 'LIST_INFO'='2010/7/23 , 2011/7/23, 2012/7/23')^ (state=,code=0)
issueID:CARBONDATA-1421
type:Bug
changed files:
texts:Auto Compaction Failing in CarbonData Loading
I ran the create query followed by multiple load queries and the auto-compaction is failing.0: jdbc:hive2://localhost:10000> LOAD DATA inpath 'hdfs://localhost:54310/data/4000_UniqData.csv' INTO table uniqdata options('DELIMITER'=',', 'FILEHEADER'='CUST_ID, CUST_NAME, ACTIVE_EMUI_VERSION, DOB, DOJ, BIGINT_COLUMN1, BIGINT_COLUMN2, DECIMAL_COLUMN1, DECIMAL_COLUMN2, Double_COLUMN1, Double_COLUMN2, INTEGER_COLUMN1');---------+ Result  ---------+---------+No rows selected (1.183 seconds)0: jdbc:hive2://localhost:10000> LOAD DATA inpath 'hdfs://localhost:54310/data/5000_UniqData.csv' INTO table uniqdata options('DELIMITER'=',', 'FILEHEADER'='CUST_ID, CUST_NAME, ACTIVE_EMUI_VERSION, DOB, DOJ, BIGINT_COLUMN1, BIGINT_COLUMN2, DECIMAL_COLUMN1, DECIMAL_COLUMN2, Double_COLUMN1, Double_COLUMN2, INTEGER_COLUMN1');Error: java.lang.Exception: Dataload is success. Auto-Compaction has failed. Please check logs. (state=,code=0)0: jdbc:hive2://localhost:10000> LOAD DATA inpath 'hdfs://localhost:54310/data/7000_UniqData.csv' INTO table uniqdata options('DELIMITER'=',', 'FILEHEADER'='CUST_ID, CUST_NAME, ACTIVE_EMUI_VERSION, DOB, DOJ, BIGINT_COLUMN1, BIGINT_COLUMN2, DECIMAL_COLUMN1, DECIMAL_COLUMN2, Double_COLUMN1, Double_COLUMN2, INTEGER_COLUMN1');Error: java.lang.Exception: Dataload is success. Auto-Compaction has failed. Please check logs. (state=,code=0)0: jdbc:hive2://localhost:10000> 0: jdbc:hive2://localhost:10000> 0: jdbc:hive2://localhost:10000> 0: jdbc:hive2://localhost:10000> 0: jdbc:hive2://localhost:10000> 0: jdbc:hive2://localhost:10000> 0: jdbc:hive2://localhost:10000> 0: jdbc:hive2://localhost:10000> show segments for table uniqdata;---------------------------------------------------------------------------- SegmentSequenceId    Status       Load Start Time            Load End Time       ---------------------------------------------------------------------------- 4                   Success   2017-08-29 10:37:13.053   2017-08-29 10:37:13.888   3                   Success   2017-08-29 10:36:57.851   2017-08-29 10:36:59.08    2                   Success   2017-08-29 10:36:49.439   2017-08-29 10:36:50.373   1                   Success   2017-08-29 10:36:37.365   2017-08-29 10:36:38.768   0                   Success   2017-08-29 10:36:21.011   2017-08-29 10:36:26.1    ----------------------------------------------------------------------------5 rows selected (0.099 seconds)0: jdbc:hive2://localhost:10000> LOAD DATA inpath 'hdfs://localhost:54310/data/7000_UniqData.csv' INTO table uniqdata options('DELIMITER'=',', 'FILEHEADER'='CUST_ID, CUST_NAME, ACTIVE_EMUI_VERSION, DOB, DOJ, BIGINT_COLUMN1, BIGINT_COLUMN2, DECIMAL_COLUMN1, DECIMAL_COLUMN2, Double_COLUMN1, Double_COLUMN2, INTEGER_COLUMN1');Error: java.lang.Exception: Dataload is success. Auto-Compaction has failed. Please check logs. (state=,code=0)0: jdbc:hive2://localhost:10000> show segments for table uniqdata;---------------------------------------------------------------------------- SegmentSequenceId    Status       Load Start Time            Load End Time       ---------------------------------------------------------------------------- 5                   Success   2017-08-29 10:38:15.727   2017-08-29 10:38:16.548   4                   Success   2017-08-29 10:37:13.053   2017-08-29 10:37:13.888   3                   Success   2017-08-29 10:36:57.851   2017-08-29 10:36:59.08    2                   Success   2017-08-29 10:36:49.439   2017-08-29 10:36:50.373   1                   Success   2017-08-29 10:36:37.365   2017-08-29 10:36:38.768   0                   Success   2017-08-29 10:36:21.011   2017-08-29 10:36:26.1    ----------------------------------------------------------------------------
issueID:CARBONDATA-1422
type:Bug
changed files:
texts:Major and Minor Compaction Failing
The major and minor compaction is failing. The Compaction is failing in the default scenario where the table property dictionary_include is not specified which is the default behaviour.Please find the error logs below :0: jdbc:hive2://localhost:10000> show segments for table uniqdata;---------------------------------------------------------------------------- SegmentSequenceId    Status       Load Start Time            Load End Time       ---------------------------------------------------------------------------- 9                   Success   2017-08-29 11:17:29.927   2017-08-29 11:17:30.555   8                   Success   2017-08-29 11:17:27.572   2017-08-29 11:17:28.363   7                   Success   2017-08-29 11:17:23.583   2017-08-29 11:17:25.112   6                   Success   2017-08-29 11:17:07.966   2017-08-29 11:17:09.322   5                   Success   2017-08-29 10:38:15.727   2017-08-29 10:38:16.548   4                   Success   2017-08-29 10:37:13.053   2017-08-29 10:37:13.888   3                   Success   2017-08-29 10:36:57.851   2017-08-29 10:36:59.08    2                   Success   2017-08-29 10:36:49.439   2017-08-29 10:36:50.373   1                   Success   2017-08-29 10:36:37.365   2017-08-29 10:36:38.768   0                   Success   2017-08-29 10:36:21.011   2017-08-29 10:36:26.1    ----------------------------------------------------------------------------10 rows selected (0.081 seconds)0: jdbc:hive2://localhost:10000> ALTER TABLE uniqdata COMPACT 'MINOR';Error: java.lang.RuntimeException: Compaction failed. Please check logs for more info. Exception in compaction java.lang.Exception: Compaction Failure in Merger Rdd. (state=,code=0)0: jdbc:hive2://localhost:10000> ALTER TABLE uniqdata COMPACT 'MAJOR';Error: java.lang.RuntimeException: Compaction failed. Please check logs for more info. Exception in compaction java.lang.Exception: Compaction Failure in Merger Rdd. (state=,code=0)0: jdbc:hive2://localhost:10000>
issueID:CARBONDATA-1423
type:Task
changed files:
texts:Add Integration Test Cases For Presto

issueID:CARBONDATA-1425
type:Improvement
changed files:
texts:Inappropriate Exception displays while creating a new partition with incorrect partition type
Inappropriate Exception displays while creating a new partition with incorrect partition typeSteps to Reproduce:1)Create table:CREATE TABLE uniqdata_part1 (CUST_NAME String,ACTIVE_EMUI_VERSION string,DOB Timestamp,DOJ timestamp, BIGINT_COLUMN1 bigint,BIGINT_COLUMN2 bigint,DECIMAL_COLUMN1 decimal(30,10), DECIMAL_COLUMN2 decimal(36,10),Double_COLUMN1 double, Double_COLUMN2 double,INTEGER_COLUMN1 int) PARTITIONED BY (CUST_ID int) STORED BY 'org.apache.carbondata.format' TBLPROPERTIES ('PARTITION_TYPE'='RANGE','RANGE_INFO'='9090,9500,9800',"TABLE_BLOCKSIZE"= "256 MB")2)Load data:LOAD DATA INPATH 'hdfs://localhost:54310/uniqdata/2000_UniqData.csv' into table uniqdata_part1 OPTIONS('DELIMITER'=',' , 'QUOTECHAR'='"','BAD_RECORDS_ACTION'='FORCE','FILEHEADER'='CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1,Double_COLUMN2,INTEGER_COLUMN1')3) Execute Query:ALTER TABLE uniqdata_part1 ADD PARTITION ('abc')4) Result on beelineError: java.lang.RuntimeException: Range info must be in ascending order, please check again! (state=,code=0)5) Expected Result: it should display a proper exception like invalid partition definition as we are providing a string value at the place of the integer value.
issueID:CARBONDATA-1428
type:Bug
changed files:
texts:Incorrect Result displays while alter drop command on partitioned and non-partitioned table
Incorrect Result displays while alter drop command on partitioned and non-partitioned tableSteps to reproduce:1) Create a partitioned tableCREATE TABLE uniqdata_part1 (CUST_NAME String,ACTIVE_EMUI_VERSION string,DOB Timestamp,DOJ timestamp, BIGINT_COLUMN1 bigint,BIGINT_COLUMN2 bigint,DECIMAL_COLUMN1 decimal(30,10), DECIMAL_COLUMN2 decimal(36,10),Double_COLUMN1 double, Double_COLUMN2 double,INTEGER_COLUMN1 int) PARTITIONED BY (CUST_ID int) STORED BY 'org.apache.carbondata.format' TBLPROPERTIES ('PARTITION_TYPE'='RANGE','RANGE_INFO'='9090,9500,9800',"TABLE_BLOCKSIZE"= "256 MB")2) Load data into partitioned tableLOAD DATA INPATH 'hdfs://localhost:54310/uniqdata/2000_UniqData.csv' into table uniqdata_part1 OPTIONS('DELIMITER'=',' , 'QUOTECHAR'='"','BAD_RECORDS_ACTION'='FORCE','FILEHEADER'='CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1,Double_COLUMN2,INTEGER_COLUMN1')3) Execute drop column query on partitioned tableALTER TABLE uniqdata_part1 drop columns(BIGINT_COLUMN1)4) Result on beelineError: java.lang.RuntimeException: Alter table drop column operation failed: Column bigint_column1 does not exists in the table partition.uniqdata_part1 (state=,code=0)5) Expected Result:it should drop the column from the partitioned table as it is not the partitioned column and existing column of the table.---------------------------------------------+       col_name           data_type     comment  ---------------------------------------------+ CUST_NAME             string           NULL      ACTIVE_EMUI_VERSION   string           NULL      DOB                   timestamp        NULL      DOJ                   timestamp        NULL      BIGINT_COLUMN1        bigint           NULL      BIGINT_COLUMN2        bigint           NULL      DECIMAL_COLUMN1       decimal(30,10)   NULL      DECIMAL_COLUMN2       decimal(36,10)   NULL      Double_COLUMN1        double           NULL      Double_COLUMN2        double           NULL      INTEGER_COLUMN1       int              NULL      CUST_ID               int              NULL
issueID:CARBONDATA-1429
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/datastore/page/SafeVarLengthColumnPage.java
core/src/main/java/org/apache/carbondata/core/datastore/page/DecimalColumnPage.java
core/src/main/java/org/apache/carbondata/core/datastore/page/LazyColumnPage.java
core/src/main/java/org/apache/carbondata/core/datastore/page/UnsafeDecimalColumnPage.java
core/src/main/java/org/apache/carbondata/core/datastore/page/UnsafeVarLengthColumnPage.java
core/src/main/java/org/apache/carbondata/core/datastore/page/VarLengthColumnPageBase.java
core/src/main/java/org/apache/carbondata/core/datastore/page/ColumnPage.java
core/src/main/java/org/apache/carbondata/core/datastore/page/SafeFixLengthColumnPage.java
core/src/main/java/org/apache/carbondata/core/metadata/datatype/DecimalConverterFactory.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/ColumnPageEncoderMeta.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/adaptive/AdaptiveIntegralCodec.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/DefaultEncodingFactory.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/adaptive/AdaptiveDeltaIntegralCodec.java
core/src/main/java/org/apache/carbondata/core/datastore/page/SafeDecimalColumnPage.java
texts:Add a value based compression for decimal data type when decimal is stored as Int or Long
Added a value based compression for decimal data type when decimal is stored as Int or LongWhen decimal precision is <= 9, decimal values are stored in 4 bytes but they are not compressed further based on min and max values as compared with other primitive data type compression. Therefore now based on min and max value decimal data falling in Integer range will be further compressed as byte or short.When decimal precision is <= 18, decimal values are stored in 8 bytes but they are not compressed further based on min and max values as compared with other primitive data type compression. Therefore now based on min and max value decimal data falling in Long range will be further compressed as byte, short or int.Advantage: This will reduce the storage space thereby decreasing the IO time while reading and decompressing the data.
issueID:CARBONDATA-1431
type:Bug
changed files:
texts:Dictionary_Include working incorrectly for date and timestamp data type.
When we create a table with date and timestamp data type with DICTIONARY_INCLUDE : Example : CREATE TABLE uniqdata_INCLUDEDICTIONARY2 (CUST_ID int,CUST_NAME String,ACTIVE_EMUI_VERSION string, DOB timestamp, DOJ timestamp, BIGINT_COLUMN1 bigint,BIGINT_COLUMN2 bigint,DECIMAL_COLUMN1 decimal(30,10), DECIMAL_COLUMN2 decimal(36,10),Double_COLUMN1 double, Double_COLUMN2 double,INTEGER_COLUMN1 int) STORED BY 'org.apache.carbondata.format' TBLPROPERTIES('DICTIONARY_INCLUDE'='CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2')It should either create the dictionary for date and timestamp field or it should throw an error that "DICTIONARY_INCLUDE" feature is not supported for date and timestamp.whereas in the current master branch,  the query executed successfully without throwing any error and neither it created dictionary files for date and timestamp field.
issueID:CARBONDATA-1432
type:Bug
changed files:
texts:Default.value property is not throwing any exception when specified column name does not matches with column name in the query
I tried to create a table with the following command:CREATE TABLE uniqdata (CUST_ID int,CUST_NAME String,ACTIVE_EMUI_VERSION string, DOB timestamp, DOJ timestamp, BIGINT_COLUMN1 bigint,BIGINT_COLUMN2 bigint,DECIMAL_COLUMN1 decimal(30,10), DECIMAL_COLUMN2 decimal(36,10),Double_COLUMN1 double, Double_COLUMN2 double,INTEGER_COLUMN1 int) STORED BY 'org.apache.carbondata.format' TBLPROPERTIES ("TABLE_BLOCKSIZE"= "256 MB")and when I tried to add a new column to this table by using:alter table uniqdata add columns(newcol5 int) tblproperties('default.value.nCol'='123456789');It does not throw any error and the query was passed successfully. The above alter command should throw an exception as the new column name to be added does not matches with the column name in the default value property.
issueID:CARBONDATA-1433
type:Bug
changed files:integration/presto/src/main/java/org/apache/carbondata/presto/PrestoCarbonVectorizedRecordReader.java
integration/presto/src/main/java/org/apache/carbondata/presto/readers/IntegerStreamReader.java
integration/presto/src/main/java/org/apache/carbondata/presto/readers/DecimalSliceStreamReader.java
integration/presto/src/main/java/org/apache/carbondata/presto/readers/DoubleStreamReader.java
integration/presto/src/main/java/org/apache/carbondata/presto/readers/LongStreamReader.java
integration/presto/src/main/java/org/apache/carbondata/presto/readers/ObjectStreamReader.java
integration/presto/src/main/java/org/apache/carbondata/presto/readers/PrestoVectorBlockBuilder.java
integration/presto/src/main/java/org/apache/carbondata/presto/readers/SliceStreamReader.java
integration/presto/src/main/java/org/apache/carbondata/presto/CarbondataPageSource.java
integration/presto/src/main/java/org/apache/carbondata/presto/CarbonColumnVectorWrapper.java
integration/presto/src/main/java/org/apache/carbondata/presto/CarbondataSplitManager.java
texts:Presto Integration - Vectorized Reader Implementation
Implementation of the Vectorized Reader for Presto including DictionaryBlock creation for improving performance
issueID:CARBONDATA-1434
type:Improvement
changed files:
texts:Remove useless class para for metastore
The `RuntimeConfig` has not been used in Carbon*Metastore class, so remove it.
issueID:CARBONDATA-1435
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/adaptive/AdaptiveIntegralCodec.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/adaptive/AdaptiveFloatingCodec.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/adaptive/AdaptiveCodec.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/EncodingFactory.java
texts:Reader is not backward compatible
Using master code, it is having NPE when reading old carbondata file
issueID:CARBONDATA-1436
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/datamap/DataMapStoreManager.java
texts:optimize concurrency control for datamap
Scenario`DataMapStoreManager` provides a synchronized interface `getDataMap` to retrieve a table's `TableDataMap`. It will cause performance problems in current query scenario when all the queries have to wait the former query finished. AnalyzeWe can make the concurrent control in table scope instead of global scope. Modification+ Synchronized by table.+ Use double checked locking to reduce lock overhead.
issueID:CARBONDATA-1437
type:Bug
changed files:
texts:Wrong Exception Mesage When Number Of Bucket is Specified as zero
steps to reproduce0: jdbc:hive2://localhost:10000> CREATE TABLE uniqData_t17(ID Int, date Timestamp, country String,name String, phonetype String, serialname String, salary Int)0: jdbc:hive2://localhost:10000> STORED BY 'CARBONDATA' TBLPROPERTIES('bucketnumber'='0', 'bucketcolumns'='name','DICTIONARY_INCLUDE'='NAME');---------+ Result  ---------+---------+No rows selected (0.501 seconds)0: jdbc:hive2://localhost:10000> load data inpath 'hdfs://localhost:54310/dataDiff1.csv' into table uniqData_t17 OPTIONS('DELIMITER'=',' , 'QUOTECHAR'='"','FILEHEADER'='ID,date,country,name,phonetype,serialname,salary');Error: java.lang.Exception: DataLoad failure (state=,code=0)logs7/08/31 12:17:07 WARN CarbonDataProcessorUtil: &#91;Executor task launch worker-9&#93;&#91;partitionID:default_uniqdata_t17_578e819e-bec8-49e5-a292-890db623e116&#93; sort scope is set to LOCAL_SORT17/08/31 12:17:07 ERROR DataLoadExecutor: &#91;Executor task launch worker-9&#93;&#91;partitionID:default_uniqdata_t17_578e819e-bec8-49e5-a292-890db623e116&#93; Data Loading failed for table uniqdata_t17java.lang.ArithmeticException: / by zero at org.apache.carbondata.processing.newflow.sort.impl.ParallelReadMergeSorterWithBucketingImpl.initialize(ParallelReadMergeSorterWithBucketingImpl.java:78)it should give meaningfull exception such as number of bucket can not be zero
issueID:CARBONDATA-1438
type:Improvement
changed files:
texts:Unify the sort column and sort scope in create table command
1 RequirementCurrently, Users can specify sort column in table properties when create table. And when load data, users can also specify sort scope in load options.In order to improve the ease of use for users, it will be better to specify the sort related parameters all in create table command.Once sort scope is specified in create table command, it will be used in load data even users have specified in load options.2 Detailed design2.1 Task-01Requirement： Create table can support specify sort scopeImplement: Take use of table properties (Map<String, String>), will specify sort scope in table properties by key/value pair, then existing interface will be called to write this key/value pair into metastore.Will support Global Sort，Local Sort and No Sort，it can be specified in sql command:CREATE TABLE tableWithGlobalSort (shortField SHORT,intField INT,bigintField LONG,doubleField DOUBLE,stringField STRING,timestampField TIMESTAMP,decimalField DECIMAL(18,2),dateField DATE,charField CHAR(5))STORED BY 'carbondata'TBLPROPERTIES('SORT_COLUMNS'='stringField', 'SORT_SCOPE'='GLOBAL_SORT')Tips：If the sort scope is global Sort, users should specify GLOBAL_SORT_PARTITIONS. If users do not specify it, it will use the number of map task. GLOBAL_SORT_PARTITIONS should be Integer type, the range is &#91;1,Integer.MaxValue&#93;，it is only used when the sort scope is global sort. Global Sort Use orderby operator in spark, data is ordered in segment level.Local Sort Node ordered, carbondata file is ordered if it is written by one task. No Sort No sortTips：key and value is case-insensitive.2.2 Task-02Requirement:Load data in will support local sort, no sort, global sort Ignore the sort scope specified in load data and use the parameter which specified in create table.Currently, user can specify the sort scope and global sort partitions in load options, After modification, it will ignore the sort scope which specified in load options and will get sort scope from table properties.Current logic: sort scope is from load optionsNumber  Prerequisite Sort scope1 isSortTable is true && Sort Scope is Global Sort Global Sort(first check)2 isSortTable is false No Sort3 isSortTable is true Local SortTips: isSortTable is true means this table contains sort column or it contains dimensions (except complex type), like string type.For example:Create table xxx1 (col1 string col2 int) stored by ‘carbondata’ &#8212; sort tableCreate table xx1 (col1 int, col2 int) stored by ‘carbondata’ &#8212; not sort tableCreate table xx (col1 int, col2 string) stored by ‘carbondata’ tblproperties (‘sort_column’=’col1’)  –- sort tableNew logic：sort scope is from create tableNumber Prerequisite Code branch1 isSortTable = true && Sort Scope is Global Sort Global Sort(first check)2 isSortTable= false || Sort Scope is No Sort No Sort3 isSortTable is true && Sort Scope is Local Sort Local Sort4 isSortTable is true，without specify Sort Scope Local Sort, (Keep current logic) 3 Acceptance standardNumber Acceptance standard1 Use can specify sort scope(global, local, no sort) when create carbon table in sql type2 Load data will ignore the sort scope specified in load options and will use the parameter which specify in create table command. If user still specify the sort scope in load options, will give warning and inform user that he will use the sort scope which specified in create table.4 Feature restrictionsNA5 DependenciesNA6 Technical riskNA
issueID:CARBONDATA-1439
type:Improvement
changed files:processing/src/main/java/org/apache/carbondata/processing/loading/converter/impl/RowConverterImpl.java
texts:Wrong Error message shown for Bad records even when BAD_RECORDS_LOGGER_ENABLE is set to true
Steps to reproduce:1. Create table :CREATE TABLE badrecordtest1 (ID int,CUST_ID int,cust_name string) STORED BY 'org.apache.carbondata.format';2.Load data into the table:LOAD DATA INPATH 'hdfs://localhost:54310/Files/test2.csv' into table badrecordtest1 OPTIONS('DELIMITER'=',' , 'QUOTECHAR'='"','BAD_RECORDS_ACTION'='FAIL','is_empty_data_bad_record'='true','BAD_RECORDS_LOGGER_ENABLE'='TRUE','FILEHEADER'='ID,CUST_ID,cust_name');RESULT:Error: java.lang.Exception: Data load failed due to bad record: The value with column name id and column data type INT is not a valid INT type.Please enable bad record logger to know the detail reason.As BAD_RECORDS_LOGGER_ENABLE is set to true, so the displayed error message should not contain the message: "Please enable bad record logger to know the detail reason."
issueID:CARBONDATA-144
type:Bug
changed files:
texts:Minor compaction is not working
1. Create table create table table1(column1 string, Amount int)  STORED BY 'org.apache.carbondata.format' 2. Load the data for 10 times3 . Run the Minor compaction queryalter table table compact 'Minor';4. Verify the resultMinor compaction is not workingDriver Log:16/08/05 14:54:07 ERROR CarbonDataRDDFactory$: Thread-1994 Exception in compaction thread java.lang.Exception: Compaction Failure in Merger Rdd.Exception in thread "Thread-1994" java.util.concurrent.ExecutionException: java.lang.Exception: Compaction Failure in Merger Rdd.        at java.util.concurrent.FutureTask.report(FutureTask.java:122)        at java.util.concurrent.FutureTask.get(FutureTask.java:192)        at org.carbondata.spark.rdd.CarbonDataRDDFactory$$anon$1$$anonfun$run$1.apply(CarbonDataRDDFactory.scala:366)        at org.carbondata.spark.rdd.CarbonDataRDDFactory$$anon$1$$anonfun$run$1.apply(CarbonDataRDDFactory.scala:365)        at scala.collection.Iterator$class.foreach(Iterator.scala:727)        at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)        at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)        at scala.collection.AbstractIterable.foreach(Iterable.scala:54)        at org.carbondata.spark.rdd.CarbonDataRDDFactory$$anon$1.run(CarbonDataRDDFactory.scala:365)Caused by: java.lang.Exception: Compaction Failure in Merger Rdd.        at org.carbondata.spark.rdd.Compactor$.triggerCompaction(Compactor.scala:151)        at org.carbondata.spark.rdd.Compactor.triggerCompaction(Compactor.scala)        at org.carbondata.integration.spark.merger.CompactionCallable.call(CompactionCallable.java:40)        at org.carbondata.integration.spark.merger.CompactionCallable.call(CompactionCallable.java:29)        at java.util.concurrent.FutureTask.run(FutureTask.java:266)        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)        at java.lang.Thread.run(Thread.java:745)
issueID:CARBONDATA-1441
type:Bug
changed files:
texts:schema change does not reflect back in hive when schema is alter in carbon
steps to reproduce1. alter table schema in hive0: jdbc:hive2://localhost:10000> desc uniqdata;---------------------------------------------+       col_name           data_type     comment  ---------------------------------------------+ CUST_ID               int              NULL      CUST_NAME             string           NULL      ACTIVE_EMUI_VERSION   string           NULL      DOB                   timestamp        NULL      DOJ                   timestamp        NULL      BIGINT_COLUMN1        bigint           NULL      BIGINT_COLUMN2        bigint           NULL      DECIMAL_COLUMN1       decimal(30,10)   NULL      DECIMAL_COLUMN2       decimal(36,10)   NULL      Double_COLUMN1        double           NULL      Double_COLUMN2        double           NULL      INTEGER_COLUMN1       int              NULL     ---------------------------------------------+12 rows selected (0.638 seconds)0: jdbc:hive2://localhost:10000> alter table uniqdata change cust_id cust_id long;---------+ Result  ---------+---------+No rows selected (7.619 seconds)0: jdbc:hive2://localhost:10000> desc uniqdata;---------------------------------------------+       col_name           data_type     comment  ---------------------------------------------+ cust_id               bigint           NULL      cust_name             string           NULL      active_emui_version   string           NULL      dob                   timestamp        NULL      doj                   timestamp        NULL      bigint_column1        bigint           NULL      bigint_column2        bigint           NULL      decimal_column1       decimal(30,10)   NULL      decimal_column2       decimal(36,10)   NULL      double_column1        double           NULL      double_column2        double           NULL      integer_column1       int              NULL     ---------------------------------------------+12 rows selected (0.431 seconds)2.start hive cli and desc same table0: jdbc:hive2://localhost:10000> desc uniqdata;---------------------------------------------+       col_name           data_type     comment  ---------------------------------------------+ cust_id               int               cust_name             string            active_emui_version   string            dob                   timestamp         doj                   timestamp         bigint_column1        bigint            bigint_column2        bigint            decimal_column1       decimal(30,10)    decimal_column2       decimal(36,10)    double_column1        double            double_column2        double            integer_column1       int              --------------------------------schema change is not reflected in hive when reading carbondata file
issueID:CARBONDATA-1442
type:Improvement
changed files:
texts:Reformat Partition-Guide.md File
Change Some markdown tags to maintain consistency.
issueID:CARBONDATA-1443
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
texts:Throwing NPE while creating table while running tests
Throwing NPE while creating table while running testsException encountered when invoking run on a nested suite. *** ABORTED ***  java.lang.NullPointerException:  at org.apache.spark.sql.hive.CarbonFileMetastore.saveToDisk(CarbonFileMetastore.scala:275)  at org.apache.spark.sql.CarbonSource$.updateCatalogTableWithCarbonSchema(CarbonSource.scala:273)  at org.apache.spark.sql.execution.command.DDLStrategy.apply(DDLStrategy.scala:135)  at org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$1.apply(QueryPlanner.scala:62)  at org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$1.apply(QueryPlanner.scala:62)  at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)  at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)  at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)  at org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:92)  at org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$2$$anonfun$apply$2.apply(QueryPlanner.scala:77)
issueID:CARBONDATA-1444
type:Improvement
changed files:processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/SortDataRows.java
core/src/main/java/org/apache/carbondata/core/datastore/page/LazyColumnPage.java
core/src/main/java/org/apache/carbondata/core/scan/expression/conditional/NotEqualsExpression.java
core/src/main/java/org/apache/carbondata/core/util/DataTypeUtil.java
core/src/main/java/org/apache/carbondata/core/scan/expression/conditional/EqualToExpression.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/ColumnPageEncoderMeta.java
core/src/main/java/org/apache/carbondata/core/scan/expression/conditional/InExpression.java
core/src/main/java/org/apache/carbondata/core/scan/expression/conditional/LessThanExpression.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
core/src/main/java/org/apache/carbondata/core/datastore/page/UnsafeFixLengthColumnPage.java
core/src/main/java/org/apache/carbondata/core/scan/expression/conditional/GreaterThanExpression.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/bool/BooleanConvert.java
core/src/main/java/org/apache/carbondata/core/datastore/page/ColumnPage.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/rle/RLECodec.java
core/src/main/java/org/apache/carbondata/core/util/comparator/Comparator.java
integration/spark-datasource/src/main/scala/org/apache/carbondata/spark/vectorreader/VectorizedCarbonRecordReader.java
core/src/main/java/org/apache/carbondata/core/scan/filter/FilterUtil.java
core/src/main/java/org/apache/carbondata/core/util/AbstractDataFileFooterConverter.java
core/src/main/java/org/apache/carbondata/core/scan/expression/conditional/LessThanEqualToExpression.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/SortTempFileChunkHolder.java
core/src/main/java/org/apache/carbondata/core/scan/expression/conditional/NotInExpression.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/UnsafeCarbonRowPage.java
core/src/main/java/org/apache/carbondata/core/scan/collector/impl/AbstractScannedResultCollector.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/ExcludeFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/scan/expression/conditional/GreaterThanEqualToExpression.java
core/src/main/java/org/apache/carbondata/core/util/CarbonMetadataUtil.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/EncodingFactory.java
core/src/main/java/org/apache/carbondata/core/datastore/page/statistics/PrimitivePageStatsCollector.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/IncludeFilterExecuterImpl.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/IntermediateFileMerger.java
core/src/main/java/org/apache/carbondata/core/scan/result/vector/MeasureDataVectorProcessor.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/DefaultEncodingFactory.java
core/src/main/java/org/apache/carbondata/core/metadata/converter/ThriftWrapperSchemaConverterImpl.java
texts:CarbonData unsupport Boolean data type
Spark/Hive table support Boolean data type, the internal table also should support Boolean data type.   Boolean data type Range: TRUE or FALSE. Do not use quotation marks around the TRUE and FALSE literal values. You can write the literal values in uppercase, lowercase, or mixed case. The values queried from a table are always returned in lowercase, true or false.    In implementing this function, we employ: endcoding: RLE data expression: byte array    CarbonData should support boolean data type in following aspects: create table: support Boolean data type insert into table values: support insert Boolean column insert overwrite insert into table select from another table select from a table load data: from a local csv file filter: including >=, >, =, <=, <, =, !=, in, not in describle: should show boolean data typeWe also add some test cases in booleantype directory of spark2
issueID:CARBONDATA-1445
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
texts:if &#39;carbon.update.persist.enable&#39;=&#39;false&#39;, it will fail to update data
When updating data, if set 'carbon.update.persist.enable'='false', it will fail.I debug code and find that in the method LoadTable.processData the 'dataFrameWithTupleId' will call udf 'getTupleId()' which is defined in CarbonEnv.init(): 'sparkSession.udf.register("getTupleId", () => "")', it will return blank string to 'CarbonUpdateUtil.getRequiredFieldFromTID', so ArrayIndexOutOfBoundsException occur.the plans (logical and physical) for dataFrameWithTupleId :== Parsed Logical Plan =='Project unresolvedalias('stringField3, None), unresolvedalias('intField, None), unresolvedalias('longField, None), unresolvedalias('int2Field, None), unresolvedalias('stringfield1-updatedColumn, None), unresolvedalias('stringfield2-updatedColumn, None), UDF('tupleId) AS segId#286+- Project stringField3#113, intField#114, longField#115L, int2Field#116, UDF:getTupleId() AS tupleId#262, concat(stringField1#111, _test) AS stringfield1-updatedColumn#263, concat(stringField2#112, _test) AS stringfield2-updatedColumn#264   +- Filter (isnotnull(stringField3#113) && (stringField3#113 = 1))      +- RelationstringField1#111,stringField2#112,stringField3#113,intField#114,longField#115L,int2Field#116 CarbonDatasourceHadoopRelation [ Database name :default, Table name :study_carbondata, Schema :Some(StructType(StructField(stringField1,StringType,true), StructField(stringField2,StringType,true), StructField(stringField3,StringType,true), StructField(intField,IntegerType,true), StructField(longField,LongType,true), StructField(int2Field,IntegerType,true))) ]== Analyzed Logical Plan ==stringField3: string, intField: int, longField: bigint, int2Field: int, stringfield1-updatedColumn: string, stringfield2-updatedColumn: string, segId: stringProject stringField3#113, intField#114, longField#115L, int2Field#116, stringfield1-updatedColumn#263, stringfield2-updatedColumn#264, UDF(tupleId#262) AS segId#286+- Project stringField3#113, intField#114, longField#115L, int2Field#116, UDF:getTupleId() AS tupleId#262, concat(stringField1#111, _test) AS stringfield1-updatedColumn#263, concat(stringField2#112, _test) AS stringfield2-updatedColumn#264   +- Filter (isnotnull(stringField3#113) && (stringField3#113 = 1))      +- RelationstringField1#111,stringField2#112,stringField3#113,intField#114,longField#115L,int2Field#116 CarbonDatasourceHadoopRelation [ Database name :default, Table name :study_carbondata, Schema :Some(StructType(StructField(stringField1,StringType,true), StructField(stringField2,StringType,true), StructField(stringField3,StringType,true), StructField(intField,IntegerType,true), StructField(longField,LongType,true), StructField(int2Field,IntegerType,true))) ]== Optimized Logical Plan ==CarbonDictionaryCatalystDecoder [CarbonDecoderRelation(Map(int2Field#116 -> int2Field#116, longField#115L -> longField#115L, stringField2#112 -> stringField2#112, stringField1#111 -> stringField1#111, stringField3#113 -> stringField3#113, intField#114 -> intField#114),CarbonDatasourceHadoopRelation [ Database name :default, Table name :study_carbondata, Schema :Some(StructType(StructField(stringField1,StringType,true), StructField(stringField2,StringType,true), StructField(stringField3,StringType,true), StructField(intField,IntegerType,true), StructField(longField,LongType,true), StructField(int2Field,IntegerType,true))) ])], ExcludeProfile(ArrayBuffer(stringField2#112, stringField1#111)), CarbonAliasDecoderRelation(), true+- Project stringField3#113, intField#114, longField#115, int2Field#116, concat(stringField1#111, _test) AS stringfield1-updatedColumn#263, concat(stringField2#112, _test) AS stringfield2-updatedColumn#264, UDF(UDF:getTupleId()) AS segId#286   +- Filter (isnotnull(stringField3#113) && (stringField3#113 = 1))      +- RelationstringField1#111,stringField2#112,stringField3#113,intField#114,longField#115L,int2Field#116 CarbonDatasourceHadoopRelation [ Database name :default, Table name :study_carbondata, Schema :Some(StructType(StructField(stringField1,StringType,true), StructField(stringField2,StringType,true), StructField(stringField3,StringType,true), StructField(intField,IntegerType,true), StructField(longField,LongType,true), StructField(int2Field,IntegerType,true))) ]== Physical Plan ==*CarbonDictionaryDecoder [CarbonDecoderRelation(Map(int2Field#116 -> int2Field#116, longField#115L -> longField#115L, stringField2#112 -> stringField2#112, stringField1#111 -> stringField1#111, stringField3#113 -> stringField3#113, intField#114 -> intField#114),CarbonDatasourceHadoopRelation [ Database name :default, Table name :study_carbondata, Schema :Some(StructType(StructField(stringField1,StringType,true), StructField(stringField2,StringType,true), StructField(stringField3,StringType,true), StructField(intField,IntegerType,true), StructField(longField,LongType,true), StructField(int2Field,IntegerType,true))) ])], ExcludeProfile(ArrayBuffer(stringField2#112, stringField1#111)), CarbonAliasDecoderRelation(), org.apache.spark.sql.CarbonSession@9e4388d+- *Project stringField3#113, intField#114, longField#115, int2Field#116, concat(stringField1#111, _test) AS stringfield1-updatedColumn#263, concat(stringField2#112, _test) AS stringfield2-updatedColumn#264, UDF(UDF:getTupleId()) AS segId#286   +- *Scan CarbonDatasourceHadoopRelation [ Database name :default, Table name :study_carbondata, Schema :Some(StructType(StructField(stringField1,StringType,true), StructField(stringField2,StringType,true), StructField(stringField3,StringType,true), StructField(intField,IntegerType,true), StructField(longField,LongType,true), StructField(int2Field,IntegerType,true))) ] default.study_carbondatastringField3#113,intField#114,longField#115,stringField2#112,int2Field#116,stringField1#111 PushedFilters: &#91;IsNotNull(stringField3), EqualTo(stringField3,1)&#93;My code:import spark.implicits._val df1 = spark.sparkContext.parallelize(0 to 50)  .map(x => ("a", x.toString(), (x % 2).toString(), x, x.toLong, x * 2))  .toDF("stringField1", "stringField2", "stringField3", "intField", "longField", "int2Field")  val df2 = spark.sparkContext.parallelize(51 to 100)  .map(x => ("b", x.toString(), (x % 2).toString(), x, x.toLong, x * 2))  .toDF("stringField1", "stringField2", "stringField3", "intField", "longField", "int2Field") val df3 = df1.union(df2)spark.sql("DROP TABLE IF EXISTS study_carbondata ").show()spark.sql("""    |  CREATE TABLE IF NOT EXISTS study_carbondata (    |    stringField1          string,    |    stringField2          string,     |    stringField3          string,     |    intField              int,     |    longField             bigint,    |    int2Field             int     |  )    |  STORED BY 'carbondata'    |  TBLPROPERTIES('DICTIONARY_INCLUDE'='stringField1, stringField2, stringField3, longField',    |    'SORT_COLUMNS'='stringField1, stringField2, stringField3, intField',    |    'NO_INVERTED_INDEX'='longField',    |    'TABLE_BLOCKSIZE'='8'    |  )   """.stripMargin)   val sortScope = "LOCAL_SORT"  //GLOBAL_SORT  LOCAL_SORTdf3.write  .format("carbondata")  .option("tableName", "study_carbondata")  .option("compress", "true")  // just valid when tempCSV is true  .option("tempCSV", "false")  .option("single_pass", "true")   .option("sort_scope", sortScope) //GLOBAL_SORT  LOCAL_SORT  .mode(SaveMode.Append)  .save()spark.sql("""          UPDATE study_carbondata a               SET (a.stringField1, a.stringField2) = (concat(a.stringField1 , "_test" ), concat(a.stringField2 , "_test" ))          WHERE a.stringField3 = '1'          """).show(false)Error logs:2017-09-04 00:39:23,354 - ERROR - org.apache.carbondata.common.logging.impl.StandardLogService.logErrorMessage(StandardLogService.java:143) - main -mainorg.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 21.0 failed 1 times, most recent failure: Lost task 0.0 in stage 21.0 (TID 27, localhost, executor driver): org.apache.spark.SparkException: Failed to execute user defined function($anonfun$7: (string) => string)        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(generated.java:146)        at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)        at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:369)        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:369)        at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:350)        at scala.collection.Iterator$class.foreach(Iterator.scala:742)        at scala.collection.AbstractIterator.foreach(Iterator.scala:1194)        at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)        at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)        at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)        at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:308)        at scala.collection.AbstractIterator.to(Iterator.scala:1194)        at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:300)        at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1194)        at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:287)        at scala.collection.AbstractIterator.toArray(Iterator.scala:1194)        at org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$29.apply(RDD.scala:1354)        at org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$29.apply(RDD.scala:1354)        at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)        at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)        at org.apache.spark.scheduler.Task.run(Task.scala:99)        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)        at java.lang.Thread.run(Thread.java:745)Caused by: java.lang.ArrayIndexOutOfBoundsException: 1        at org.apache.carbondata.core.mutate.CarbonUpdateUtil.getRequiredFieldFromTID(CarbonUpdateUtil.java:67)        at org.apache.spark.sql.execution.command.LoadTable$$anonfun$7.apply(carbonTableSchema.scala:866)        at org.apache.spark.sql.execution.command.LoadTable$$anonfun$7.apply(carbonTableSchema.scala:865)        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(generated.java:144)        ... 26 moreDriver stacktrace:        at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)        at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)        at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)        at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)        at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)        at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)        at scala.Option.foreach(Option.scala:257)        at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)        at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)        at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)        at org.apache.spark.SparkContext.runJob(SparkContext.scala:1925)        at org.apache.spark.SparkContext.runJob(SparkContext.scala:1938)        at org.apache.spark.SparkContext.runJob(SparkContext.scala:1951)        at org.apache.spark.rdd.RDD$$anonfun$take$1.apply(RDD.scala:1354)        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)        at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)        at org.apache.spark.rdd.RDD.take(RDD.scala:1327)        at org.apache.spark.rdd.RDD$$anonfun$isEmpty$1.apply$mcZ$sp(RDD.scala:1462)        at org.apache.spark.rdd.RDD$$anonfun$isEmpty$1.apply(RDD.scala:1462)        at org.apache.spark.rdd.RDD$$anonfun$isEmpty$1.apply(RDD.scala:1462)
issueID:CARBONDATA-1446
type:Bug
changed files:
texts:Alter query throws invalid exception while executing on range partitioned table
Alter query throws invalid exception while executing on range partitioned table with invalid partition idSteps to reproduce:1) Create tableCREATE TABLE uniqdata_part1 (CUST_NAME String,ACTIVE_EMUI_VERSION string,DOB Timestamp,DOJ timestamp, BIGINT_COLUMN1 bigint,BIGINT_COLUMN2 bigint,DECIMAL_COLUMN1 decimal(30,10), DECIMAL_COLUMN2 decimal(36,10),Double_COLUMN1 double, Double_COLUMN2 double,INTEGER_COLUMN1 int) PARTITIONED BY (CUST_ID int) STORED BY 'org.apache.carbondata.format' TBLPROPERTIES ('PARTITION_TYPE'='RANGE','RANGE_INFO'='9090,9500,9800',"TABLE_BLOCKSIZE"= "256 MB")2)Load data into the table:LOAD DATA INPATH 'hdfs://localhost:54310/uniqdata/2000_UniqData.csv' into table uniqdata_part1 OPTIONS('DELIMITER'=',' , 'QUOTECHAR'='"','BAD_RECORDS_ACTION'='FORCE','FILEHEADER'='CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1,Double_COLUMN2,INTEGER_COLUMN1')3)Execute Query:ALTER TABLE uniqdata_part1 SPLIT PARTITION(6) INTO ('9800','9900')4) Result On beeline:Error: java.lang.RuntimeException: Range info must be in ascending order, please check again! (state=,code=0)5) Expected Result:As we are giving invalid partition id then error/exception should be like invalid partition id (related to partition id rather than range info).
issueID:CARBONDATA-1447
type:Improvement
changed files:
texts:Add the CNCF link in the CarbonDataWebsite

issueID:CARBONDATA-1448
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/metadata/schema/BucketingInfo.java
core/src/main/java/org/apache/carbondata/core/metadata/converter/ThriftWrapperSchemaConverterImpl.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/PartitionInfo.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/TableSchema.java
texts:PartitionInfo is null in CarbonTable
PartitionInfo is null in CarbonTable
issueID:CARBONDATA-1449
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelFilterExecuterImpl.java
texts:GC issue in case of date filter if it is going to rowlevel executor
GC issue in case of date filter if it is going to rowlevel executor
issueID:CARBONDATA-1450
type:New Feature
changed files:core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RangeValueFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/util/DataTypeUtil.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/store/impl/safe/SafeVariableLengthDimensionDataChunkStore.java
core/src/main/java/org/apache/carbondata/core/scan/executor/util/RestructureUtil.java
core/src/main/java/org/apache/carbondata/core/scan/executor/impl/AbstractQueryExecutor.java
core/src/main/java/org/apache/carbondata/core/scan/collector/impl/RestructureBasedRawResultCollector.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RestructureEvaluatorImpl.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/store/impl/unsafe/UnsafeVariableLengthDimensionDataChunkStore.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelRangeLessThanEqualFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/scan/collector/impl/RestructureBasedVectorResultCollector.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
core/src/main/java/org/apache/carbondata/core/datastore/TableSpec.java
core/src/main/java/org/apache/carbondata/core/scan/collector/impl/DictionaryBasedVectorResultCollector.java
core/src/main/java/org/apache/carbondata/core/scan/filter/FilterExpressionProcessor.java
core/src/main/java/org/apache/carbondata/core/scan/filter/FilterUtil.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelRangeLessThanFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/RowLevelRangeFilterResolverImpl.java
processing/src/main/java/org/apache/carbondata/processing/loading/converter/impl/NonDictionaryFieldConverterImpl.java
texts:Support timestamp more than 68 years, Enhance NoDictionary Datatypes - int , long
Problem:Current implementation supports timestamp as direct dictionary only. As dictionary is always integer only 68 years of range is supported.Solution:So this issue will support timestamp as default DICTIONARY_EXCLUDE. Allowing to store internally as unix long timestamp.Problem:Int and Bigint(long) types are supported only as Dictionary include or measure, they are not allowed in dictionary exclude.Solution:Support Int and Bigint(long) as Dictionary exclude and also Sort columns support for int,long, bigint.To be resolved - CARBONDATA-1485
issueID:CARBONDATA-1451
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/scan/scanner/impl/BlockletFullScanner.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonV3DataFormatConstants.java
processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactDataHandlerColumnar.java
core/src/main/java/org/apache/carbondata/core/util/CarbonProperties.java
core/src/main/java/org/apache/carbondata/core/scan/filter/FilterUtil.java
texts:Removing configuration for number_of_rows_per_blocklet_column_page

issueID:CARBONDATA-1452
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/keygenerator/directdictionary/timestamp/TimeStampDirectDictionaryGenerator.java
texts:Issue with loading timestamp data beyond cutoff
While generating surrogate for timestamp dictionary column, we are casting the value to int. Then we are considering only the +ve values for generating dictionary, when the value is out of range,overflow occurs and cyclic rotation happens while casting, in the cyclic rotation there is possibility of getting +ve values in overflow cases too.
issueID:CARBONDATA-1453
type:Bug
changed files:
texts:Optimize the cluster test case ID and make it more general

issueID:CARBONDATA-1454
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/scan/filter/intf/FilterExecuterType.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/resolverinfo/FalseConditionalResolverImpl.java
core/src/main/java/org/apache/carbondata/core/scan/filter/FilterExpressionProcessor.java
core/src/main/java/org/apache/carbondata/core/scan/filter/FilterUtil.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/FalseFilterExecutor.java
texts:Block pruning not working when wrong data is given in filter
Load ~13 Million records all with 2013-07-25' dateQ1select *  from test where  my_date='2013-07-25' ;This took ~.0.5 secQ2 select * from test where  my_date='' limit 1;This tool ~30 sec .Q2 data will not be found in carbon so actually it should be handled in the block pruning . and no job should be fired for it.Same need to hanlde for timestamp also.Solution :- This to be consider as FalseExpression and handle same.
issueID:CARBONDATA-1456
type:Bug
changed files:
texts:Regenerate cached hive results if cluster testcases fail
Hive results are cached to speed up subsequent test runs.But some times the result might have to be changedReasons:1.May be the test case changed2.May be the input data changed3.May be the environment changed
issueID:CARBONDATA-1458
type:Bug
changed files:integration/presto/src/main/java/org/apache/carbondata/presto/readers/DecimalSliceStreamReader.java
integration/presto/src/main/java/org/apache/carbondata/presto/CarbondataPageSource.java
core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
integration/hive/src/main/java/org/apache/carbondata/hive/CarbonDictionaryDecodeReadSupport.java
texts:Error in fetching decimal type data loaded with Carbondata 1.1.0 in Carbondata 1.2.0
I have a carbon data table created and loaded with the Carbondata 1.1.0CREATE TABLE uniqdata1 (CUST_ID int,CUST_NAME String,ACTIVE_EMUI_VERSION string, DOB timestamp, DOJ timestamp, BIGINT_COLUMN1 bigint,BIGINT_COLUMN2 bigint,DECIMAL_COLUMN1 decimal(30,10), DECIMAL_COLUMN2 decimal(36,10),Double_COLUMN1 double, Double_COLUMN2 double,INTEGER_COLUMN1 int) STORED BY 'org.apache.carbondata.format' ; LOAD DATA INPATH 'hdfs://localhost:54310/data/2000_UniqData.csv' into table uniqdata1 OPTIONS('DELIMITER'=',' , 'QUOTECHAR'='"','FILEHEADER'='CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1,Double_COLUMN2,INTEGER_COLUMN1') ;Now, when I am trying to fetch the data using Carbondata 1.2.0, I am getting exceptionThis is happening for the decimal data type, the command and error message is given below:select DECIMAL_COLUMN1 from uniqdata1;Error: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 29.0 failed 4 times, most recent failure: Lost task 0.3 in stage 29.0 (TID 93, 192.168.2.188, executor 0): org.apache.spark.util.TaskCompletionListenerException: java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.lang.ClassCastException: java.lang.Double cannot be cast to java.math.BigDecimalAnd also if the table contains decimal, then the select * query also fails because of above issue
issueID:CARBONDATA-146
type:Bug
changed files:
texts:Data loading failure using carbon-spark-sql and carbon-spark-shell
Carbon plugin's path is not considering from carbon.properties file and it is refering from CARBON_HOME path. Because of that Data loading is failing  using carbon-spark-sql and carbon-spark-shell.
issueID:CARBONDATA-1460
type:Bug
changed files:
texts:Drop column in alter table working incorrectly when connected to same thrift using different beeline sessions
I am trying to do concurrent testing on the same table. For that, I have started my thrift server with the given command:sudo /home/hduser/spark-2.1.0-bin-hadoop2.7/bin/spark-submit  --master spark://host-name:7077 --class org.apache.carbondata.spark.thriftserver.CarbonThriftServer carbondata_2.11-1.2.0-SNAPSHOT-shade-hadoop2.2.0.jar <carbon-store-path>and two different nodes are connecting to same thrift using two beeline sessions.While beeline1 session executes a query :  alter table uniqdata drop columns(cust_id);Then after execution of the query, beeline1 session gets error: Error: org.apache.spark.sql.AnalysisException: cannot resolve '`cust_id`' given input columns: &#91;bigint_column1, double_column1, dob, doj, active_emui_version, decimal_column2, bigint_column2, integer_column1, cust_name, double_column2, decimal_column1&#93;; line 1 pos 7;While beeline2 session is able to get data for cust_id from that table using the below command: select cust_id from uniqdata;But, when both the beeline session try to run "describe table uniqdata" they see the same result which does not include cust_id as a column in the table.
issueID:CARBONDATA-1461
type:Bug
changed files:integration/hive/src/main/java/org/apache/carbondata/hive/CarbonDictionaryDecodeReadSupport.java
texts:Unable to Read Date And TimeStamp Type in HIve
val connection = DriverManager.getConnection(s"jdbc:hive2://localhost:$port/default", "", "")    val statement: Statement = connection.createStatement    logger.info(s"============HIVE CLI IS STARTED ON PORT $port ==============")    statement.execute("CREATE TABLE IF NOT EXISTS carbon_table(shortField SMALLINT,intField INT,bigintField " +                      "bigint,doubleField DOUBLE,stringField STRING,timestampField TIMESTAMP," +                      "decimalField DECIMAL(18,2),dateField DATE,\n charField CHAR(5),floatField " +                      "double,complexData ARRAY<STRING>)")    statement      .execute(        "ALTER TABLE carbon_table SET FILEFORMAT INPUTFORMAT \"org.apache.carbondata." +        "hive.MapredCarbonInputFormat\"OUTPUTFORMAT \"org.apache.carbondata.hive." +        "MapredCarbonOutputFormat\"SERDE \"org.apache.carbondata.hive." +        "CarbonHiveSerDe\" ")    statement      .execute(        "ALTER TABLE carbon_table SET LOCATION 'file:///home/vinod/Desktop/carbondata/examples/spark2/target/store/default/carbon_table' ")    val sql = "SELECT dateField FROM carbon_table"    val resultSet: ResultSet = statement.executeQuery(sql)     logsexception in thread "main" org.apache.hive.service.cli.HiveSQLException: java.io.IOException: java.lang.ClassCastException: java.lang.Integer cannot be cast to java.lang.Long at org.apache.hive.jdbc.Utils.verifySuccess(Utils.java:256) at org.apache.hive.jdbc.Utils.verifySuccessWithInfo(Utils.java:242) at org.apache.hive.jdbc.HiveQueryResultSet.next(HiveQueryResultSet.java:365)when i tried to read timestamp type it gives error  val sql = "SELECT timestampField FROM carbon_table"    val resultSet: ResultSet = statement.executeQuery(sql)    var rowsFetched = 0    while (resultSet.next) {println("*****"+resultSet.getString("timestampField"))}java.lang.IllegalArgumentException: Timestamp format must be yyyy-mm-dd hh:mm:ss&#91;.fffffffff&#93;
issueID:CARBONDATA-1462
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
core/src/main/java/org/apache/carbondata/core/util/CarbonProperties.java
texts:Add an option &#39;carbon.update.storage.level&#39; to support configuring the storage level when updating data with &#39;carbon.update.persist.enable&#39;=&#39;true&#39;
When updating data with 'carbon.update.persist.enable'='true'(default), the storage level of dataset is 'MEMORY_AND_DISK', it should support configuring the storage level to correspond to different environment.
issueID:CARBONDATA-1463
type:Improvement
changed files:
texts:Compare Test should validate result size
CompareTest for spark2.1 should only validate result size instead of result value, because some test case include aggregation on double column which will give different result since carbon records are sorted
issueID:CARBONDATA-1464
type:Bug
changed files:
texts:SparkSessionExample is not working
SparkSessionExample is not working
issueID:CARBONDATA-1465
type:Bug
changed files:integration/hive/src/main/java/org/apache/carbondata/hive/MapredCarbonInputFormat.java
texts:Hive unable to query carbondata when column names is in small letters
inside hive example i tried this    hiveEmbeddedServer2 = new HiveEmbeddedServer2    hiveEmbeddedServer2.start()    hiveEmbeddedServer2      .execute(s"CREATE TABLE IF NOT EXISTS TESTTABLE(ID INT,JOINNINGDATE date,COUNTRY string,NAME STRING," +               s" PHONETYPE STRING, SERIALNAME STRING, SALARY DOUBLE,BONUS DECIMAL(10,4)) ROW FORMAT " +               s"SERDE 'org.apache.carbondata.hive.CarbonHiveSerDe' STORED AS INPUTFORMAT 'org" +               s".apache.carbondata.hive.MapredCarbonInputFormat' OUTPUTFORMAT 'org.apache" +               s".carbondata.hive.MapredCarbonOutputFormat' TBLPROPERTIES ('spark.sql.sources" +               s".provider'='org.apache.spark.sql.CarbonSource')")hiveEmbeddedServer2      .execute(s"ALTER TABLE TESTTABLE SET LOCATION " +               s"'file:///$rootPath/integration/hive/target/store/testdb/testtable' ")hiveEmbeddedServer2.execute("SELECT name FROM TESTTABLE")Caused by: java.lang.StringIndexOutOfBoundsException: String index out of range: -1 at java.lang.AbstractStringBuilder.substring(AbstractStringBuilder.java:935)
issueID:CARBONDATA-1466
type:Improvement
changed files:
texts:Presto Integration - Performance Improvement
Presto Integration : Performance ImprovementImplemented Better handling of Nulls in Stream ReadersAdded Short and Timestamp Reader to resolve it properlyOptimized Filters to ensure better push down.
issueID:CARBONDATA-1467
type:Improvement
changed files:
texts:Presto Integration - Performance Improvement
Presto Integration : Performance ImprovementImplemented Better handling of Nulls in Stream ReadersAdded Short and Timestamp Reader to resolve it properlyOptimized Filters to ensure better push down.
issueID:CARBONDATA-1468
type:Improvement
changed files:
texts:Presto Integration - Performance Improvement
Presto Integration : Performance ImprovementImplemented Better handling of Nulls in Stream ReadersAdded Short and Timestamp Reader to resolve it properlyOptimized Filters to ensure better push down.
issueID:CARBONDATA-1469
type:Improvement
changed files:integration/presto/src/main/java/org/apache/carbondata/presto/PrestoFilterUtil.java
integration/presto/src/main/java/org/apache/carbondata/presto/readers/IntegerStreamReader.java
integration/presto/src/main/java/org/apache/carbondata/presto/readers/DecimalSliceStreamReader.java
integration/presto/src/main/java/org/apache/carbondata/presto/readers/DoubleStreamReader.java
integration/presto/src/main/java/org/apache/carbondata/presto/readers/LongStreamReader.java
integration/presto/src/main/java/org/apache/carbondata/presto/readers/ShortStreamReader.java
integration/presto/src/main/java/org/apache/carbondata/presto/readers/TimestampStreamReader.java
texts:Presto Integration - Performance Improvement
Presto Integration : Performance ImprovementImplemented Better handling of Nulls in Stream ReadersAdded Short and Timestamp Reader to resolve it properlyOptimized Filters to ensure better push down.
issueID:CARBONDATA-147
type:Bug
changed files:
texts:Describe formatted command failing
Describe formatted command failing with below errorjava.lang.IndexOutOfBoundsException: Index: 0, Size: 0        at java.util.ArrayList.rangeCheck(ArrayList.java:653)        at java.util.ArrayList.get(ArrayList.java:429)        at org.apache.hive.service.cli.Column.get(Column.java:281)        at org.apache.hive.service.cli.ColumnBasedSet$1.next(ColumnBasedSet.java:130)        at org.apache.hive.service.cli.ColumnBasedSet$1.next(ColumnBasedSet.java:117)        at org.apache.hive.jdbc.HiveQueryResultSet.next(HiveQueryResultSet.java:374)        at org.apache.hive.beeline.BufferedRows.<init>(BufferedRows.java:42)        at org.apache.hive.beeline.BeeLine.print(BeeLine.java:1794)        at org.apache.hive.beeline.Commands.execute(Commands.java:860)        at org.apache.hive.beeline.Commands.sql(Commands.java:713)        at org.apache.hive.beeline.BeeLine.dispatch(BeeLine.java:973)        at org.apache.hive.beeline.BeeLine.execute(BeeLine.java:813)        at org.apache.hive.beeline.BeeLine.begin(BeeLine.java:771)        at org.apache.hive.beeline.BeeLine.mainWithInputRedirection(BeeLine.java:484)        at org.apache.hive.beeline.BeeLine.main(BeeLine.java:467)Error: Error retrieving next row (state=,code=0)
issueID:CARBONDATA-1470
type:Bug
changed files:processing/src/main/java/org/apache/carbondata/processing/loading/csvinput/CSVRecordReaderIterator.java
processing/src/main/java/org/apache/carbondata/processing/util/CarbonDataProcessorUtil.java
texts:csv data should not show in error log when data column length is greater than 100000 characters

issueID:CARBONDATA-1471
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/adaptive/AdaptiveFloatingCodec.java
texts:Replace BigDecimal to double to improve performance
Replace BigDecimal to double in AdaptiveFloatingCodec to improve performance
issueID:CARBONDATA-1472
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RangeValueFilterExecuterImpl.java
hadoop/src/main/java/org/apache/carbondata/hadoop/AbstractRecordReader.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
core/src/main/java/org/apache/carbondata/core/memory/UnsafeMemoryManager.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/store/impl/unsafe/UnsafeAbstractDimensionDataChunkStore.java
core/src/main/java/org/apache/carbondata/core/scan/executor/impl/AbstractQueryExecutor.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelRangeLessThanFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelRangeLessThanEqualFilterExecuterImpl.java
texts:Optimize memory and fix nosort queries
Use UnsafeManager for dimension chunks as well to avoid leaksFix filters on nosort columns.Optimize scanRDD
issueID:CARBONDATA-1473
type:Bug
changed files:
texts:Unable To use Greater than Operator on Date Type In Hive
HiveEmbeddedServer.execute("SELECT ID FROM ALLDATATYPETEST WHERE JOININGDATE > 2016-04-14")org.apache.hive.service.cli.HiveSQLException: java.io.IOException: java.lang.ArrayIndexOutOfBoundsException: 1  at org.apache.hive.service.cli.operation.SQLOperation.getNextRowSet(SQLOperation.java:352)  at org.apache.hive.service.cli.operation.OperationManager.getOperationNextRowSet(OperationManager.java:220)  at org.apache.hive.service.cli.session.HiveSessionImpl.fetchResults(HiveSessionImpl.java:685)  at org.apache.hive.service.cli.CLIService.fetchResults(CLIService.java:454)  at org.apache.hive.service.cli.CLIService.fetchResults(CLIService.java:447)  at server.HiveEmbeddedServer$.execute(HiveEmbeddedServer.scala:150)  at org.apache.carbondata.hive.integrationtest.AllDataTypeTest$$anonfun$2.apply$mcV$sp(AllDataTypeTest.scala:50)  at org.apache.carbondata.hive.integrationtest.AllDataTypeTest$$anonfun$2.apply(AllDataTypeTest.scala:50)  at org.apache.carbondata.hive.integrationtest.AllDataTypeTest$$anonfun$2.apply(AllDataTypeTest.scala:50)  at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)  ...  Cause: java.io.IOException: java.lang.ArrayIndexOutOfBoundsException: 1  at org.apache.hadoop.hive.ql.exec.FetchTask.fetch(FetchTask.java:154)  at org.apache.hadoop.hive.ql.Driver.getResults(Driver.java:1670)  at org.apache.hive.service.cli.operation.SQLOperation.getNextRowSet(SQLOperation.java:347)  at org.apache.hive.service.cli.operation.OperationManager.getOperationNextRowSet(OperationManager.java:220)  at org.apache.hive.service.cli.session.HiveSessionImpl.fetchResults(HiveSessionImpl.java:685)  at org.apache.hive.service.cli.CLIService.fetchResults(CLIService.java:454)  at org.apache.hive.service.cli.CLIService.fetchResults(CLIService.java:447)
issueID:CARBONDATA-1474
type:Bug
changed files:
texts:Memory leak issue in case of vector reader
Memory leak issue in case of vector reader
issueID:CARBONDATA-1475
type:Improvement
changed files:
texts:fixed dependencies for Intellij Idea
When we choose profile spark-1.6, libraries contain dependencies of scala-2.11.
issueID:CARBONDATA-1477
type:Bug
changed files:integration/hive/src/main/java/org/apache/carbondata/hive/CarbonDictionaryDecodeReadSupport.java
texts:Wrong values shown when fetching date type values in hive
package org.apache.carbondata.hiveexampleimport java.io.Fileimport java.sql.{DriverManager, ResultSet, Statement}import org.apache.spark.sql.SparkSessionimport org.apache.carbondata.common.logging.LogServiceFactoryimport org.apache.carbondata.hive.server.HiveEmbeddedServer2// scalastyle:off printlnobject HiveExample {  private val driverName: String = "org.apache.hive.jdbc.HiveDriver"  def main(args: Array&#91;String&#93;) {    val rootPath = new File(this.getClass.getResource("/").getPath                            + "../../../..").getCanonicalPath    val store = s"$rootPath/integration/hive/target/store"    val warehouse = s"$rootPath/integration/hive/target/warehouse"    val metaStore_Db = s"$rootPath/integration/hive/target/carbon_metaStore_db"    val logger = LogServiceFactory.getLogService(this.getClass.getCanonicalName)    var resultId = ""    var resultName = ""    var resultSalary = ""    import org.apache.spark.sql.CarbonSession._    val carbonSession = SparkSession      .builder()      .master("local")      .appName("HiveExample")      .config("carbonSession.sql.warehouse.dir", warehouse).enableHiveSupport()      .getOrCreateCarbonSession(        store, metaStore_Db)    carbonSession.sql("DROP TABLE IF EXISTS HIVE_CARBON_EXAMPLE ")    carbonSession      .sql(        """CREATE TABLE IF NOT EXISTS HIVE_CARBON_EXAMPLE (ID int,NAME string,SALARY double,JOININGDATE date) STORED BY'CARBONDATA' """          .stripMargin)    carbonSession.sql(      s"""           LOAD DATA LOCAL INPATH '$rootPath/integration/hive/src/main/resources/data.csv' INTO           TABLE         HIVE_CARBON_EXAMPLE           """)    carbonSession.sql("SELECT * FROM HIVE_CARBON_EXAMPLE").show()    carbonSession.stop()    try {      Class.forName(driverName)    }    catch {      case classNotFoundException: ClassNotFoundException =>        classNotFoundException.printStackTrace()    }    val hiveEmbeddedServer2 = new HiveEmbeddedServer2()    hiveEmbeddedServer2.start()    val port = hiveEmbeddedServer2.getFreePort    val connection = DriverManager.getConnection(s"jdbc:hive2://localhost:$port/default", "", "")    val statement: Statement = connection.createStatement    logger.info(s"============HIVE CLI IS STARTED ON PORT $port ==============")    statement.execute("CREATE TABLE IF NOT EXISTS " + "HIVE_CARBON_EXAMPLE " +                      " (ID int, NAME string,SALARY double,JOININGDATE date)")    statement      .execute(        "ALTER TABLE HIVE_CARBON_EXAMPLE SET FILEFORMAT INPUTFORMAT \"org.apache.carbondata." +        "hive.MapredCarbonInputFormat\"OUTPUTFORMAT \"org.apache.carbondata.hive." +        "MapredCarbonOutputFormat\"SERDE \"org.apache.carbondata.hive." +        "CarbonHiveSerDe\" ")    statement      .execute(        "ALTER TABLE HIVE_CARBON_EXAMPLE SET LOCATION " +        s"'file:///$store/default/hive_carbon_example' ")    val sql = "SELECT * FROM HIVE_CARBON_EXAMPLE"    val resultSet: ResultSet = statement.executeQuery(sql)    var rowsFetched = 0    while (resultSet.next) {      println("*********"+resultSet.getString("JOININGDATE"))    }    println(s"******Total Number Of Rows Fetched ****** $rowsFetched")    logger.info("Fetching the Individual Columns ")    hiveEmbeddedServer2.stop()    System.exit(0)  }}values that i get for date type is 1970-01-01 which was wrong
issueID:CARBONDATA-1478
type:Improvement
changed files:
texts:Update compaction documentation
Some examples about compaction in documentation should be updated.
issueID:CARBONDATA-1480
type:Bug
changed files:processing/src/main/java/org/apache/carbondata/processing/datamap/DataMapWriterListener.java
processing/src/main/java/org/apache/carbondata/processing/store/writer/v3/CarbonFactDataWriterImplV3.java
core/src/main/java/org/apache/carbondata/core/indexstore/SegmentPropertiesFetcher.java
core/src/main/java/org/apache/carbondata/core/datamap/DataMapStoreManager.java
datamap/examples/src/minmaxdatamap/main/java/org/apache/carbondata/datamap/examples/BlockletMinMax.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockletDataMapFactory.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockletDataMap.java
datamap/examples/src/minmaxdatamap/main/java/org/apache/carbondata/datamap/examples/MinMaxIndexDataMapFactory.java
datamap/examples/src/minmaxdatamap/main/java/org/apache/carbondata/datamap/examples/MinMaxIndexBlockDetails.java
core/src/main/java/org/apache/carbondata/core/datamap/dev/DataMap.java
processing/src/main/java/org/apache/carbondata/processing/store/writer/AbstractFactDataWriter.java
core/src/main/java/org/apache/carbondata/core/datamap/TableDataMap.java
datamap/examples/src/minmaxdatamap/main/java/org/apache/carbondata/datamap/examples/MinMaxDataWriter.java
datamap/examples/src/minmaxdatamap/main/java/org/apache/carbondata/datamap/examples/MinMaxIndexDataMap.java
core/src/main/java/org/apache/carbondata/core/datamap/dev/DataMapWriter.java
texts:Datamap Example. Min Max Index implementation.
Example of DataMap Use Case. Creating a Min Max Index through DataMap implementation and using it in Pruning.
issueID:CARBONDATA-1481
type:Improvement
changed files:
texts:Compaction support global_sort
We should add some test cases and evaluate performance for Compaction support global_sort.       Test cases:             1. compaction type: major and monor             2. parameter:  global_sort_partitions               3. scort_column: data type             4. data size              5. load times             6. clean files and delete by segment.id    performance evaluation:            It should faster for querying after loading 1000 times and compaction
issueID:CARBONDATA-1482
type:Bug
changed files:
texts:fix the failing integration test cases of presto

issueID:CARBONDATA-1483
type:Improvement
changed files:
texts:Open auto merge when loading and inserting into
auto compaction when loading and inserting into set  DEFAULT_ENABLE_AUTO_LOAD_MERGE as true add test case and verify the change is ok
issueID:CARBONDATA-1484
type:Bug
changed files:
texts:carbonthriftserver cache is not refreshed
i am use carbondata version is 1.1.0 and spark version is 1.6.0.     and I reproduce in accordance with the official quick-start-guide case again,     1.Creating a Table cc.sql("create table IF NOT EXISTS  carbondb.test_table(id string,name String,city String,age int) stored by 'carbondata' ")     2.load data into table   cc.sql("load data inpath 'hdfs://nameservice1/user/zz/sample.csv' into table carbondb.test_table")     3.start carbonthriftserver    /home/zz/spark-1.6.0-bin-hadoop2.6/bin/spark-submit  --master local&#91;*&#93; --driver-java-options="-Dcarbon.properties.filepath=/home/zz/spark-1.6.0-bin-hadoop2.6/conf/carbon.properties" --executor-memory 4G  --driver-memory 2g  --conf spark.serializer=org.apache.spark.serializer.KryoSerializer   --conf "spark.sql.shuffle.partitions=3" --conf spark.speculation=true   --class org.apache.carbondata.spark.thriftserver.CarbonThriftServer /home/zz/spark-1.6.0-bin-hadoop2.6/carbonlib/carbondata_2.10-1.1.0-shade-hadoop2.2.0.jar hdfs://nameservice1/user/zz/rp_carbon_store    4.Connecting to CarbonData Thrift Server Using Beeline.          <http://chuantu.biz/t6/47/1505284993x2890202558.jpg>    5.drop table    cc.sql("drop table carbondb.test_table")    6.recreate table and load data     cc.sql("create table IF NOT EXISTS  carbondb.test_table(id string,name String,city String,age int) stored by 'carbondata' ")     cc.sql("load data inpath 'hdfs://nameservice1/user/zz/sample.csv' into table carbondb.test_table")    7.select data use beeline     <http://chuantu.biz/t6/47/1505284937x1034817476.jpg>    Like the above error, the cache is not updated  in my case,1,2,5 and 6 step is one session on spark-shell ,4 and 7 is one session on beeline, and i test this case on the current carbondata master branch(1.2.0),  when i use beeline there is no Btree load failed info,but in my table there is no data,All the data is null,but in spark-shell is ok. spark-shell: -------------+  id name    cityage-------------+   1davidshenzhen 31  2easonshenzhen 27  3jarry   wuhan 35-------------+ beeline: 0: jdbc:hive2://localhost:10000> select * from  carbondb.test_table; ----------------------   id    name   city    age  ----------------------  NULL   NULL   NULL   NULL   NULL   NULL   NULL   NULL   NULL   NULL   NULL   NULL  ----------------------
issueID:CARBONDATA-1486
type:Bug
changed files:processing/src/main/java/org/apache/carbondata/processing/util/CarbonLoaderUtil.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/merger/UnsafeSingleThreadFinalSortFilesMerger.java
texts:Fixed issue of table status updation on insert overwrite failure and exception thrown while deletion of stale folders
Problems to be fixed:1. HDFS file system throwing exception while deleting a non-existent folder. This happens because on multiple run of insert overwrite operation, same folder is getting added to the stale folder list even though it has been deleted during first time insert overwrite operation run and when the same folder is tried to be deleted again HDFS file system throws an IO exception.2. Insert or Load operation should throw exception if:       a. If insert overwrite is in progress and any other load or insert operation is triggered       b. If load or insert into operation is in progress and insert overwrite operation is triggered3. Table status not getting updated after failure of insert/load job. In case of insert overwrite job status in table status file is "Overwrite in Progress". In this case no other load for same table can run in parallel and if this status is not changes on success of failure jobs, the system will still assume that insert overwrite is in progress and fail all next insert/load jobs4. Removed Unused Code
issueID:CARBONDATA-1488
type:Bug
changed files:integration/spark-datasource/src/main/scala/org/apache/carbondata/spark/vectorreader/VectorizedCarbonRecordReader.java
core/src/main/java/org/apache/carbondata/core/datastore/page/UnsafeVarLengthColumnPage.java
core/src/main/java/org/apache/carbondata/core/datastore/page/VarLengthColumnPageBase.java
texts:JVM crashes when unsafe columnpage is enabled.
The below testcase fails when unsafe column page is enabledBadRecordEmptyDataTest.test load multiple loads- pne with valid record and one with invalid
issueID:CARBONDATA-1490
type:Bug
changed files:processing/src/main/java/org/apache/carbondata/processing/loading/steps/CarbonRowDataWriterProcessorStepImpl.java
texts:Unnecessary space is being allocated for measures in carbon row
Consider the following query which has only one column of complex type. While loading when we create carbon row for each of the records, unnecessary space in the carbon row is being allocated for measures which do not exist.spark.sql("CREATE TABLE carbon_table( complexData ARRAY<STRING>) STORED BY 'carbondata'
issueID:CARBONDATA-1491
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/scan/executor/util/RestructureUtil.java
texts:Dictionary_exclude columns are not going into no_dictionary flow
Dictionary_exclude columns are not going into dictionary_exclude flow. This is visible in case of alter add measure column with default value.Alter add int column and do select column, below issue exists.java.lang.IllegalArgumentException: Wrong length: 2, expected 4 at org.apache.carbondata.core.util.ByteUtil.explainWrongLengthOrOffset(ByteUtil.java:581) at org.apache.carbondata.core.util.ByteUtil.toInt(ByteUtil.java:500) at org.apache.carbondata.core.scan.executor.util.RestructureUtil.getNoDictionaryDefaultValue(RestructureUtil.java:222) at org.apache.carbondata.core.scan.executor.util.RestructureUtil.validateAndGetDefaultValue(RestructureUtil.java:162) at org.apache.carbondata.core.scan.executor.util.RestructureUtil.createDimensionInfoAndGetCurrentBlockQueryDimension(RestructureUtil.java:118) at org.apache.carbondata.core.scan.executor.impl.AbstractQueryExecutor.getBlockExecutionInfoForBlock(AbstractQueryExecutor.java:259) at org.apache.carbondata.core.scan.executor.impl.AbstractQueryExecutor.getBlockExecutionInfos(AbstractQueryExecutor.java:223) at org.apache.carbondata.core.scan.executor.impl.VectorDetailQueryExecutor.execute(VectorDetailQueryExecutor.java:36) at org.apache.carbondata.spark.vectorreader.VectorizedCarbonRecordReader.initialize(VectorizedCarbonRecordReader.java:116) at org.apache.carbondata.spark.rdd.CarbonScanRDD.internalCompute(CarbonScanRDD.scala:229) at org.apache.carbondata.spark.rdd.CarbonRDD.compute(CarbonRDD.scala:62) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) at org.apache.spark.rdd.RDD.iterator(RDD.scala:287) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) at org.apache.spark.rdd.RDD.iterator(RDD.scala:287) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) at org.apache.spark.rdd.RDD.iterator(RDD.scala:287) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87) at org.apache.spark.scheduler.Task.run(Task.scala:99) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:748)
issueID:CARBONDATA-15
type:Bug
changed files:
texts:filter query throwing error if the query applied over a table having no data.
Steps:1. Create carbon table2. do not load any data3. do filter query for ex. select * from table where column_1='shahid';Actualthrowing RuntimeExecption: Exception Occured in query execution.Expected:Query should be successful with empty data.
issueID:CARBONDATA-150
type:Bug
changed files:
texts:Aggregate function with sub query using Order by is not working
Aggregate function with sub query using Order by is not working 1. Create a table2. Load the data 3. query  as belowselect Min(imei) from (select imei from table1 order by imei) t; Error: org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 312.0 failed 4 times, most recent failure: Lost task 7.3 in stage 312.0 (TID 7266, BLR1000007847): java.lang.ClassCastException
issueID:CARBONDATA-1504
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/datamap/DataMapStoreManager.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonTableInputFormat.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockletDataMapFactory.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockletDataMap.java
texts:Refresh of segments in datamap for update and partition is not working if the segments are cached
Currently datamap scans complete segment every time for query execution to get all the carbonindex files. It will be slow when data/number of files are big.
issueID:CARBONDATA-1505
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/datamap/DataMapStoreManager.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonTableInputFormat.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockletDataMapFactory.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockletDataMap.java
core/src/main/java/org/apache/carbondata/core/datamap/DataMapJob.java
core/src/main/java/org/apache/carbondata/core/indexstore/TableBlockIndexUniqueIdentifier.java
core/src/main/java/org/apache/carbondata/core/indexstore/BlockletDetailsFetcher.java
core/src/main/java/org/apache/carbondata/core/indexstore/Blocklet.java
core/src/main/java/org/apache/carbondata/core/datamap/TableDataMap.java
core/src/main/java/org/apache/carbondata/core/indexstore/ExtendedBlocklet.java
core/src/main/java/org/apache/carbondata/core/datamap/DistributableDataMapFormat.java
texts:Get the detailed blocklet information using default BlockletDataMap for other datamaps
All the detail information of blocklet which is need for exceuting query is present only BlockletDataMap. It is actually default datamap.So if new datamap is added then it gives only information of blocklet and blockid, it is insuffucient information to exceute query.Please add the functionality to retrieve detailed blocklet information from the BlockletDataMap based on block and blocklet id
issueID:CARBONDATA-1506
type:Bug
changed files:
texts:SDV tests error in CI
Sometimes, there is error in SDV test:== Results ==!== Correct Answer - 1 ==   == Spark Answer - 1 ==!&#91;0.0&#93;                      &#91;1.0E-5&#93;sometimes it is correct.It's better to fix this error.failed CI like：http://144.76.159.231:8080/job/ApacheSDVTests/879/testReport/junit/org.apache.carbondata.cluster.sdv.generated/QueriesBasicTestCase/PushUP_FILTER_uniqdata_TC075/http://144.76.159.231:8080/job/ApacheSDVTests/841/testReport/junit/org.apache.carbondata.cluster.sdv.generated/QueriesBasicTestCase/PushUP_FILTER_uniqdata_TC074/http://144.76.159.231:8080/job/ApacheSDVTests/797/testReport/junit/org.apache.carbondata.cluster.sdv.generated/QueriesBasicTestCase/PushUP_FILTER_uniqdata_TC074/http://144.76.159.231:8080/job/ApacheSDVTests/1037/testReport/junit/org.apache.carbondata.cluster.sdv.generated/QueriesBasicTestCase/PushUP_FILTER_uniqdata_TC076/http://144.76.159.231:8080/job/ApacheSDVTests/1042/testReport/junit/org.apache.carbondata.cluster.sdv.generated/QueriesBasicTestCase/PushUP_FILTER_uniqdata_TC076/
issueID:CARBONDATA-1507
type:Bug
changed files:
texts:Dataload global sort fails with RpcTimeOutException: Futures timed out after [120 seconds]
Dataload global sort fails with RpcTimeOutException: Futures timed out after &#91;120 seconds&#93;
issueID:CARBONDATA-1509
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/datastore/TableSpec.java
texts:Fixed bug for maintaining compatibility of decimal type with older releases of Carbondata
In old Carbondata releases, precision and scale is not stored for decimal data type and both values are initialized to -1. In TableSpec.ColumnSpec  default values for precision and scale are initialized to 0 because of which exception is thrown while reading the old store with decimal column. Exception traceCaused by: java.lang.ClassCastException: [B cannot be cast to java.lang.Long at org.apache.carbondata.core.metadata.datatype.DecimalConverterFactory$DecimalIntConverter.getDecimal(DecimalConverterFactory.java:98) at org.apache.carbondata.core.datastore.page.UnsafeDecimalColumnPage.getDecimal(UnsafeDecimalColumnPage.java:260) at org.apache.carbondata.core.datastore.page.LazyColumnPage.getDecimal(LazyColumnPage.java:111) at org.apache.carbondata.core.scan.result.vector.MeasureDataVectorProcessor$DecimalMeasureVectorFiller.fillMeasureVector(MeasureDataVectorProcessor.java:215) at org.apache.carbondata.core.scan.result.AbstractScannedResult.fillColumnarMeasureBatch(AbstractScannedResult.java:257) at org.apache.carbondata.core.scan.collector.impl.DictionaryBasedVectorResultCollector.scanAndFillResult(DictionaryBasedVectorResultCollector.java:164) at org.apache.carbondata.core.scan.collector.impl.DictionaryBasedVectorResultCollector.collectVectorBatch(DictionaryBasedVectorResultCollector.java:155) at org.apache.carbondata.core.scan.processor.impl.DataBlockIteratorImpl.processNextBatch(DataBlockIteratorImpl.java:65) at org.apache.carbondata.core.scan.result.iterator.VectorDetailQueryResultIterator.processNextBatch(VectorDetailQueryResultIterator.java:46) at org.apache.carbondata.spark.vectorreader.VectorizedCarbonRecordReader.nextBatch(VectorizedCarbonRecordReader.java:258) at org.apache.carbondata.spark.vectorreader.VectorizedCarbonRecordReader.nextKeyValue(VectorizedCarbonRecordReader.java:145) at org.apache.carbondata.spark.rdd.CarbonScanRDD$$anon$1.hasNext(CarbonScanRDD.scala:246)
issueID:CARBONDATA-151
type:Bug
changed files:
texts:count & count distinct column on same query is not working
count & count distinct column on same query is not working 1. Create a table2. Load the data3. Run the below queryselect count(imei), count(distinct(imei)) from table1; Error: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 316.0 failed 4 times, most recent failure: Lost task 0.3 in stage 316.0 (TID 7276, BLR1000007780): java.lang.ClassCastException: org.apache.spark.unsafe.types.UTF8String cannot be cast to java.lang.Integer
issueID:CARBONDATA-1512
type:Bug
changed files:integration/presto/src/main/java/org/apache/carbondata/presto/impl/CarbonTableReader.java
texts:Failed to run sqls concurrently
Hi,    I run 20 sqls concurrently,but it appears error.The attachment is the error log.sql: select l_orderkey from carbondata.car0921.lineitem where l_comment='ke furiously final deposits. fur'
issueID:CARBONDATA-1514
type:Bug
changed files:
texts:Sort Column Property is not getting added in case of alter operation
Sort Column Property is not getting added in case of alter operation
issueID:CARBONDATA-1515
type:Bug
changed files:processing/src/main/java/org/apache/carbondata/processing/loading/steps/DataConverterProcessorStepImpl.java
texts:Fixed NPE in Data loading
Scenario: Data size: 3.5 billion rows(4.1 tb data)3 node clusterNumber of core while data loading 12.No. of loads 100 timesProblem: In DataConverterProcessorStepImpl it is using array list for adding all the local converter, in case of multiple thread scenario it is creating a hole (null value)(as array list if not synchronized). while closing the converter it is it is throwing NPESolution: Add local converter in synchronized block
issueID:CARBONDATA-1516
type:New Feature
changed files:common/src/main/java/org/apache/carbondata/common/exceptions/sql/MalformedDataMapCommandException.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/datamap/DataMapClassProvider.java
common/src/main/java/org/apache/carbondata/common/exceptions/sql/NoSuchDataMapException.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/datamap/Granularity.java
texts:Support pre-aggregate tables and timeseries in carbondata
Currently Carbondata has standard SQL capability on distributed data sets.Carbondata should support pre-aggregating tables for timeseries and improve query performance.
issueID:CARBONDATA-1517
type:Sub-task
changed files:core/src/main/java/org/apache/carbondata/core/metadata/schema/table/column/ParentColumnTableRelation.java
texts:1. Support CTAS in carbon and support creating aggregation tables using CTAS.And update aggregation table information to main table schema.
User should add aggregation tables with following syntaxCREATE TABLE agg_sales STORED BY 'carbondata'TBLPROPERTIES ('parent_table'='sales') AS SELECT user_id, sum(quantity), avg(price) FROM sales GROUP BY user_id
issueID:CARBONDATA-1518
type:Sub-task
changed files:core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
core/src/main/java/org/apache/carbondata/core/preagg/TimeSeriesUDF.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/column/ColumnSchema.java
core/src/main/java/org/apache/carbondata/core/util/AbstractDataFileFooterConverter.java
core/src/main/java/org/apache/carbondata/core/preagg/TimeSeriesFunctionEnum.java
core/src/main/java/org/apache/carbondata/core/metadata/converter/ThriftWrapperSchemaConverterImpl.java
texts:2. Support creating timeseries while creating main table.
User can give timeseries option while creating the main table itself and carbon will create aggregate tables automatically.CREATE TABLE agg_salesSTORED BY 'carbondata'TBLPROPERTIES ('parent_table'='sales', ‘timeseries_column’=’order_time’, ‘granualarity’=’hour’, ‘rollup’ =’quantity:sum, max # user_id: count # price: sum, max, min, avg’) In the above case, user choose timeseries_column, granularity and aggregation types for measures, so carbon generates the aggregation tables automatically for year, month, day and hour level aggregation tables (totally 4 tables, their table name will be prefixed with agg_sales).
issueID:CARBONDATA-1519
type:Sub-task
changed files:core/src/main/java/org/apache/carbondata/core/preagg/TimeSeriesUDF.java
core/src/main/java/org/apache/carbondata/core/preagg/TimeSeriesFunctionEnum.java
core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
texts:3. Create UDF for timestamp to extract year,month,day,hour and minute from timestamp and date
Create UDF for timestamp to extract year,month,day,hour and minute from timestamp and date
issueID:CARBONDATA-152
type:Bug
changed files:processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactDataHandlerColumnar.java
texts:Carbon is not giving proper result with double value
create table double(name string, value double) stored by 'org.apache.carbondata.format'load below dataa,0.9b,1result isname, valuea, -1.5E-4b, 1.0
issueID:CARBONDATA-1520
type:Sub-task
changed files:processing/src/main/java/org/apache/carbondata/processing/loading/converter/impl/FieldEncoderFactory.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/TableInfo.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/RelationIdentifier.java
core/src/main/java/org/apache/carbondata/events/OperationListenerBus.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockletDataMap.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/CarbonTable.java
core/src/main/java/org/apache/carbondata/core/indexstore/UnsafeMemoryDMStore.java
core/src/main/java/org/apache/carbondata/core/writer/CarbonIndexFileMergeWriter.java
core/src/main/java/org/apache/carbondata/core/scan/executor/util/QueryUtil.java
core/src/main/java/org/apache/carbondata/core/indexstore/row/DataMapRowImpl.java
core/src/main/java/org/apache/carbondata/core/indexstore/schema/CarbonRowSchema.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonTableInputFormat.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
core/src/main/java/org/apache/carbondata/core/indexstore/row/DataMapRow.java
core/src/main/java/org/apache/carbondata/core/indexstore/row/UnsafeDataMapRow.java
core/src/main/java/org/apache/carbondata/core/util/SessionParams.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/DataMapSchema.java
core/src/main/java/org/apache/carbondata/core/metadata/converter/SchemaConverter.java
processing/src/main/java/org/apache/carbondata/processing/loading/model/CarbonLoadModel.java
texts:4 Load aggregation tables from main table after finish.
Generate 'insert into ' query for every aggregation table and execute it. insert into table agg_table select UDF(order_time, day) , user_id, sum(quantity), sum(price) from ordersHere the selection of aggregation types should be depending on aggtypes mentioned while creating aggregation tables.Here AVG should be stored as COUNT and SUM separately.
issueID:CARBONDATA-1522
type:Sub-task
changed files:hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonTableInputFormat.java
texts:6. Loading aggregation tables for streaming data tables.
we can finish the segment load after receives configurable amount of data and create a new segment to load new streaming data. While finishing the segment we can trigger the agg tables on it. So there would not be any agg tables on ongoing streaming segment but querying can be done on the streaming segment of actual table.For example user configures stream_segment size as 1 GB, so for every 1 GB of stream data it receives it creates a new segment and finishes the current segment. While finishing the current segment we can trigger agg table loading and compaction of segments.While querying of data we change the query plan to apply union of agg table and streaming segment of actual table to get the current data.
issueID:CARBONDATA-1523
type:Sub-task
changed files:core/src/main/java/org/apache/carbondata/core/metadata/schema/table/TableInfo.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/CarbonTable.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/DataMapSchemaFactory.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/DataMapSchema.java
core/src/main/java/org/apache/carbondata/core/metadata/converter/ThriftWrapperSchemaConverterImpl.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/TableSchema.java
texts:. Add the API in carbon layer to get suitable aggregation table for group by query. Update query plan in carbon optimizer to support aggregation tables for group by queries.
Use the wrapper around main table and keep the agg table hint inside to let the strategy to choose the aggregation table.
issueID:CARBONDATA-1524
type:Sub-task
changed files:core/src/main/java/org/apache/carbondata/core/metadata/schema/table/column/ParentColumnTableRelation.java
texts:8. Refresh the cache of main table after droping of aggregation table.

issueID:CARBONDATA-1526
type:Sub-task
changed files:processing/src/main/java/org/apache/carbondata/processing/util/CarbonLoaderUtil.java
texts:10. Handle compaction in aggregation tables.
User can trigger compaction on pre-aggregate table directly, it will further merge the segments inside pre-aggregation table. To do that, use ALTER TABLE COMPACT command on the pre-aggregate table just like the main table. For implementation, there are two kinds of implementation for compaction. 1. Mergable pre-aggregate tables: if aggregate functions are count, max, min, sum, avg, the pre-aggregate table segments can be merged directly without re-computing it.2. Non-mergable pre-aggregate tables: if aggregate function include distinct_count, it needs to re-compute when doing compaction on pre-aggregate table.
issueID:CARBONDATA-1528
type:Sub-task
changed files:core/src/main/java/org/apache/carbondata/core/metadata/schema/table/RelationIdentifier.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/CarbonTable.java
texts:12. Handle alter table scenarios for aggregation table

issueID:CARBONDATA-1529
type:Bug
changed files:
texts:Partition Table link not working in the README.md

issueID:CARBONDATA-153
type:Bug
changed files:integration/spark-common/src/main/java/org/apache/carbondata/spark/load/CarbonLoaderUtil.java
texts:Record count is not matching while loading the data when one data node went down in HA setup
Record count is not matching while loading the data when one data node went down in HA setup1. Make HA setup2. Kill one data node3. Load the data for 1000000 records CSV4. Verify the record countActual Result: Record count is not matching with actual records
issueID:CARBONDATA-1531
type:Improvement
changed files:
texts:Format module should support BOOLEAN
format module support Boolean data type when I was developing Boolean function, and there are some error in CI after push request into github. So fromat module should support Boolean.
issueID:CARBONDATA-1532
type:Bug
changed files:
texts:Add Link for Partition Table Guide in Website

issueID:CARBONDATA-1533
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/adaptive/AdaptiveIntegralCodec.java
processing/src/main/java/org/apache/carbondata/processing/loading/csvinput/CSVInputFormat.java
texts:Fixed decimal data load fail issue and restricted max characters per column
1. Data load fails when both precision and data falls in integer range for decimal data type with exception as below:17/09/28 16:36:28 ERROR CarbonFactDataHandlerColumnar: pool-20-thread-1 Error in producerjava.lang.RuntimeException: internal error: org.apache.carbondata.core.datastore.page.encoding.adaptive.AdaptiveIntegralCodec&#91;src type: DECIMAL, target type: INT, stats(org.apache.carbondata.core.datastore.page.statistics.PrimitivePageStatsCollector@2afaa54e)&#93; at org.apache.carbondata.core.datastore.page.encoding.adaptive.AdaptiveIntegralCodec$3.encode(AdaptiveIntegralCodec.java:142) at org.apache.carbondata.core.datastore.page.SafeDecimalColumnPage.convertValue(SafeDecimalColumnPage.java:209) at org.apache.carbondata.core.datastore.page.encoding.adaptive.AdaptiveIntegralCodec$1.encodeData(AdaptiveIntegralCodec.java:67) at org.apache.carbondata.core.datastore.page.encoding.ColumnPageEncoder.encode(ColumnPageEncoder.java:57) at org.apache.carbondata.processing.store.TablePage.encodeAndCompressMeasures(TablePage.java:284) at org.apache.carbondata.processing.store.TablePage.encode(TablePage.java:269) at org.apache.carbondata.processing.store.CarbonFactDataHandlerColumnar.processDataRows(CarbonFactDataHandlerColumnar.java:350) at org.apache.carbondata.processing.store.CarbonFactDataHandlerColumnar.access$500(CarbonFactDataHandlerColumnar.java:62) at org.apache.carbondata.processing.store.CarbonFactDataHandlerColumnar$Producer.call(CarbonFactDataHandlerColumnar.java:724) at org.apache.carbondata.processing.store.CarbonFactDataHandlerColumnar$Producer.call(CarbonFactDataHandlerColumnar.java:701)2. Negative Array size exception is thrown when max characters per column is greater than Short max value during data load.
issueID:CARBONDATA-1534
type:New Feature
changed files:
texts:Update CarbonData 1.2.0 Release details in the website

issueID:CARBONDATA-1536
type:Bug
changed files:
texts:Default value of carbon.bad.records.action is FORCE
Default value of carbon.bad.records.action is FORCE in source code, but the value is fail in documentation
issueID:CARBONDATA-1537
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/metadata/blocklet/BlockletInfo.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockletDataMap.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/adaptive/AdaptiveDeltaFloatingCodec.java
core/src/main/java/org/apache/carbondata/core/metadata/datatype/DataType.java
core/src/main/java/org/apache/carbondata/core/util/DataFileFooterConverter.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/ColumnPageEncoderMeta.java
core/src/main/java/org/apache/carbondata/core/datastore/block/TableBlockInfo.java
core/src/main/java/org/apache/carbondata/core/metadata/ValueEncoderMeta.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockletDataRefNode.java
core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/EncodingFactory.java
core/src/main/java/org/apache/carbondata/core/datastore/page/statistics/PrimitivePageStatsCollector.java
core/src/main/java/org/apache/carbondata/core/metadata/blocklet/datachunk/DataChunk.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/DefaultEncodingFactory.java
core/src/main/java/org/apache/carbondata/core/util/AbstractDataFileFooterConverter.java
texts:Can&#39;t use data which is loaded in V1 format (0.2 version) in V3 format (current master )

issueID:CARBONDATA-1538
type:Sub-task
changed files:core/src/main/java/org/apache/carbondata/core/datastore/chunk/impl/AbstractDimensionColumnPage.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RestructureExcludeFilterExecutorImpl.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RangeValueFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/AndFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/scan/scanner/impl/BlockletFilterScanner.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/DimensionColumnPage.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/TrueFilterExecutor.java
core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelRangeLessThanEqualFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/ExcludeFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelRangeGrtrThanEquaToFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/store/impl/safe/SafeAbsractDimensionDataChunkStore.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/FilterExecuter.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/store/ColumnPageWrapper.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/OrFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RestructureIncludeFilterExecutorImpl.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/store/impl/unsafe/UnsafeAbstractDimensionDataChunkStore.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/IncludeFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/store/DimensionDataChunkStore.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelRangeGrtThanFiterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelRangeLessThanFilterExecuterImpl.java
texts:Implement bitset pipe-lining in carbondata filtering. It will improve performance and needed for FG Datamap
Implement bitset pipe-lining in carbondata filtering. It will improve performance and needed for FG Datamap
issueID:CARBONDATA-1539
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/metadata/datatype/BooleanType.java
core/src/main/java/org/apache/carbondata/core/datastore/page/VarLengthColumnPageBase.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/merger/UnsafeIntermediateFileMerger.java
core/src/main/java/org/apache/carbondata/core/scan/expression/conditional/EqualToExpression.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/ColumnPageEncoderMeta.java
core/src/main/java/org/apache/carbondata/core/scan/expression/conditional/LessThanExpression.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RestructureEvaluatorImpl.java
processing/src/main/java/org/apache/carbondata/processing/datatypes/StructDataType.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/compress/DirectCompressCodec.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
core/src/main/java/org/apache/carbondata/core/datastore/page/UnsafeDecimalColumnPage.java
core/src/main/java/org/apache/carbondata/core/scan/expression/logical/RangeExpression.java
core/src/main/java/org/apache/carbondata/core/datastore/block/SegmentProperties.java
core/src/main/java/org/apache/carbondata/core/datastore/page/SafeFixLengthColumnPage.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/rle/RLECodec.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/holder/UnsafeSortTempFileChunkHolder.java
integration/spark-datasource/src/main/scala/org/apache/carbondata/spark/vectorreader/VectorizedCarbonRecordReader.java
core/src/main/java/org/apache/carbondata/core/metadata/datatype/DecimalType.java
processing/src/main/java/org/apache/carbondata/processing/merger/CompactionResultSortProcessor.java
core/src/main/java/org/apache/carbondata/core/util/AbstractDataFileFooterConverter.java
core/src/main/java/org/apache/carbondata/core/metadata/datatype/DoubleType.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelRangeLessThanFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RangeValueFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/scan/expression/conditional/NotInExpression.java
core/src/main/java/org/apache/carbondata/core/scan/expression/logical/FalseExpression.java
core/src/main/java/org/apache/carbondata/core/indexstore/UnsafeMemoryDMStore.java
core/src/main/java/org/apache/carbondata/core/cache/dictionary/ColumnDictionaryInfo.java
core/src/main/java/org/apache/carbondata/core/metadata/datatype/LongType.java
core/src/main/java/org/apache/carbondata/core/scan/executor/impl/AbstractQueryExecutor.java
processing/src/main/java/org/apache/carbondata/processing/loading/partition/impl/HashPartitionerImpl.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/column/ColumnSchema.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/ExcludeFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/metadata/datatype/ByteType.java
processing/src/main/java/org/apache/carbondata/processing/util/CarbonDataProcessorUtil.java
core/src/main/java/org/apache/carbondata/core/keygenerator/directdictionary/timestamp/TimeStampDirectDictionaryGenerator.java
core/src/main/java/org/apache/carbondata/core/util/CarbonMetadataUtil.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/EncodingFactory.java
core/src/main/java/org/apache/carbondata/core/metadata/datatype/TimestampType.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/IncludeFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/keygenerator/directdictionary/timestamp/DateDirectDictionaryGenerator.java
core/src/main/java/org/apache/carbondata/core/datamap/dev/DataMapFactory.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/DefaultEncodingFactory.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/adaptive/AdaptiveIntegralCodec.java
core/src/main/java/org/apache/carbondata/core/metadata/datatype/ShortIntType.java
core/src/main/java/org/apache/carbondata/core/scan/result/vector/MeasureDataVectorProcessor.java
core/src/main/java/org/apache/carbondata/core/metadata/datatype/DateType.java
core/src/main/java/org/apache/carbondata/core/scan/complextypes/PrimitiveQueryType.java
processing/src/main/java/org/apache/carbondata/processing/loading/converter/impl/FieldEncoderFactory.java
core/src/main/java/org/apache/carbondata/core/datastore/page/LazyColumnPage.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/SortDataRows.java
core/src/main/java/org/apache/carbondata/core/scan/expression/RangeExpressionEvaluator.java
core/src/main/java/org/apache/carbondata/core/util/DataTypeUtil.java
core/src/main/java/org/apache/carbondata/core/metadata/datatype/DataType.java
core/src/main/java/org/apache/carbondata/core/scan/executor/util/RestructureUtil.java
core/src/main/java/org/apache/carbondata/core/scan/expression/ExpressionResult.java
core/src/main/java/org/apache/carbondata/core/scan/expression/conditional/InExpression.java
processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactDataHandlerColumnar.java
integration/hive/src/main/java/org/apache/carbondata/hive/CarbonDictionaryDecodeReadSupport.java
core/src/main/java/org/apache/carbondata/core/scan/executor/util/QueryUtil.java
core/src/main/java/org/apache/carbondata/core/datastore/page/statistics/LVStringStatsCollector.java
core/src/main/java/org/apache/carbondata/core/datastore/page/SafeDecimalColumnPage.java
core/src/main/java/org/apache/carbondata/core/datastore/page/UnsafeFixLengthColumnPage.java
core/src/main/java/org/apache/carbondata/core/scan/expression/logical/OrExpression.java
core/src/main/java/org/apache/carbondata/core/metadata/datatype/StringType.java
processing/src/main/java/org/apache/carbondata/processing/loading/parser/CarbonParserFactory.java
core/src/main/java/org/apache/carbondata/core/scan/expression/conditional/GreaterThanExpression.java
core/src/main/java/org/apache/carbondata/core/indexstore/row/DataMapRowImpl.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/adaptive/AdaptiveFloatingCodec.java
core/src/main/java/org/apache/carbondata/core/datastore/page/ColumnPage.java
core/src/main/java/org/apache/carbondata/core/datastore/TableSpec.java
core/src/main/java/org/apache/carbondata/core/memory/HeapMemoryAllocator.java
core/src/main/java/org/apache/carbondata/core/scan/filter/FilterExpressionProcessor.java
processing/src/main/java/org/apache/carbondata/processing/store/TablePage.java
core/src/main/java/org/apache/carbondata/core/metadata/datatype/DataTypes.java
core/src/main/java/org/apache/carbondata/core/util/comparator/Comparator.java
core/src/main/java/org/apache/carbondata/core/scan/complextypes/ArrayQueryType.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/SortTempFileChunkHolder.java
core/src/main/java/org/apache/carbondata/core/scan/filter/FilterUtil.java
core/src/main/java/org/apache/carbondata/core/scan/expression/conditional/LessThanEqualToExpression.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/ConditionalFilterResolverImpl.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/RowLevelRangeFilterResolverImpl.java
processing/src/main/java/org/apache/carbondata/processing/loading/converter/impl/NonDictionaryFieldConverterImpl.java
core/src/main/java/org/apache/carbondata/core/metadata/datatype/NullType.java
integration/presto/src/main/java/org/apache/carbondata/presto/PrestoFilterUtil.java
core/src/main/java/org/apache/carbondata/core/metadata/datatype/ByteArrayType.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/column/CarbonColumn.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockletDataMap.java
core/src/main/java/org/apache/carbondata/core/metadata/datatype/ShortType.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/CarbonTable.java
core/src/main/java/org/apache/carbondata/core/scan/collector/impl/DictionaryBasedResultCollector.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/impl/FixedLengthDimensionColumnPage.java
core/src/main/java/org/apache/carbondata/core/keygenerator/directdictionary/DirectDictionaryKeyGeneratorFactory.java
core/src/main/java/org/apache/carbondata/core/scan/collector/impl/AbstractScannedResultCollector.java
core/src/main/java/org/apache/carbondata/core/metadata/datatype/FloatType.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/UnsafeCarbonRowPage.java
core/src/main/java/org/apache/carbondata/core/metadata/datatype/StructType.java
core/src/main/java/org/apache/carbondata/core/scan/collector/impl/RestructureBasedRawResultCollector.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/ColumnPageEncoder.java
core/src/main/java/org/apache/carbondata/core/scan/expression/logical/TrueExpression.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/adaptive/AdaptiveDeltaIntegralCodec.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelRangeLessThanEqualFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/datastore/impl/FileFactory.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/column/CarbonImplicitDimension.java
core/src/main/java/org/apache/carbondata/core/scan/collector/impl/RestructureBasedVectorResultCollector.java
core/src/main/java/org/apache/carbondata/core/metadata/datatype/MapType.java
core/src/main/java/org/apache/carbondata/core/scan/expression/logical/AndExpression.java
core/src/main/java/org/apache/carbondata/core/scan/expression/conditional/GreaterThanEqualToExpression.java
core/src/main/java/org/apache/carbondata/core/metadata/datatype/IntType.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/resolverinfo/visitor/CustomTypeDictionaryVisitor.java
integration/presto/src/main/java/org/apache/carbondata/presto/PrestoCarbonVectorizedRecordReader.java
core/src/main/java/org/apache/carbondata/core/datastore/page/statistics/PrimitivePageStatsCollector.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/IntermediateFileMerger.java
core/src/main/java/org/apache/carbondata/core/metadata/datatype/ArrayType.java
core/src/main/java/org/apache/carbondata/core/scan/expression/conditional/NotEqualsExpression.java
core/src/main/java/org/apache/carbondata/core/metadata/converter/ThriftWrapperSchemaConverterImpl.java
texts:Change DataType from enum to class
DataType should be java class instead of enum, it enables data type object to hold more information for decimal and complex type. This is required so that:1. ColumnPage does not need to store extra information for decimal and complex type. It can process for all datatype in unified way.2. It is needed to decouple carbon core and spark, so core does not depend on spark.
issueID:CARBONDATA-154
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/FilterResolverIntf.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/LogicalFilterResolverImpl.java
core/src/main/java/org/apache/carbondata/core/scan/filter/FilterUtil.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/ConditionalFilterResolverImpl.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/RowLevelRangeFilterResolverImpl.java
texts:Block prune can not get the right blocks and query result is wrong

issueID:CARBONDATA-1547
type:Improvement
changed files:
texts:Table should be dropped after running test
Table should be dropped after running test
issueID:CARBONDATA-1550
type:Bug
changed files:
texts:There is an error when table has Array<STRING> column and ENABLE_AUTO_LOAD_MERGE is true
There is an error when table has Array<STRING> column and ENABLE_AUTO_LOAD_MERGE is true    //unfinish    sql("drop table if exists array_table")    CarbonProperties.getInstance().addProperty(CarbonCommonConstants.ENABLE_AUTO_LOAD_MERGE, "true")    sql(      s"""         | CREATE TABLE array_table(         | complexData ARRAY<STRING>         | )         | STORED BY 'carbondata'         | TBLPROPERTIES('sort_columns'='')       """.stripMargin)    val storeLocation = s"$rootPath/integration/spark-common-test/src/test/resources/bool/ArrayString.csv"    for (i <- 0 until 4) {      sql(        s"""           | LOAD DATA LOCAL INPATH '${storeLocation}'           | INTO TABLE array_table           | options('FILEHEADER'='complexData')           """.stripMargin)    }    checkAnswer(      sql("select count(*) from array_table"),      Seq(Row(40))    )    val segments = sql("SHOW SEGMENTS FOR TABLE array_table")    val SegmentSequenceIds = segments.collect().map { each => (each.toSeq) (0) }    assert(!SegmentSequenceIds.contains("0.1"))    assert(SegmentSequenceIds.length == 4)    CarbonProperties.getInstance().addProperty(CarbonCommonConstants.ENABLE_AUTO_LOAD_MERGE,      CarbonCommonConstants.DEFAULT_ENABLE_AUTO_LOAD_MERGE)    sql("drop table if exists array_table")
issueID:CARBONDATA-1552
type:New Feature
changed files:hadoop/src/main/java/org/apache/carbondata/hadoop/util/CarbonInputFormatUtil.java
texts:Add support of Spark-2.2 in carbon.
Carbon should support spark 2.2.
issueID:CARBONDATA-1568
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
core/src/main/java/org/apache/carbondata/core/util/CarbonProperties.java
core/src/main/java/org/apache/carbondata/core/util/path/CarbonTablePath.java
processing/src/main/java/org/apache/carbondata/processing/merger/CarbonDataMergerUtil.java
texts:Optimize annotation of code
There are some improper places in code annotation  by IDEA inspect codeWe need to optimize annotation of code
issueID:CARBONDATA-157
type:Bug
changed files:
texts:for decimal(n,n) column, when filter has int value, then will trow exception
when do  as followingcreate table Test (c1_int int,c2_Bigint Bigint,c3_Decimal Decimal(38,38)) STORED BY 'org.apache.carbondata.format'LOAD DATA INPATH 'hdfs:/Data/Test_Data.csv' INTO table TestOPTIONS('DELIMITER'=',','QUOTECHAR'='','FILEHEADER'='')select c3_decimal from Test where c3_decimal =-0.12345678900987654321123456789009876538 or c3_decimal =2345 then the query will throw exceptionError: java.lang.RuntimeException: Exception occurred in query execution :: org.carbondata.query.expression.LiteralExpression cannot be cast to org.carbondata.query.expression.conditional.ConditionalExpression (state=,code=0).
issueID:CARBONDATA-1572
type:New Feature
changed files:core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RestructureExcludeFilterExecutorImpl.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/AndFilterExecuterImpl.java
streaming/src/main/java/org/apache/carbondata/streaming/CarbonStreamRecordReader.java
core/src/main/java/org/apache/carbondata/core/statusmanager/FileFormat.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelFilterExecuterImpl.java
streaming/src/main/java/org/apache/carbondata/streaming/parser/CSVStreamParserImp.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/TrueFilterExecutor.java
core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
streaming/src/main/java/org/apache/carbondata/streaming/CarbonStreamException.java
core/src/main/java/org/apache/carbondata/core/util/path/CarbonTablePath.java
processing/src/main/java/org/apache/carbondata/processing/loading/csvinput/CSVInputFormat.java
streaming/src/main/java/org/apache/carbondata/streaming/CarbonStreamOutputFormat.java
core/src/main/java/org/apache/carbondata/core/statusmanager/SegmentStatusManager.java
core/src/main/java/org/apache/carbondata/core/scan/model/QueryModel.java
core/src/main/java/org/apache/carbondata/core/datastore/compression/Compressor.java
streaming/src/main/java/org/apache/carbondata/streaming/CarbonStreamRecordWriter.java
streaming/src/main/java/org/apache/carbondata/streaming/segment/StreamSegment.java
core/src/main/java/org/apache/carbondata/core/scan/filter/FilterUtil.java
core/src/main/java/org/apache/carbondata/core/metadata/datatype/DataTypes.java
core/src/main/java/org/apache/carbondata/core/statusmanager/LoadMetadataDetails.java
core/src/main/java/org/apache/carbondata/core/locks/LockUsage.java
integration/hive/src/main/java/org/apache/carbondata/hive/MapredCarbonInputFormat.java
hadoop/src/main/java/org/apache/carbondata/hadoop/CarbonMultiBlockSplit.java
streaming/src/main/java/org/apache/carbondata/streaming/StreamBlockletWriter.java
streaming/src/main/java/org/apache/carbondata/streaming/StreamBlockletReader.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/CarbonTable.java
processing/src/main/java/org/apache/carbondata/processing/store/writer/AbstractFactDataWriter.java
processing/src/main/java/org/apache/carbondata/processing/util/CarbonLoaderUtil.java
streaming/src/main/java/org/apache/carbondata/streaming/CarbonStreamInputFormat.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/ExcludeFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/datastore/impl/FileFactory.java
processing/src/main/java/org/apache/carbondata/processing/merger/CarbonDataMergerUtil.java
processing/src/main/java/org/apache/carbondata/processing/util/CarbonDataProcessorUtil.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonTableInputFormat.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
integration/presto/src/main/java/org/apache/carbondata/presto/PrestoCarbonVectorizedRecordReader.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/FilterExecuter.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/OrFilterExecuterImpl.java
hadoop/src/main/java/org/apache/carbondata/hadoop/util/CarbonInputFormatUtil.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RestructureIncludeFilterExecutorImpl.java
core/src/main/java/org/apache/carbondata/core/datastore/compression/SnappyCompressor.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/IncludeFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/hadoop/CarbonInputSplit.java
streaming/src/main/java/org/apache/carbondata/streaming/parser/CarbonStreamParser.java
texts:Support Streaming Ingest
CarbonData should support streaming ingest.CarbonData Streaming Ingest_v1.6.pdf
issueID:CARBONDATA-1573
type:New Feature
changed files:processing/src/main/java/org/apache/carbondata/processing/loading/converter/impl/FieldEncoderFactory.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/TableInfo.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockletDataMapFactory.java
streaming/src/main/java/org/apache/carbondata/streaming/CarbonStreamRecordReader.java
core/src/main/java/org/apache/carbondata/core/indexstore/BlockletDataMapIndexStore.java
core/src/main/java/org/apache/carbondata/core/scan/result/iterator/AbstractDetailQueryResultIterator.java
integration/hive/src/main/java/org/apache/carbondata/hive/CarbonDictionaryDecodeReadSupport.java
core/src/main/java/org/apache/carbondata/core/scan/executor/util/QueryUtil.java
integration/presto/src/main/java/org/apache/carbondata/presto/impl/CarbonTableReader.java
core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
core/src/main/java/org/apache/carbondata/core/util/path/CarbonTablePath.java
core/src/main/java/org/apache/carbondata/core/memory/HeapMemoryAllocator.java
core/src/main/java/org/apache/carbondata/core/statusmanager/SegmentStatusManager.java
core/src/main/java/org/apache/carbondata/core/locks/CarbonLockFactory.java
processing/src/main/java/org/apache/carbondata/processing/loading/DataLoadProcessBuilder.java
core/src/main/java/org/apache/carbondata/core/scan/filter/FilterUtil.java
core/src/main/java/org/apache/carbondata/core/locks/CarbonLockUtil.java
processing/src/main/java/org/apache/carbondata/processing/merger/AbstractResultProcessor.java
processing/src/main/java/org/apache/carbondata/processing/loading/model/CarbonLoadModel.java
integration/hive/src/main/java/org/apache/carbondata/hive/MapredCarbonInputFormat.java
core/src/main/java/org/apache/carbondata/core/locks/HdfsFileLock.java
processing/src/main/java/org/apache/carbondata/processing/datatypes/PrimitiveDataType.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/CarbonTable.java
hadoop/src/main/java/org/apache/carbondata/hadoop/readsupport/impl/DictionaryDecodeReadSupport.java
core/src/main/java/org/apache/carbondata/core/mutate/CarbonUpdateUtil.java
core/src/main/java/org/apache/carbondata/core/cache/CacheProvider.java
core/src/main/java/org/apache/carbondata/core/scan/executor/impl/AbstractQueryExecutor.java
core/src/main/java/org/apache/carbondata/core/locks/LocalFileLock.java
processing/src/main/java/org/apache/carbondata/processing/util/CarbonLoaderUtil.java
processing/src/main/java/org/apache/carbondata/processing/loading/converter/impl/RowConverterImpl.java
streaming/src/main/java/org/apache/carbondata/streaming/CarbonStreamInputFormat.java
core/src/main/java/org/apache/carbondata/core/locks/ZooKeeperLocking.java
core/src/main/java/org/apache/carbondata/core/datamap/DistributableDataMapFormat.java
core/src/main/java/org/apache/carbondata/core/util/DeleteLoadFolders.java
core/src/main/java/org/apache/carbondata/core/datastore/filesystem/LocalCarbonFile.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/SchemaReader.java
processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactDataHandlerModel.java
processing/src/main/java/org/apache/carbondata/processing/merger/CarbonDataMergerUtil.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonTableInputFormat.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
hadoop/src/main/java/org/apache/carbondata/hadoop/util/CarbonInputFormatUtil.java
core/src/main/java/org/apache/carbondata/core/cache/dictionary/DictionaryColumnUniqueIdentifier.java
core/src/main/java/org/apache/carbondata/core/metadata/converter/ThriftWrapperSchemaConverterImpl.java
core/src/main/java/org/apache/carbondata/core/metadata/AbsoluteTableIdentifier.java
texts:Support Database Location Configuration while Creating Database/ Support Creation of carbon Table in the database location
Support Creation of carbon table at the database locationPlease refer to  for Design and discussion:http://apache-carbondata-dev-mailing-list-archive.1130556.n5.nabble.com/DISCUSSION-Support-Database-Location-Configuration-while-Creating-Database-td23492.html
issueID:CARBONDATA-1574
type:Bug
changed files:
texts:No_Inverted is applied for all newly added column irrespect of specified in tableproperties
If No_Inverted_Index is specified in tableproperties during addition of several column. All columns are being considered as  No_Inverted_Index, even if few of them is marked as No_Inverted_Index in tableproperties .*Steps to Reproduce : *sql(      """           CREATE TABLE IF NOT EXISTS NO_INVERTED_CARBON           (id Int, name String, city String)           STORED BY 'org.apache.carbondata.format'           TBLPROPERTIES('NO_INVERTED_INDEX'='city')      """)    sql("alter table NO_INVERTED_CARBON add columns(col1 string,col2 string) tblproperties('NO_INVERTED_INDEX'='col2')")sql("desc formatted NO_INVERTED_CARBON").show()Expected : 2 columns should be marked as delete(city and col2)Actual :  3 columns is being marked as delete(city,col1 and col2)
issueID:CARBONDATA-1575
type:Sub-task
changed files:
texts:Support large scale data on DataMap
This ticket is to track the data scale test on datamapFixes need to be handle if any problems are found in large scale test.
issueID:CARBONDATA-1576
type:Sub-task
changed files:core/src/main/java/org/apache/carbondata/core/metadata/schema/table/TableInfo.java
core/src/main/java/org/apache/carbondata/core/datamap/DataMapStoreManager.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/DataMapSchema.java
core/src/main/java/org/apache/carbondata/core/metadata/converter/ThriftWrapperSchemaConverterImpl.java
integration/presto/src/main/java/org/apache/carbondata/presto/impl/CarbonTableReader.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/TableSchema.java
texts:Support create and drop datamap by SQL

issueID:CARBONDATA-1579
type:Sub-task
changed files:
texts:Support show/describe datamap information by SQL

issueID:CARBONDATA-158
type:Bug
changed files:
texts:Load data failed when first line is null in data
step1:create table t5 (name String,age int) STORED BY 'org.apache.carbondata.format';step2:data:+++++++++            ++ gin,24 ++jay,20   +++++++++LOAD DATA INPATH 'D:/hadoop/srcdata/max_bigint.csv' INTO table t5 OPTIONS('DELIMITER'=',','QUOTECHAR'='', 'FILEHEADER'='name,age');then load data failed, java.lang.NullPointerException
issueID:CARBONDATA-1580
type:Sub-task
changed files:
texts:1. create/alter table with stream properties
CREATE TABLE sensor_readingSTORED BY 'carbondata'TBLPROPERTIES (‘streaming’='true’, ‘streaming.maxFileSize’=’128m’) Table property  name Description Default valuestreaming Whether to enable streaming ingest feature for this tablefalsestreaming.maxFileSizeThe maximum file size for streaming data 128MBstreaming.handoff.numFiles The number of files before handoff is triggered 16
issueID:CARBONDATA-1581
type:Sub-task
changed files:
texts:2. row format writer and support to append batch data
Carbon will write the streaming data into a row format. The row format is appendable, so forevery time ingestion is triggered, the data will be appended to the row format file. Thus,currently, carbon will only support APPEND mode in structured streaming.
issueID:CARBONDATA-1582
type:Sub-task
changed files:
texts:3. support StreamSinkProvider and append batch data to row format file
When data batch comes to carbon, carbon will checkwhether the batch id of the data is already committed. If yes, skip this batch,otherwise convert data fields and append to row format file directly. So, therecords are either flushed or not flushed, data consistency is ensured.When writing to the row format file, carbon will convert the data fields the same way as forcolumnar format, like global dictionary conversion, timestamp conversion, complex typeconversion, etc.
issueID:CARBONDATA-1583
type:Sub-task
changed files:
texts:4. row format reader and support to split row format file to small blocks
Once the data is written to streaming table, it can be query along with any historydata by submitting spark SQL query
issueID:CARBONDATA-1584
type:Sub-task
changed files:
texts:5. query with streaming row format file.
Once the data is written to streaming table, it can be query along with any historydata by submitting spark SQL query
issueID:CARBONDATA-1585
type:Sub-task
changed files:
texts:Support show/describe streaming table information by SQL

issueID:CARBONDATA-1586
type:Sub-task
changed files:core/src/main/java/org/apache/carbondata/core/locks/LockUsage.java
processing/src/main/java/org/apache/carbondata/processing/merger/CarbonCompactionUtil.java
processing/src/main/java/org/apache/carbondata/processing/merger/CarbonDataMergerUtil.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonTableInputFormat.java
streaming/src/main/java/org/apache/carbondata/streaming/CarbonStreamOutputFormat.java
streaming/src/main/java/org/apache/carbondata/streaming/CarbonStreamRecordReader.java
processing/src/main/java/org/apache/carbondata/processing/merger/CompactionType.java
core/src/main/java/org/apache/carbondata/core/statusmanager/SegmentStatusManager.java
processing/src/main/java/org/apache/carbondata/processing/merger/CompactionResultSortProcessor.java
processing/src/main/java/org/apache/carbondata/processing/merger/AbstractResultProcessor.java
texts:Support handoff from row format to columnar format
When number of files in the streaming segment exceeds configured value, system should support converting row files into columnar files to avoid too many row files and improve query performance on streaming table continuously.
issueID:CARBONDATA-1587
type:Sub-task
changed files:
texts:Integrate with Kafka and Spark structured streaming to ingest data
Should support ingest data from kafka and spark structured streaming into carbon streaming table.The solution should provide E2E exactly once semantic and fault tolerance.
issueID:CARBONDATA-159
type:Improvement
changed files:
texts:carbon should support primary key & keep mapping table table_property
As we know , carbon support MDK index , according the design ,if we have filter or filter combination on the left side columns , we can get a good performance . but if the leading key is a high cardinality column (>100million cardinality etc), only the filter on leading key can gain good performance， the filter on following columns and other high cardinality columns can not , because the they are close to un-sort .i suggest we add one key mapping function , the table property will look like : create table (low cardinality column to high cardinality column)table_property(primary_key h_col3,index_key_mapping(h_col1,h_col2))                low cardinality-> high cardinalitycol1,col2,col3,col4.....col10,h_col1,h_col2,h_col3during data loading , carbon will create a internal index table A,it will records all the (values --> position) of primary_key,look like:        h_col3                           list of block let  18682114091        &#91;blockid1+blokletid1&#93;,&#91;blockid4+blokletid10&#93;....  18683343442        &#91;blockid2+blokletid4&#93;,&#91;blockid23+blokletid5&#93;....  ...                           .....and will create another two key mapping table:table 1:---------------------------------------h_col2            hcol3 jarray        18682114091 ramana     18683343442  ......              .......table2:-----------------------------------------h_col1            hcol3 77647        18682114091 99899       18683343442  ......              .......1)if the filter on col1-col10, will use original MDK capacity ;2)if the filter on h_col1, system will scan index table to get the block let position , then use it to fetch the data directly; 3)if the filter on h_col2 or h_col3 , system first scan the key mapping table to get the primary key list , then 2)
issueID:CARBONDATA-1591
type:Sub-task
changed files:
texts:Support query from specified Segment (ThreadSafe)

issueID:CARBONDATA-1592
type:New Feature
changed files:core/src/main/java/org/apache/carbondata/events/OperationEventListener.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
core/src/main/java/org/apache/carbondata/events/OperationListenerBus.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockletDataMapFactory.java
core/src/main/java/org/apache/carbondata/events/OperationContext.java
core/src/main/java/org/apache/carbondata/events/Event.java
core/src/main/java/org/apache/carbondata/core/datamap/dev/DataMapFactory.java
core/src/main/java/org/apache/carbondata/core/datamap/TableDataMap.java
core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
texts:Add Event Listener interface to Carbondata
Add Event Listener interface to Carbondata. This will allow extending the current functionality of various commands to perform various other operations.Example: After completion of load process, if any aggregate tables are created on that table, then data load operation need to be done for the aggregate table also. In this case we can create a listener such as AggregateLoadListener and register it as an event bus. Then this listener can be called once the load operation is completed which will take care of loading the aggregate table.
issueID:CARBONDATA-1594
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/datastore/page/LazyColumnPage.java
core/src/main/java/org/apache/carbondata/core/datastore/page/statistics/SimpleStatsResult.java
core/src/main/java/org/apache/carbondata/core/scan/expression/conditional/NotEqualsExpression.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/SortDataRows.java
core/src/main/java/org/apache/carbondata/core/datastore/page/VarLengthColumnPageBase.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/merger/UnsafeIntermediateFileMerger.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/IntermediateFileMerger.java
core/src/main/java/org/apache/carbondata/core/util/DataTypeUtil.java
core/src/main/java/org/apache/carbondata/core/metadata/datatype/DataType.java
core/src/main/java/org/apache/carbondata/core/scan/expression/conditional/EqualToExpression.java
core/src/main/java/org/apache/carbondata/core/scan/executor/util/RestructureUtil.java
core/src/main/java/org/apache/carbondata/core/scan/expression/ExpressionResult.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/ColumnPageEncoderMeta.java
core/src/main/java/org/apache/carbondata/core/scan/expression/conditional/InExpression.java
core/src/main/java/org/apache/carbondata/core/scan/expression/conditional/LessThanExpression.java
integration/hive/src/main/java/org/apache/carbondata/hive/CarbonDictionaryDecodeReadSupport.java
processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactDataHandlerColumnar.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/compress/DirectCompressCodec.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
core/src/main/java/org/apache/carbondata/core/datastore/page/statistics/LVStringStatsCollector.java
core/src/main/java/org/apache/carbondata/core/datastore/page/UnsafeFixLengthColumnPage.java
core/src/main/java/org/apache/carbondata/core/scan/expression/conditional/GreaterThanExpression.java
core/src/main/java/org/apache/carbondata/core/datastore/page/UnsafeDecimalColumnPage.java
core/src/main/java/org/apache/carbondata/core/datastore/page/ColumnPage.java
core/src/main/java/org/apache/carbondata/core/datastore/block/SegmentProperties.java
core/src/main/java/org/apache/carbondata/core/datastore/TableSpec.java
core/src/main/java/org/apache/carbondata/core/metadata/datatype/DecimalConverterFactory.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/holder/UnsafeSortTempFileChunkHolder.java
core/src/main/java/org/apache/carbondata/core/util/comparator/Comparator.java
core/src/main/java/org/apache/carbondata/core/metadata/datatype/DecimalType.java
core/src/main/java/org/apache/carbondata/core/metadata/datatype/DataTypes.java
integration/spark-datasource/src/main/scala/org/apache/carbondata/spark/vectorreader/VectorizedCarbonRecordReader.java
processing/src/main/java/org/apache/carbondata/processing/merger/CompactionResultSortProcessor.java
core/src/main/java/org/apache/carbondata/core/scan/filter/FilterUtil.java
core/src/main/java/org/apache/carbondata/core/util/AbstractDataFileFooterConverter.java
core/src/main/java/org/apache/carbondata/core/scan/expression/conditional/LessThanEqualToExpression.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/SortTempFileChunkHolder.java
processing/src/main/java/org/apache/carbondata/processing/store/TablePage.java
integration/presto/src/main/java/org/apache/carbondata/presto/PrestoFilterUtil.java
core/src/main/java/org/apache/carbondata/core/scan/expression/conditional/NotInExpression.java
core/src/main/java/org/apache/carbondata/core/datastore/page/statistics/KeyPageStatsCollector.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockletDataMap.java
core/src/main/java/org/apache/carbondata/core/cache/dictionary/ColumnDictionaryInfo.java
core/src/main/java/org/apache/carbondata/core/scan/collector/impl/AbstractScannedResultCollector.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/UnsafeCarbonRowPage.java
processing/src/main/java/org/apache/carbondata/processing/loading/partition/impl/HashPartitionerImpl.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/column/ColumnSchema.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/ColumnPageEncoder.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/adaptive/AdaptiveDeltaIntegralCodec.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/ExcludeFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/scan/collector/impl/RestructureBasedVectorResultCollector.java
core/src/main/java/org/apache/carbondata/core/scan/expression/conditional/GreaterThanEqualToExpression.java
core/src/main/java/org/apache/carbondata/core/util/CarbonMetadataUtil.java
integration/presto/src/main/java/org/apache/carbondata/presto/PrestoCarbonVectorizedRecordReader.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/EncodingFactory.java
core/src/main/java/org/apache/carbondata/core/datastore/page/statistics/PrimitivePageStatsCollector.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/IncludeFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/adaptive/AdaptiveIntegralCodec.java
core/src/main/java/org/apache/carbondata/core/scan/result/vector/MeasureDataVectorProcessor.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/DefaultEncodingFactory.java
core/src/main/java/org/apache/carbondata/core/metadata/converter/ThriftWrapperSchemaConverterImpl.java
texts:Add scale and decimal to DecimalType
DecimalType should include scale and precision, and all scale and precision class member outside DecimalType should be removed
issueID:CARBONDATA-1596
type:Bug
changed files:processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/IntermediateFileMerger.java
texts:ClassCastException is thrown by IntermediateFileMerger for decimal columns
When intermediate file merger tries to merge the sort files it converts the row data to their appropriate datatypes.While converting decimal types ClassCastException was being thrown.
issueID:CARBONDATA-1597
type:Improvement
changed files:
texts:Remove spark1 integration
As voted by community at : http://apache-carbondata-dev-mailing-list-archive.1130556.n5.nabble.com/DISCUSSION-Support-only-spark-2-in-carbon-1-3-0-td23589.htmlspark1 integration with carbon can be removed
issueID:CARBONDATA-1598
type:Task
changed files:
texts:Remove all spark 1.x info(CI, readme, documents, etc.)
released JIRA: https://issues.apache.org/jira/browse/CARBONDATA-1597
issueID:CARBONDATA-1599
type:Improvement
changed files:
texts:Optimize pull request template for reminding contributors to provide full info.

issueID:CARBONDATA-16
type:Bug
changed files:integration/spark-common/src/main/java/org/apache/carbondata/spark/load/CarbonLoaderUtil.java
texts:BLOCK distribution in query is not correct in query when number of executors are less than the cluster size.

issueID:CARBONDATA-160
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/scan/filter/FilterUtil.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/IncludeFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/RowLevelRangeFilterResolverImpl.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/ExcludeFilterExecuterImpl.java
texts:Data mismatch issue issue in case of multiple loads with dictionary column with different key size
Data mismatch issue issue in case of multiple loads with dictionary column with different key size
issueID:CARBONDATA-1601
type:Sub-task
changed files:
texts:Add carbon store module
Add carbondata-store module and move corresponding code from integration module
issueID:CARBONDATA-1602
type:Improvement
changed files:
texts:Remove unused declaration in spark-common module

issueID:CARBONDATA-1604
type:New Feature
changed files:
texts:Support Column comment for carbon table

issueID:CARBONDATA-1605
type:New Feature
changed files:
texts:Support Column comment for carbon table
support column comment for table, so when table is described, we can show the comment for that specific column, if comment is not mentioned, comment default value is null
issueID:CARBONDATA-1606
type:New Feature
changed files:
texts:Support Column comment for carbon table
support column comment for table, so when table is described, we can show the comment for that specific column, if comment is not mentioned, comment default value is null
issueID:CARBONDATA-1607
type:New Feature
changed files:
texts:Support Column comment for carbon table
support column comment for table, so when table is described, we can show the comment for that specific column, if comment is not mentioned, comment default value is null
issueID:CARBONDATA-1608
type:New Feature
changed files:
texts:Support Column comment for carbon table
support column comment for table, so when table is described, we can show the comment for that specific column, if comment is not mentioned, comment default value is null
issueID:CARBONDATA-1610
type:Sub-task
changed files:
texts:ALTER TABLE set streaming property
For existing table, user should be able to use ALTER TABLE source SET TBLPROPERTIES('streaming'='true')  to set the table property so that this table can be streaming ingested
issueID:CARBONDATA-1611
type:Sub-task
changed files:core/src/main/java/org/apache/carbondata/core/metadata/schema/table/CarbonTable.java
texts:Block UPDATE/DELETE command for streaming table
In streaming table, row file format is used, which is not updatable. So UPDATE/DELETE command should be rejected
issueID:CARBONDATA-1612
type:Sub-task
changed files:
texts:Block DELETE SEGMENT BY ID for streaming table
Streaming segment should be managed by carbon internally and it should not be deleted by user
issueID:CARBONDATA-1614
type:Sub-task
changed files:hadoop/src/main/java/org/apache/carbondata/hadoop/CarbonMultiBlockSplit.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonTableInputFormat.java
core/src/main/java/org/apache/carbondata/hadoop/CarbonInputSplit.java
core/src/main/java/org/apache/carbondata/core/statusmanager/FileFormat.java
streaming/src/main/java/org/apache/carbondata/streaming/segment/StreamSegment.java
core/src/main/java/org/apache/carbondata/core/statusmanager/LoadMetadataDetails.java
texts:SHOW SEGMENT should include the streaming property

issueID:CARBONDATA-1615
type:Sub-task
changed files:
texts:DELETE SEGMENT BY DATE should ignore the streaming segment

issueID:CARBONDATA-1616
type:Sub-task
changed files:
texts:Add document for streaming ingestion usage

issueID:CARBONDATA-1617
type:New Feature
changed files:core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockletDataMapModel.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockletDataMapFactory.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockletDataMap.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/SegmentIndexFileStore.java
core/src/main/java/org/apache/carbondata/core/writer/CarbonIndexFileMergeWriter.java
core/src/main/java/org/apache/carbondata/core/indexstore/BlockletDataMapIndexStore.java
core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
core/src/main/java/org/apache/carbondata/core/datamap/dev/DataMapModel.java
core/src/main/java/org/apache/carbondata/core/util/path/CarbonTablePath.java
processing/src/main/java/org/apache/carbondata/processing/merger/CarbonDataMergerUtil.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonTableInputFormat.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
core/src/main/java/org/apache/carbondata/core/reader/CarbonIndexFileReader.java
processing/src/main/java/org/apache/carbondata/processing/merger/CompactionType.java
core/src/main/java/org/apache/carbondata/core/datamap/dev/DataMap.java
core/src/main/java/org/apache/carbondata/core/datamap/dev/DataMapFactory.java
core/src/main/java/org/apache/carbondata/core/datamap/TableDataMap.java
core/src/main/java/org/apache/carbondata/core/util/AbstractDataFileFooterConverter.java
core/src/main/java/org/apache/carbondata/core/reader/ThriftReader.java
texts:Merging carbonindex files for each segment.
Hi,Problem : The first-time query of carbon becomes very slow. It is because of reading many small carbonindex files and cache to the driver at the first time.  Many carbonindex files are created in below case Loading data in large cluster   For example, if the cluster size is 100 nodes then for each load 100 index files are created per segment. So after 100 loads, the number of carbonindex files becomes 10000. .It will be slower to read all the files from the driver since a lot of namenode calls and IO operations.Solution :Merge the carbonindex files in two levels.so that we can reduce the IO calls to namenode and improves the read performance.Merge within a segment.Merge the carbonindex files to single file immediately after load completes within the segment. It would be named as a .carbonindexmerge file. It is actually not a true data merging but a simple file merge. So that the current structure of carbonindex files does not change. While reading we just read one file instead of many carbonindex files within the segment.
issueID:CARBONDATA-1618
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/TableInfo.java
texts:Fix issue of not supporting table comment

issueID:CARBONDATA-1619
type:Bug
changed files:
texts:Loading data to a carbondata table with overwrite=true many times will cause NullPointerException
If you loading data to a carbondata table with overwrite=true many times will cause NullPointerException. The following is the code snippet:Welcome to      ____              __     / __/__  ___ _____/ /__    _\ \/ _ \/ _ `/ __/  '_/   /___/ .__/\_,_/_/ /_/\_\   version 2.1.0      /_/         Using Scala version 2.11.8 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_77)Type in expressions to have them evaluated.Type :help for more information.scala> import org.apache.spark.sql.SparkSessionimport org.apache.spark.sql.SparkSessionscala> import org.apache.spark.sql.CarbonSession._import org.apache.spark.sql.CarbonSession._scala> val carbon = SparkSession.builder().config(sc.getConf).getOrCreateCarbonSession("hdfs://mycluster/user/wyp/carb")17/10/26 12:58:25 WARN spark.SparkContext: Using an existing SparkContext; some configuration may not take effect.17/10/26 12:58:25 WARN util.CarbonProperties: main The custom block distribution value "null" is invalid. Using the default value "false17/10/26 12:58:25 WARN util.CarbonProperties: main The enable vector reader value "null" is invalid. Using the default value "true17/10/26 12:58:25 WARN util.CarbonProperties: main The value "LOCALLOCK" configured for key carbon.lock.type is invalid for current file system. Use the default value HDFSLOCK instead.17/10/26 12:58:43 WARN metastore.ObjectStore: Failed to get database global_temp, returning NoSuchObjectExceptioncarbon: org.apache.spark.sql.SparkSession = org.apache.spark.sql.CarbonSession@718b9d56scala> carbon.sql("CREATE TABLE temp.my_table(id bigint)  STORED BY 'carbondata'")17/10/26 12:59:03 AUDIT command.CreateTable: [l-sparkcluster1.test.com][wyp][Thread-1]Creating Table with Database name [temp] and Table name [my_table]17/10/26 12:59:03 WARN hive.HiveExternalCatalog: Couldn't find corresponding Hive SerDe for data source provider org.apache.spark.sql.CarbonSource. Persisting data source table `temp`.`my_table` into Hive metastore in Spark SQL specific format, which is NOT compatible with Hive.17/10/26 12:59:03 AUDIT command.CreateTable: [l-sparkcluster1.test.com][wyp][Thread-1]Table created with Database name [temp] and Table name [my_table]res0: org.apache.spark.sql.DataFrame = []scala> carbon.sql("insert overwrite table temp.my_table select id from  co.order_common_p where dt = '2010-10'")17/10/26 12:59:23 AUDIT rdd.CarbonDataRDDFactory$: [l-sparkcluster1.test.com][wyp][Thread-1]Data load request has been received for table temp.my_table17/10/26 12:59:23 WARN util.CarbonDataProcessorUtil: main sort scope is set to LOCAL_SORT17/10/26 12:59:26 AUDIT rdd.CarbonDataRDDFactory$: [l-sparkcluster1.test.com][wyp][Thread-1]Data load is successful for temp.my_tableres1: org.apache.spark.sql.DataFrame = []scala> carbon.sql("insert overwrite table temp.my_table select id from  co.order_common_p where dt = '2010-10'")17/10/26 12:59:33 AUDIT rdd.CarbonDataRDDFactory$: [l-sparkcluster1.test.com][wyp][Thread-1]Data load request has been received for table temp.my_table17/10/26 12:59:33 WARN util.CarbonDataProcessorUtil: main sort scope is set to LOCAL_SORT17/10/26 12:59:52 AUDIT rdd.CarbonDataRDDFactory$: [l-sparkcluster1.test.com][wyp][Thread-1]Data load is successful for temp.my_tableres2: org.apache.spark.sql.DataFrame = []scala> carbon.sql("insert overwrite table temp.my_table select id from  co.order_common_p where dt = '2012-10'")17/10/26 13:00:05 AUDIT rdd.CarbonDataRDDFactory$: [l-sparkcluster1.test.com][wyp][Thread-1]Data load request has been received for table temp.my_table17/10/26 13:00:05 WARN util.CarbonDataProcessorUtil: main sort scope is set to LOCAL_SORT17/10/26 13:00:08 ERROR filesystem.AbstractDFSCarbonFile: main Exception occurred:File does not exist: hdfs://mycluster/user/wyp/carb/temp/my_table/Fact/Part0/Segment_017/10/26 13:00:09 ERROR command.LoadTable: main java.lang.NullPointerException  at org.apache.carbondata.core.datastore.filesystem.AbstractDFSCarbonFile.isDirectory(AbstractDFSCarbonFile.java:88)  at org.apache.carbondata.core.util.CarbonUtil.deleteRecursive(CarbonUtil.java:364)  at org.apache.carbondata.core.util.CarbonUtil.access$100(CarbonUtil.java:93)  at org.apache.carbondata.core.util.CarbonUtil$2.run(CarbonUtil.java:326)  at org.apache.carbondata.core.util.CarbonUtil$2.run(CarbonUtil.java:322)  at java.security.AccessController.doPrivileged(Native Method)  at javax.security.auth.Subject.doAs(Subject.java:422)  at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1491)  at org.apache.carbondata.core.util.CarbonUtil.deleteFoldersAndFiles(CarbonUtil.java:322)  at org.apache.carbondata.spark.load.CarbonLoaderUtil.recordLoadMetadata(CarbonLoaderUtil.java:333)  at org.apache.carbondata.spark.rdd.CarbonDataRDDFactory$.updateStatus$1(CarbonDataRDDFactory.scala:595)  at org.apache.carbondata.spark.rdd.CarbonDataRDDFactory$.loadCarbonData(CarbonDataRDDFactory.scala:1107)  at org.apache.spark.sql.execution.command.LoadTable.processData(carbonTableSchema.scala:1046)  at org.apache.spark.sql.execution.command.LoadTable.run(carbonTableSchema.scala:754)  at org.apache.spark.sql.execution.command.LoadTableByInsert.processData(carbonTableSchema.scala:651)  at org.apache.spark.sql.execution.command.LoadTableByInsert.run(carbonTableSchema.scala:637)  at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)  at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)  at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114)  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114)  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:135)  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:132)  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:113)  at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:87)  at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:87)  at org.apache.spark.sql.Dataset.<init>(Dataset.scala:185)  at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:64)  at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:592)  at $line20.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:31)  at $line20.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:36)  at $line20.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:38)  at $line20.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:40)  at $line20.$read$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:42)  at $line20.$read$$iw$$iw$$iw$$iw$$iw.<init>(<console>:44)  at $line20.$read$$iw$$iw$$iw$$iw.<init>(<console>:46)  at $line20.$read$$iw$$iw$$iw.<init>(<console>:48)  at $line20.$read$$iw$$iw.<init>(<console>:50)  at $line20.$read$$iw.<init>(<console>:52)  at $line20.$read.<init>(<console>:54)  at $line20.$read$.<init>(<console>:58)  at $line20.$read$.<clinit>(<console>)  at $line20.$eval$.$print$lzycompute(<console>:7)  at $line20.$eval$.$print(<console>:6)  at $line20.$eval.$print(<console>)  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)  at java.lang.reflect.Method.invoke(Method.java:498)  at scala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:786)  at scala.tools.nsc.interpreter.IMain$Request.loadAndRun(IMain.scala:1047)  at scala.tools.nsc.interpreter.IMain$WrappedRequest$$anonfun$loadAndRunReq$1.apply(IMain.scala:638)  at scala.tools.nsc.interpreter.IMain$WrappedRequest$$anonfun$loadAndRunReq$1.apply(IMain.scala:637)  at scala.reflect.internal.util.ScalaClassLoader$class.asContext(ScalaClassLoader.scala:31)  at scala.reflect.internal.util.AbstractFileClassLoader.asContext(AbstractFileClassLoader.scala:19)  at scala.tools.nsc.interpreter.IMain$WrappedRequest.loadAndRunReq(IMain.scala:637)  at scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:569)  at scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:565)  at scala.tools.nsc.interpreter.ILoop.interpretStartingWith(ILoop.scala:807)  at scala.tools.nsc.interpreter.ILoop.command(ILoop.scala:681)  at scala.tools.nsc.interpreter.ILoop.processLine(ILoop.scala:395)  at scala.tools.nsc.interpreter.ILoop.loop(ILoop.scala:415)  at scala.tools.nsc.interpreter.ILoop$$anonfun$process$1.apply$mcZ$sp(ILoop.scala:923)  at scala.tools.nsc.interpreter.ILoop$$anonfun$process$1.apply(ILoop.scala:909)  at scala.tools.nsc.interpreter.ILoop$$anonfun$process$1.apply(ILoop.scala:909)  at scala.reflect.internal.util.ScalaClassLoader$.savingContextLoader(ScalaClassLoader.scala:97)  at scala.tools.nsc.interpreter.ILoop.process(ILoop.scala:909)  at org.apache.spark.repl.Main$.doMain(Main.scala:68)  at org.apache.spark.repl.Main$.main(Main.scala:51)  at org.apache.spark.repl.Main.main(Main.scala)  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)  at java.lang.reflect.Method.invoke(Method.java:498)  at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:738)  at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187)  at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212)  at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126)  at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)17/10/26 13:00:09 AUDIT command.LoadTable: [l-sparkcluster1.test.com][wyp][Thread-1]Dataload failure for temp.my_table. Please check the logsjava.lang.NullPointerException  at org.apache.carbondata.core.datastore.filesystem.AbstractDFSCarbonFile.isDirectory(AbstractDFSCarbonFile.java:88)  at org.apache.carbondata.core.util.CarbonUtil.deleteRecursive(CarbonUtil.java:364)  at org.apache.carbondata.core.util.CarbonUtil.access$100(CarbonUtil.java:93)  at org.apache.carbondata.core.util.CarbonUtil$2.run(CarbonUtil.java:326)  at org.apache.carbondata.core.util.CarbonUtil$2.run(CarbonUtil.java:322)  at java.security.AccessController.doPrivileged(Native Method)  at javax.security.auth.Subject.doAs(Subject.java:422)  at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1491)  at org.apache.carbondata.core.util.CarbonUtil.deleteFoldersAndFiles(CarbonUtil.java:322)  at org.apache.carbondata.spark.load.CarbonLoaderUtil.recordLoadMetadata(CarbonLoaderUtil.java:333)  at org.apache.carbondata.spark.rdd.CarbonDataRDDFactory$.updateStatus$1(CarbonDataRDDFactory.scala:595)  at org.apache.carbondata.spark.rdd.CarbonDataRDDFactory$.loadCarbonData(CarbonDataRDDFactory.scala:1107)  at org.apache.spark.sql.execution.command.LoadTable.processData(carbonTableSchema.scala:1046)  at org.apache.spark.sql.execution.command.LoadTable.run(carbonTableSchema.scala:754)  at org.apache.spark.sql.execution.command.LoadTableByInsert.processData(carbonTableSchema.scala:651)  at org.apache.spark.sql.execution.command.LoadTableByInsert.run(carbonTableSchema.scala:637)  at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)  at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)  at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114)  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114)  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:135)  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:132)  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:113)  at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:87)  at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:87)  at org.apache.spark.sql.Dataset.<init>(Dataset.scala:185)  at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:64)  at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:592)  ... 50 elidedscala> As you see, if you run insert overwrite table xxxx sql greater than or equal to three times, then you will get java.lang.NullPointerException.
issueID:CARBONDATA-162
type:Bug
changed files:
texts:Javadoc doesn&#39;t pass
Using the release profile, the javadoc generation doesn't pass.
issueID:CARBONDATA-1620
type:Improvement
changed files:
texts:Ignore empty line when load from csv
if csv have many empty line, then will store null for empty line on CarbonData, but this is unused and waste space.for example:in csv the data is------------------1,a// emptyline2,b// emptyline------------------store to CarbonData is------------------1,anull,null2,bnull,null------------------after change, then it will be------------------1,a2,b------------------
issueID:CARBONDATA-1621
type:Improvement
changed files:
texts:Ignore empty line when load from csv
if csv have many empty line, then will store null for empty line on CarbonData, but this is unused and waste space.for example:in csv the data is------------------1,a// emptyline2,b// emptyline------------------store to CarbonData is------------------1,anull,null2,bnull,null------------------after change, then it will be------------------1,a2,b------------------
issueID:CARBONDATA-1622
type:Improvement
changed files:
texts:Ignore empty line when load from csv
if csv have many empty line, then will store null for empty line on CarbonData, but this is unused and waste space.for example:in csv the data is------------------1,a// emptyline2,b// emptyline------------------store to CarbonData is------------------1,anull,null2,bnull,null------------------after change, then it will be------------------1,a2,b------------------
issueID:CARBONDATA-1624
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/reader/CarbonDeleteFilesDataReader.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/SortParameters.java
core/src/main/java/org/apache/carbondata/core/util/CarbonProperties.java
processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactDataHandlerColumnar.java
texts:If SORT_SCOPE is non-GLOBAL_SORT with Spark, set &#39;carbon.number.of.cores.while.loading&#39; dynamically as per the available executor cores
If we are using carbondata + spark to load data, we can set carbon.number.of.cores.while.loading to the  number of executor cores. For example, when set the number of executor cores to 6, it shows that there are at least 6 cores per node for loading data, so we can set carbon.number.of.cores.while.loading to 6 automatically.
issueID:CARBONDATA-1625
type:New Feature
changed files:
texts:Introduce new datatype of  varchar(size) to store column length more than short limit.
I am using Spark 2.1 + CarbonData 1.2, and find that if enable.unsafe.sort=true, the length of bytes of column exceed 32768, it will load data unsuccessfully. My test code: val longStr = sb.toString()  // the getBytes length of longStr exceeds 32768     println(longStr.length())     println(longStr.getBytes("UTF-8").length)         import spark.implicits._     val df1 = spark.sparkContext.parallelize(0 to 1000)       .map(x => ("a", x.toString(), longStr, x, x.toLong, x * 2))       .toDF("stringField1", "stringField2", "stringField3", "intField", "longField", "int2Field")           val df2 = spark.sparkContext.parallelize(1001 to 2000)       .map(x => ("b", x.toString(), (x % 2).toString(), x, x.toLong, x * 2))       .toDF("stringField1", "stringField2", "stringField3", "intField", "longField", "int2Field")           val df3 = df1.union(df2)     val tableName = "study_carbondata_test"     spark.sql(s"DROP TABLE IF EXISTS ${tableName} ").show()     val sortScope = "LOCAL_SORT"   // LOCAL_SORT   GLOBAL_SORT     spark.sql(s"""         |  CREATE TABLE IF NOT EXISTS ${tableName} (         |    stringField1          string,         |    stringField2          string,         |    stringField3          string,         |    intField              int,         |    longField             bigint,         |    int2Field             int         |  )         |  STORED BY 'carbondata'         |  TBLPROPERTIES('DICTIONARY_INCLUDE'='stringField1, stringField2',         |    'SORT_COLUMNS'='stringField1, stringField2, intField, longField',         |    'SORT_SCOPE'='${sortScope}',         |    'NO_INVERTED_INDEX'='stringField3, int2Field',         |    'TABLE_BLOCKSIZE'='64'         |  )        """.stripMargin)     df3.write       .format("carbondata")         .option("tableName", "study_carbondata_test")       .option("compress", "true")  // just valid when tempCSV is true       .option("tempCSV", "false")       .option("single_pass", "true")       .mode(SaveMode.Append)       .save()The error message: *java.lang.NegativeArraySizeException         at org.apache.carbondata.processing.newflow.sort.unsafe.UnsafeCarbonRowPage.getRow(UnsafeCarbonRowPage.java:182)         at org.apache.carbondata.processing.newflow.sort.unsafe.holder.UnsafeInmemoryHolder.readRow(UnsafeInmemoryHolder.java:63)         at org.apache.carbondata.processing.newflow.sort.unsafe.merger.UnsafeSingleThreadFinalSortFilesMerger.startSorting(UnsafeSingleThreadFinalSortFilesMerger.java:114)         at org.apache.carbondata.processing.newflow.sort.unsafe.merger.UnsafeSingleThreadFinalSortFilesMerger.startFinalMerge(UnsafeSingleThreadFinalSortFilesMerger.java:81)         at org.apache.carbondata.processing.newflow.sort.impl.UnsafeParallelReadMergeSorterImpl.sort(UnsafeParallelReadMergeSorterImpl.java:105)         at org.apache.carbondata.processing.newflow.steps.SortProcessorStepImpl.execute(SortProcessorStepImpl.java:62)         at org.apache.carbondata.processing.newflow.steps.DataWriterProcessorStepImpl.execute(DataWriterProcessorStepImpl.java:87)         at org.apache.carbondata.processing.newflow.DataLoadExecutor.execute(DataLoadExecutor.java:51)         at org.apache.carbondata.spark.rdd.NewDataFrameLoaderRDD$$anon$2.<init>(NewCarbonDataLoadRDD.scala:442)         at org.apache.carbondata.spark.rdd.NewDataFrameLoaderRDD.internalCompute(NewCarbonDataLoadRDD.scala:405)         at org.apache.carbondata.spark.rdd.CarbonRDD.compute(CarbonRDD.scala:62)         at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)         at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)* Currently, the length of column was stored by short type.Introduce new datatype of  varchar(size) to store column length more than short limit.
issueID:CARBONDATA-1626
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/datastore/impl/FileFactory.java
core/src/main/java/org/apache/carbondata/core/util/path/CarbonTablePath.java
processing/src/main/java/org/apache/carbondata/processing/merger/CarbonDataMergerUtil.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
core/src/main/java/org/apache/carbondata/core/statusmanager/LoadMetadataDetails.java
core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
texts:add datasize and index size to table status file
if carbondata is used in cloud which will have charging or billing for the queries ran, adding datasize and indexsize in table status will help in billing features.
issueID:CARBONDATA-1627
type:Bug
changed files:
texts:one job failed among 100 job while performing select operation with 100 different thread
1) create query (with any 5 column)2) load data: only 5 records3) perform select operation by launching 100 threads in parallel (can use Jmeter tool to launch 100 thread)All request will be success only one job failed with an error message:- java.lang.illegalArgumentException:- Config entry enable.unsafe.sort already registered
issueID:CARBONDATA-1628
type:Improvement
changed files:
texts:Re-factory LoadTableCommand to reuse code for streaming ingest in the future
Re-factory LoadTableCommand to reuse code for streaming ingest in the future
issueID:CARBONDATA-163
type:New Feature
changed files:
texts:Tool to merge Github Pull Requests
As per discussion on the mailing list, this JIRA is for creating a tool to help merge pull requests.
issueID:CARBONDATA-1630
type:Bug
changed files:
texts:load data into hive table fail
import org.apache.spark.sql.SparkSessionimport org.apache.spark.sql.CarbonSession._val rootPath = "hdfs://namenodeb:8020/app/carbondata"val storeLocation = s"$rootPath/store"val warehouse = s"$rootPath/warehouse"val metastoredb = s"$rootPath/metastore_db"val carbon = SparkSession.builder().enableHiveSupport().config("spark.sql.warehouse.dir", warehouse).config(org.apache.carbondata.core.constants.CarbonCommonConstants.STORE_LOCATION, storeLocation).getOrCreateCarbonSession(storeLocation, metastoredb)import org.apache.spark.sql.types._import org.apache.spark.sql.Rowval rdd = sc.textFile("/data/home/hadoop/test.txt");val schemaString = "id name city"val fields = schemaString.split(" ").map(fieldName => StructField(fieldName, StringType, nullable = true))val schema = StructType(fields)val rowRDD = rdd.map(_.split(",")).map(attributes => Row(attributes(0),attributes(1),attributes(2)))val peopleDF = spark.createDataFrame(rowRDD, schema)peopleDF.createOrReplaceTempView("tmp_table")spark.sql("insert into target_table SELECT * FROM tmp_table")java.lang.RuntimeException: Failed to add entry in table status for default.target_table  at scala.sys.package$.error(package.scala:27)  at org.apache.carbondata.spark.util.CommonUtil$.readAndUpdateLoadProgressInTableMeta(CommonUtil.scala:533)  at org.apache.spark.sql.execution.command.LoadTable.processData(carbonTableSchema.scala:928)  at org.apache.spark.sql.execution.command.LoadTable.run(carbonTableSchema.scala:754)  at org.apache.spark.sql.execution.command.LoadTableByInsert.processData(carbonTableSchema.scala:651)  at org.apache.spark.sql.execution.command.LoadTableByInsert.run(carbonTableSchema.scala:637)  at org.apache.spark.sql.CarbonDatasourceHadoopRelation.insert(CarbonDatasourceHadoopRelation.scala:98)  at org.apache.spark.sql.execution.datasources.InsertIntoDataSourceCommand.run(InsertIntoDataSourceCommand.scala:43)  at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)  at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)  at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114)  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114)  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:135)  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:132)  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:113)  at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:92)  at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:92)  at org.apache.spark.sql.Dataset.<init>(Dataset.scala:185)  at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:64)  at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:592)  ... 52 elided
issueID:CARBONDATA-164
type:New Feature
changed files:
texts:Add template for pull requests
As per discussion on the mailing list, this JIRA is to add a template for pull requests.
issueID:CARBONDATA-165
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
texts:Behavior need to be corrected when fact csv have header for ALL_DICTIONARY
When fact csv already have header and  giving FILEHEADER along with  ALL_DICTIONARY_PATH option ,  header will be considered as data row , which is not correct.FILEHEADER option must be given only when CSV do not have header .  We can read the header from fact file when FILEHEADER  is not  given with ALL_DICTIONARY_PATH
issueID:CARBONDATA-1651
type:Bug
changed files:
texts:Unsupported Spark2 BooleanType
Unable to save Dataset if it contains field with BooleanTypeclass CarbonDataFrameWritermethod convertToCarbonType doesn't support it
issueID:CARBONDATA-1652
type:Improvement
changed files:
texts:Add example for spark integration
It is good to have more examples for user reference.This PR adds back examples from spark-example module in earlier spark 1 integration
issueID:CARBONDATA-1653
type:Improvement
changed files:processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactDataHandlerModel.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/IntermediateFileMerger.java
core/src/main/java/org/apache/carbondata/core/scan/executor/impl/AbstractQueryExecutor.java
processing/src/main/java/org/apache/carbondata/processing/merger/CompactionResultSortProcessor.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/SortTempFileChunkHolder.java
core/src/main/java/org/apache/carbondata/core/scan/executor/util/QueryUtil.java
texts:Rename aggType to measureType
There are many 'aggType' in the code but its meaning is not clear
issueID:CARBONDATA-1656
type:Sub-task
changed files:
texts:Reject ALTER TABLE command for streaming table

issueID:CARBONDATA-1658
type:Bug
changed files:processing/src/main/java/org/apache/carbondata/processing/loading/steps/CarbonRowDataWriterProcessorStepImpl.java
texts:Thread Leak Issue in No Sort
Threads are not getting closed in case of no sort
issueID:CARBONDATA-1659
type:Task
changed files:
texts:Remove spark 1.x info in module &#39;carbondata-spark-common-cluster-test&#39;

issueID:CARBONDATA-166
type:Bug
changed files:
texts:create table contains shared dictionary,and the shared dictionary keywords is not complete,create table can success and load failed

issueID:CARBONDATA-1660
type:Bug
changed files:integration/presto/src/main/java/org/apache/carbondata/presto/PrestoFilterUtil.java
texts:Incorrect result displays while executing select query with where clause for decimal data type
Incorrect result displays while executing select query with where clause for decimal data typeSteps to reproduce:On Beeline:1) Create Table:CREATE TABLE uniqdata (CUST_ID int,CUST_NAME String,ACTIVE_EMUI_VERSION string, DOB timestamp, DOJ timestamp, BIGINT_COLUMN1 bigint,BIGINT_COLUMN2 bigint,DECIMAL_COLUMN1 decimal(30,10), DECIMAL_COLUMN2 decimal(36,10),Double_COLUMN1 double, Double_COLUMN2 double,INTEGER_COLUMN1 int) STORED BY 'org.apache.carbondata.format' TBLPROPERTIES ("TABLE_BLOCKSIZE"= "256 MB")2)Load Data:LOAD DATA INPATH 'HDFS_URL/BabuStore/Data/uniqdata/2000_UniqData.csv' into table uniqdata OPTIONS('DELIMITER'=',' , 'QUOTECHAR'='"','BAD_RECORDS_ACTION'='FORCE','FILEHEADER'='CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1,Double_COLUMN2,INTEGER_COLUMN1')3) Start presto server:bin/launcher run4) run presto CLI:./presto --server localhost:9000 --catalog carbondata --schema newprestoOn presto CLI1) Execute select Query:select cust_name from uniqdata where decimal_column1=12345678902.1234000000;Expected Result: it should display the cust_name as on beeline------------------+    cust_name     ------------------+ CUST_NAME_00001  ------------------+Actual Result:it throws an error saying error while setting filter expression to job.presto:newpresto> select cust_name from uniqdata where decimal_column1=12345678902.1234000000;Query 20171031_074909_00013_k9q68 failed: Error while setting filter expression to Job
issueID:CARBONDATA-1661
type:Bug
changed files:integration/presto/src/main/java/org/apache/carbondata/presto/PrestoFilterUtil.java
texts:Incorrect output of select query with timestamp data type on presto CLI
Incorrect output of select query with timestamp data type on presto CLISteps to Reproduce: On Beeline:1) Create Table:CREATE TABLE uniqdata (CUST_ID int,CUST_NAME String,ACTIVE_EMUI_VERSION string, DOB timestamp, DOJ timestamp, BIGINT_COLUMN1 bigint,BIGINT_COLUMN2 bigint,DECIMAL_COLUMN1 decimal(30,10), DECIMAL_COLUMN2 decimal(36,10),Double_COLUMN1 double, Double_COLUMN2 double,INTEGER_COLUMN1 int) STORED BY 'org.apache.carbondata.format' TBLPROPERTIES ("TABLE_BLOCKSIZE"= "256 MB")2)Load Data:LOAD DATA INPATH 'HDFS_URL/BabuStore/Data/uniqdata/2000_UniqData.csv' into table uniqdata OPTIONS('DELIMITER'=',' , 'QUOTECHAR'='"','BAD_RECORDS_ACTION'='FORCE','FILEHEADER'='CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1,Double_COLUMN2,INTEGER_COLUMN1')3) Start presto server:bin/launcher run4) run presto CLI:./presto --server localhost:9000 --catalog carbondata --schema newprestoOn presto CLI1) Execute select Query:select cust_name from uniqdata where dob= cast('1970-01-11 01:00:03.000' as timestamp);2)Expected Result: it should display correct output as on beeline:------------------+    cust_name     ------------------+ CUST_NAME_00010  ------------------+3) Actual Result:cust_name -----------(0 rows)Query 20171031_084306_00030_k9q68, FINISHED, 1 nodeSplits: 17 total, 17 done (100.00%)0:00 &#91;0 rows, 0B&#93; &#91;0 rows/s, 0B/s&#93;
issueID:CARBONDATA-1662
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/metadata/datatype/BooleanType.java
processing/src/main/java/org/apache/carbondata/processing/loading/converter/impl/FieldEncoderFactory.java
core/src/main/java/org/apache/carbondata/core/scan/expression/RangeExpressionEvaluator.java
streaming/src/main/java/org/apache/carbondata/streaming/CarbonStreamRecordReader.java
integration/hive/src/main/java/org/apache/carbondata/hive/CarbonDictionaryDecodeReadSupport.java
core/src/main/java/org/apache/carbondata/core/metadata/converter/ThriftWrapperSchemaConverterImpl.java
core/src/main/java/org/apache/carbondata/core/scan/executor/util/QueryUtil.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelFilterExecuterImpl.java
processing/src/main/java/org/apache/carbondata/processing/loading/parser/CarbonParserFactory.java
core/src/main/java/org/apache/carbondata/core/metadata/datatype/StringType.java
core/src/main/java/org/apache/carbondata/core/indexstore/row/DataMapRowImpl.java
core/src/main/java/org/apache/carbondata/core/datastore/block/SegmentProperties.java
core/src/main/java/org/apache/carbondata/core/scan/filter/FilterExpressionProcessor.java
core/src/main/java/org/apache/carbondata/core/metadata/datatype/DataTypes.java
core/src/main/java/org/apache/carbondata/core/metadata/datatype/StructField.java
streaming/src/main/java/org/apache/carbondata/streaming/CarbonStreamRecordWriter.java
core/src/main/java/org/apache/carbondata/core/util/AbstractDataFileFooterConverter.java
core/src/main/java/org/apache/carbondata/core/metadata/datatype/DoubleType.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/ConditionalFilterResolverImpl.java
core/src/main/java/org/apache/carbondata/core/metadata/datatype/NullType.java
core/src/main/java/org/apache/carbondata/core/metadata/datatype/ByteArrayType.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockletDataMap.java
core/src/main/java/org/apache/carbondata/core/metadata/datatype/ShortType.java
core/src/main/java/org/apache/carbondata/core/metadata/datatype/FloatType.java
core/src/main/java/org/apache/carbondata/core/metadata/datatype/LongType.java
core/src/main/java/org/apache/carbondata/core/scan/executor/impl/AbstractQueryExecutor.java
core/src/main/java/org/apache/carbondata/core/metadata/datatype/StructType.java
streaming/src/main/java/org/apache/carbondata/streaming/CarbonStreamInputFormat.java
core/src/main/java/org/apache/carbondata/core/metadata/datatype/ByteType.java
core/src/main/java/org/apache/carbondata/core/metadata/datatype/MapType.java
processing/src/main/java/org/apache/carbondata/processing/util/CarbonDataProcessorUtil.java
core/src/main/java/org/apache/carbondata/core/metadata/datatype/IntType.java
core/src/main/java/org/apache/carbondata/core/metadata/datatype/TimestampType.java
core/src/main/java/org/apache/carbondata/core/metadata/datatype/ArrayType.java
core/src/main/java/org/apache/carbondata/core/metadata/datatype/ShortIntType.java
core/src/main/java/org/apache/carbondata/core/metadata/datatype/DateType.java
texts:Make ArrayType and StructType contain child DataType

issueID:CARBONDATA-1663
type:Improvement
changed files:
texts:Decouple spark in carbon modules
carbon-core, carbon-processing, carbon-hadoop modules should not depend on spark
issueID:CARBONDATA-1664
type:Bug
changed files:
texts:Abnormal behavior of timestamp data type in carbondata
Abnormal behavior of timestamp data type in carbondataSteps to Reproduce:1) Create Table:CREATE TABLE uniqdata (CUST_ID int,CUST_NAME String,ACTIVE_EMUI_VERSION string, DOB timestamp, DOJ timestamp, BIGINT_COLUMN1 bigint,BIGINT_COLUMN2 bigint,DECIMAL_COLUMN1 decimal(30,10), DECIMAL_COLUMN2 decimal(36,10),Double_COLUMN1 double, Double_COLUMN2 double,INTEGER_COLUMN1 int) STORED BY 'org.apache.carbondata.format' TBLPROPERTIES ("TABLE_BLOCKSIZE"= "256 MB")2)Load Data:LOAD DATA INPATH 'HDFS_URL/BabuStore/Data/uniqdata/2000_UniqData.csv' into table uniqdata OPTIONS('DELIMITER'=',' , 'QUOTECHAR'='"','BAD_RECORDS_ACTION'='FORCE','FILEHEADER'='CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1,Double_COLUMN2,INTEGER_COLUMN1')3) Execute Query:a) select DOB from UNIQDATA where DOB ='1970-01-01 10:00:03.0' or DOB = '1970-01-04 01:00:03.0';output:------------------------+          DOB           ------------------------+ 1970-01-01 10:00:03.0   1970-01-04 01:00:03.0  ------------------------+b) select DOB from UNIQDATA where DOB in ('1970-01-01 10:00:03.0','1970-01-04 01:00:03.0');output:------+ DOB  ------+------+c)select DOB from UNIQDATA where DOB in (cast('1970-01-01 10:00:03.0' as timestamp),cast('1970-01-04 01:00:03.0' as timestamp));output:------------------------+          DOB           ------------------------+ 1970-01-01 10:00:03.0   1970-01-04 01:00:03.0  ------------------------+Abnormality of timestamp datatype:In the select query (a) it fetch the records containing DOB  1970-01-01 10:00:03.0 and 1970-01-04 01:00:03.0 but for query (b) while using IN operator it shows no data and again in the same query when we cast it to timestamp as in query (c) it displays result.There should be a strict type checking for timestamp values.
issueID:CARBONDATA-1666
type:Improvement
changed files:
texts:Clean up redundant code
There are some removed feature in carbon project, it is better to remove redundant code for better readability.
issueID:CARBONDATA-1667
type:Sub-task
changed files:processing/src/main/java/org/apache/carbondata/processing/loading/model/CarbonLoadModel.java
texts:Remove DirectLoad feature
Currently carbon is always using DirectLoad from CSV. So this option can be removed in the code
issueID:CARBONDATA-1668
type:Sub-task
changed files:
texts:Remove isTableSplitPartition while loading
This option is always false, related code can be removed
issueID:CARBONDATA-1669
type:Sub-task
changed files:core/src/main/java/org/apache/carbondata/core/locks/CarbonLockUtil.java
texts:Clean up code in CarbonDataRDDFactory
Inside CarbonDataRDDFactory.loadCarbonData, there are many function defined inside function, makes the loading logic very hard to read
issueID:CARBONDATA-167
type:Bug
changed files:
texts:UndeclaredThrowableException thrown instead of data loading fail when fileheader has unsupported characters in file/command

issueID:CARBONDATA-1670
type:Bug
changed files:
texts:Incorrect result displays while select query on presto CLI after recreating a table.
Incorrect result displays while select query on presto CLI after recreating a table.Steps to reproduce:On Beeline:1) Create Table:CREATE TABLE list_partition_table_short(intField INT, bigintField LONG, doubleField DOUBLE, stringField STRING, timestampField TIMESTAMP, decimalField DECIMAL(18,2), dateField DATE, charField CHAR(5), floatField FLOAT) PARTITIONED BY (shortField SHORT) STORED BY 'carbondata' TBLPROPERTIES('PARTITION_TYPE'='LIST', 'LIST_INFO'='10,20,30');2)Load Data:load data inpath 'hdfs://localhost:54310/Data/partition_table.csv' into table list_partition_table_short options('FILEHEADER'='shortfield,intfield,bigintfield,doublefield,stringfield,timestampfield,decimalfield,datefield,charfield,floatfield');3) Execute select Query:select * from list_partition_table_short;Output:-------------------------------------------------------------------------------------------------------------------------------- intField   bigintField   doubleField         stringField        timestampField   decimalField   dateField   charField   floatField   shortField  -------------------------------------------------------------------------------------------------------------------------------- 19         109           1009.0        HashPartition            NULL             19.25          NULL        W           109.01       10           11         101           1001.0        HashPartition            NULL             11.25          NULL        Z           101.01       2            21         111           1011.0        HashPartition            NULL             21.25          NULL        Z           111.01       12           10         100           1000.0        ListPartition            NULL             10.25          NULL        A           100.01       1            22         112           1012.0        ListPartition            NULL             22.25          NULL        F           112.01       13           23         113           1013.0        ListPartition            NULL             23.25          NULL        M           113.01       14           16         106           1006.0        ListPartition            NULL             16.25          NULL        Y           106.01       7            12         102           1002.0        NoPartition              NULL             12.25          NULL        F           102.01       3            15         105           1005.0        NoPartition              NULL             15.25          NULL        K           105.01       6            20         110           1010.0        NoPartition              NULL             20.25          NULL        K           110.01       11           18         108           1008.0        RangeIntervalPartition   NULL             18.25          NULL        A           108.01       9            14         104           1004.0        RangePartition           NULL             14.25          NULL        L           104.01       5            13         103           1003.0        RangePartition           NULL             13.25          NULL        M           103.01       4            17         107           1007.0        RangePartition           NULL             17.25          NULL        T           107.01       8           --------------------------------------------------------------------------------------------------------------------------------Start presto server:bin/launcher run run presto CLI:./presto --server localhost:9000 --catalog carbondata --schema newprestoOn Presto CLI: 1)Execute Queries:a) show tables;b) select * from list_partition_table_short;Output: same as beeline. intfield | bigintfield | doublefield |      stringfield       | timestampfield | decimalfield | datefield | charfield | floatfield | shortfield -----------------------------------------------------------------------------------------------------------+-----------       11 |         101 |      1001.0 | HashPartition          | NULL           | 11.25        | NULL      | Z         |     101.01 |          2        21 |         111 |      1011.0 | HashPartition          | NULL           | 21.25        | NULL      | Z         |     111.01 |         12        10 |         100 |      1000.0 | ListPartition          | NULL           | 10.25        | NULL      | A         |     100.01 |          1        22 |         112 |      1012.0 | ListPartition          | NULL           | 22.25        | NULL      | F         |     112.01 |         13        23 |         113 |      1013.0 | ListPartition          | NULL           | 23.25        | NULL      | M         |     113.01 |         14        16 |         106 |      1006.0 | ListPartition          | NULL           | 16.25        | NULL      | Y         |     106.01 |          7        12 |         102 |      1002.0 | NoPartition            | NULL           | 12.25        | NULL      | F         |     102.01 |          3        15 |         105 |      1005.0 | NoPartition            | NULL           | 15.25        | NULL      | K         |     105.01 |          6        20 |         110 |      1010.0 | NoPartition            | NULL           | 20.25        | NULL      | K         |     110.01 |         11        18 |         108 |      1008.0 | RangeIntervalPartition | NULL           | 18.25        | NULL      | A         |     108.01 |          9        14 |         104 |      1004.0 | RangePartition         | NULL           | 14.25        | NULL      | L         |     104.01 |          5        13 |         103 |      1003.0 | RangePartition         | NULL           | 13.25        | NULL      | M         |     103.01 |          4        17 |         107 |      1007.0 | RangePartition         | NULL           | 17.25        | NULL      | T         |     107.01 |          8        19 |         109 |      1009.0 | HashPartition          | NULL           | 19.25        | NULL      | W         |     109.01 |         10 (14 rows)Now On beeline drop the table and recreate it using same commands as described  and after recreating run the select query on it in presto CLI.(don't restart the server)Output:1)Expected Result: it should display the same result as on beeline.2)Actual result:intfield | bigintfield | doublefield | stringfield | timestampfield | decimalfield | datefield | charfield | floatfield | shortfield ------------------------------------------------------------------------------------------------+-----------(0 rows)
issueID:CARBONDATA-1671
type:Sub-task
changed files:
texts:Support set/unset table comment for ALTER table
Table comment set/unset using *ALTER TABLE  SET/UNSET TBLPROPERTIES*
issueID:CARBONDATA-1674
type:Improvement
changed files:
texts:Carbon 1.3.0-Partitioning:Describe Formatted Should show the type of partition as well.
Describe Formatted should show type of partitions as well.
issueID:CARBONDATA-1675
type:Bug
changed files:
texts:Incorrect result displays after applying drop column query on a table
Incorrect result displays after applying drop column query on a tableSteps to reproduce:1) Create table:CREATE TABLE uniqdata(CUST_ID int,CUST_NAME String,ACTIVE_EMUI_VERSION string, DOB timestamp, DOJ timestamp, BIGINT_COLUMN1 bigint,BIGINT_COLUMN2 bigint,DECIMAL_COLUMN1 decimal(30,10), DECIMAL_COLUMN2 decimal(36,10),Double_COLUMN1 double, Double_COLUMN2 double,INTEGER_COLUMN1 int) STORED BY 'org.apache.carbondata.format' TBLPROPERTIES ("TABLE_BLOCKSIZE"= "256 MB")2) Load dataLOAD DATA INPATH 'hdfs://localhost:54310/Data/uniqdata/2000_UniqData.csv' into table uniqdata OPTIONS('DELIMITER'=',' , 'QUOTECHAR'='"','BAD_RECORDS_ACTION'='FORCE','FILEHEADER'='CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1,Double_COLUMN2,INTEGER_COLUMN1')3)Execute Query:desc uniqdataoutput:---------------------------------------------+       col_name           data_type     comment  ---------------------------------------------+ CUST_ID               int              NULL      CUST_NAME             string           NULL      ACTIVE_EMUI_VERSION   string           NULL      DOB                   timestamp        NULL      DOJ                   timestamp        NULL      BIGINT_COLUMN1        bigint           NULL      BIGINT_COLUMN2        bigint           NULL      DECIMAL_COLUMN1       decimal(30,10)   NULL      DECIMAL_COLUMN2       decimal(36,10)   NULL      Double_COLUMN1        double           NULL      Double_COLUMN2        double           NULL      INTEGER_COLUMN1       int              NULL     ---------------------------------------------+12 rows selected (0.041 seconds)Start Presto serversudo ./bin/launcher runrun presto CLI:./presto --server localhost:9000 --catalog carbondata --schema newprestoOn Presto CLI:1) Execute Query:a) desc uniqdata;output:       Column        |      Type      | Extra | Comment ---------------------------------------+-------- cust_id             | integer        |       |          cust_name           | varchar        |       |          active_emui_version | varchar        |       |          dob                 | timestamp      |       |          doj                 | timestamp      |       |          bigint_column1      | bigint         |       |          bigint_column2      | bigint         |       |          decimal_column1     | decimal(30,10) |       |          decimal_column2     | decimal(36,10) |       |          double_column1      | double         |       |          double_column2      | double         |       |          integer_column1     | integer        |       |         (12 rows)b) Now on Beeline execute the drop column query on the table which is :alter table uniqdata drop columns (CUST_ID)c)desc uniqdata;Expected output: it should display updated table description as on beelineActual output: on beeline:0: jdbc:hive2://localhost:10000> desc uniqdata;---------------------------------------------+       col_name           data_type     comment  ---------------------------------------------+ cust_name             string           NULL      active_emui_version   string           NULL      dob                   timestamp        NULL      doj                   timestamp        NULL      bigint_column1        bigint           NULL      bigint_column2        bigint           NULL      decimal_column1       decimal(30,10)   NULL      decimal_column2       decimal(36,10)   NULL      double_column1        double           NULL      double_column2        double           NULL      integer_column1       int              NULL     ---------------------------------------------+11 rows selected (0.039 seconds)On presto CLIpresto:newpresto> desc uniqdata;       Column        |      Type      | Extra | Comment ---------------------------------------+-------- cust_id             | integer        |       |          cust_name           | varchar        |       |          active_emui_version | varchar        |       |          dob                 | timestamp      |       |          doj                 | timestamp      |       |          bigint_column1      | bigint         |       |          bigint_column2      | bigint         |       |          decimal_column1     | decimal(30,10) |       |          decimal_column2     | decimal(36,10) |       |          double_column1      | double         |       |          double_column2      | double         |       |          integer_column1     | integer        |       |         (12 rows)
issueID:CARBONDATA-1677
type:Bug
changed files:
texts:Incorrect result displays on presto CLI after applying drop table command
Incorrect result displays on presto CLI after applying drop table commandSteps to reproduce:On Beeline:1) Create Table:CREATE TABLE uniqdata (CUST_ID int,CUST_NAME String,ACTIVE_EMUI_VERSION string, DOB timestamp, DOJ timestamp, BIGINT_COLUMN1 bigint,BIGINT_COLUMN2 bigint,DECIMAL_COLUMN1 decimal(30,10), DECIMAL_COLUMN2 decimal(36,10),Double_COLUMN1 double, Double_COLUMN2 double,INTEGER_COLUMN1 int) STORED BY 'org.apache.carbondata.format' TBLPROPERTIES ("TABLE_BLOCKSIZE"= "256 MB");2) Load DataLOAD DATA INPATH 'hdfs://localhost:54310/Data/uniqdata/2000_UniqData.csv' into table uniqdata OPTIONS('DELIMITER'=',' , 'QUOTECHAR'='"','BAD_RECORDS_ACTION'='FORCE','FILEHEADER'='CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1,Double_COLUMN2,INTEGER_COLUMN1');3) Execute Queryselect * from uniqdata;Start presto server:bin/launcher runrun presto CLI:./presto --server localhost:9000 --catalog carbondata --schema newprestoOn Presto CLI:1)Execute Queries:a) show tables;b) select * from uniqdata;c)Now on beeline drop the table and execute query:select * from uniqdata;Expected Result: it should throw an error as table or view does not exist or not found Actual Result :On beeline:Error: org.apache.spark.sql.AnalysisException: Table or view not found: uniqdata; line 1 pos 14 (state=,code=0)on Presto:cust_id | cust_name | active_emui_version | dob | doj | bigint_column1 | bigint_column2 | decimal_column1 | decimal_column2 | double_column1 | double_column2 | integer_column1 --------------------------------------------------------------------------------------------------------------------------------+----------------(0 rows)Query 20171108_115415_00002_34smd, FINISHED, 1 nodeSplits: 16 total, 16 done (100.00%)0:00 &#91;0 rows, 0B&#93; &#91;0 rows/s, 0B/s&#93;
issueID:CARBONDATA-168
type:Bug
changed files:
texts:File an INFRA ticket for changing CarbonData&#39;s JIRA notification schema

issueID:CARBONDATA-1680
type:Bug
changed files:
texts:Carbon 1.3.0-Partitioning:Show Partition for Hash Partition doesn&#39;t display the partition id
CREATE TABLE IF NOT EXISTS t9( id Int, logdate Timestamp, phonenumber Int, country String, area String ) PARTITIONED BY (vin String) STORED BY 'carbondata' TBLPROPERTIES('PARTITION_TYPE'='HASH','NUM_PARTITIONS'='5');show partitions t9;
issueID:CARBONDATA-1682
type:Bug
changed files:
texts:Incorrect output on presto CLI after applying alter query on a table
Incorrect output on presto CLI after applying alter query on a tableSteps to reproduce:On beeline 1) Create table:CREATE TABLE uniqdata (CUST_ID int,CUST_NAME String,ACTIVE_EMUI_VERSION string, DOB timestamp, DOJ timestamp, BIGINT_COLUMN1 bigint,BIGINT_COLUMN2 bigint,DECIMAL_COLUMN1 decimal(30,10), DECIMAL_COLUMN2 decimal(36,10),Double_COLUMN1 double, Double_COLUMN2 double,INTEGER_COLUMN1 int) STORED BY 'org.apache.carbondata.format' TBLPROPERTIES ("TABLE_BLOCKSIZE"= "256 MB");2) Execute Query:a)desc uniqdata;b) alter table uniqdata drop columns (cust_id);c)select cust_id from uniqdata;output:Error: org.apache.spark.sql.AnalysisException: cannot resolve '`cust_id`' given input columns: &#91;doj, dob, double_column1, double_column2, active_emui_version, bigint_column1, decimal_column1, decimal_column2, cust_name, bigint_column2, integer_column1&#93;; line 1 pos 7;'Project &#91;&#39;cust_id&#93;+- SubqueryAlias uniqdata   +- Relationcust_name#3097,active_emui_version#3098,dob#3099,doj#3100,bigint_column1#3101L,bigint_column2#3102L,decimal_column1#3103,decimal_column2#3104,double_column1#3105,double_column2#3106,integer_column1#3107 CarbonDatasourceHadoopRelation [ Database name :newpresto, Table name :uniqdata, Schema :Some(StructType(StructField(cust_name,StringType,true), StructField(active_emui_version,StringType,true), StructField(dob,TimestampType,true), StructField(doj,TimestampType,true), StructField(bigint_column1,LongType,true), StructField(bigint_column2,LongType,true), StructField(decimal_column1,DecimalType(30,10),true), StructField(decimal_column2,DecimalType(36,10),true), StructField(double_column1,DoubleType,true), StructField(double_column2,DoubleType,true), StructField(integer_column1,IntegerType,true))) ] (state=,code=0)On Presto CLI:1) Execute Query:select cust_id from uniqdata;2)Expected Output: It should through an error same as on Beeline3)Actual Output: cust_id ---------(0 rows)
issueID:CARBONDATA-1683
type:Improvement
changed files:
texts:Upgrade Presto Version in CarbonData to 0.186
Upgrade the Presto Version in carbondata to 0.186
issueID:CARBONDATA-1684
type:Improvement
changed files:
texts:Upgrade Presto Version in CarbonData to 0.186
Upgrade the Presto Version in carbondata to 0.186
issueID:CARBONDATA-1685
type:Improvement
changed files:
texts:Upgrade Presto Version in CarbonData to 0.186
Upgrade the Presto Version in carbondata to 0.186
issueID:CARBONDATA-1686
type:Improvement
changed files:integration/presto/src/main/java/org/apache/carbondata/presto/readers/DecimalSliceStreamReader.java
integration/presto/src/main/java/org/apache/carbondata/presto/CarbondataPageSource.java
integration/presto/src/main/java/org/apache/carbondata/presto/CarbondataColumnConstraint.java
texts:Upgrade Presto Version of Current Carbon Data to 0.186
Upgrade Presto Version of Current Carbon Data to 0.186
issueID:CARBONDATA-1689
type:Bug
changed files:
texts:Fix parent pom issues and correct CI link of README
1. Move 2.11 of scala.binary.version and scala.version to common properties , for supporting default compilation.    <scala.binary.version>2.11</scala.binary.version>    <scala.version>2.11.8</scala.version>2.Add <streaming> module's source code to coverage report.3.Correct CI link of README
issueID:CARBONDATA-169
type:Bug
changed files:
texts:COLUMNDICT and ALL_DICT_PATH can not be used together
when user use "load data ... Options('COLUMNDICT'='', 'ALL_DICT_PATH'='')", the load sucess, but the query is not right. the reason is that columnDict and All_dict are not compatible, they should not be used together.
issueID:CARBONDATA-1690
type:Bug
changed files:
texts:Query failed after swap table by renaming
SCENARIOI encountered query error after swap table by renaming table. Steps to reproduce this bug are listed as below.These steps work fine:1. CREATE TABLE `t1`;2. LOAD DATA TO `t1`;3. CREATE TABLE `t2`;4. LOAD DATA TO `t2`;5. RENAME `t1` TO `t3`;6. RENAME `t2` TO `t1`;7. QUERY `t1`;These steps work wrong:1. CREATE TABLE `t1`;2. LOAD DATA TO `t1`;3. CREATE TABLE `t2`;4. LOAD DATA TO `t2`;*5. QUERY `t1`;*   &#8212; Added this step6. RENAME `t1` TO `t3`;7. RENAME `t2` TO `t1`;8. QUERY `t1`;   &#8212; This step will cause failureThe above two scenario differs from that the second one add Step5 and the error will be thrown in Step8. The error message in sparksql shell looks like```Error: java.io.FileNotFoundException: File hdfs://slave1:9000/carbonstore/default/test_table/Fact/Part0/Segment_0/part-0-0_batchno0-0-1510144676427.carbondata does not exist. (state=,code=0)``` AnalyzeRenaming table name in carbondata actually is done through renaming the corresponding data folder name. In addition, carbondata also refresh the metadata and its cache.Having seen from the error message above, we find that the file name is exactly the one before rename operation. We guess the problems may lies in data map.In the second scenario, before renaming, when we query `t1 ` (Step5), the corresponding data map will be loaded and cached. Since data map is table name based, when we query `t1` again (Step8) after renaming, the previous data map will be used, which is outdated and incorrect, thus will cause the `FileNotFoundException` error.In the first scenario, when we query `t1` (Step7), it is the first time to load the data map, so the correct data will be readed, that's why it acts OK. ResolveThere are two ways to fix this bug:1. Change the index key of Data Map. Use `table_name + table_schema_last_update_time` in replace of `table_name`.2. Clear corresponding Data Map when doing renaming operation.I prefer the second one since it is easy to implement —— just one line of code.
issueID:CARBONDATA-1691
type:Bug
changed files:
texts:Carbon 1.3.0-Partitioning:Document needs to be updated for Table properties (Sort_Scope) in create table
Document needs to be updated for Table properties (Sort_Scope) in create table.As per JIRA-1438, the sort_scope will be supported in the create statement itself, but the same thing is not mentioned in the document.Document Site- https://carbondata.apache.org/ddl-operation-on-carbondata.html
issueID:CARBONDATA-1694
type:Bug
changed files:integration/presto/src/main/java/org/apache/carbondata/presto/impl/CarbonTableReader.java
texts:Incorrect exception on presto CLI while executing select query after applying alter drop column query on a table
Incorrect exception on presto CLI while executing select query after applying alter drop column query on a tableSteps to Reproduce:On Beeline:1) Create Table:CREATE TABLE uniqdata (CUST_ID int,CUST_NAME String,ACTIVE_EMUI_VERSION string, DOB timestamp, DOJ timestamp, BIGINT_COLUMN1 bigint,BIGINT_COLUMN2 bigint,DECIMAL_COLUMN1 decimal(30,10), DECIMAL_COLUMN2 decimal(36,10),Double_COLUMN1 double, Double_COLUMN2 double,INTEGER_COLUMN1 int) STORED BY 'org.apache.carbondata.format' TBLPROPERTIES ("TABLE_BLOCKSIZE"= "256 MB");2) Load DataLOAD DATA INPATH 'hdfs://localhost:54310/Data/uniqdata/2000_UniqData.csv' into table uniqdata OPTIONS('DELIMITER'=',' , 'QUOTECHAR'='"','BAD_RECORDS_ACTION'='FORCE','FILEHEADER'='CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1,Double_COLUMN2,INTEGER_COLUMN1');3) Execute Query:a) alter table uniqdata drop columns (cust_id);b)select * from uniqdata;ouput:------------------------------------------------------------------------------------------------    cust_name         active_emui_version               dob                     doj            bigint_column1   bigint_column2       decimal_column1          decimal_column2         double_column1        double_column2      integer_column1  -----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ CUST_NAME_01987   ACTIVE_EMUI_VERSION_01987   1975-06-11 01:00:03.0   1975-06-11 02:00:03.0   123372038841     -223372034867    12345680888.1234000000   22345680888.1234000000   1.12345674897976E10   -1.12345674897976E10   1988              CUST_NAME_01988   ACTIVE_EMUI_VERSION_01988   1975-06-12 01:00:03.0   1975-06-12 02:00:03.0   123372038842     -223372034866    12345680889.1234000000   22345680889.1234000000   1.12345674897976E10   -1.12345674897976E10   1989              CUST_NAME_01989   ACTIVE_EMUI_VERSION_01989   1975-06-13 01:00:03.0   1975-06-13 02:00:03.0   123372038843     -223372034865    12345680890.1234000000   22345680890.1234000000   1.12345674897976E10   -1.12345674897976E10   1990              CUST_NAME_01990   ACTIVE_EMUI_VERSION_01990   1975-06-14 01:00:03.0   1975-06-14 02:00:03.0   123372038844     -223372034864    12345680891.1234000000   22345680891.1234000000   1.12345674897976E10   -1.12345674897976E10   1991              CUST_NAME_01991   ACTIVE_EMUI_VERSION_01991   1975-06-15 01:00:03.0   1975-06-15 02:00:03.0   123372038845     -223372034863    12345680892.1234000000   22345680892.1234000000   1.12345674897976E10   -1.12345674897976E10   1992              CUST_NAME_01992   ACTIVE_EMUI_VERSION_01992   1975-06-16 01:00:03.0   1975-06-16 02:00:03.0   123372038846     -223372034862    12345680893.1234000000   22345680893.1234000000   1.12345674897976E10   -1.12345674897976E10   1993              CUST_NAME_01993   ACTIVE_EMUI_VERSION_01993   1975-06-17 01:00:03.0   1975-06-17 02:00:03.0   123372038847     -223372034861    12345680894.1234000000   22345680894.1234000000   1.12345674897976E10   -1.12345674897976E10   1994              CUST_NAME_01994   ACTIVE_EMUI_VERSION_01994   1975-06-18 01:00:03.0   1975-06-18 02:00:03.0   123372038848     -223372034860    12345680895.1234000000   22345680895.1234000000   1.12345674897976E10   -1.12345674897976E10   1995              CUST_NAME_01995   ACTIVE_EMUI_VERSION_01995   1975-06-19 01:00:03.0   1975-06-19 02:00:03.0   123372038849     -223372034859    12345680896.1234000000   22345680896.1234000000   1.12345674897976E10   -1.12345674897976E10   1996              CUST_NAME_01996   ACTIVE_EMUI_VERSION_01996   1975-06-20 01:00:03.0   1975-06-20 02:00:03.0   123372038850     -223372034858    12345680897.1234000000   22345680897.1234000000   1.12345674897976E10   -1.12345674897976E10   1997              CUST_NAME_01997   ACTIVE_EMUI_VERSION_01997   1975-06-21 01:00:03.0   1975-06-21 02:00:03.0   123372038851     -223372034857    12345680898.1234000000   22345680898.1234000000   1.12345674897976E10   -1.12345674897976E10   1998              CUST_NAME_01998   ACTIVE_EMUI_VERSION_01998   1975-06-22 01:00:03.0   1975-06-22 02:00:03.0   123372038852     -223372034856    12345680899.1234000000   22345680899.1234000000   1.12345674897976E10   -1.12345674897976E10   1999              CUST_NAME_01999   ACTIVE_EMUI_VERSION_01999   1975-06-23 01:00:03.0   1975-06-23 02:00:03.0   123372038853     -223372034855    12345680900.1234000000   22345680900.1234000000   1.12345674897976E10   -1.12345674897976E10   2000             -----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+2,013 rows selected (0.547 seconds)On Presto CLI:1)Execute Queries:a) desc uniqdata;b)select * from uniqdata;Expected Output: its should display all data except dropped column as in beeline.Actual output:Query 20171110_053245_00003_ucwgx, FAILED, 1 nodeSplits: 17 total, 0 done (0.00%)0:01 &#91;0 rows, 0B&#93; &#91;0 rows/s, 0B/s&#93;Query 20171110_053245_00003_ucwgx failed: cust_id column not found in the table uniqdata
issueID:CARBONDATA-1695
type:Bug
changed files:
texts:Inaccurate result displays on presto CLI while select Query after change data type Query
Inaccurate result displays on presto CLI while select Query after change data type QuerySteps to reproduce:On Beeline:1) Create Table:CREATE TABLE uniqdata (CUST_ID int,CUST_NAME String,ACTIVE_EMUI_VERSION string, DOB timestamp, DOJ timestamp, BIGINT_COLUMN1 bigint,BIGINT_COLUMN2 bigint,DECIMAL_COLUMN1 decimal(30,10), DECIMAL_COLUMN2 decimal(36,10),Double_COLUMN1 double, Double_COLUMN2 double,INTEGER_COLUMN1 int) STORED BY 'org.apache.carbondata.format' TBLPROPERTIES ("TABLE_BLOCKSIZE"= "256 MB");2) Load DataLOAD DATA INPATH 'hdfs://localhost:54310/Data/uniqdata/2000_UniqData.csv' into table uniqdata OPTIONS('DELIMITER'=',' , 'QUOTECHAR'='"','BAD_RECORDS_ACTION'='FORCE','FILEHEADER'='CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1,Double_COLUMN2,INTEGER_COLUMN1');3) Execute Query:a)desc uniqdata:outpurt:---------------------------------------------+       col_name           data_type     comment  ---------------------------------------------+ CUST_ID               int              NULL      CUST_NAME             string           NULL      ACTIVE_EMUI_VERSION   string           NULL      DOB                   timestamp        NULL      DOJ                   timestamp        NULL      BIGINT_COLUMN1        bigint           NULL      BIGINT_COLUMN2        bigint           NULL      DECIMAL_COLUMN1       decimal(30,10)   NULL      DECIMAL_COLUMN2       decimal(36,10)   NULL      Double_COLUMN1        double           NULL      Double_COLUMN2        double           NULL      INTEGER_COLUMN1       int              NULL     ---------------------------------------------+b)alter table uniqdata change decimal_column1 decimal_column1 decimal(36,12);c)select decimal_column1 from uniqdata;output:---------------------------+      decimal_column1      ---------------------------+ NULL                       NULL                       NULL                       NULL                       12345678901.123400000000   NULL                       NULL                       NULL                       NULL                       NULL                       NULL                       NULL                       NULL                       12345678901.123400000000   12345678902.123400000000   12345678903.123400000000   12345678904.123400000000   12345678905.123400000000   12345678906.123400000000   12345678907.123400000000   12345678908.123400000000   12345678909.123400000000   12345678910.123400000000   12345678911.123400000000   12345678912.123400000000   12345678913.123400000000   12345678914.123400000000  On Presto CLI:1)Execute Queries:a) desc uniqdata;b) select decimal_column1 from uniqdata;Expected Output: Same as in beeline(no change in value before the decimal.Actual Output:    decimal_column1      -------------------------- NULL                      NULL                      NULL                      NULL                      1234567890112.3400000000  NULL                      NULL                      NULL                      NULL                      NULL                      NULL                      NULL                      NULL                      1234567890112.3400000000  1234567890212.3400000000  1234567890312.3400000000  1234567890412.3400000000  1234567890512.3400000000  1234567890612.3400000000  1234567890712.3400000000  1234567890812.3400000000  1234567890912.3400000000  1234567891012.3400000000  1234567891112.3400000000  1234567891212.3400000000  1234567891312.3400000000  1234567891412.3400000000
issueID:CARBONDATA-1696
type:Bug
changed files:
texts:Incorrect output on presto CLI after rename table query on table
Incorrect output on presto CLI after rename table query on tableSteps to Reproduce:On Beeline:1) Create Table:CREATE TABLE uniqdata (CUST_ID bigint,CUST_NAME String,ACTIVE_EMUI_VERSION string, DOB timestamp, DOJ timestamp, BIGINT_COLUMN1 bigint,BIGINT_COLUMN2 bigint,DECIMAL_COLUMN1 decimal(30,10), DECIMAL_COLUMN2 decimal(36,10),Double_COLUMN1 double, Double_COLUMN2 double,INTEGER_COLUMN1 int) STORED BY 'org.apache.carbondata.format' TBLPROPERTIES ("TABLE_BLOCKSIZE"= "256 MB");2) Load DataLOAD DATA INPATH 'hdfs://localhost:54310/Data/uniqdata/2000_UniqData.csv' into table uniqdata OPTIONS('DELIMITER'=',' , 'QUOTECHAR'='"','BAD_RECORDS_ACTION'='FORCE','FILEHEADER'='CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1,Double_COLUMN2,INTEGER_COLUMN1');3) Execute Query:a)alter table uniqdata rename to newuniqdata;b) desc uniqdata;output:Error: org.apache.spark.sql.catalyst.analysis.NoSuchTableException: Table or view 'uniqdata' not found in database 'newpresto'; (state=,code=0)On presto CLI:1) Execute Query:a) desc uniqdata;2) Expected Output: It should display an error saying table or view not found, similar to beeline result.3) Actual Result:      Column        |      Type      | Extra | Comment ---------------------------------------+-------- cust_id             | bigint         |       |          cust_name           | varchar        |       |          active_emui_version | varchar        |       |          dob                 | timestamp      |       |          doj                 | timestamp      |       |          bigint_column1      | bigint         |       |          bigint_column2      | bigint         |       |          decimal_column1     | decimal(30,10) |       |          decimal_column2     | decimal(36,10) |       |          double_column1      | double         |       |          double_column2      | double         |       |          integer_column1     | integer        |       |         (12 rows)
issueID:CARBONDATA-1697
type:Bug
changed files:
texts:Incorrect output displays on presto CLI after alter table query
Incorrect output displays on presto CLI after alter table querySteps to reproduce:On Beeline:1) Create Table:CREATE TABLE uniqdata (CUST_ID int,CUST_NAME String,ACTIVE_EMUI_VERSION string, DOB timestamp, DOJ timestamp, BIGINT_COLUMN1 bigint,BIGINT_COLUMN2 bigint,DECIMAL_COLUMN1 decimal(30,10), DECIMAL_COLUMN2 decimal(36,10),Double_COLUMN1 double, Double_COLUMN2 double,INTEGER_COLUMN1 int) STORED BY 'org.apache.carbondata.format' TBLPROPERTIES ("TABLE_BLOCKSIZE"= "256 MB");2) Load DataLOAD DATA INPATH 'hdfs://localhost:54310/Data/uniqdata/2000_UniqData.csv' into table uniqdata OPTIONS('DELIMITER'=',' , 'QUOTECHAR'='"','BAD_RECORDS_ACTION'='FORCE','FILEHEADER'='CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1,Double_COLUMN2,INTEGER_COLUMN1');3) Execute Query:a)  alter table uniqdata change decimal_column1 decimal_column1 decimal(36,12);b) desc uniqdata;output:      col_name       |    data_type    | comment  |---------------------------------------------+ cust_id               int              NULL      cust_name             string           NULL      active_emui_version   string           NULL      dob                   timestamp        NULL      doj                   timestamp        NULL      bigint_column1        bigint           NULL      bigint_column2        bigint           NULL      decimal_column1       decimal(36,12)   NULL      decimal_column2       decimal(36,10)   NULL      double_column1        double           NULL      double_column2        double           NULL      integer_column1       int              NULL     ---------------------------------------------+On Presto CLI1)Execute Query:a) desc uniqdata;2) Expected Result: it should display the updated precision and scale value of decimal_column1 column as on beeline3) Actual Result:      Column        |      Type      | Extra | Comment ---------------------------------------+-------- cust_id             | integer        |       |          cust_name           | varchar        |       |          active_emui_version | varchar        |       |          dob                 | timestamp      |       |          doj                 | timestamp      |       |          bigint_column1      | bigint         |       |          bigint_column2      | bigint         |       |          decimal_column1     | decimal(30,10) |       |          decimal_column2     | decimal(36,10) |       |          double_column1      | double         |       |          double_column2      | double         |       |          integer_column1     | integer        |       |         (12 rows)
issueID:CARBONDATA-1698
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
core/src/main/java/org/apache/carbondata/core/util/CarbonProperties.java
processing/src/main/java/org/apache/carbondata/processing/merger/CarbonDataMergerUtil.java
texts:support table level compaction configuration
support table level compaction configuration
issueID:CARBONDATA-1699
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/cache/dictionary/ColumnDictionaryInfo.java
texts:Filter is not working properly
Scenario:Load data like belowa,11234567489.7976b,11234567489.7976000000Filter query on double_column = 11234567489.7976Result - only either one of the row is selectedActual Result - all the two rows should be selected
issueID:CARBONDATA-17
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
texts:select count(*) from table where column_x =  &#39;value&#39; is not returnig the correct count
select count from table where column_x =  'value' is not returning the correct countif the first measure is having all values null.
issueID:CARBONDATA-170
type:Bug
changed files:
texts:Delete the lock files which are created after unlock.
Delete the lock files which are created after unlock.
issueID:CARBONDATA-1700
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/metadata/schema/table/TableSchema.java
texts:Failed to load data to existed table after spark session restarted
scenarioI encounterd loading data to existed carbondata table failure after query the table after restarting spark session. I have this failure in spark local mode (found it during local test) and haven't test in other scenarioes.The problem can be reproduced by following steps：0. START: start a session;1. CREATE: create table `t1`;2. LOAD: create a dataframe and write apppend to `t1`;3. STOP: stop current session;4. START: start a session;5. QUERY: query table `t1`;  ----  This step is essential to reproduce the problem.6. LOAD: create a dataframe and write append to `t1`;  &#8212; This step will be failed.Error will be thrown in Step6. The error message in console looks like```java.lang.NullPointerException was thrown.java.lang.NullPointerExceptionat org.apache.spark.sql.execution.command.management.LoadTableCommand.processData(LoadTableCommand.scala:92)at org.apache.spark.sql.execution.command.management.LoadTableCommand.run(LoadTableCommand.scala:60)at org.apache.spark.sql.CarbonDataFrameWriter.loadDataFrame(CarbonDataFrameWriter.scala:141)at org.apache.spark.sql.CarbonDataFrameWriter.writeToCarbonFile(CarbonDataFrameWriter.scala:50)at org.apache.spark.sql.CarbonDataFrameWriter.appendToCarbonFile(CarbonDataFrameWriter.scala:42)at org.apache.spark.sql.CarbonSource.createRelation(CarbonSource.scala:110)at org.apache.spark.sql.execution.datasources.DataSource.write(DataSource.scala:426)at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:215)```The following code can be pasted in `TestLoadDataFrame.scala` to reproduce this problem —— but keepin mind you should manually run the first test and then the second in different iteration (to make sure that the sparksession is restarted).```  test("prepare") {    sql("drop table if exists carbon_stand_alone")    sql( "create table if not exists carbon_stand_alone (c1 string, c2 string, c3 int)" +    " stored by 'carbondata'").collect()    sql("select * from carbon_stand_alone").show()    df.write      .format("carbondata")      .option("tableName", "carbon_stand_alone")      .option("tempCSV", "false")      .mode(SaveMode.Append)      .save()  }  test("test load dataframe after query") {    sql("select * from carbon_stand_alone").show()    // the following line will cause failure    df.write      .format("carbondata")      .option("tableName", "carbon_stand_alone")      .option("tempCSV", "false")      .mode(SaveMode.Append)      .save()    // if it works fine, it sould be true    checkAnswer(      sql("select count(*) from carbon_stand_alone where c3 > 500"), Row(31500 * 2)    )  }``` ANALYSEI went through the code and found the problem was caused by NULL `tableProperties` in `tablemeta: tableMeta.carbonTable.getTableInfo      .getFactTable.getTableProperties` (we will name it `propertyInTableInfo` for short) is null in Line89 in `LoadTableCommand.scala`.After debug, I found that the `propertyInTableInfo` sett in `CarbonTableInputFormat.setTableInfo(...)` had the correct value. But `CarbonTableInputFormat.getTableInfo(...)` had the incorrect value. The setter is used to serialized TableInfo, while the getter is used to deserialized TableInfo ———— That means there are something wrong in serialization-deserialization.Keep diving into the code, I found that serialization and deserialization in `TableSchema`, a member of `TableInfo`, ignores the `tableProperties` member, thus causing this value empty after deserialization. Since this value has not been initialized in construtor, so the value remains `NULL` and cause the NPE problem. RESOLVE1. Initialize `tableProperties` in `TableSchema`2. Include `tableProperties` in serialization-deserialization of `TableSchema` NotesAlthough the bug has been fix, I still can't understand why the problem can be triggered in above way.Tests need the sparksession to be restarted, which is impossible currently, so no tests will be added.
issueID:CARBONDATA-1701
type:Sub-task
changed files:core/src/main/java/org/apache/carbondata/core/util/CarbonSessionInfo.java
core/src/main/java/org/apache/carbondata/core/util/SessionParams.java
texts:support thread safe api for segment reading
Interface to support setting thread local session parameters. Segments to access is supported usingCarbonSession.threadSet("carbon.input.segments.default.carbon_table1", "1,2,3")CarbonSession.threadSet("carbon.input.segments.default.carbon_table1", "*")
issueID:CARBONDATA-1702
type:Sub-task
changed files:
texts:Add Documentation for SEGMENT READING feature

issueID:CARBONDATA-1703
type:Bug
changed files:
texts:Difference in result set count of carbon and hive during select query with Null values in  IN expression
Incorrect result displays after applying select query.Steps to reproduce:1) Create table stored by carbondata and load data in it:a) CREATE TABLE uniqdata (CUST_ID int,CUST_NAME String,ACTIVE_EMUI_VERSION string, DOB timestamp, DOJ timestamp, BIGINT_COLUMN1 bigint,BIGINT_COLUMN2 bigint,DECIMAL_COLUMN1 decimal(30,10), DECIMAL_COLUMN2 decimal(36,10),Double_COLUMN1 double, Double_COLUMN2 double,INTEGER_COLUMN1 int) STORED BY 'org.apache.carbondata.format' TBLPROPERTIES ("TABLE_BLOCKSIZE"= "256 MB");b) LOAD DATA INPATH 'hdfs://localhost:54310/Data/uniqdata/2000_UniqData.csv' into table uniqdata OPTIONS('DELIMITER'=',' , 'QUOTECHAR'='"','BAD_RECORDS_ACTION'='FORCE','FILEHEADER'='CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1,Double_COLUMN2,INTEGER_COLUMN1');2) Create hive table:a) CREATE TABLE uniqdata_h (CUST_ID int,CUST_NAME String,ACTIVE_EMUI_VERSION string, DOB timestamp, DOJ timestamp, BIGINT_COLUMN1 bigint,BIGINT_COLUMN2 bigint,DECIMAL_COLUMN1 decimal(30,10), DECIMAL_COLUMN2 decimal(36,10),Double_COLUMN1 double, Double_COLUMN2 double,INTEGER_COLUMN1 int) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',';b) load data local inpath '/home/knoldus/Desktop/csv/TestData/Data/uniqdata/2000_UniqData.csv' into table uniqdata_h;3) Execute Query:a) SELECT CUST_ID,CUST_NAME,DOB,BIGINT_COLUMN1,DECIMAL_COLUMN1,Double_COLUMN2,INTEGER_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN2 from (select * from uniqdata) SUB_QRY WHERE (CUST_ID in (10020,10030,10032,10035,10040,10060,NULL) or INTEGER_COLUMN1 not in (1021,1031,1032,1033,NULL)) and (Double_COLUMN1 not in (1.12345674897976E10,NULL) or DECIMAL_COLUMN2 in (22345679921.1234000000,NULL));b) SELECT CUST_ID,CUST_NAME,DOB,BIGINT_COLUMN1,DECIMAL_COLUMN1,Double_COLUMN2,INTEGER_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN2 from (select * from uniqdata_h) SUB_QRY WHERE (CUST_ID in (10020,10030,10032,10035,10040,10060,NULL) or INTEGER_COLUMN1 not in (1021,1031,1032,1033,NULL)) and (Double_COLUMN1 not in (1.12345674897976E10,NULL) or DECIMAL_COLUMN2 in (22345679921.1234000000,NULL));4) Expected Result: both results should be same.5) Actual Result:a) carbondata table result:----------------------------------------------+ CUST_ID      CUST_NAME               DOB            BIGINT_COLUMN1       DECIMAL_COLUMN1         Double_COLUMN2      INTEGER_COLUMN1       DECIMAL_COLUMN2         Double_COLUMN2     -----------------------------------------------------------------------------------------------------------------------------------------------------------------------+ NULL       NULL                    NULL             NULL                     NULL                   NULL              NULL                     NULL                   NULL       NULL                    1233720368578    NULL                     NULL                   NULL              NULL                     NULL                   NULL       NULL                    NULL             NULL                     NULL                   NULL              NULL                     NULL                   NULL       NULL                    NULL             12345678901.1234000000   NULL                   NULL              NULL                     NULL                   NULL       NULL                    NULL             NULL                     NULL                   NULL              NULL                     NULL                   NULL       NULL                    NULL             NULL                     -1.12345674897976E10   NULL              NULL                     -1.12345674897976E10   NULL       NULL                    NULL             NULL                     NULL                   0                 NULL                     NULL                   NULL       NULL                    NULL             NULL                     NULL                   NULL              NULL                     NULL                   NULL       1970-01-01 11:00:03.0   NULL             NULL                     NULL                   NULL              NULL                     NULL                   NULL       NULL                    NULL             NULL                     NULL                   NULL              NULL                     NULL                   NULL      CUST_NAME_00000   NULL                    NULL             NULL                     NULL                   NULL              NULL                     NULL                   10020     CUST_NAME_01020   1972-10-17 01:00:03.0   123372037874     12345679921.1234000000   -1.12345674897976E10   1021              22345679921.1234000000   -1.12345674897976E10  -----------------------------------------------------------------------------------------------------------------------------------------------------------------------+12 rows selected (1.391 seconds)b) hive table result:----------------------------------------------+ CUST_ID      CUST_NAME               DOB            BIGINT_COLUMN1       DECIMAL_COLUMN1         Double_COLUMN2      INTEGER_COLUMN1       DECIMAL_COLUMN2         Double_COLUMN2     -----------------------------------------------------------------------------------------------------------------------------------------------------------------------+ 10020     CUST_NAME_01020   1972-10-17 01:00:03.0   123372037874     12345679921.1234000000   -1.12345674897976E10   1021              22345679921.1234000000   -1.12345674897976E10  -----------------------------------------------------------------------------------------------------------------------------------------------------------------------+1 row selected (0.408 seconds)
issueID:CARBONDATA-1704
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/resolverinfo/visitor/DictionaryColumnVisitor.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/resolverinfo/visitor/RangeDictionaryColumnVisitor.java
core/src/main/java/org/apache/carbondata/core/scan/filter/FilterUtil.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/resolverinfo/visitor/CustomTypeDictionaryVisitor.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/DimColumnExecuterFilterInfo.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/resolverinfo/visitor/RangeDirectDictionaryVisitor.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RestructureEvaluatorImpl.java
core/src/main/java/org/apache/carbondata/core/scan/filter/ColumnFilterInfo.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/ConditionalFilterResolverImpl.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/RowLevelRangeFilterResolverImpl.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/ExcludeFilterExecuterImpl.java
texts:Filter Optimization
If the include filter can give us more than 60% of the block data, then include filter can be converted to exclude filter
issueID:CARBONDATA-1706
type:Improvement
changed files:
texts:Making index merge DDL insensitive to the property

issueID:CARBONDATA-1707
type:Improvement
changed files:
texts:Log the taken time of each stream batch and fix StreamExample issue
Log the taken time of each stream batch and fix StreamExample issue
issueID:CARBONDATA-1708
type:Bug
changed files:
texts:Carbon1.3.0 Dictionary creation: By default dictionary is not created for string column
By default dictionary is not created for string column.  steps: 1: create a table with one column of string data type:create table check_dict(id int, name string)2: insert into check_dict select 1,'abc'3: describe the table to check dictionary column:desc formatted check_dict;4: Observe that name column is not DICTIONARY column.Issue: This is not as per document. Link: https://carbondata.apache.org/ddl-operation-on-carbondata.htmlExpected : Dictionary encoding is enabled by default for all String columns, and disabled for non-String columns
issueID:CARBONDATA-1709
type:Improvement
changed files:
texts:Support sort_columns option in dataframe writer
While creating carbondata table from dataframe, `sort_column` property is not specified, which by default will use all string columns as `sort_column`. So an option is required to specify it as below:```scaladf.write  .format("carbondata")  .options(...)  .option("sort_columns", "c1,c2")  .save```
issueID:CARBONDATA-171
type:Bug
changed files:integration/spark-common/src/main/java/org/apache/carbondata/spark/load/CarbonLoaderUtil.java
texts:Block distribution not proper when the number of active executors more than the node size

issueID:CARBONDATA-1711
type:Bug
changed files:
texts:Carbon1.3.0-DataMap - Show datamap  on table <par_table> does not work
0: jdbc:hive2://10.18.98.34:23040> create datamap agr_lineitem ON TABLE lineitem USING "org.apache.carbondata.datamap.AggregateDataMapHandler" as select L_RETURNFLAG,L_LINESTATUS,sum(L_QUANTITY),sum(L_EXTENDEDPRICE) from lineitem group by  L_RETURNFLAG, L_LINESTATUS;Error: java.lang.RuntimeException: Table &#91;lineitem_agr_lineitem&#93; already exists under database &#91;default&#93; (state=,code=0)0: jdbc:hive2://10.18.98.34:23040> show tables;--------------------------------------------------------+ database               tableName              isTemporary  --------------------------------------------------------+ default    flow_carbon_test4                  false         default    jl_r3                              false         default    lineitem                           false         default    lineitem_agr_lineitem              false         default    sensor_reading_blockblank_false    false         default    sensor_reading_blockblank_false1   false         default    sensor_reading_blockblank_false2   false         default    sensor_reading_false               false         default    sensor_reading_true                false         default    t1                                 false         default    t1_agg_t1                          false         default    tc4                                false         default    uniqdata                           false        --------------------------------------------------------+13 rows selected (0.04 seconds)0: jdbc:hive2://10.18.98.34:23040> show datamap on table lineitem;Error: java.lang.RuntimeException:BaseSqlParser>>>>missing 'FUNCTIONS' at 'on'(line 1, pos 13)== SQL ==show datamap on table lineitem-------------^^^CarbonSqlParser>>>> &#91;1.6&#93; failure: identifier matching regex (?i)SEGMENTS expectedshow datamap on table lineitem
issueID:CARBONDATA-1712
type:New Feature
changed files:
texts:Add Lucene Datamap
Add Lucene Datamap (index) into CarbonData, Support fuzzy text query .
issueID:CARBONDATA-1713
type:Bug
changed files:
texts:Carbon1.3.0-Pre-AggregateTable - Aggregate query on main table fails after creating pre-aggregate table when upper case used for column name
0: jdbc:hive2://10.18.98.34:23040> load data inpath "hdfs://hacluster/user/test/lineitem.tbl.1" into table lineitem options('DELIMITER'='|','FILEHEADER'='L_ORDERKEY,L_PARTKEY,L_SUPPKEY,L_LINENUMBER,L_QUANTITY,L_EXTENDEDPRICE,L_DISCOUNT,L_TAX,L_RETURNFLAG,L_LINESTATUS,L_SHIPDATE,L_COMMITDATE,L_RECEIPTDATE,L_SHIPINSTRUCT,L_SHIPMODE,L_COMMENT');Error: org.apache.spark.sql.catalyst.analysis.NoSuchTableException: Table or view 'lineitem' not found in database 'default'; (state=,code=0)0: jdbc:hive2://10.18.98.34:23040> create table if not exists lineitem(0: jdbc:hive2://10.18.98.34:23040> L_SHIPDATE string,0: jdbc:hive2://10.18.98.34:23040> L_SHIPMODE string,0: jdbc:hive2://10.18.98.34:23040> L_SHIPINSTRUCT string,0: jdbc:hive2://10.18.98.34:23040> L_RETURNFLAG string,0: jdbc:hive2://10.18.98.34:23040> L_RECEIPTDATE string,0: jdbc:hive2://10.18.98.34:23040> L_ORDERKEY string,0: jdbc:hive2://10.18.98.34:23040> L_PARTKEY string,0: jdbc:hive2://10.18.98.34:23040> L_SUPPKEY   string,0: jdbc:hive2://10.18.98.34:23040> L_LINENUMBER int,0: jdbc:hive2://10.18.98.34:23040> L_QUANTITY double,0: jdbc:hive2://10.18.98.34:23040> L_EXTENDEDPRICE double,0: jdbc:hive2://10.18.98.34:23040> L_DISCOUNT double,0: jdbc:hive2://10.18.98.34:23040> L_TAX double,0: jdbc:hive2://10.18.98.34:23040> L_LINESTATUS string,0: jdbc:hive2://10.18.98.34:23040> L_COMMITDATE string,0: jdbc:hive2://10.18.98.34:23040> L_COMMENT  string0: jdbc:hive2://10.18.98.34:23040> ) STORED BY 'org.apache.carbondata.format'0: jdbc:hive2://10.18.98.34:23040> TBLPROPERTIES ('table_blocksize'='128','NO_INVERTED_INDEX'='L_SHIPDATE,L_SHIPMODE,L_SHIPINSTRUCT,L_RETURNFLAG,L_RECEIPTDATE,L_ORDERKEY,L_PARTKEY,L_SUPPKEY','sort_columns'='');---------+ Result  ---------+---------+No rows selected (0.338 seconds)0: jdbc:hive2://10.18.98.34:23040> load data inpath "hdfs://hacluster/user/test/lineitem.tbl.1" into table lineitem options('DELIMITER'='|','FILEHEADER'='L_ORDERKEY,L_PARTKEY,L_SUPPKEY,L_LINENUMBER,L_QUANTITY,L_EXTENDEDPRICE,L_DISCOUNT,L_TAX,L_RETURNFLAG,L_LINESTATUS,L_SHIPDATE,L_COMMITDATE,L_RECEIPTDATE,L_SHIPINSTRUCT,L_SHIPMODE,L_COMMENT');---------+ Result  ---------+---------+No rows selected (48.634 seconds)0: jdbc:hive2://10.18.98.34:23040> create datamap agr_lineitem ON TABLE lineitem USING "org.apache.carbondata.datamap.AggregateDataMapHandler" as select L_RETURNFLAG,L_LINESTATUS,sum(L_QUANTITY),sum(L_EXTENDEDPRICE) from lineitem group by  L_RETURNFLAG, L_LINESTATUS;---------+ Result  ---------+---------+No rows selected (16.552 seconds)0: jdbc:hive2://10.18.98.34:23040> select L_RETURNFLAG,L_LINESTATUS,sum(L_QUANTITY),sum(L_EXTENDEDPRICE) from lineitem group by  L_RETURNFLAG, L_LINESTATUS;Error: org.apache.spark.sql.AnalysisException: Column doesnot exists in Pre Aggregate table; (state=,code=0)
issueID:CARBONDATA-1714
type:Bug
changed files:hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonTableInputFormat.java
texts:Carbon1.3.0-Alter Table - Select columns with is null and limit throws ArrayIndexOutOfBoundsException after multiple alter
Steps -Execute the below queries in sequence.create database test;use test;CREATE TABLE uniqdata111785 (CUST_ID int,CUST_NAME String,ACTIVE_EMUI_VERSION string, DOB timestamp, DOJ timestamp, BIGINT_COLUMN1 bigint,BIGINT_COLUMN2 bigint,DECIMAL_COLUMN1 decimal(30,10), DECIMAL_COLUMN2 decimal(36,10),Double_COLUMN1 double, Double_COLUMN2 double,INTEGER_COLUMN1 int) STORED BY 'org.apache.carbondata.format' TBLPROPERTIES('DICTIONARY_INCLUDE'='INTEGER_COLUMN1,CUST_ID');LOAD DATA INPATH 'hdfs://hacluster/chetan/2000_UniqData.csv' into table uniqdata111785 OPTIONS('DELIMITER'=',' , 'QUOTECHAR'='"','BAD_RECORDS_ACTION'='FORCE','FILEHEADER'='CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1,Double_COLUMN2,INTEGER_COLUMN1');alter table test.uniqdata111785 RENAME TO  uniqdata1117856;select * from test.uniqdata1117856 limit 100;ALTER TABLE test.uniqdata1117856 ADD COLUMNS (cust_name1 int);select * from test.uniqdata1117856 where cust_name1 is null limit 100;ALTER TABLE test.uniqdata1117856 DROP COLUMNS (cust_name1);select * from test.uniqdata1117856 where cust_name1 is null limit 100;ALTER TABLE test.uniqdata1117856 CHANGE CUST_ID CUST_ID BIGINT;select * from test.uniqdata1117856 where CUST_ID in (10013,10011,10000,10019) limit 10;ALTER  TABLE test.uniqdata1117856 ADD COLUMNS (a1 INT, b1 STRING) TBLPROPERTIES('DICTIONARY_EXCLUDE'='b1');select a1,b1 from test.uniqdata1117856  where a1 is null and b1 is null limit 100;Actual Issue : Select columns with is null and limit throws ArrayIndexOutOfBoundsException after multiple alter operations.0: jdbc:hive2://10.18.98.34:23040> select a1,b1 from test.uniqdata1117856  where a1 is null and b1 is null limit 100;Error: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 9.0 failed 4 times, most recent failure: Lost task 0.3 in stage 9.0 (TID 14, BLR1000014269, executor 2): java.lang.ArrayIndexOutOfBoundsException: 7        at org.apache.carbondata.core.scan.model.QueryModel.setDimAndMsrColumnNode(QueryModel.java:223)        at org.apache.carbondata.core.scan.model.QueryModel.processFilterExpression(QueryModel.java:172)        at org.apache.carbondata.core.scan.model.QueryModel.processFilterExpression(QueryModel.java:181)        at org.apache.carbondata.hadoop.util.CarbonInputFormatUtil.processFilterExpression(CarbonInputFormatUtil.java:118)        at org.apache.carbondata.hadoop.api.CarbonTableInputFormat.getQueryModel(CarbonTableInputFormat.java:791)        at org.apache.carbondata.spark.rdd.CarbonScanRDD.internalCompute(CarbonScanRDD.scala:250)        at org.apache.carbondata.spark.rdd.CarbonRDD.compute(CarbonRDD.scala:60)        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)        at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)        at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)        at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)        at org.apache.spark.scheduler.Task.run(Task.scala:99)        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)        at java.lang.Thread.run(Thread.java:745)Driver stacktrace: (state=,code=0)Expected : The select query should be successful after multiple alter operations.
issueID:CARBONDATA-1715
type:Bug
changed files:
texts:Carbon 1.3.0- Datamap BAD_RECORD_ACTION is not working as per the Document link.
By default the BAD_RECORDS_ACTION = FORCE should be written in "http://carbondata.apache.org/dml-operation-on-carbondata.html" document link but it is written as BAD_RECORDS_ACTION = FAIL.Expected result: BAD_RECORDS_ACTION = FORCE should be mentioned BAD RECORDS HANDLING section in document.Actual issue: BAD_RECORDS_ACTION = FAIL is present in the Document link.
issueID:CARBONDATA-1716
type:Bug
changed files:
texts:Carbon 1.3.0-Table Comment- When unset the header is not removed in describe formatted.
When UNSET, the header is not removed in the Describe Formatted.Create a table with comment.SET the commentUNSET the commentDescribe formattedExpected Result: When UNSET the header should be removed in the Describe Formatted.Actual result: When UNSET the header is not removed in the describe Formatted.
issueID:CARBONDATA-1717
type:Bug
changed files:
texts:remove sc broadcast to get hadoop configuration

issueID:CARBONDATA-1718
type:Bug
changed files:
texts:carbon.options.bad.records.action=REDIRECT configured in carbon.properties is not working as expected.
carbon.options.bad.records.action=REDIRECT configured in carbon.properties is not working as expected.When we set carbon properties as carbon.options.bad.records.action=REDIRECT , csv should be created. But Csv is not created.Expected Result: When we set carbon properties as carbon.options.bad.records.action=REDIRECT , Csv should be created.Actual Result: When we set carbon properties as carbon.options.bad.records.action=REDIRECT , Csv is not  created.
issueID:CARBONDATA-1719
type:Bug
changed files:
texts:Carbon1.3.0-Pre-AggregateTable - Empty segment is created when pre-aggr table created in parallel with table load, aggregate query returns no data
1. Create a tablecreate table if not exists lineitem3(L_SHIPDATE string,L_SHIPMODE string,L_SHIPINSTRUCT string,L_RETURNFLAG string,L_RECEIPTDATE string,L_ORDERKEY string,L_PARTKEY string,L_SUPPKEY   string,L_LINENUMBER int,L_QUANTITY double,L_EXTENDEDPRICE double,L_DISCOUNT double,L_TAX double,L_LINESTATUS string,L_COMMITDATE string,L_COMMENT  string) STORED BY 'org.apache.carbondata.format' TBLPROPERTIES ('table_blocksize'='128','NO_INVERTED_INDEX'='L_SHIPDATE,L_SHIPMODE,L_SHIPINSTRUCT,L_RETURNFLAG,L_RECEIPTDATE,L_ORDERKEY,L_PARTKEY,L_SUPPKEY','sort_columns'='');2. Run load queries and create pre-agg table queries in diff console:load data inpath "hdfs://hacluster/user/test/lineitem.tbl.1" into table lineitem3 options('DELIMITER'='|','FILEHEADER'='L_ORDERKEY,L_PARTKEY,L_SUPPKEY,L_LINENUMBER,L_QUANTITY,L_EXTENDEDPRICE,L_DISCOUNT,L_TAX,L_RETURNFLAG,L_LINESTATUS,L_SHIPDATE,L_COMMITDATE,L_RECEIPTDATE,L_SHIPINSTRUCT,L_SHIPMODE,L_COMMENT');create datamap agr_lineitem3 ON TABLE lineitem3 USING "org.apache.carbondata.datamap.AggregateDataMapHandler" as select L_RETURNFLAG,L_LINESTATUS,sum(L_QUANTITY),sum(L_EXTENDEDPRICE) from lineitem3 group by  L_RETURNFLAG, L_LINESTATUS;3.  Check table content using aggregate query:select l_returnflag,l_linestatus,sum(l_quantity),sum(l_extendedprice) from lineitem3 group by l_returnflag, l_linestatus;0: jdbc:hive2://10.18.98.34:23040> select l_returnflag,l_linestatus,sum(l_quantity),sum(l_extendedprice) from lineitem3 group by l_returnflag, l_linestatus;----------------------------------------------------------------- l_returnflag   l_linestatus   sum(l_quantity)   sum(l_extendedprice)  ----------------------------------------------------------------------------------------------------------------------------------No rows selected (1.258 seconds)HDFS data:BLR1000014307:/srv/spark2.2Bigdata/install/hadoop/datanode # bin/hadoop fs -ls /carbonstore/default/lineitem3_agr_lineitem3/Fact/Part0/Segment_0BLR1000014307:/srv/spark2.2Bigdata/install/hadoop/datanode # bin/hadoop fs -ls /carbonstore/default/lineitem3/Fact/Part0/Segment_0Found 27 itemsrw-rr-   2 root users      22148 2017-11-15 18:05 /carbonstore/default/lineitem3/Fact/Part0/Segment_0/1510740293106.carbonindexmergerw-rr-   2 root users   58353052 2017-11-15 18:05 /carbonstore/default/lineitem3/Fact/Part0/Segment_0/part-0-0_batchno0-0-1510740300247.carbondatarw-rr-   2 root users   58351680 2017-11-15 18:05 /carbonstore/default/lineitem3/Fact/Part0/Segment_0/part-0-0_batchno1-0-1510740300247.carbondatarw-rr-   2 root users   58364823 2017-11-15 18:05 /carbonstore/default/lineitem3/Fact/Part0/Segment_0/part-0-1_batchno0-0-1510740300247.carbondatarw-rr-   2 root users   58356303 2017-11-15 18:05 /carbonstore/default/lineitem3/Fact/Part0/Segment_0/part-0-2_batchno0-0-1510740300247.carbondatarw-rr-   2 root users   58342246 2017-11-15 18:05 /carbonstore/default/lineitem3/Fact/Part0/Segment_0/part-1-0_batchno0-0-1510740300247.carbondatarw-rr-   2 root users   58353186 2017-11-15 18:05 /carbonstore/default/lineitem3/Fact/Part0/Segment_0/part-1-0_batchno1-0-1510740300247.carbondatarw-rr-   2 root users   58352964 2017-11-15 18:05 /carbonstore/default/lineitem3/Fact/Part0/Segment_0/part-1-1_batchno0-0-1510740300247.carbondatarw-rr-   2 root users   58357183 2017-11-15 18:05 /carbonstore/default/lineitem3/Fact/Part0/Segment_0/part-1-2_batchno0-0-1510740300247.carbondatarw-rr-   2 root users   58345739 2017-11-15 18:05 /carbonstore/default/lineitem3/Fact/Part0/Segment_0/part-2-0_batchno0-0-1510740300247.carbondataYarn job stages:29 load data inpath "hdfs://hacluster/user/test/lineitem.tbl.1" into table lineitem3 options('DELIMITER'='|','FILEHEADER'='L_ORDERKEY,L_PARTKEY,L_SUPPKEY,L_LINENUMBER,L_QUANTITY,L_EXTENDEDPRICE,L_DISCOUNT,L_TAX,L_RETURNFLAG,L_LINESTATUS,L_SHIPDATE,L_COMMITDATE,L_RECEIPTDATE,L_SHIPINSTRUCT,L_SHIPMODE,L_COMMENT')collect at CommonUtil.scala:858 +details 2017/11/15 18:10:51 0.1 s 1/1    28 load data inpath "hdfs://hacluster/user/test/lineitem.tbl.1" into table lineitem3 options('DELIMITER'='|','FILEHEADER'='L_ORDERKEY,L_PARTKEY,L_SUPPKEY,L_LINENUMBER,L_QUANTITY,L_EXTENDEDPRICE,L_DISCOUNT,L_TAX,L_RETURNFLAG,L_LINESTATUS,L_SHIPDATE,L_COMMITDATE,L_RECEIPTDATE,L_SHIPINSTRUCT,L_SHIPMODE,L_COMMENT')collect at CarbonDataRDDFactory.scala:918 +details 2017/11/15 18:10:50 1 s 3/3   10.8 KB 27 load data inpath "hdfs://hacluster/user/test/lineitem.tbl.1" into table lineitem3 options('DELIMITER'='|','FILEHEADER'='L_ORDERKEY,L_PARTKEY,L_SUPPKEY,L_LINENUMBER,L_QUANTITY,L_EXTENDEDPRICE,L_DISCOUNT,L_TAX,L_RETURNFLAG,L_LINESTATUS,L_SHIPDATE,L_COMMITDATE,L_RECEIPTDATE,L_SHIPINSTRUCT,L_SHIPMODE,L_COMMENT')onEvent at OperationListenerBus.java:116 +details 2017/11/15 18:10:40 10 s 30/30 189.1 MB   10.8 KB26 load data inpath "hdfs://hacluster/user/test/lineitem.tbl.1" into table lineitem3 options('DELIMITER'='|','FILEHEADER'='L_ORDERKEY,L_PARTKEY,L_SUPPKEY,L_LINENUMBER,L_QUANTITY,L_EXTENDEDPRICE,L_DISCOUNT,L_TAX,L_RETURNFLAG,L_LINESTATUS,L_SHIPDATE,L_COMMITDATE,L_RECEIPTDATE,L_SHIPINSTRUCT,L_SHIPMODE,L_COMMENT')collect at CommonUtil.scala:858 +details 2017/11/15 18:10:34 0.4 s 1/1    25 load data inpath "hdfs://hacluster/user/test/lineitem.tbl.1" into table lineitem3 options('DELIMITER'='|','FILEHEADER'='L_ORDERKEY,L_PARTKEY,L_SUPPKEY,L_LINENUMBER,L_QUANTITY,L_EXTENDEDPRICE,L_DISCOUNT,L_TAX,L_RETURNFLAG,L_LINESTATUS,L_SHIPDATE,L_COMMITDATE,L_RECEIPTDATE,L_SHIPINSTRUCT,L_SHIPMODE,L_COMMENT')collect at CarbonDataRDDFactory.scala:1006
issueID:CARBONDATA-1720
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RangeValueFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelRangeLessThanFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelRangeLessThanEqualFilterExecuterImpl.java
texts:Wrong data displayed for <= filter for timestamp column(dictionary column)
Issue:<= filter is giving wrong results for timestamp dictioinary columnSteps to reproduce:(1) Create a table with a timestamp dictionary columncreate table t1(dob timestamp) stored by 'carbondata' TBLPROPERTIES ('DICTIONARY_INCLUDE'='dob')(2) Load data1970-01-01 05:30:00 (same value as 300 records)(3) Apply filter on tableselect count from t1 where dob=cast('1970-01-01 05:30:00' as timestamp);--------count(1)--------300     --------select count from t1 where dob<=cast('1970-01-01 05:30:00' as timestamp);--------count(1)--------1         --------Both the queries should give us the same results.Solution:In less than equal to filter, we are considering surrogate 2 as default value. But surrogate 1 is for default value.
issueID:CARBONDATA-1721
type:Bug
changed files:
texts:Wrong data displayed for <= filter for timestamp column(dictionary column)
Issue:<= filter is giving wrong results for timestamp dictioinary columnSteps to reproduce:(1) Create a table with a timestamp dictionary columncreate table t1(dob timestamp) stored by 'carbondata' TBLPROPERTIES ('DICTIONARY_INCLUDE'='dob')(2) Load data1970-01-01 05:30:00 (same value as 300 records)(3) Apply filter on tableselect count from t1 where dob=cast('1970-01-01 05:30:00' as timestamp);--------count(1)--------300     --------select count from t1 where dob<=cast('1970-01-01 05:30:00' as timestamp);--------count(1)--------1         --------Both the queries should give us the same results.Solution:In less than equal to filter, we are considering surrogate 2 as default value. But surrogate 1 is for default value.
issueID:CARBONDATA-1722
type:Bug
changed files:
texts:Wrong data displayed for <= filter for timestamp column(dictionary column)
Issue:<= filter is giving wrong results for timestamp dictioinary columnSteps to reproduce:(1) Create a table with a timestamp dictionary columncreate table t1(dob timestamp) stored by 'carbondata' TBLPROPERTIES ('DICTIONARY_INCLUDE'='dob')(2) Load data1970-01-01 05:30:00 (same value as 300 records)(3) Apply filter on tableselect count from t1 where dob=cast('1970-01-01 05:30:00' as timestamp);--------count(1)--------300     --------select count from t1 where dob<=cast('1970-01-01 05:30:00' as timestamp);--------count(1)--------1         --------Both the queries should give us the same results.Solution:In less than equal to filter, we are considering surrogate 2 as default value. But surrogate 1 is for default value.
issueID:CARBONDATA-1723
type:Bug
changed files:
texts:Wrong data displayed for <= filter for timestamp column(dictionary column)

issueID:CARBONDATA-1724
type:Bug
changed files:
texts:Wrong data displayed for <= filter for timestamp column(dictionary column)
Issue:<= filter is giving wrong results for timestamp dictioinary columnSteps to reproduce:(1) Create a table with a timestamp dictionary columncreate table t1(dob timestamp) stored by 'carbondata' TBLPROPERTIES ('DICTIONARY_INCLUDE'='dob')(2) Load data1970-01-01 05:30:00 (same value as 300 records)(3) Apply filter on tableselect count from t1 where dob=cast('1970-01-01 05:30:00' as timestamp);--------count(1)--------300     --------select count from t1 where dob<=cast('1970-01-01 05:30:00' as timestamp);--------count(1)--------1         --------Both the queries should give us the same results.Solution:In less than equal to filter, we are considering surrogate 2 as default value. But surrogate 1 is for default value.
issueID:CARBONDATA-1725
type:Bug
changed files:
texts:Wrong data displayed for <= filter for timestamp column(dictionary column)
Issue:<= filter is giving wrong results for timestamp dictioinary columnSteps to reproduce:(1) Create a table with a timestamp dictionary columncreate table t1(dob timestamp) stored by 'carbondata' TBLPROPERTIES ('DICTIONARY_INCLUDE'='dob')(2) Load data1970-01-01 05:30:00 (same value as 300 records)(3) Apply filter on tableselect count from t1 where dob=cast('1970-01-01 05:30:00' as timestamp);--------count(1)--------300     --------select count from t1 where dob<=cast('1970-01-01 05:30:00' as timestamp);--------count(1)--------1         --------Both the queries should give us the same results.Solution:In less than equal to filter, we are considering surrogate 2 as default value. But surrogate 1 is for default value.
issueID:CARBONDATA-1726
type:Bug
changed files:
texts:Carbon1.3.0-Streaming - Null pointer exception is thrown when streaming is started in spark-shell
Steps :// prepare csv file for batch loadingcd /srv/spark2.2Bigdata/install/hadoop/datanode/bin// generate streamSample.csv100000001,batch_1,city_1,0.1,school_1:school_11$20100000002,batch_2,city_2,0.2,school_2:school_22$30100000003,batch_3,city_3,0.3,school_3:school_33$40100000004,batch_4,city_4,0.4,school_4:school_44$50100000005,batch_5,city_5,0.5,school_5:school_55$60// put to hdfs /tmp/streamSample.csv./hadoop fs -put streamSample.csv /tmp// spark-beelinecd /srv/spark2.2Bigdata/install/spark/sparkJdbcbin/spark-submit --master yarn-client --executor-memory 10G --executor-cores 5 --driver-memory 5G --num-executors 3 --class org.apache.carbondata.spark.thriftserver.CarbonThriftServer /srv/spark2.2Bigdata/install/spark/sparkJdbc/carbonlib/carbondata_2.11-1.3.0-SNAPSHOT-shade-hadoop2.7.2.jar "hdfs://hacluster/user/sparkhive/warehouse"bin/beeline -u jdbc:hive2://10.18.98.34:23040CREATE TABLE stream_table(id INT,name STRING,city STRING,salary FLOAT)STORED BY 'carbondata'TBLPROPERTIES('streaming'='true', 'sort_columns'='name');LOAD DATA LOCAL INPATH 'hdfs://hacluster/chetan/streamSample.csv' INTO TABLE stream_table OPTIONS('HEADER'='false');// spark-shell cd /srv/spark2.2Bigdata/install/spark/sparkJdbcbin/spark-shell --master yarn-client --executor-memory 10G --executor-cores 5 --driver-memory 5G --num-executors 3 --jars /srv/spark2.2Bigdata/install/spark/sparkJdbc/carbonlib/carbondata_2.11-1.3.0-SNAPSHOT-shade-hadoop2.7.2.jarimport java.io.{File, PrintWriter}import java.net.ServerSocketimport org.apache.spark.sql.{CarbonEnv, SparkSession}import org.apache.spark.sql.hive.CarbonRelationimport org.apache.spark.sql.streaming.{ProcessingTime, StreamingQuery}import org.apache.carbondata.core.constants.CarbonCommonConstantsimport org.apache.carbondata.core.util.CarbonPropertiesimport org.apache.carbondata.core.util.path.{CarbonStorePath, CarbonTablePath}CarbonProperties.getInstance().addProperty(CarbonCommonConstants.CARBON_TIMESTAMP_FORMAT, "yyyy/MM/dd")import org.apache.spark.sql.CarbonSession._val carbonSession = SparkSession.  builder().  appName("StreamExample").  config("spark.sql.warehouse.dir", "hdfs://hacluster/user/sparkhive/warehouse").  config("javax.jdo.option.ConnectionURL", "jdbc:mysql://10.18.98.34:3306/sparksql?characterEncoding=UTF-8").  config("javax.jdo.option.ConnectionDriverName", "com.mysql.jdbc.Driver").  config("javax.jdo.option.ConnectionPassword", "huawei").  config("javax.jdo.option.ConnectionUserName", "sparksql").  getOrCreateCarbonSession()   carbonSession.sparkContext.setLogLevel("ERROR")carbonSession.sql("select * from stream_table").showdef writeSocket(serverSocket: ServerSocket): Thread = {  val thread = new Thread() {    override def run(): Unit = {      // wait for client to connection request and accept      val clientSocket = serverSocket.accept()      val socketWriter = new PrintWriter(clientSocket.getOutputStream())      var index = 0      for (_ <- 1 to 1000) {        // write 5 records per iteration        for (_ <- 0 to 100) {          index = index + 1          socketWriter.println(index.toString + ",name_" + index                               + ",city_" + index + "," + (index * 10000.00).toString +                               ",school_" + index + ":school_" + index + index + "$" + index)        }        socketWriter.flush()        Thread.sleep(2000)      }      socketWriter.close()      System.out.println("Socket closed")    }  }  thread.start()  thread}  def startStreaming(spark: SparkSession, tablePath: CarbonTablePath): Thread = {  val thread = new Thread() {    override def run(): Unit = {      var qry: StreamingQuery = null      try {        val readSocketDF = spark.readStream          .format("socket")          .option("host", "10.18.98.34")          .option("port", 7071)          .load()        // Write data from socket stream to carbondata file        qry = readSocketDF.writeStream          .format("carbondata")          .trigger(ProcessingTime("5 seconds"))          .option("checkpointLocation", tablePath.getStreamingCheckpointDir)          .option("tablePath", tablePath.getPath)          .start()        qry.awaitTermination()      } catch {        case _: InterruptedException =>          println("Done reading and writing streaming data")      } finally {        qry.stop()      }    }  }  thread.start()  thread}val streamTableName = s"stream_table"val carbonTable = CarbonEnv.getInstance(carbonSession).carbonMetastore.  lookupRelation(Some("default"), streamTableName)(carbonSession).asInstanceOf&#91;CarbonRelation&#93;.  tableMeta.carbonTableval tablePath = CarbonStorePath.getCarbonTablePath(carbonTable.getAbsoluteTableIdentifier)val serverSocket = new ServerSocket(7071)val socketThread = writeSocket(serverSocket)val streamingThread = startStreaming(carbonSession, tablePath)*Issue : There is a null pointer exception when streaming is started.When the executor and driver cores and memory is increased while launching the spark shell the issue still occurs.scala> import java.io.{File, PrintWriter}import java.io.{File, PrintWriter}scala> import java.net.ServerSocketimport java.net.ServerSocketscala>scala> import org.apache.spark.sql.{CarbonEnv, SparkSession}import org.apache.spark.sql.{CarbonEnv, SparkSession}scala> import org.apache.spark.sql.hive.CarbonRelationimport org.apache.spark.sql.hive.CarbonRelationscala> import org.apache.spark.sql.streaming.{ProcessingTime, StreamingQuery}import org.apache.spark.sql.streaming.{ProcessingTime, StreamingQuery}scala>scala> import org.apache.carbondata.core.constants.CarbonCommonConstantsimport org.apache.carbondata.core.constants.CarbonCommonConstantsscala> import org.apache.carbondata.core.util.CarbonPropertiesimport org.apache.carbondata.core.util.CarbonPropertiesscala> import org.apache.carbondata.core.util.path.{CarbonStorePath, CarbonTablePath}import org.apache.carbondata.core.util.path.{CarbonStorePath, CarbonTablePath}scala>scala> CarbonProperties.getInstance().addProperty(CarbonCommonConstants.CARBON_TIMESTAMP_FORMAT, "yyyy/MM/dd")res0: org.apache.carbondata.core.util.CarbonProperties = org.apache.carbondata.core.util.CarbonProperties@7212b28escala>scala> import org.apache.spark.sql.CarbonSession._import org.apache.spark.sql.CarbonSession._scala>scala> val carbonSession = SparkSession.   builder().   appName("StreamExample").   config("spark.sql.warehouse.dir", "hdfs://hacluster/user/sparkhive/warehouse").   config("javax.jdo.option.ConnectionURL", "jdbc:mysql://10.18.98.34:3306/sparksql?characterEncoding=UTF-8").   config("javax.jdo.option.ConnectionDriverName", "com.mysql.jdbc.Driver").   config("javax.jdo.option.ConnectionPassword", "huawei").   config("javax.jdo.option.ConnectionUserName", "sparksql").   getOrCreateCarbonSession()carbonSession: org.apache.spark.sql.SparkSession = org.apache.spark.sql.CarbonSession@7593716dscala> carbonSession.sparkContext.setLogLevel("ERROR")scala>scala> carbonSession.sql("select * from stream_table").show----------------------+       id   name  citysalary----------------------+100000001batch_1city_1   0.1100000002batch_2city_2   0.2100000003batch_3city_3   0.3100000004batch_4city_4   0.4100000005batch_5city_5   0.5----------------------+scala> def writeSocket(serverSocket: ServerSocket): Thread = {   val thread = new Thread() {     override def run(): Unit = {       // wait for client to connection request and accept       val clientSocket = serverSocket.accept()       val socketWriter = new PrintWriter(clientSocket.getOutputStream())       var index = 0       for (_ <- 1 to 1000) {         // write 5 records per iteration         for (_ <- 0 to 100) {     |           index = index + 1     |           socketWriter.println(index.toString + ",name_" + index     |                                + ",city_" + index + "," + (index * 10000.00).toString +     |                                ",school_" + index + ":school_" + index + index + "$" + index)     |         }         socketWriter.flush()         Thread.sleep(2000)       }       socketWriter.close()       System.out.println("Socket closed")     }   }   thread.start()   thread }writeSocket: (serverSocket: java.net.ServerSocket)Threadscala> def startStreaming(spark: SparkSession, tablePath: CarbonTablePath): Thread = {   val thread = new Thread() {     override def run(): Unit = {       var qry: StreamingQuery = null       try {     |         val readSocketDF = spark.readStream     |           .format("socket")     |           .option("host", "10.18.98.34")     |           .option("port", 7071)     |           .load()     |     |         // Write data from socket stream to carbondata file     |         qry = readSocketDF.writeStream     |           .format("carbondata")     |           .trigger(ProcessingTime("5 seconds"))     |           .option("checkpointLocation", tablePath.getStreamingCheckpointDir)     |           .option("tablePath", tablePath.getPath)     |           .start()     |     |         qry.awaitTermination()     |       } catch {     |         case _: InterruptedException =>     |           println("Done reading and writing streaming data")     |       } finally {     |         qry.stop()     |       }     }   }   thread.start()   thread }startStreaming: (spark: org.apache.spark.sql.SparkSession, tablePath: org.apache.carbondata.core.util.path.CarbonTablePath)Threadscala>scala> val streamTableName = s"stream_table"streamTableName: String = stream_tablescala>scala> val carbonTable = CarbonEnv.getInstance(carbonSession).carbonMetastore.   lookupRelation(Some("default"), streamTableName)(carbonSession).asInstanceOf&#91;CarbonRelation&#93;.   tableMeta.carbonTablecarbonTable: org.apache.carbondata.core.metadata.schema.table.CarbonTable = org.apache.carbondata.core.metadata.schema.table.CarbonTable@62cf8fdascala>scala> val tablePath = CarbonStorePath.getCarbonTablePath(carbonTable.getAbsoluteTableIdentifier)tablePath: org.apache.carbondata.core.util.path.CarbonTablePath = hdfs://hacluster/user/hive/warehouse/carbon.store/default/stream_tablescala>scala> val serverSocket = new ServerSocket(7071)serverSocket: java.net.ServerSocket = ServerSocket&#91;addr=0.0.0.0/0.0.0.0,localport=7071&#93;scala> val socketThread = writeSocket(serverSocket)socketThread: Thread = Thread&#91;Thread-103,5,main&#93;scala> val streamingThread = startStreaming(carbonSession, tablePath)streamingThread: Thread = Thread&#91;Thread-104,5,main&#93;***scala> Exception in thread "Thread-104" java.lang.NullPointerException        at $line29.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$anon$1.run(<console>:59)***Expected : The startstreaming should not throw exception and should be successful.
issueID:CARBONDATA-1728
type:Bug
changed files:
texts:Carbon1.3.0- DB creation external path : Delete data with select in where clause not successful for large data
Steps :0: jdbc:hive2://10.18.98.34:23040> create database test_db1 location 'hdfs://hacluster/user/test1';---------+ Result  ---------+---------+No rows selected (0.032 seconds)0: jdbc:hive2://10.18.98.34:23040> use test_db1;---------+ Result  ---------+---------+No rows selected (0.01 seconds)0: jdbc:hive2://10.18.98.34:23040> create table if not exists ORDERS(O_ORDERDATE string,O_ORDERPRIORITY string,O_ORDERSTATUS string,O_ORDERKEY string,O_CUSTKEY string,O_TOTALPRICE double,O_CLERK string,O_SHIPPRIORITY int,O_COMMENT string) STORED BY 'org.apache.carbondata.format' TBLPROPERTIES ('table_blocksize'='128');---------+ Result  ---------+---------+No rows selected (0.174 seconds)0: jdbc:hive2://10.18.98.34:23040> load data inpath "hdfs://hacluster/chetan/orders.tbl.1" into table ORDERS options('DELIMITER'='|','FILEHEADER'='O_ORDERKEY,O_CUSTKEY,O_ORDERSTATUS,O_TOTALPRICE,O_ORDERDATE,O_ORDERPRIORITY,O_CLERK,O_SHIPPRIORITY,O_COMMENT','batch_sort_size_inmb'='32');---------+ Result  ---------+---------+No rows selected (27.421 seconds)0: jdbc:hive2://10.18.98.34:23040> create table h_orders as select * from orders;---------+ Result  ---------+---------+No rows selected (9.779 seconds)0: jdbc:hive2://10.18.98.34:23040> Delete from test_db1.orders a where exists (select 1 from test_db1.h_orders b where b.o_ORDERKEY=a.O_ORDERKEY);---------+ Result  ---------+---------+No rows selected (48.998 seconds)select count from test_db1.orders;Actual Issue : Select count displays shows all records present which means the records are not deleted.0: jdbc:hive2://10.18.98.34:23040> select count from test_db1.orders;-----------+ count(1)  -----------+ 7500000   -----------+1 row selected (7.967 seconds)This indicates Delete data with select in where clause not successful for large data. Expected : The Delete data with select in where clause should be successful for large data. The select count should return 0 records which indicates that the records are deleted successfully.
issueID:CARBONDATA-1729
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/datastore/impl/FileFactory.java
texts:The compatibility issue with hadoop <= 2.6 and 2.7
On branch master, when compiled with hadoop <= 2.6, it failed, the root cause is using new API FileSystem.truncate which is added in hadoop 2.7. It needs to implement a method called 'truncate' in file 'FileFactory.java' to support hadoop <= 2.6.
issueID:CARBONDATA-173
type:Bug
changed files:
texts:Error info is not proper when measure use COLUMNDICT
in CREATE TABLE IF NOT EXISTS t3           (ID Int, date Timestamp, country String,           name String, phonetype String, serialname String, salary Int)           STORED BY 'carbondata',then user use LOAD DATA LOCAL INPATH '$testData' into table t3 OPTIONS('COLUMNDICT'='id:FilePath'), because the ID is measure, so exception should be thrown to indicate that measure can not use COLUMNDICT.
issueID:CARBONDATA-1731
type:Bug
changed files:
texts:Carbon1.3.0- DB creation external path: Update fails incorrectly with error for table created in external db location
Steps :0: jdbc:hive2://10.18.98.34:23040> drop database if exists test_db1 cascade;---------+ Result  ---------+---------+No rows selected (0.279 seconds)0: jdbc:hive2://10.18.98.34:23040> create database test_db1 location 'hdfs://hacluster/user/test1';---------+ Result  ---------+---------+No rows selected (0.04 seconds)0: jdbc:hive2://10.18.98.34:23040> use test_db1;---------+ Result  ---------+---------+No rows selected (0.011 seconds)0: jdbc:hive2://10.18.98.34:23040> create table if not exists ORDERS(O_ORDERDATE string,O_ORDERPRIORITY string,O_ORDERSTATUS string,O_ORDERKEY string,O_CUSTKEY string,O_TOTALPRICE double,O_CLERK string,O_SHIPPRIORITY int,O_COMMENT string) STORED BY 'org.apache.carbondata.format' TBLPROPERTIES ('table_blocksize'='128');---------+ Result  ---------+---------+No rows selected (0.15 seconds)0: jdbc:hive2://10.18.98.34:23040> load data inpath "hdfs://hacluster/chetan/orders.tbl.1" into table ORDERS options('DELIMITER'='|','FILEHEADER'='O_ORDERKEY,O_CUSTKEY,O_ORDERSTATUS,O_TOTALPRICE,O_ORDERDATE,O_ORDERPRIORITY,O_CLERK,O_SHIPPRIORITY,O_COMMENT','batch_sort_size_inmb'='32');---------+ Result  ---------+---------+No rows selected (23.228 seconds)0: jdbc:hive2://10.18.98.34:23040> update test_Db1.ORDERS set (o_comment) = ('yyy');Issue : Update fails incorrectly with error for table created in external db location.0: jdbc:hive2://10.18.98.34:23040> update test_Db1.ORDERS set (o_comment) = ('yyy');Error: java.lang.RuntimeException: Update operation failed. Multiple input rows matched for same row. (state=,code=0)Expected : The update should be success for table created in external db location.
issueID:CARBONDATA-1732
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/datastore/impl/FileFactory.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/SchemaReader.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
core/src/main/java/org/apache/carbondata/core/util/path/HDFSLeaseUtils.java
core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
texts:Add S3 support in FileFactory
Add S3 file prefix support to FileFactory
issueID:CARBONDATA-1733
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/statusmanager/LoadMetadataDetails.java
texts:While load is in progress, Show segments is throwing NPE
Scenario:Concurrent execution of load and show segments.Show segments is thorwing NullPointerException
issueID:CARBONDATA-1734
type:Improvement
changed files:processing/src/main/java/org/apache/carbondata/processing/loading/constants/DataLoadProcessorConstants.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
processing/src/main/java/org/apache/carbondata/processing/loading/csvinput/CSVInputFormat.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonLoadOptionConstants.java
processing/src/main/java/org/apache/carbondata/processing/loading/DataLoadProcessBuilder.java
processing/src/main/java/org/apache/carbondata/processing/loading/model/CarbonLoadModel.java
texts:Ignore empty line while reading CSV
Ignore empty line while reading CSV file in LOAD
issueID:CARBONDATA-1735
type:Bug
changed files:
texts:Carbon1.3.0 Load: Segment created during load is not marked for delete if beeline session is closed  while load is still in progress
Load: Segment created during load is not marked for delete if beeline session is closed  while load is still in progress.Steps: 1: Create a table with dictionary include2: Start a load job3: close the beeline session when global dictionary generation job is still in progress.4: Observe that global dictionary generation job is completed but next job is not triggered.5:  Also observe that table status file is not updated and status of job is still in progress.6: show segment  will show this segment with status as in progress.Expected behaviour: Either job should be completed or load should fail and segment should be marked for delete.
issueID:CARBONDATA-1736
type:Bug
changed files:
texts:Carbon1.3.0-Pre-AggregateTable -Query from segment set is not effective when pre-aggregate table is present
1. Create a tablecreate table if not exists lineitem1(L_SHIPDATE string,L_SHIPMODE string,L_SHIPINSTRUCT string,L_RETURNFLAG string,L_RECEIPTDATE string,L_ORDERKEY string,L_PARTKEY string,L_SUPPKEY   string,L_LINENUMBER int,L_QUANTITY double,L_EXTENDEDPRICE double,L_DISCOUNT double,L_TAX double,L_LINESTATUS string,L_COMMITDATE string,L_COMMENT  string) STORED BY 'org.apache.carbondata.format' TBLPROPERTIES ('table_blocksize'='128','NO_INVERTED_INDEX'='L_SHIPDATE,L_SHIPMODE,L_SHIPINSTRUCT,L_RETURNFLAG,L_RECEIPTDATE,L_ORDERKEY,L_PARTKEY,L_SUPPKEY','sort_columns'='');2. Run load :load data inpath "hdfs://hacluster/user/test/lineitem.tbl.1" into table lineitem1 options('DELIMITER'='|','FILEHEADER'='L_ORDERKEY,L_PARTKEY,L_SUPPKEY,L_LINENUMBER,L_QUANTITY,L_EXTENDEDPRICE,L_DISCOUNT,L_TAX,L_RETURNFLAG,L_LINESTATUS,L_SHIPDATE,L_COMMITDATE,L_RECEIPTDATE,L_SHIPINSTRUCT,L_SHIPMODE,L_COMMENT');3. create pre-agg table create datamap agr_lineitem3 ON TABLE lineitem3 USING "org.apache.carbondata.datamap.AggregateDataMapHandler" as select L_RETURNFLAG,L_LINESTATUS,sum(L_QUANTITY),sum(L_EXTENDEDPRICE) from lineitem3 group by  L_RETURNFLAG, L_LINESTATUS;3.  Check table content using aggregate query:select l_returnflag,l_linestatus,sum(l_quantity),sum(l_extendedprice) from lineitem3 group by l_returnflag, l_linestatus;------------------------------------------------------------------ l_returnflag   l_linestatus   sum(l_quantity)    sum(l_extendedprice)  ------------------------------------------------------------------ N              F              4913382.0         7.369901176949993E9     A              F              1.88818373E8      2.8310705145736383E11   N              O              3.82400594E8      5.734650756707479E11    R              F              1.88960009E8      2.833523780876951E11   ------------------------------------------------------------------4 rows selected (1.568 seconds)4. Load one more time:load data inpath "hdfs://hacluster/user/test/lineitem.tbl.1" into table lineitem1 options('DELIMITER'='|','FILEHEADER'='L_ORDERKEY,L_PARTKEY,L_SUPPKEY,L_LINENUMBER,L_QUANTITY,L_EXTENDEDPRICE,L_DISCOUNT,L_TAX,L_RETURNFLAG,L_LINESTATUS,L_SHIPDATE,L_COMMITDATE,L_RECEIPTDATE,L_SHIPINSTRUCT,L_SHIPMODE,L_COMMENT');5.  Check table content using aggregate query:select l_returnflag,l_linestatus,sum(l_quantity),sum(l_extendedprice) from lineitem3 group by l_returnflag, l_linestatus;------------------------------------------------------------------ l_returnflag   l_linestatus   sum(l_quantity)    sum(l_extendedprice)  ------------------------------------------------------------------ N              F              9826764.0         1.4739802353899986E10   A              F              3.77636746E8      5.662141029147278E11    N              O              7.64801188E8      1.1469301513414958E12   R              F              3.77920018E8      5.667047561753901E11   ------------------------------------------------------------------6. Set query from segment 1:0: jdbc:hive2://10.18.98.48:23040> set carbon.input.segments.test_db1.lilneitem1=1;--------------------------------------------------                    key                      value  -------------------------------------------------- carbon.input.segments.test_db1.lilneitem1   1      --------------------------------------------------7. Check table content using aggregate query:select l_returnflag,l_linestatus,sum(l_quantity),sum(l_extendedprice) from lineitem3 group by l_returnflag, l_linestatus;Expected: It should return the values from segment 1 alone.Actual : : It returns values from both segments------------------------------------------------------------------ l_returnflag   l_linestatus   sum(l_quantity)    sum(l_extendedprice)  ------------------------------------------------------------------ N              F              9826764.0         1.4739802353899986E10   A              F              3.77636746E8      5.662141029147278E11    N              O              7.64801188E8      1.1469301513414958E12   R              F              3.77920018E8      5.667047561753901E11   ------------------------------------------------------------------
issueID:CARBONDATA-1737
type:Bug
changed files:
texts:Carbon1.3.0-Pre-AggregateTable - Pre-aggregate table loads partially when segment filter is set on the main table
1. Create a tablecreate table if not exists lineitem2(L_SHIPDATE string,L_SHIPMODE string,L_SHIPINSTRUCT string,L_RETURNFLAG string,L_RECEIPTDATE string,L_ORDERKEY string,L_PARTKEY string,L_SUPPKEY   string,L_LINENUMBER int,L_QUANTITY double,L_EXTENDEDPRICE double,L_DISCOUNT double,L_TAX double,L_LINESTATUS string,L_COMMITDATE string,L_COMMENT  string) STORED BY 'org.apache.carbondata.format' TBLPROPERTIES ('table_blocksize'='128','NO_INVERTED_INDEX'='L_SHIPDATE,L_SHIPMODE,L_SHIPINSTRUCT,L_RETURNFLAG,L_RECEIPTDATE,L_ORDERKEY,L_PARTKEY,L_SUPPKEY','sort_columns'='');2. Load 2 times to create 2 segments load data inpath "hdfs://hacluster/user/test/lineitem.tbl.5" into table lineitem2 options('DELIMITER'='|','FILEHEADER'='L_ORDERKEY,L_PARTKEY,L_SUPPKEY,L_LINENUMBER,L_QUANTITY,L_EXTENDEDPRICE,L_DISCOUNT,L_TAX,L_RETURNFLAG,L_LINESTATUS,L_SHIPDATE,L_COMMITDATE,L_RECEIPTDATE,L_SHIPINSTRUCT,L_SHIPMODE,L_COMMENT');3. Check the table content without setting any filter:select l_returnflag,l_linestatus,sum(l_quantity),sum(l_extendedprice) from lineitem2 group by l_returnflag, l_linestatus;------------------------------------------------------------------ l_returnflag   l_linestatus   sum(l_quantity)    sum(l_extendedprice)  ------------------------------------------------------------------ N              F              327800.0          4.913876776200004E8     A              F              1.263625E7        1.893851542524009E10    N              O              2.5398626E7       3.810981608977967E10    R              F              1.2643878E7       1.8948524305619976E10  ------------------------------------------------------------------4. Set segment filter on the main table:set carbon.input.segments.test_db1.lineitem2=1;-------------------------------------------------                    key                     value  ------------------------------------------------- carbon.input.segments.test_db1.lineitem2   1      -------------------------------------------------5. Create pre-aggregate table create datamap agr_lineitem2 ON TABLE lineitem2 USING "org.apache.carbondata.datamap.AggregateDataMapHandler" as select L_RETURNFLAG,L_LINESTATUS,sum(L_QUANTITY),sum(L_EXTENDEDPRICE) from lineitem2 group by  L_RETURNFLAG, L_LINESTATUS;6. Check table content: select l_returnflag,l_linestatus,sum(l_quantity),sum(l_extendedprice) from lineitem2 group by l_returnflag, l_linestatus;------------------------------------------------------------------ l_returnflag   l_linestatus   sum(l_quantity)    sum(l_extendedprice)  ------------------------------------------------------------------ N              F              163900.0          2.4569383881000024E8    A              F              6318125.0         9.469257712620043E9     N              O              1.2699313E7       1.9054908044889835E10   R              F              6321939.0         9.474262152809986E9    ------------------------------------------------------------------7. remove the filter on segment0: jdbc:hive2://10.18.98.48:23040> reset;8. Check the table conent: select l_returnflag,l_linestatus,sum(l_quantity),sum(l_extendedprice) from lineitem2 group by l_returnflag, l_linestatus;------------------------------------------------------------------ l_returnflag   l_linestatus   sum(l_quantity)    sum(l_extendedprice)  ------------------------------------------------------------------ N              F              163900.0          2.4569383881000024E8    A              F              6318125.0         9.469257712620043E9     N              O              1.2699313E7       1.9054908044889835E10   R              F              6321939.0         9.474262152809986E9    ------------------------------------------------------------------4 rows selected (2.341 seconds)9. Load one more time:10. Check table contentselect l_returnflag,l_linestatus,sum(l_quantity),sum(l_extendedprice) from lineitem2 group by l_returnflag, l_linestatus;------------------------------------------------------------------ l_returnflag   l_linestatus   sum(l_quantity)    sum(l_extendedprice)  ------------------------------------------------------------------ N              F              327800.0          4.913876776200005E8     A              F              1.263625E7        1.8938515425240086E10   N              O              2.5398626E7       3.810981608977967E10    R              F              1.2643878E7       1.8948524305619972E10  ------------------------------------------------------------------4 rows selected (0.936 seconds)Expected:: one of these should have been the behavour:1.Ignore segment filter and use all segments for pre-aggregate load. At the time of query run, if segment filter is set then ignore the pre-aggr table and fetch data from main table. (Preferred)Or2. Reject pre-aggregate creation when segment filter is set or vis-a-versa.Actual: Partial data returned
issueID:CARBONDATA-1738
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
texts:Block direct load on pre-aggregate table
The user should not be able to execute load directly on the pre-aggregate table therefore load and insert have to be blocked.
issueID:CARBONDATA-1739
type:Improvement
changed files:processing/src/main/java/org/apache/carbondata/processing/merger/CarbonCompactionUtil.java
processing/src/main/java/org/apache/carbondata/processing/merger/CarbonCompactionExecutor.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/CarbonTable.java
streaming/src/main/java/org/apache/carbondata/streaming/CarbonStreamRecordReader.java
core/src/main/java/org/apache/carbondata/core/mutate/CarbonUpdateUtil.java
processing/src/main/java/org/apache/carbondata/processing/store/writer/AbstractFactDataWriter.java
core/src/main/java/org/apache/carbondata/core/metadata/CarbonMetadata.java
processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactDataHandlerColumnar.java
processing/src/main/java/org/apache/carbondata/processing/util/CarbonLoaderUtil.java
core/src/main/java/org/apache/carbondata/core/util/CarbonProperties.java
processing/src/main/java/org/apache/carbondata/processing/merger/CarbonDataMergerUtil.java
processing/src/main/java/org/apache/carbondata/processing/util/CarbonDataProcessorUtil.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonTableInputFormat.java
hadoop/src/main/java/org/apache/carbondata/hadoop/util/CarbonInputFormatUtil.java
core/src/main/java/org/apache/carbondata/core/scan/model/QueryModel.java
processing/src/main/java/org/apache/carbondata/processing/loading/DataLoadProcessBuilder.java
streaming/src/main/java/org/apache/carbondata/streaming/CarbonStreamRecordWriter.java
streaming/src/main/java/org/apache/carbondata/streaming/segment/StreamSegment.java
integration/presto/src/main/java/org/apache/carbondata/presto/impl/CarbonTableReader.java
texts:Clean up store path interface
There are many getStorePath API, it should be unified in one place
issueID:CARBONDATA-174
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
texts:When hadoop.tmp.dir configured incorrectly, hdfs lock of carbon would throw exception.
1. The hdfs lock file for one table should be put inside this table's store path, this is more reasonable, and if the store path is not set, then we put it into hadoop.tmp.dir. For example: if the store path of carbon on hdfs is /user/hive/warehouse/carbon.store, then the lock file for this table woud be: /user/hive/warehouse/carbon.store/default/table_name/meta.lock2. This bug is found by : Some times, hadoop configured wrong hadoop.tmp.dir, hadoop can still work normally, but carbon's hdfs lock can not work normally, it will throws exception: "Table is locked for updation. Please try after some time".
issueID:CARBONDATA-1740
type:Bug
changed files:
texts:Carbon1.3.0-Pre-AggregateTable - Query plan exception for aggregate query with order by when main table is having pre-aggregate table
lineitem3: has a pre-aggregate table select l_returnflag,l_linestatus,sum(l_quantity),sum(l_extendedprice) from lineitem3 group by l_returnflag, l_linestatus order by l_returnflag, l_linestatus;Error: org.apache.spark.sql.AnalysisException: expression '`lineitem3_l_returnflag`' is neither present in the group by, nor is it an aggregate function. Add to group by or wrap in first() (or first_value) if you don't care which value you get.;;Project l_returnflag#2356, l_linestatus#2366, sum(l_quantity)#2791, sum(l_extendedprice)#2792+- Sort aggOrder#2795 ASC NULLS FIRST, aggOrder#2796 ASC NULLS FIRST, true   +- !Aggregate l_returnflag#2356, l_linestatus#2366, l_returnflag#2356, l_linestatus#2366, sum(l_quantity#2362) AS sum(l_quantity)#2791, sum(l_extendedprice#2363) AS sum(l_extendedprice)#2792, lineitem3_l_returnflag#2341 AS aggOrder#2795, lineitem3_l_linestatus#2342 AS aggOrder#2796      +- SubqueryAlias lineitem3         +- RelationL_SHIPDATE#2353,L_SHIPMODE#2354,L_SHIPINSTRUCT#2355,L_RETURNFLAG#2356,L_RECEIPTDATE#2357,L_ORDERKEY#2358,L_PARTKEY#2359,L_SUPPKEY#2360,L_LINENUMBER#2361,L_QUANTITY#2362,L_EXTENDEDPRICE#2363,L_DISCOUNT#2364,L_TAX#2365,L_LINESTATUS#2366,L_COMMITDATE#2367,L_COMMENT#2368 CarbonDatasourceHadoopRelation [ Database name :test_db1, Table name :lineitem3, Schema :Some(StructType(StructField(L_SHIPDATE,StringType,true), StructField(L_SHIPMODE,StringType,true), StructField(L_SHIPINSTRUCT,StringType,true), StructField(L_RETURNFLAG,StringType,true), StructField(L_RECEIPTDATE,StringType,true), StructField(L_ORDERKEY,StringType,true), StructField(L_PARTKEY,StringType,true), StructField(L_SUPPKEY,StringType,true), StructField(L_LINENUMBER,IntegerType,true), StructField(L_QUANTITY,DoubleType,true), StructField(L_EXTENDEDPRICE,DoubleType,true), StructField(L_DISCOUNT,DoubleType,true), StructField(L_TAX,DoubleType,true), StructField(L_LINESTATUS,StringType,true), StructField(L_COMMITDATE,StringType,true), StructField(L_COMMENT,StringType,true))) ] (state=,code=0)lineitem4: no pre-aggregate table createdselect l_returnflag,l_linestatus,sum(l_quantity),sum(l_extendedprice) from lineitem4 group by l_returnflag, l_linestatus order by l_returnflag, l_linestatus;------------------------------------------------------------------ l_returnflag   l_linestatus   sum(l_quantity)    sum(l_extendedprice)  ------------------------------------------------------------------ A              F              1.263625E7        1.8938515425239815E10   N              F              327800.0          4.913876776200002E8     N              O              2.5398626E7       3.810981608977963E10    R              F              1.2643878E7       1.8948524305619884E10  ------------------------------------------------------------------Expected:: aggregate query with order by should run fineActual: aggregate query with order failed
issueID:CARBONDATA-1741
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
texts:Remove AKSK in Log
In order to provide better security, AKSK credential information should be removed in log
issueID:CARBONDATA-1742
type:Bug
changed files:
texts:Fix NullPointerException in SegmentStatusManager
when loadFolderDetailsArray is null ,there is NullPointerException. We should fix it.
issueID:CARBONDATA-1743
type:Bug
changed files:
texts:Carbon1.3.0-Pre-AggregateTable - Query returns no value if run at the time of pre-aggregate table creation
Steps:1. Create table and load with large datacreate table if not exists lineitem4(L_SHIPDATE string,L_SHIPMODE string,L_SHIPINSTRUCT string,L_RETURNFLAG string,L_RECEIPTDATE string,L_ORDERKEY string,L_PARTKEY string,L_SUPPKEY   string,L_LINENUMBER int,L_QUANTITY double,L_EXTENDEDPRICE double,L_DISCOUNT double,L_TAX double,L_LINESTATUS string,L_COMMITDATE string,L_COMMENT  string) STORED BY 'org.apache.carbondata.format' TBLPROPERTIES ('table_blocksize'='128','NO_INVERTED_INDEX'='L_SHIPDATE,L_SHIPMODE,L_SHIPINSTRUCT,L_RETURNFLAG,L_RECEIPTDATE,L_ORDERKEY,L_PARTKEY,L_SUPPKEY','sort_columns'='');load data inpath "hdfs://hacluster/user/test/lineitem.tbl.1" into table lineitem4 options('DELIMITER'='|','FILEHEADER'='L_ORDERKEY,L_PARTKEY,L_SUPPKEY,L_LINENUMBER,L_QUANTITY,L_EXTENDEDPRICE,L_DISCOUNT,L_TAX,L_RETURNFLAG,L_LINESTATUS,L_SHIPDATE,L_COMMITDATE,L_RECEIPTDATE,L_SHIPINSTRUCT,L_SHIPMODE,L_COMMENT');2. Create a pre-aggregate table create datamap agr_lineitem4 ON TABLE lineitem4 USING "org.apache.carbondata.datamap.AggregateDataMapHandler" as select L_RETURNFLAG,L_LINESTATUS,sum(L_QUANTITY),sum(L_EXTENDEDPRICE) from lineitem4 group by  L_RETURNFLAG, L_LINESTATUS;3. Run aggregate query at the same time select l_returnflag,l_linestatus,sum(l_quantity),sum(l_extendedprice) from lineitem4 group by l_returnflag, l_linestatus;Expected:: aggregate query should fetch data either from main table or pre-aggregate table.Actual: aggregate query does not return data until the pre-aggregate table is created0: jdbc:hive2://10.18.98.48:23040> select l_returnflag,l_linestatus,sum(l_quantity),sum(l_extendedprice) from lineitem4 group by l_returnflag, l_linestatus;----------------------------------------------------------------- l_returnflag   l_linestatus   sum(l_quantity)   sum(l_extendedprice)  ----------------------------------------------------------------------------------------------------------------------------------No rows selected (1.74 seconds)0: jdbc:hive2://10.18.98.48:23040> select l_returnflag,l_linestatus,sum(l_quantity),sum(l_extendedprice) from lineitem4 group by l_returnflag, l_linestatus;----------------------------------------------------------------- l_returnflag   l_linestatus   sum(l_quantity)   sum(l_extendedprice)  ----------------------------------------------------------------------------------------------------------------------------------No rows selected (0.746 seconds)0: jdbc:hive2://10.18.98.48:23040> select l_returnflag,l_linestatus,sum(l_quantity),sum(l_extendedprice) from lineitem4 group by l_returnflag, l_linestatus;------------------------------------------------------------------ l_returnflag   l_linestatus   sum(l_quantity)    sum(l_extendedprice)  ------------------------------------------------------------------ N              F              2.9808092E7       4.471079473931997E10    A              F              1.145546488E9     1.717580824169429E12    N              O              2.31980219E9      3.4789002701143467E12   R              F              1.146403932E9     1.7190627928317903E12  ------------------------------------------------------------------4 rows selected (0.8 seconds)0: jdbc:hive2://10.18.98.48:23040> select l_returnflag,l_linestatus,sum(l_quantity),sum(l_extendedprice) from lineitem4 group by l_returnflag, l_linestatus;------------------------------------------------------------------ l_returnflag   l_linestatus   sum(l_quantity)    sum(l_extendedprice)  ------------------------------------------------------------------ N              F              2.9808092E7       4.471079473931997E10    A              F              1.145546488E9     1.717580824169429E12    N              O              2.31980219E9      3.4789002701143467E12   R              F              1.146403932E9     1.7190627928317903E12  ------------------------------------------------------------------
issueID:CARBONDATA-1745
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/metadata/AbsoluteTableIdentifier.java
texts:Remove local metastore path
If user does not specify metastore path, use default metastore path from Hive
issueID:CARBONDATA-1746
type:New Feature
changed files:hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonTableInputFormat.java
core/src/main/java/org/apache/carbondata/core/mutate/CarbonUpdateUtil.java
texts:Count Star optimization
Since carbon records number of row in metadata, count star query can leverage it to improve performance
issueID:CARBONDATA-1747
type:Bug
changed files:
texts:Carbon1.3.0- DB creation external path : Owner name of compacted segment and segment after update is not correct
Steps :In spark Beeline user executes the following queriesdrop database if exists test_db1 cascade;create database test_db1 location 'hdfs://hacluster/user/test1';use test_db1;create table if not exists ORDERS(O_ORDERDATE string,O_ORDERPRIORITY string,O_ORDERSTATUS string,O_ORDERKEY string,O_CUSTKEY string,O_TOTALPRICE double,O_CLERK string,O_SHIPPRIORITY int,O_COMMENT string) STORED BY 'org.apache.carbondata.format'TBLPROPERTIES ('table_blocksize'='128');load data inpath "hdfs://hacluster/chetan/orders.tbl.1" into table ORDERS options('DELIMITER'='|','FILEHEADER'='O_ORDERKEY,O_CUSTKEY,O_ORDERSTATUS,O_TOTALPRICE,O_ORDERDATE,O_ORDERPRIORITY,O_CLERK,O_SHIPPRIORITY,O_COMMENT','batch_sort_size_inmb'='32');load data inpath "hdfs://hacluster/chetan/orders.tbl.1" into table ORDERS options('DELIMITER'='|','FILEHEADER'='O_ORDERKEY,O_CUSTKEY,O_ORDERSTATUS,O_TOTALPRICE,O_ORDERDATE,O_ORDERPRIORITY,O_CLERK,O_SHIPPRIORITY,O_COMMENT','batch_sort_size_inmb'='32');load data inpath "hdfs://hacluster/chetan/orders.tbl.1" into table ORDERS options('DELIMITER'='|','FILEHEADER'='O_ORDERKEY,O_CUSTKEY,O_ORDERSTATUS,O_TOTALPRICE,O_ORDERDATE,O_ORDERPRIORITY,O_CLERK,O_SHIPPRIORITY,O_COMMENT','batch_sort_size_inmb'='32');load data inpath "hdfs://hacluster/chetan/orders.tbl.1" into table ORDERS options('DELIMITER'='|','FILEHEADER'='O_ORDERKEY,O_CUSTKEY,O_ORDERSTATUS,O_TOTALPRICE,O_ORDERDATE,O_ORDERPRIORITY,O_CLERK,O_SHIPPRIORITY,O_COMMENT','batch_sort_size_inmb'='32');alter table ORDERS compact 'major';update orders set (O_ORDERKEY)=(1) where O_CUSTKEY=6259021;After compaction and update user checks the Owner name of compacted segment and segment name after update in HDFS UI.Issue : In HDFS UI before compaction and update the owner name of the existing segment folders was "anonymous". After compaction and update the owner name of the compacted segment folder and segment which is impacted by update is displayed as "root".Expected : After compaction and update the owner name of the compacted segment folder and segment which is impacted by update should be "anonymous".
issueID:CARBONDATA-1748
type:Bug
changed files:
texts:Carbon1.3.0- DB creation external path : Permission of created table and database folder in carbon store not correct
Steps : In spark Beeline user executes the following queries.drop database if exists test_db1 cascade;create database test_db1 location 'hdfs://hacluster/user/test1';use test_db1;create table if not exists ORDERS(O_ORDERDATE string,O_ORDERPRIORITY string,O_ORDERSTATUS string,O_ORDERKEY string,O_CUSTKEY string,O_TOTALPRICE double,O_CLERK string,O_SHIPPRIORITY int,O_COMMENT string) STORED BY 'org.apache.carbondata.format'TBLPROPERTIES ('table_blocksize'='128');User checks the permission of the created database and table in carbon store using the  bin/hadoop fs -getfacl command.Issue : The Permission of created table and database folder in carbon store not correct. i.e  file: /user/test1/orders owner: anonymous group: usersuser::rwxgroup::r-xother::r-xExpected : Correct permissions for the created table and database folder in carbon store should be  file: /user/test1/orders owner: anonymous group: usersuser::rwxgroup::---other::---
issueID:CARBONDATA-1749
type:Bug
changed files:
texts:Carbon1.3.0- DB creation external path : mdt file is not created in directory as per configuration in carbon.properties
Steps : In carbon.properties the mdt file directory path is configured as Carbon.update.sync.folder=hdfs://hacluster/user/test1 or /tmp/test1/In beeline user creates a database by specifying the carbon store path and creates a carbon table in the db. drop database if exists test_db1 cascade; create database test_db1 location 'hdfs://hacluster/user/test1'; use test_db1; create table if not exists ORDERS(O_ORDERDATE string,O_ORDERPRIORITY string,O_ORDERSTATUS string,O_ORDERKEY string,O_CUSTKEY string,O_TOTALPRICE double,O_CLERK string,O_SHIPPRIORITY int,O_COMMENT string) STORED BY 'org.apache.carbondata.format'TBLPROPERTIES ('table_blocksize'='128');User checks in HDFS UI if the mdt file is created in directory specified (hdfs://hacluster/user/test1 as per configuration in carbon.properties.Issue : mdt file is not created in directory specified (hdfs://hacluster/user/test1) as per configuration in carbon.properties. Also the folder is not created if the user configures the folder path as Carbon.update.sync.folder=/tmp/test1/Expected : mdt file should be created in directory specified (hdfs://hacluster/user/test1) or /tmp/test1/ as per configuration in carbon.properties.
issueID:CARBONDATA-175
type:Bug
changed files:
texts:Load Decimal Data to BigInt Column, query is NULL
when Loading Decimal Data to BigInt Column, the data is handle as NULL, like loading 3.14 into a BigInt Column, 3.14 is handle as NULL.
issueID:CARBONDATA-1750
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/statusmanager/SegmentStatusManager.java
texts:SegmentStatusManager.readLoadMetadata showing NPE if tablestatus file is empty
SegmentStatusManager.readLoadMetadata showing NPE if tablestatus file is empty
issueID:CARBONDATA-1751
type:Bug
changed files:
texts:Modify sys.err to AnalysisException when  uses run related operation except IUD,compaction and alter
carbon printout improper error message, for example, it printout system error when users run create table with the same column name, but it should printout related exception informationSo we modify sys.error method to AnalysisException when uses run related operation except IUD,compaction and alterMake the type of exception and message correctly,including Spark2 and spark-common module
issueID:CARBONDATA-1752
type:Bug
changed files:integration/presto/src/main/java/org/apache/carbondata/presto/readers/SliceStreamReader.java
integration/presto/src/main/java/org/apache/carbondata/presto/readers/ObjectStreamReader.java
integration/presto/src/main/java/org/apache/carbondata/presto/CarbondataSplitManager.java
texts:There are some scalastyle error should be optimized in CarbonData
There are some scalastyle error should be optimized in CarbonData, including removing useless import, optimizing method definition and so on
issueID:CARBONDATA-1753
type:Bug
changed files:
texts:Missing &#39;org.scalatest.tools.Runner&#39; when run test with streaming module
Missing 'org.scalatest.tools.Runner' when run test with streaming module.Need to add scalatest to pom.xml of streaming module.
issueID:CARBONDATA-1755
type:Bug
changed files:
texts:Carbon1.3.0 Concurrent Insert overwrite-update: User is able to run insert overwrite and update job concurrently.
Carbon1.3.0 Concurrent Insert overwrite-update: User is able to run insert overwrite and update job concurrently.updated data will be overwritten by insert overwrite job. So there is no meaning of running update job if insert overwrite is in progress.Steps:1: Create a table2: Do a data load3: run insert overwrite job.4: run a update job while overwrite job is still running.5: Observe that update job is finished and after that overwrite job is also finished.6: All previous segments are marked for delete and there is no impact of update job. Update job will use the resources unnecessary.
issueID:CARBONDATA-1756
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/DefaultEncodingFactory.java
core/src/main/java/org/apache/carbondata/core/datastore/page/LazyColumnPage.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/EncodingFactory.java
texts:Improve Boolean data compress rate by changing RLE to SNAPPY algorithm
Improve Boolean data compress rate by changing RLE to SNAPPY algorithmBecause Boolean data compress rate that uses RLE algorithm is lower than SNAPPY algorithm in most scenario.
issueID:CARBONDATA-1757
type:Bug
changed files:
texts:Carbon 1.3.0- Pre_aggregate: After creating datamap on parent table, avg is not correct.
Steps:1. create table cust_2 (c_custkey int, c_name string, c_address string, c_nationkey bigint, c_phone string,c_acctbal decimal, c_mktsegment string, c_comment string) STORED BY 'org.apache.carbondata.format'; 2. load data  inpath 'hdfs://hacluster/customer/customer3.csv' into table cust_2 options('DELIMITER'='|','QUOTECHAR'='"','FILEHEADER'='c_custkey,c_name,c_address,c_nationkey,c_phone,c_acctbal,c_mktsegment,c_comment');load data  inpath 'hdfs://hacluster/customer/customer3.csv' into table cust_2 options('DELIMITER'='|','QUOTECHAR'='"','FILEHEADER'='c_custkey,c_name,c_address,c_nationkey,c_phone,c_acctbal,c_mktsegment,c_comment');load data  inpath 'hdfs://hacluster/customer/customer4.csv' into table cust_2 options('DELIMITER'='|','QUOTECHAR'='"','FILEHEADER'='c_custkey,c_name,c_address,c_nationkey,c_phone,c_acctbal,c_mktsegment,c_comment');load data  inpath 'hdfs://hacluster/customer/customer5.csv' into table cust_2 options('DELIMITER'='|','QUOTECHAR'='"','FILEHEADER'='c_custkey,c_name,c_address,c_nationkey,c_phone,c_acctbal,c_mktsegment,c_comment');load data  inpath 'hdfs://hacluster/customer/customer6.csv' into table cust_2 options('DELIMITER'='|','QUOTECHAR'='"','FILEHEADER'='c_custkey,c_name,c_address,c_nationkey,c_phone,c_acctbal,c_mktsegment,c_comment');load data  inpath 'hdfs://hacluster/customer/customer7.csv' into table cust_2 options('DELIMITER'='|','QUOTECHAR'='"','FILEHEADER'='c_custkey,c_name,c_address,c_nationkey,c_phone,c_acctbal,c_mktsegment,c_comment');load data  inpath 'hdfs://hacluster/customer/customer8.csv' into table cust_2 options('DELIMITER'='|','QUOTECHAR'='"','FILEHEADER'='c_custkey,c_name,c_address,c_nationkey,c_phone,c_acctbal,c_mktsegment,c_comment');load data  inpath 'hdfs://hacluster/customer/customer9.csv' into table cust_2 options('DELIMITER'='|','QUOTECHAR'='"','FILEHEADER'='c_custkey,c_name,c_address,c_nationkey,c_phone,c_acctbal,c_mktsegment,c_comment');load data  inpath 'hdfs://hacluster/customer/customer10.csv' into table cust_2 options('DELIMITER'='|','QUOTECHAR'='"','FILEHEADER'='c_custkey,c_name,c_address,c_nationkey,c_phone,c_acctbal,c_mktsegment,c_comment');load data  inpath 'hdfs://hacluster/customer/customer11.csv' into table cust_2 options('DELIMITER'='|','QUOTECHAR'='"','FILEHEADER'='c_custkey,c_name,c_address,c_nationkey,c_phone,c_acctbal,c_mktsegment,c_comment');load data  inpath 'hdfs://hacluster/customer/customer12.csv' into table cust_2 options('DELIMITER'='|','QUOTECHAR'='"','FILEHEADER'='c_custkey,c_name,c_address,c_nationkey,c_phone,c_acctbal,c_mktsegment,c_comment');load data  inpath 'hdfs://hacluster/customer/customer13.csv' into table cust_2 options('DELIMITER'='|','QUOTECHAR'='"','FILEHEADER'='c_custkey,c_name,c_address,c_nationkey,c_phone,c_acctbal,c_mktsegment,c_comment');load data  inpath 'hdfs://hacluster/customer/customer14.csv' into table cust_2 options('DELIMITER'='|','QUOTECHAR'='"','FILEHEADER'='c_custkey,c_name,c_address,c_nationkey,c_phone,c_acctbal,c_mktsegment,c_comment');3. SELECT c_custkey, c_name, sum(c_acctbal), avg(c_acctbal) FROM cust_2 GROUP BY c_custkey, c_name;4. set carbon.input.segments.default.cust_2=0,1;5. SELECT c_custkey, c_name, sum(c_acctbal), avg(c_acctbal) FROM cust_2 GROUP BY c_custkey, c_name;6. CREATE DATAMAP tt1 ON TABLE cust_2 USING "org.apache.carbondata.datamap.AggregateDataMapHandler" AS SELECT c_custkey, c_name, sum(c_acctbal), avg(c_acctbal) FROM cust_2 GROUP BY c_custkey, c_name;7.  SELECT c_custkey, c_name, sum(c_acctbal), avg(c_acctbal) FROM cust_2 GROUP BY c_custkey, c_name;8. set carbon.input.segments.default.cust_2=*;9. SELECT c_custkey, c_name, sum(c_acctbal), avg(c_acctbal) FROM cust_2 GROUP BY c_custkey, c_name;Issue:After creating datamap, avg is not correctExpected Output:Avg should have been displayed correctly.
issueID:CARBONDATA-1758
type:Bug
changed files:
texts:Carbon1.3.0- No Inverted Index : Select column with is null for no_inverted_index column throws java.lang.ArrayIndexOutOfBoundsException
Steps :In Beeline user executes the queries in sequence.CREATE TABLE uniqdata_DI_int (CUST_ID int,CUST_NAME String,ACTIVE_EMUI_VERSION string, DOB timestamp, DOJ timestamp, BIGINT_COLUMN1 bigint,BIGINT_COLUMN2 bigint,DECIMAL_COLUMN1 decimal(30,10), DECIMAL_COLUMN2 decimal(36,10),Double_COLUMN1 double, Double_COLUMN2 double,INTEGER_COLUMN1 int) STORED BY 'org.apache.carbondata.format' TBLPROPERTIES('DICTIONARY_INCLUDE'='cust_id','NO_INVERTED_INDEX'='cust_id');LOAD DATA INPATH 'hdfs://hacluster/chetan/3000_UniqData.csv' into table uniqdata_DI_int OPTIONS('DELIMITER'=',', 'QUOTECHAR'='"','BAD_RECORDS_ACTION'='FORCE','FILEHEADER'='CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1,Double_COLUMN2,INTEGER_COLUMN1');Select count(CUST_ID) from uniqdata_DI_int;Select count(CUST_ID)*10 as multiple from uniqdata_DI_int;Select avg(CUST_ID) as average from uniqdata_DI_int;Select floor(CUST_ID) as average from uniqdata_DI_int;Select ceil(CUST_ID) as average from uniqdata_DI_int;Select ceiling(CUST_ID) as average from uniqdata_DI_int;Select CUST_ID*integer_column1 as multiple from uniqdata_DI_int;Select CUST_ID from uniqdata_DI_int where CUST_ID is null;Issue : Select column with is null for no_inverted_index column throws java.lang.ArrayIndexOutOfBoundsException0: jdbc:hive2://10.18.98.34:23040> Select CUST_ID from uniqdata_DI_int where CUST_ID is null;Error: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 79.0 failed 4 times, most recent failure: Lost task 0.3 in stage 79.0 (TID 123, BLR1000014278, executor 18): org.apache.spark.util.TaskCompletionListenerException: java.util.concurrent.ExecutionException: java.lang.ArrayIndexOutOfBoundsException: 0        at org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:105)        at org.apache.spark.scheduler.Task.run(Task.scala:112)        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)        at java.lang.Thread.run(Thread.java:745)Driver stacktrace: (state=,code=0)Expected : Select column with is null for no_inverted_index column should be successful displaying the correct result set.
issueID:CARBONDATA-1759
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/util/DeleteLoadFolders.java
texts:(Carbon1.3.0 - Clean Files) Clean command is not working correctly for  segments marked for delete due to insert overwrite job
Carbon1.3.0  Clean command is not working correctly for  segments marked for delete due to insert overwrite job.1: Create a tableCREATE TABLE IF NOT EXISTS flow_carbon_new999(txn_dte String,dt String,txn_bk String,txn_br String,own_bk String,own_br String,opp_bk String,bus_opr_cde String,opt_prd_cde String,cus_no String,cus_ac String,opp_ac_nme  String,opp_ac String,bv_no  String,aco_ac String,ac_dte String,txn_cnt int,jrn_par int,mfm_jrn_no String,cbn_jrn_no String,ibs_jrn_no String,vch_no String,vch_seq String,srv_cde String,bus_cd_no  String,id_flg String,bv_cde String,txn_time  String,txn_tlr String,ety_tlr String,ety_bk String,ety_br String,bus_pss_no String,chk_flg String,chk_tlr String,chk_jrn_no String,  bus_sys_no String,txn_sub_cde String,fin_bus_cde String,fin_bus_sub_cde String,chl  String,tml_id String,sus_no String,sus_seq String,  cho_seq String,  itm_itm String,itm_sub String,itm_sss String,dc_flg String,amt  decimal(15,2),bal  decimal(15,2),ccy  String,spv_flg String,vch_vld_dte String,pst_bk String,pst_br String,ec_flg String,aco_tlr String,gen_flg String,his_rec_sum_flg String,his_flg String,vch_typ String,val_dte String,opp_ac_flg String,cmb_flg String,ass_vch_flg String,cus_pps_flg String,bus_rmk_cde String,vch_bus_rmk String,tec_rmk_cde String,vch_tec_rmk String,gems_last_upd_d String,maps_date String,maps_job String)STORED BY 'org.apache.carbondata.format' TBLPROPERTIES('DICTIONARY_INCLUDE'='txn_cnt,jrn_par,amt,bal','No_Inverted_Index'= 'txn_dte,dt,txn_bk,txn_br,own_bk ,own_br ,opp_bk ,bus_opr_cde ,opt_prd_cde ,cus_no ,cus_ac ,opp_ac_nme  ,opp_ac ,bv_no  ,aco_ac ,ac_dte ,txn_cnt  ,jrn_par  ,mfm_jrn_no ,cbn_jrn_no ,ibs_jrn_no ,vch_no ,vch_seq ,srv_cde ,bus_cd_no  ,id_flg ,bv_cde ,txn_time  ,txn_tlr ,ety_tlr ,ety_bk ,ety_br ,bus_pss_no ,chk_flg ,chk_tlr ,chk_jrn_no , bus_sys_no ,txn_sub_cde ,fin_bus_cde ,fin_bus_sub_cde ,chl  ,tml_id ,sus_no ,sus_seq , cho_seq , itm_itm ,itm_sub ,itm_sss ,dc_flg ,amt,bal,ccy  ,spv_flg ,vch_vld_dte ,pst_bk ,pst_br ,ec_flg ,aco_tlr ,gen_flg ,his_rec_sum_flg ,his_flg ,vch_typ ,val_dte ,opp_ac_flg ,cmb_flg ,ass_vch_flg ,cus_pps_flg ,bus_rmk_cde ,vch_bus_rmk ,tec_rmk_cde ,vch_tec_rmk ,gems_last_upd_d ,maps_date ,maps_job' );2: start a data load.LOAD DATA inpath 'hdfs://hacluster/user/test/20140101_1_144444444.csv' into table flow_carbon_new999 options('DELIMITER'=',', 'QUOTECHAR'='"','header'='false');3: run a insert overwrite job insert into table  flow_carbon_new999 select * from flow_carbon_new666;4: run show segment query:show segments for table ajeet.flow_carbon_new9995: Observe that all previous segments are marked for delete6: run clean queryCLEAN FILES FOR TABLE ajeet.flow_carbon_new999;7: again run show segment query8: Observe that still all previous segments which are marked for delete are shown as result.
issueID:CARBONDATA-176
type:Bug
changed files:
texts:Should not allow deletion of compacted segment.
If a segment is compacted then user can not delete that segment.
issueID:CARBONDATA-1760
type:Bug
changed files:
texts:Carbon 1.3.0- Pre_aggregate: Proper Error message should be displayed, when parent table name is not correct while creating datamap.
Steps:1. CREATE DATAMAP tt3 ON TABLE cust_2 USING "org.apache.carbondata.datamap.AggregateDataMapHandler" AS SELECT c_custkey, c_name, sum(c_acctbal), avg(c_acctbal), count(c_acctbal) FROM tstcust GROUP BY c_custkey, c_name;Issue:Proper error message is not displayed. It throws "assertion failed" error.Expected:Proper error message should be displayed, if parent table name has any ambiguity.
issueID:CARBONDATA-1761
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/statusmanager/SegmentStatusManager.java
texts:(Carbon1.3.0 - DELETE SEGMENT BY ID) In Progress Segment is marked for delete if respective id is given in delete segment by id query
(Carbon1.3.0 - DELETE SEGMENT BY ID) In Progress Segment is marked for delete if respective id is given in delete segment by id query.1: Create a tableCREATE TABLE IF NOT EXISTS flow_carbon_new999(txn_dte String,dt String,txn_bk String,txn_br String,own_bk String,own_br String,opp_bk String,bus_opr_cde String,opt_prd_cde String,cus_no String,cus_ac String,opp_ac_nme String,opp_ac String,bv_no String,aco_ac String,ac_dte String,txn_cnt int,jrn_par int,mfm_jrn_no String,cbn_jrn_no String,ibs_jrn_no String,vch_no String,vch_seq String,srv_cde String,bus_cd_no String,id_flg String,bv_cde String,txn_time String,txn_tlr String,ety_tlr String,ety_bk String,ety_br String,bus_pss_no String,chk_flg String,chk_tlr String,chk_jrn_no String, bus_sys_no String,txn_sub_cde String,fin_bus_cde String,fin_bus_sub_cde String,chl String,tml_id String,sus_no String,sus_seq String, cho_seq String, itm_itm String,itm_sub String,itm_sss String,dc_flg String,amt decimal(15,2),bal decimal(15,2),ccy String,spv_flg String,vch_vld_dte String,pst_bk String,pst_br String,ec_flg String,aco_tlr String,gen_flg String,his_rec_sum_flg String,his_flg String,vch_typ String,val_dte String,opp_ac_flg String,cmb_flg String,ass_vch_flg String,cus_pps_flg String,bus_rmk_cde String,vch_bus_rmk String,tec_rmk_cde String,vch_tec_rmk String,gems_last_upd_d String,maps_date String,maps_job String)STORED BY 'org.apache.carbondata.format' TBLPROPERTIES('DICTIONARY_INCLUDE'='txn_cnt,jrn_par,amt,bal','No_Inverted_Index'= 'txn_dte,dt,txn_bk,txn_br,own_bk ,own_br ,opp_bk ,bus_opr_cde ,opt_prd_cde ,cus_no ,cus_ac ,opp_ac_nme ,opp_ac ,bv_no ,aco_ac ,ac_dte ,txn_cnt ,jrn_par ,mfm_jrn_no ,cbn_jrn_no ,ibs_jrn_no ,vch_no ,vch_seq ,srv_cde ,bus_cd_no ,id_flg ,bv_cde ,txn_time ,txn_tlr ,ety_tlr ,ety_bk ,ety_br ,bus_pss_no ,chk_flg ,chk_tlr ,chk_jrn_no , bus_sys_no ,txn_sub_cde ,fin_bus_cde ,fin_bus_sub_cde ,chl ,tml_id ,sus_no ,sus_seq , cho_seq , itm_itm ,itm_sub ,itm_sss ,dc_flg ,amt,bal,ccy ,spv_flg ,vch_vld_dte ,pst_bk ,pst_br ,ec_flg ,aco_tlr ,gen_flg ,his_rec_sum_flg ,his_flg ,vch_typ ,val_dte ,opp_ac_flg ,cmb_flg ,ass_vch_flg ,cus_pps_flg ,bus_rmk_cde ,vch_bus_rmk ,tec_rmk_cde ,vch_tec_rmk ,gems_last_upd_d ,maps_date ,maps_job' );2: start a data load.LOAD DATA inpath 'hdfs://hacluster/user/test/20140101_1_144444444.csv' into table flow_carbon_new999 options('DELIMITER'=',', 'QUOTECHAR'='"','header'='false');3: run a insert into/overwrite jobinsert into table flow_carbon_new999 select * from flow_carbon_new666;4: show segments for table flow_carbon_new999;5: Observe that load/insert/overwrite job is started with new segment id6: now run a delete segment by id query with this id.DELETE FROM TABLE ajeet.flow_carbon_new999 WHERE SEGMENT.ID IN (34)7: again run show segment and see this segment which is still in progress is marked for delete.8: Observe that insert/load job is still running and after some time(in next job of load/insert/overwrite), this job fails with below error:Error: java.lang.RuntimeException: It seems insert overwrite has been issued during load (state=,code=0)This is not correct behaviour and it should be handled.
issueID:CARBONDATA-1763
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/locks/CarbonLockUtil.java
core/src/main/java/org/apache/carbondata/core/metadata/AbsoluteTableIdentifier.java
texts:Carbon1.3.0-Pre-AggregateTable - Recreating a failed pre-aggregate table fails due to table exists
Steps:1. Create table and load with  data2. Run update query on the table - this will take table metalock3. In parallel run the pre-aggregate table create step - this will not be allowed due to table lock4. Rerun pre-aggegate table create stepExpected: Pre-aggregate table should be created Actual: Pre-aggregate table creation failsCreate, Load & Update:0: jdbc:hive2://10.18.98.136:23040> create table if not exists lineitem4(L_SHIPDATE string,L_SHIPMODE string,L_SHIPINSTRUCT string,L_RETURNFLAG string,L_RECEIPTDATE string,L_ORDERKEY string,L_PARTKEY string,L_SUPPKEY   string,L_LINENUMBER int,L_QUANTITY double,L_EXTENDEDPRICE double,L_DISCOUNT double,L_TAX double,L_LINESTATUS string,L_COMMITDATE string,L_COMMENT  string) STORED BY 'org.apache.carbondata.format' TBLPROPERTIES ('table_blocksize'='128','NO_INVERTED_INDEX'='L_SHIPDATE,L_SHIPMODE,L_SHIPINSTRUCT,L_RETURNFLAG,L_RECEIPTDATE,L_ORDERKEY,L_PARTKEY,L_SUPPKEY','sort_columns'='');---------+ Result  ---------+---------+No rows selected (0.266 seconds)0: jdbc:hive2://10.18.98.136:23040> load data inpath "hdfs://hacluster/user/test/lineitem.tbl.5" into table lineitem4 options('DELIMITER'='|','FILEHEADER'='L_ORDERKEY,L_PARTKEY,L_SUPPKEY,L_LINENUMBER,L_QUANTITY,L_EXTENDEDPRICE,L_DISCOUNT,L_TAX,L_RETURNFLAG,L_LINESTATUS,L_SHIPDATE,L_COMMITDATE,L_RECEIPTDATE,L_SHIPINSTRUCT,L_SHIPMODE,L_COMMENT');---------+ Result  ---------+---------+No rows selected (6.331 seconds)0: jdbc:hive2://10.18.98.136:23040> update lineitem4 set (l_linestatus) = ('xx');Create Datamap:0: jdbc:hive2://10.18.98.136:23040> create datamap agr_lineitem4 ON TABLE lineitem4 USING "org.apache.carbondata.datamap.AggregateDataMapHandler" as select l_returnflag,l_linestatus,sum(l_quantity),avg(l_quantity),count(l_quantity) from lineitem4  group by l_returnflag, l_linestatus;Error: java.lang.RuntimeException: Acquire table lock failed after retry, please try after some time (state=,code=0)0: jdbc:hive2://10.18.98.136:23040> select l_returnflag,l_linestatus,sum(l_quantity),avg(l_quantity),count(l_quantity) from lineitem4 group by l_returnflag, l_linestatus;---------------------------------------------------------------------------------+ l_returnflag   l_linestatus   sum(l_quantity)     avg(l_quantity)    count(l_quantity)  ---------------------------------------------------------------------------------+ N              xx             1.2863213E7       25.48745561614304    504688              A              xx             6318125.0         25.506342144783375   247708              R              xx             6321939.0         25.532459087898417   247604             ---------------------------------------------------------------------------------+3 rows selected (1.033 seconds)0: jdbc:hive2://10.18.98.136:23040> create datamap agr_lineitem4 ON TABLE lineitem4 USING "org.apache.carbondata.datamap.AggregateDataMapHandler" as select l_returnflag,l_linestatus,sum(l_quantity),avg(l_quantity),count(l_quantity) from lineitem4  group by l_returnflag, l_linestatus;Error: java.lang.RuntimeException: Table &#91;lineitem4_agr_lineitem4&#93; already exists under database &#91;test_db1&#93; (state=,code=0)
issueID:CARBONDATA-1764
type:Bug
changed files:
texts:Fix issue of when create table with short data type
Fix issue of when create table with short data type
issueID:CARBONDATA-1765
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/util/DataTypeUtil.java
texts:Remove repeat code of Boolean

issueID:CARBONDATA-1766
type:Bug
changed files:streaming/src/main/java/org/apache/carbondata/streaming/segment/StreamSegment.java
streaming/src/main/java/org/apache/carbondata/streaming/CarbonStreamRecordReader.java
streaming/src/main/java/org/apache/carbondata/streaming/CarbonStreamRecordWriter.java
texts:fix serialization issue for CarbonAppendableStreamSink
fix serialization issue for CarbonAppendableStreamSink
issueID:CARBONDATA-1767
type:Bug
changed files:integration/presto/src/main/java/org/apache/carbondata/presto/impl/CarbonTableReader.java
texts:Remove dependency of Java 1.8
carbon should b enable to compile with Java 1.7
issueID:CARBONDATA-1768
type:Improvement
changed files:
texts:Upgrade univocity parser to 2.2.1
Univocity CSV parser has improved performance in 2.2.1, upgrade dependency to use it
issueID:CARBONDATA-1769
type:Improvement
changed files:
texts:Change alterTableCompaction to support transfer  tableInfo
Change alterTableCompaction to support transfer  tableInfo
issueID:CARBONDATA-177
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelRangeGrtThanFiterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelRangeLessThanFiterExecuterImpl.java
texts:Greater than and Less than filter returning wrong result
DDL:      "CREATE TABLE IF NOT EXISTS carbonTable(date Timestamp, country String, salary Int) STORED " +      "BY " +      "'carbondata'Data:2015-7-23 12:07:28,china,150002018-7-24 12:07:28,china,150012017-7-25 12:07:28,china,15002Load command:"LOAD DATA LOCAL INPATH '" + csvFilePath + "' into table carbonTable OPTIONS " +      "('FILEHEADER'='date,country,salary')"greather than less than filter not returning correct result
issueID:CARBONDATA-1770
type:Improvement
changed files:
texts:Update documents and consolidate DDL,DML,Partition docs
1. Update documents : there are some error description.2. Consolidate Data management, DDL,DML,Partition docs, to ensure one feature which only be described in one place.
issueID:CARBONDATA-1771
type:Improvement
changed files:
texts:While segment_index compaction, .carbonindex files of invalid segments are also getting merged

issueID:CARBONDATA-1774
type:Bug
changed files:integration/presto/src/main/java/org/apache/carbondata/presto/readers/BooleanStreamReader.java
integration/presto/src/main/java/org/apache/carbondata/presto/PrestoCarbonVectorizedRecordReader.java
texts:Not able to fetch data from a table with Boolean data type in presto
Not able to fetch data from a table with Boolean data type in prestoSteps to Reproduce:On Beeline1)Create table:create table boolean ( id int, employee boolean) stored by 'carbondata';2) Insert values in table:insert into boolean values (1,true);insert into boolean values (2,false);insert into boolean values (5,true);insert into boolean values (3,true);insert into boolean values (4,false);3) Execute select query with and without boolean datatype;a)select id from boolean;output:-----+ id  -----+ 2    3    4    5    1   -----+b)select employee from boolean;output:----------- employee  -----------+ false      true       true       false      true      -----------+c) select * from boolean;output:-------------- id   employee  -------------- 1    true       3    true       4    false      5    true       2    false     --------------On Presto CLI:Execute queries with and without boolean data type:a)select id from boolean;output: id   2   5   1   3   4 (5 rows)b)select employee from boolean;output:Expected output: it should display the boolean data type values of employee column as on beeline.Actual output:Query 20171120_054640_00011_2ppsk, FAILED, 1 nodeSplits: 21 total, 0 done (0.00%)0:01 &#91;0 rows, 0B&#93; &#91;0 rows/s, 0B/s&#93;Query 20171120_054640_00011_2ppsk failed: com.facebook.presto.spi.type.BooleanTypec)select * from boolean;output:Expected output: it should display the boolean data type values of employee column as on beeline.Actual output:Query 20171120_054858_00012_2ppsk, FAILED, 1 nodeSplits: 21 total, 0 done (0.00%)0:00 &#91;0 rows, 0B&#93; &#91;0 rows/s, 0B/s&#93;Query 20171120_054858_00012_2ppsk failed: com.facebook.presto.spi.type.BooleanType
issueID:CARBONDATA-1775
type:Bug
changed files:
texts:(Carbon1.3.0 - Streaming) Select query fails with  java.io.EOFException when data streaming is in progress
Steps :User starts the thrift server using the command - bin/spark-submit --master yarn-client --executor-memory 10G --executor-cores 5 --driver-memory 5G --num-executors 3 --class org.apache.carbondata.spark.thriftserver.CarbonThriftServer /srv/spark2.2Bigdata/install/spark/sparkJdbc/carbonlib/carbondata_2.11-1.3.0-SNAPSHOT-shade-hadoop2.7.2.jar "hdfs://hacluster/user/hive/warehouse/carbon.store"User connects to spark shell using the command - bin/spark-shell --master yarn-client --executor-memory 10G --executor-cores 5 --driver-memory 5G --num-executors 3 --jars /srv/spark2.2Bigdata/install/spark/sparkJdbc/carbonlib/carbondata_2.11-1.3.0-SNAPSHOT-shade-hadoop2.7.2.jarIn spark shell User creates a table and does streaming load in the table as per the below socket streaming script.import java.io.{File, PrintWriter}import java.net.ServerSocketimport org.apache.spark.sql.{CarbonEnv, SparkSession}import org.apache.spark.sql.hive.CarbonRelationimport org.apache.spark.sql.streaming.{ProcessingTime, StreamingQuery}import org.apache.carbondata.core.constants.CarbonCommonConstantsimport org.apache.carbondata.core.util.CarbonPropertiesimport org.apache.carbondata.core.util.path.{CarbonStorePath, CarbonTablePath}CarbonProperties.getInstance().addProperty(CarbonCommonConstants.CARBON_TIMESTAMP_FORMAT, "yyyy/MM/dd")import org.apache.spark.sql.CarbonSession._val carbonSession = SparkSession.  builder().  appName("StreamExample").  getOrCreateCarbonSession("hdfs://hacluster/user/hive/warehouse/david")carbonSession.sparkContext.setLogLevel("INFO")def sql(sql: String) = carbonSession.sql(sql)def writeSocket(serverSocket: ServerSocket): Thread = {  val thread = new Thread() {    override def run(): Unit = {      // wait for client to connection request and accept      val clientSocket = serverSocket.accept()      val socketWriter = new PrintWriter(clientSocket.getOutputStream())      var index = 0      for (_ <- 1 to 1000) {        // write 5 records per iteration        for (_ <- 0 to 100) {          index = index + 1          socketWriter.println(index.toString + ",name_" + index                               + ",city_" + index + "," + (index * 10000.00).toString +                               ",school_" + index + ":school_" + index + index + "$" + index)        }        socketWriter.flush()        Thread.sleep(2000)      }      socketWriter.close()      System.out.println("Socket closed")    }  }  thread.start()  thread}def startStreaming(spark: SparkSession, tablePath: CarbonTablePath, tableName: String, port: Int): Thread = {    val thread = new Thread() {      override def run(): Unit = {        var qry: StreamingQuery = null        try {          val readSocketDF = spark.readStream            .format("socket")            .option("host", "10.18.98.34")            .option("port", port)            .load()          qry = readSocketDF.writeStream            .format("carbondata")            .trigger(ProcessingTime("5 seconds"))            .option("checkpointLocation", tablePath.getStreamingCheckpointDir)            .option("tablePath", tablePath.getPath).option("tableName", tableName)            .start()          qry.awaitTermination()        } catch {          case ex: Throwable =>            ex.printStackTrace()            println("Done reading and writing streaming data")        } finally {          qry.stop()        }      }    }    thread.start()    thread}val streamTableName = "stream_table"sql(s"CREATE TABLE $streamTableName (id INT,name STRING,city STRING,salary FLOAT) STORED BY 'carbondata' TBLPROPERTIES('streaming'='true', 'sort_columns'='name')")sql(s"LOAD DATA LOCAL INPATH 'hdfs://hacluster/tmp/streamSample.csv' INTO TABLE $streamTableName OPTIONS('HEADER'='true')")sql(s"select * from $streamTableName").showval carbonTable = CarbonEnv.getInstance(carbonSession).carbonMetastore.  lookupRelation(Some("default"), streamTableName)(carbonSession).asInstanceOf&#91;CarbonRelation&#93;.carbonTableval tablePath = CarbonStorePath.getCarbonTablePath(carbonTable.getAbsoluteTableIdentifier)val port = 7995val serverSocket = new ServerSocket(port)val socketThread = writeSocket(serverSocket)val streamingThread = startStreaming(carbonSession, tablePath, streamTableName, port)While load is in progress user executes select query on the streaming table from beeline.0: jdbc:hive2://10.18.98.34:23040> select * from stream_table;Issue : The Select query fails with  java.io.EOFException when socket streaming is in progress.0: jdbc:hive2://10.18.98.34:23040> select * from stream_table;Error: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 1.0 failed 4 times, most recent failure: Lost task 3.3 in stage 1.0 (TID 38, BLR1000014278, executor 7): java.io.EOFException        at org.apache.carbondata.hadoop.streaming.StreamBlockletReader.readBytesFromStream(StreamBlockletReader.java:182)        at org.apache.carbondata.hadoop.streaming.StreamBlockletReader.readBlockletData(StreamBlockletReader.java:116)        at org.apache.carbondata.hadoop.streaming.CarbonStreamRecordReader.scanBlockletAndFillVector(CarbonStreamRecordReader.java:406)        at org.apache.carbondata.hadoop.streaming.CarbonStreamRecordReader.nextColumnarBatch(CarbonStreamRecordReader.java:317)        at org.apache.carbondata.hadoop.streaming.CarbonStreamRecordReader.nextKeyValue(CarbonStreamRecordReader.java:298)        at org.apache.carbondata.spark.rdd.CarbonScanRDD$$anon$1.hasNext(CarbonScanRDD.scala:298)        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.scan_nextBatch$(Unknown Source)        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)        at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)        at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)        at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:231)        at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:225)        at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:826)        at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:826)        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)        at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)        at org.apache.spark.scheduler.Task.run(Task.scala:99)        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)        at java.lang.Thread.run(Thread.java:745)Driver stacktrace: (state=,code=0)Also when user checks the spark shell terminal there are exceptions thrown.scala> org.apache.spark.sql.streaming.StreamingQueryException: Offsets committed out of order: 100999 followed by 100 scala.sys.package$.error(package.scala:27)        org.apache.spark.sql.execution.streaming.TextSocketSource.commit(socket.scala:151)        org.apache.spark.sql.execution.streaming.StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$constructNextBatch$2$$anonfun$apply$mcV$sp$4.apply(StreamExecution.scala:421)        org.apache.spark.sql.execution.streaming.StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$constructNextBatch$2$$anonfun$apply$mcV$sp$4.apply(StreamExecution.scala:420)        scala.collection.Iterator$class.foreach(Iterator.scala:893)        scala.collection.AbstractIterator.foreach(Iterator.scala:1336)        scala.collection.IterableLike$class.foreach(IterableLike.scala:72)        org.apache.spark.sql.execution.streaming.StreamProgress.foreach(StreamProgress.scala:25)        org.apache.spark.sql.execution.streaming.StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$constructNextBatch$2.apply$mcV$sp(StreamExecution.scala:420)        org.apache.spark.sql.execution.streaming.StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$constructNextBatch$2.apply(StreamExecution.scala:404)=== Streaming Query ===Identifier: &#91;id = d23c5633-e747-457e-a5c0-69ec09bb183f, runId = 2db93553-fe97-4fa6-b425-278128a42f50&#93;Current Offsets: {org.apache.spark.sql.execution.streaming.TextSocketSource@750267f5: 100}Current State: ACTIVEThread State: RUNNABLELogical Plan:org.apache.spark.sql.execution.streaming.TextSocketSource@750267f5        at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runBatches(StreamExecution.scala:284)        at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:177)Caused by: java.lang.RuntimeException: Offsets committed out of order: 100999 followed by 100        at scala.sys.package$.error(package.scala:27)        at org.apache.spark.sql.execution.streaming.TextSocketSource.commit(socket.scala:151)        at org.apache.spark.sql.execution.streaming.StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$constructNextBatch$2$$anonfun$apply$mcV$sp$4.apply(StreamExecution.scala:421)        at org.apache.spark.sql.execution.streaming.StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$constructNextBatch$2$$anonfun$apply$mcV$sp$4.apply(StreamExecution.scala:420)        at scala.collection.Iterator$class.foreach(Iterator.scala:893)        at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)        at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)        at org.apache.spark.sql.execution.streaming.StreamProgress.foreach(StreamProgress.scala:25)        at org.apache.spark.sql.execution.streaming.StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$constructNextBatch$2.apply$mcV$sp(StreamExecution.scala:420)        at org.apache.spark.sql.execution.streaming.StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$constructNextBatch$2.apply(StreamExecution.scala:404)        at org.apache.spark.sql.execution.streaming.StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$constructNextBatch$2.apply(StreamExecution.scala:404)        at org.apache.spark.sql.execution.streaming.ProgressReporter$class.reportTimeTaken(ProgressReporter.scala:262)        at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:46)        at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$constructNextBatch(StreamExecution.scala:404)        at org.apache.spark.sql.execution.streaming.StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$runBatches$1$$anonfun$1.apply$mcV$sp(StreamExecution.scala:250)        at org.apache.spark.sql.execution.streaming.StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$runBatches$1$$anonfun$1.apply(StreamExecution.scala:244)        at org.apache.spark.sql.execution.streaming.StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$runBatches$1$$anonfun$1.apply(StreamExecution.scala:244)        at org.apache.spark.sql.execution.streaming.ProgressReporter$class.reportTimeTaken(ProgressReporter.scala:262)        at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:46)        at org.apache.spark.sql.execution.streaming.StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$runBatches$1.apply$mcZ$sp(StreamExecution.scala:244)        at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:43)        at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runBatches(StreamExecution.scala:239)        ... 1 moreDone reading and writing streaming dataExpected Output : select query should be successful from beeline on the streaming table.
issueID:CARBONDATA-1776
type:Bug
changed files:
texts:Fix some possible test error that are related to compaction
Fix some possible test error that are related to compaction
issueID:CARBONDATA-1777
type:Bug
changed files:
texts:Carbon1.3.0-Pre-AggregateTable - Pre-aggregate tables created in Spark-shell sessions are not used in the beeline session
Steps:Beeline:1. Create table and load with  dataSpark-shell:1. create a pre-aggregate tableBeeline:1. Run aggregate queryExpected: Pre-aggregate table should be used in the aggregate query Actual: Pre-aggregate table is not used1.create table if not exists lineitem1(L_SHIPDATE string,L_SHIPMODE string,L_SHIPINSTRUCT string,L_RETURNFLAG string,L_RECEIPTDATE string,L_ORDERKEY string,L_PARTKEY string,L_SUPPKEY   string,L_LINENUMBER int,L_QUANTITY double,L_EXTENDEDPRICE double,L_DISCOUNT double,L_TAX double,L_LINESTATUS string,L_COMMITDATE string,L_COMMENT  string) STORED BY 'org.apache.carbondata.format' TBLPROPERTIES ('table_blocksize'='128','NO_INVERTED_INDEX'='L_SHIPDATE,L_SHIPMODE,L_SHIPINSTRUCT,L_RETURNFLAG,L_RECEIPTDATE,L_ORDERKEY,L_PARTKEY,L_SUPPKEY','sort_columns'='');load data inpath "hdfs://hacluster/user/test/lineitem.tbl.5" into table lineitem1 options('DELIMITER'='|','FILEHEADER'='L_ORDERKEY,L_PARTKEY,L_SUPPKEY,L_LINENUMBER,L_QUANTITY,L_EXTENDEDPRICE,L_DISCOUNT,L_TAX,L_RETURNFLAG,L_LINESTATUS,L_SHIPDATE,L_COMMITDATE,L_RECEIPTDATE,L_SHIPINSTRUCT,L_SHIPMODE,L_COMMENT');2.  carbon.sql("create datamap agr1_lineitem1 ON TABLE lineitem1 USING 'org.apache.carbondata.datamap.AggregateDataMapHandler' as select l_returnflag,l_linestatus,sum(l_quantity),avg(l_quantity),count(l_quantity) from lineitem1 group by l_returnflag, l_linestatus").show();3. select l_returnflag,l_linestatus,sum(l_quantity),avg(l_quantity),count(l_quantity) from lineitem1 where l_returnflag = 'R' group by l_returnflag, l_linestatus;Actual:0: jdbc:hive2://10.18.98.136:23040> show tables;------------------------------------------------+ database           tableName          isTemporary  ------------------------------------------------+ test_db2   lineitem1                  false         test_db2   lineitem1_agr1_lineitem1   false        ------------------------------------------------+2 rows selected (0.047 seconds)Logs:2017-11-20 15:46:48,314 | INFO  | &#91;pool-23-thread-53&#93; | Running query 'select l_returnflag,l_linestatus,sum(l_quantity),avg(l_quantity),count(l_quantity) from lineitem1 where l_returnflag = 'R' group by l_returnflag, l_linestatus' with 7f3091a8-4d7b-40ac-840f-9db6f564c9cf | org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)2017-11-20 15:46:48,314 | INFO  | &#91;pool-23-thread-53&#93; | Parsing command: select l_returnflag,l_linestatus,sum(l_quantity),avg(l_quantity),count(l_quantity) from lineitem1 where l_returnflag = 'R' group by l_returnflag, l_linestatus | org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)2017-11-20 15:46:48,353 | INFO  | &#91;pool-23-thread-53&#93; | 55: get_table : db=test_db2 tbl=lineitem1 | org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.logInfo(HiveMetaStore.java:746)2017-11-20 15:46:48,353 | INFO  | &#91;pool-23-thread-53&#93; | ugi=anonymous ip=unknown-ip-addr cmd=get_table : db=test_db2 tbl=lineitem1  | org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.logAuditEvent(HiveMetaStore.java:371)2017-11-20 15:46:48,354 | INFO  | &#91;pool-23-thread-53&#93; | 55: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore | org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:589)2017-11-20 15:46:48,355 | INFO  | &#91;pool-23-thread-53&#93; | ObjectStore, initialize called | org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:289)2017-11-20 15:46:48,360 | INFO  | &#91;pool-23-thread-53&#93; | Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing | org.datanucleus.util.Log4JLogger.info(Log4JLogger.java:77)2017-11-20 15:46:48,362 | INFO  | &#91;pool-23-thread-53&#93; | Using direct SQL, underlying DB is MYSQL | org.apache.hadoop.hive.metastore.MetaStoreDirectSql.<init>(MetaStoreDirectSql.java:139)2017-11-20 15:46:48,362 | INFO  | &#91;pool-23-thread-53&#93; | Initialized ObjectStore | org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:272)2017-11-20 15:46:48,376 | INFO  | &#91;pool-23-thread-53&#93; | Parsing command: array<string> | org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)2017-11-20 15:46:48,399 | INFO  | &#91;pool-23-thread-53&#93; | Schema changes have been detected for table: `lineitem1` | org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)2017-11-20 15:46:48,399 | INFO  | &#91;pool-23-thread-53&#93; | 55: get_table : db=test_db2 tbl=lineitem1 | org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.logInfo(HiveMetaStore.java:746)2017-11-20 15:46:48,400 | INFO  | &#91;pool-23-thread-53&#93; | ugi=anonymous ip=unknown-ip-addr cmd=get_table : db=test_db2 tbl=lineitem1  | org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.logAuditEvent(HiveMetaStore.java:371)2017-11-20 15:46:48,413 | INFO  | &#91;pool-23-thread-53&#93; | Parsing command: array<string> | org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)2017-11-20 15:46:48,415 | INFO  | &#91;pool-23-thread-53&#93; | 55: get_table : db=test_db2 tbl=lineitem1 | org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.logInfo(HiveMetaStore.java:746)2017-11-20 15:46:48,415 | INFO  | &#91;pool-23-thread-53&#93; | ugi=anonymous ip=unknown-ip-addr cmd=get_table : db=test_db2 tbl=lineitem1  | org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.logAuditEvent(HiveMetaStore.java:371)2017-11-20 15:46:48,428 | INFO  | &#91;pool-23-thread-53&#93; | Parsing command: array<string> | org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)2017-11-20 15:46:48,431 | INFO  | &#91;pool-23-thread-53&#93; | 55: get_database: test_db2 | org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.logInfo(HiveMetaStore.java:746)2017-11-20 15:46:48,431 | INFO  | &#91;pool-23-thread-53&#93; | ugi=anonymous ip=unknown-ip-addr cmd=get_database: test_db2  | org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.logAuditEvent(HiveMetaStore.java:371)2017-11-20 15:46:48,434 | INFO  | &#91;pool-23-thread-53&#93; | 55: get_database: test_db2 | org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.logInfo(HiveMetaStore.java:746)2017-11-20 15:46:48,434 | INFO  | &#91;pool-23-thread-53&#93; | ugi=anonymous ip=unknown-ip-addr cmd=get_database: test_db2  | org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.logAuditEvent(HiveMetaStore.java:371)2017-11-20 15:46:48,437 | INFO  | &#91;pool-23-thread-53&#93; | 55: get_tables: db=test_db2 pat=* | org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.logInfo(HiveMetaStore.java:746)2017-11-20 15:46:48,437 | INFO  | &#91;pool-23-thread-53&#93; | ugi=anonymous ip=unknown-ip-addr cmd=get_tables: db=test_db2 pat=*  | org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.logAuditEvent(HiveMetaStore.java:371)2017-11-20 15:46:48,522 | INFO  | &#91;pool-23-thread-53&#93; | pool-23-thread-53 Starting to optimize plan | org.apache.carbondata.common.logging.impl.StandardLogService.logInfoMessage(StandardLogService.java:150)2017-11-20 15:46:48,536 | INFO  | &#91;pool-23-thread-53&#93; | pool-23-thread-53 Skip CarbonOptimizer | org.apache.carbondata.common.logging.impl.StandardLogService.logInfoMessage(StandardLogService.java:150)2017-11-20 15:46:48,679 | INFO  | &#91;pool-23-thread-53&#93; | Code generated in 41.000919 ms | org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)2017-11-20 15:46:48,766 | INFO  | &#91;pool-23-thread-53&#93; | Code generated in 61.651832 ms | org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)2017-11-20 15:46:48,821 | INFO  | &#91;pool-23-thread-53&#93; | pool-23-thread-53 Table block size not specified for test_db2_lineitem1. Therefore considering the default value 1024 MB | org.apache.carbondata.common.logging.impl.StandardLogService.logInfoMessage(StandardLogService.java:150)2017-11-20 15:46:48,872 | INFO  | &#91;pool-23-thread-53&#93; | pool-23-thread-53 Time taken to load blocklet datamap from file : hdfs://hacluster/user/test2/lineitem1/Fact/Part0/Segment_0/1_batchno0-0-1511163544085.carbonindexis 2 | org.apache.carbondata.common.logging.impl.StandardLogService.logInfoMessage(StandardLogService.java:150)2017-11-20 15:46:48,873 | INFO  | &#91;pool-23-thread-53&#93; | pool-23-thread-53 Time taken to load blocklet datamap from file : hdfs://hacluster/user/test2/lineitem1/Fact/Part0/Segment_0/0_batchno0-0-1511163544085.carbonindexis 1 | org.apache.carbondata.common.logging.impl.StandardLogService.logInfoMessage(StandardLogService.java:150)2017-11-20 15:46:48,884 | INFO  | &#91;pool-23-thread-53&#93; |  Identified no.of.blocks: 2, no.of.tasks: 2, no.of.nodes: 0, parallelism: 2
issueID:CARBONDATA-1778
type:Improvement
changed files:
texts:Support clean garbage segments for all

issueID:CARBONDATA-1779
type:Improvement
changed files:integration/presto/src/main/java/org/apache/carbondata/presto/PrestoFilterUtil.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/merger/UnsafeIntermediateFileMerger.java
core/src/main/java/org/apache/carbondata/core/scan/result/vector/impl/CarbonColumnVectorImpl.java
integration/presto/src/main/java/org/apache/carbondata/presto/readers/IntegerStreamReader.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/store/impl/safe/SafeVariableLengthDimensionDataChunkStore.java
integration/presto/src/main/java/org/apache/carbondata/presto/readers/DecimalSliceStreamReader.java
integration/presto/src/main/java/org/apache/carbondata/presto/readers/DoubleStreamReader.java
integration/presto/src/main/java/org/apache/carbondata/presto/readers/LongStreamReader.java
integration/presto/src/main/java/org/apache/carbondata/presto/CarbondataPageSource.java
integration/presto/src/main/java/org/apache/carbondata/presto/impl/CarbonTableReader.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/store/impl/unsafe/UnsafeVariableLengthDimensionDataChunkStore.java
core/src/main/java/org/apache/carbondata/core/scan/collector/impl/RestructureBasedVectorResultCollector.java
integration/presto/src/main/java/org/apache/carbondata/presto/CarbondataConnectorFactory.java
core/src/main/java/org/apache/carbondata/core/scan/result/vector/CarbonColumnVector.java
integration/presto/src/main/java/org/apache/carbondata/presto/CarbonVectorBatch.java
integration/presto/src/main/java/org/apache/carbondata/presto/PrestoCarbonVectorizedRecordReader.java
integration/presto/src/main/java/org/apache/carbondata/presto/readers/ObjectStreamReader.java
integration/presto/src/main/java/org/apache/carbondata/presto/readers/ShortStreamReader.java
integration/presto/src/main/java/org/apache/carbondata/presto/readers/PrestoVectorBlockBuilder.java
integration/presto/src/main/java/org/apache/carbondata/presto/readers/SliceStreamReader.java
integration/presto/src/main/java/org/apache/carbondata/presto/readers/TimestampStreamReader.java
core/src/main/java/org/apache/carbondata/core/scan/result/vector/MeasureDataVectorProcessor.java
integration/spark-datasource/src/main/scala/org/apache/carbondata/spark/vectorreader/ColumnarVectorWrapper.java
integration/presto/src/main/java/org/apache/carbondata/presto/CarbonColumnVectorWrapper.java
texts:GeneriVectorizedReader for Presto
Write a Generic Vectorized Reader for Presto to remove the dependencies on spark
issueID:CARBONDATA-178
type:Bug
changed files:
texts:table not exist when execute show segments using spark-sql and beeline the same time
1 When using beeline and sparksql the same time, if create a table and load data into it by sparksql, beeline and sparksql would see the same table by show tables, but if execute show segments by beeline, it would throws exception that this table is not exists.2. But if restart beeline or select the table before show segements, it is OK.
issueID:CARBONDATA-1780
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/datastore/impl/FileFactory.java
core/src/main/java/org/apache/carbondata/core/datastore/filesystem/HDFSCarbonFile.java
core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
core/src/main/java/org/apache/carbondata/core/datastore/filesystem/AbstractDFSCarbonFile.java
texts:Create configuration from SparkSession for data loading
Create configuration form SparkSession for data loading
issueID:CARBONDATA-1781
type:Bug
changed files:streaming/src/main/java/org/apache/carbondata/streaming/StreamBlockletReader.java
texts:(Carbon1.3.0 - Streaming) Select * & select column fails but select count(*) is success when .streaming file is removed from HDFS
Steps :Thrift server is started using the command - bin/spark-submit --master yarn-client --executor-memory 10G --executor-cores 5 --driver-memory 5G --num-executors 3 --class org.apache.carbondata.spark.thriftserver.CarbonThriftServer /srv/spark2.2Bigdata/install/spark/sparkJdbc/carbonlib/carbondata_2.11-1.3.0-SNAPSHOT-shade-hadoop2.7.2.jar "hdfs://hacluster/user/hive/warehouse/carbon.store"Spark shell is opened using the command - bin/spark-shell --master yarn-client --executor-memory 10G --executor-cores 5 --driver-memory 5G --num-executors 3 --jars /srv/spark2.2Bigdata/install/spark/sparkJdbc/carbonlib/carbondata_2.11-1.3.0-SNAPSHOT-shade-hadoop2.7.2.jarFrom spark shell the below code is executed -import java.io.{File, PrintWriter}import java.net.ServerSocketimport org.apache.spark.sql.{CarbonEnv, SparkSession}import org.apache.spark.sql.hive.CarbonRelationimport org.apache.spark.sql.streaming.{ProcessingTime, StreamingQuery}import org.apache.carbondata.core.constants.CarbonCommonConstantsimport org.apache.carbondata.core.util.CarbonPropertiesimport org.apache.carbondata.core.util.path.{CarbonStorePath, CarbonTablePath}CarbonProperties.getInstance().addProperty(CarbonCommonConstants.CARBON_TIMESTAMP_FORMAT, "yyyy/MM/dd")import org.apache.spark.sql.CarbonSession._val carbonSession = SparkSession.  builder().  appName("StreamExample").  getOrCreateCarbonSession("hdfs://hacluster/user/hive/warehouse/carbon.store")carbonSession.sparkContext.setLogLevel("INFO")def sql(sql: String) = carbonSession.sql(sql)def writeSocket(serverSocket: ServerSocket): Thread = {  val thread = new Thread() {    override def run(): Unit = {      // wait for client to connection request and accept      val clientSocket = serverSocket.accept()      val socketWriter = new PrintWriter(clientSocket.getOutputStream())      var index = 0      for (_ <- 1 to 1000) {        // write 5 records per iteration        for (_ <- 0 to 100) {          index = index + 1          socketWriter.println(index.toString + ",name_" + index                               + ",city_" + index + "," + (index * 10000.00).toString +                               ",school_" + index + ":school_" + index + index + "$" + index)        }        socketWriter.flush()        Thread.sleep(2000)      }      socketWriter.close()      System.out.println("Socket closed")    }  }  thread.start()  thread}def startStreaming(spark: SparkSession, tablePath: CarbonTablePath, tableName: String, port: Int): Thread = {    val thread = new Thread() {      override def run(): Unit = {        var qry: StreamingQuery = null        try {          val readSocketDF = spark.readStream            .format("socket")            .option("host", "10.18.98.34")            .option("port", port)            .load()          qry = readSocketDF.writeStream            .format("carbondata")            .trigger(ProcessingTime("5 seconds"))            .option("checkpointLocation", tablePath.getStreamingCheckpointDir)            .option("tablePath", tablePath.getPath).option("tableName", tableName)            .start()          qry.awaitTermination()        } catch {          case ex: Throwable =>            ex.printStackTrace()            println("Done reading and writing streaming data")        } finally {          qry.stop()        }      }    }    thread.start()    thread}val streamTableName = "brinjal"sql(s"drop table brinjal").showsql(s"create table brinjal (imei string,AMSize string,channelsId string,ActiveCountry string, Activecity string,gamePointId double,deviceInformationId double,productionDate Timestamp,deliveryDate timestamp,deliverycharge double) STORED BY 'org.apache.carbondata.format' TBLPROPERTIES('streaming'='true','table_blocksize'='1')")sql(s"LOAD DATA INPATH 'hdfs://hacluster/chetan/vardhandaterestruct.csv' INTO TABLE brinjal OPTIONS('DELIMITER'=',', 'QUOTECHAR'= '','BAD_RECORDS_ACTION'='FORCE','FILEHEADER'= 'imei,deviceInformationId,AMSize,channelsId,ActiveCountry,Activecity,gamePointId,productionDate,deliveryDate,deliverycharge')")val carbonTable = CarbonEnv.getInstance(carbonSession).carbonMetastore.  lookupRelation(Some("default"), streamTableName)(carbonSession).asInstanceOf&#91;CarbonRelation&#93;.carbonTableval tablePath = CarbonStorePath.getCarbonTablePath(carbonTable.getAbsoluteTableIdentifier)val port = 8002val serverSocket = new ServerSocket(port)val socketThread = writeSocket(serverSocket)val streamingThread = startStreaming(carbonSession, tablePath, streamTableName, port)From other terminal user deletes the .streaming file - BLR1000014307:/srv/spark2.2Bigdata/install/hadoop/datanode # bin/hadoop fs -rm -r /user/hive/warehouse/carbon.store/default/brinjal/.streaming17/11/20 19:02:11 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicableDeleted /user/hive/warehouse/carbon.store/default/brinjal/.streamingAlso the thrift server spark submit process is killed when streaming is in progress.From beeline session user executes the select * , select column and select count queries.Issue : Select * and select column queries fail and throw exception while select count displays result set.. 0: jdbc:hive2://10.18.98.34:23040> select * from brinjal;Error: org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 17.0 failed 4 times, most recent failure: Lost task 4.3 in stage 17.0 (TID 120, BLR1000014307, executor 12): java.io.EOFException        at org.apache.carbondata.hadoop.streaming.StreamBlockletReader.readBytesFromStream(StreamBlockletReader.java:182)        at org.apache.carbondata.hadoop.streaming.StreamBlockletReader.readBlockletData(StreamBlockletReader.java:116)        at org.apache.carbondata.hadoop.streaming.CarbonStreamRecordReader.scanBlockletAndFillVector(CarbonStreamRecordReader.java:406)        at org.apache.carbondata.hadoop.streaming.CarbonStreamRecordReader.nextColumnarBatch(CarbonStreamRecordReader.java:317)        at org.apache.carbondata.hadoop.streaming.CarbonStreamRecordReader.nextKeyValue(CarbonStreamRecordReader.java:298)        at org.apache.carbondata.spark.rdd.CarbonScanRDD$$anon$1.hasNext(CarbonScanRDD.scala:298)        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.scan_nextBatch$(Unknown Source)        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)        at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)        at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)        at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:231)        at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:225)        at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:826)        at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:826)        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)        at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)        at org.apache.spark.scheduler.Task.run(Task.scala:99)        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)        at java.lang.Thread.run(Thread.java:745)Driver stacktrace: (state=,code=0)0: jdbc:hive2://10.18.98.34:23040> select imei from brinjal;Error: org.apache.spark.SparkException: Job aborted due to stage failure: Task 5 in stage 18.0 failed 4 times, most recent failure: Lost task 5.3 in stage 18.0 (TID 136, BLR1000014307, executor 13): java.io.EOFException        at org.apache.carbondata.hadoop.streaming.StreamBlockletReader.readBytesFromStream(StreamBlockletReader.java:182)        at org.apache.carbondata.hadoop.streaming.StreamBlockletReader.readBlockletData(StreamBlockletReader.java:116)        at org.apache.carbondata.hadoop.streaming.CarbonStreamRecordReader.scanBlockletAndFillVector(CarbonStreamRecordReader.java:406)        at org.apache.carbondata.hadoop.streaming.CarbonStreamRecordReader.nextColumnarBatch(CarbonStreamRecordReader.java:317)        at org.apache.carbondata.hadoop.streaming.CarbonStreamRecordReader.nextKeyValue(CarbonStreamRecordReader.java:298)        at org.apache.carbondata.spark.rdd.CarbonScanRDD$$anon$1.hasNext(CarbonScanRDD.scala:298)        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.scan_nextBatch$(Unknown Source)        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)        at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)        at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)        at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:231)        at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:225)        at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:826)        at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:826)        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)        at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)        at org.apache.spark.scheduler.Task.run(Task.scala:99)        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)        at java.lang.Thread.run(Thread.java:745)Driver stacktrace: (state=,code=0)0: jdbc:hive2://10.18.98.34:23040> select count from brinjal;-----------+ count(1)  -----------+ 11916     -----------+1 row selected (0.545 seconds)Expected : The behavior of select * and select column should be consistent with the select count behavior for streaming query. when .streaming file is deleted when streaming is in progress.
issueID:CARBONDATA-1782
type:Bug
changed files:
texts:(Carbon1.3.0 - Streaming) Select regexp_extract from table with where clause having is null throws indexoutofbounds exception
Steps :Thrift server is started using the command - bin/spark-submit --master yarn-client --executor-memory 10G --executor-cores 5 --driver-memory 5G --num-executors 3 --class org.apache.carbondata.spark.thriftserver.CarbonThriftServer /srv/spark2.2Bigdata/install/spark/sparkJdbc/carbonlib/carbondata_2.11-1.3.0-SNAPSHOT-shade-hadoop2.7.2.jar "hdfs://hacluster/user/sparkhive/warehouse"Spark shell is launched using the command - bin/spark-shell --master yarn-client --executor-memory 10G --executor-cores 5 --driver-memory 5G --num-executors 3 --jars /srv/spark2.2Bigdata/install/spark/sparkJdbc/carbonlib/carbondata_2.11-1.3.0-SNAPSHOT-shade-hadoop2.7.2.jarFrom Spark shell the streaming table is created and data is loaded to the streaming table.import java.io.{File, PrintWriter}import java.net.ServerSocketimport org.apache.spark.sql.{CarbonEnv, SparkSession}import org.apache.spark.sql.hive.CarbonRelationimport org.apache.spark.sql.streaming.{ProcessingTime, StreamingQuery}import org.apache.carbondata.core.constants.CarbonCommonConstantsimport org.apache.carbondata.core.util.CarbonPropertiesimport org.apache.carbondata.core.util.path.{CarbonStorePath, CarbonTablePath}CarbonProperties.getInstance().addProperty(CarbonCommonConstants.CARBON_TIMESTAMP_FORMAT, "yyyy/MM/dd")import org.apache.spark.sql.CarbonSession._val carbonSession = SparkSession.  builder().  appName("StreamExample").  getOrCreateCarbonSession("hdfs://hacluster/user/hive/warehouse/carbon.store")carbonSession.sparkContext.setLogLevel("INFO")def sql(sql: String) = carbonSession.sql(sql)def writeSocket(serverSocket: ServerSocket): Thread = {  val thread = new Thread() {    override def run(): Unit = {      // wait for client to connection request and accept      val clientSocket = serverSocket.accept()      val socketWriter = new PrintWriter(clientSocket.getOutputStream())      var index = 0      for (_ <- 1 to 1000) {        // write 5 records per iteration        for (_ <- 0 to 100) {          index = index + 1          socketWriter.println(index.toString + ",name_" + index                               + ",city_" + index + "," + (index * 10000.00).toString +                               ",school_" + index + ":school_" + index + index + "$" + index)        }        socketWriter.flush()        Thread.sleep(2000)      }      socketWriter.close()      System.out.println("Socket closed")    }  }  thread.start()  thread}def startStreaming(spark: SparkSession, tablePath: CarbonTablePath, tableName: String, port: Int): Thread = {    val thread = new Thread() {      override def run(): Unit = {        var qry: StreamingQuery = null        try {          val readSocketDF = spark.readStream            .format("socket")            .option("host", "10.18.98.34")            .option("port", port)            .load()          qry = readSocketDF.writeStream            .format("carbondata")            .trigger(ProcessingTime("5 seconds"))            .option("checkpointLocation", tablePath.getStreamingCheckpointDir)            .option("tablePath", tablePath.getPath).option("tableName", tableName)            .start()          qry.awaitTermination()        } catch {          case ex: Throwable =>            ex.printStackTrace()            println("Done reading and writing streaming data")        } finally {          qry.stop()        }      }    }    thread.start()    thread}val streamTableName = "uniqdata"sql(s"CREATE TABLE uniqdata (CUST_ID int,CUST_NAME String,ACTIVE_EMUI_VERSION string, DOB timestamp, DOJ timestamp, BIGINT_COLUMN1 bigint,BIGINT_COLUMN2 bigint,DECIMAL_COLUMN1 decimal(30,10), DECIMAL_COLUMN2 decimal(36,36),Double_COLUMN1 double, Double_COLUMN2 double,INTEGER_COLUMN1 int) STORED BY 'org.apache.carbondata.format' TBLPROPERTIES('streaming'='true')")sql(s"LOAD DATA INPATH 'hdfs://hacluster/chetan/2000_UniqData.csv' into table uniqdata OPTIONS( 'BAD_RECORDS_ACTION'='FORCE','FILEHEADER'='CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1,Double_COLUMN2,INTEGER_COLUMN1')")val carbonTable = CarbonEnv.getInstance(carbonSession).carbonMetastore.  lookupRelation(Some("default"), streamTableName)(carbonSession).asInstanceOf&#91;CarbonRelation&#93;.carbonTableval tablePath = CarbonStorePath.getCarbonTablePath(carbonTable.getAbsoluteTableIdentifier)val port = 8006val serverSocket = new ServerSocket(port)val socketThread = writeSocket(serverSocket)val streamingThread = startStreaming(carbonSession, tablePath, streamTableName, port)From Beeline user executes the queryselect regexp_extract(CUST_NAME,'a',1)from uniqdata where regexp_extract(CUST_NAME,'a',1) IS NULL or regexp_extract(DOB,'b',2) is NULL;Issue : Select regexp_extract from table with where clause having is null throws indexoutofbounds exception0: jdbc:hive2://10.18.98.34:23040> select regexp_extract(CUST_NAME,'a',1)from uniqdata where regexp_extract(CUST_NAME,'a',1) IS NULL or regexp_extract(DOB,'b',2) is NULL;Error: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 198.0 failed 4 times, most recent failure: Lost task 1.3 in stage 198.0 (TID 1634, BLR1000014269, executor 8): jagroup 1va.lang.IndexOutOfBoundsException: No         at java.util.regex.Matcher.group(Matcher.java:538)        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)        at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)        at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)        at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:231)        at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:225)        at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:826)        at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:826)        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)        at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)        at org.apache.spark.scheduler.Task.run(Task.scala:99)        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)        at java.lang.Thread.run(Thread.java:745)Driver stacktrace: (state=,code=0)Expected : Select regexp_extract from table with where clause having is null should be successful.
issueID:CARBONDATA-1783
type:Bug
changed files:
texts:(Carbon1.3.0 - Streaming) Error "Failed to filter row in vector reader" when filter query executed on streaming data
Steps :-Spark submit thrift server is started using the command - bin/spark-submit --master yarn-client --executor-memory 10G --executor-cores 5 --driver-memory 5G --num-executors 3 --class org.apache.carbondata.spark.thriftserver.CarbonThriftServer /srv/spark2.2Bigdata/install/spark/sparkJdbc/carbonlib/carbondata_2.11-1.3.0-SNAPSHOT-shade-hadoop2.7.2.jar "hdfs://hacluster/user/hive/warehouse/carbon.store"Spark shell is launched using the command - bin/spark-shell --master yarn-client --executor-memory 10G --executor-cores 5 --driver-memory 5G --num-executors 3 --jars /srv/spark2.2Bigdata/install/spark/sparkJdbc/carbonlib/carbondata_2.11-1.3.0-SNAPSHOT-shade-hadoop2.7.2.jarFrom Spark shell user creates table and loads data in the table as shown below.import java.io.{File, PrintWriter}import java.net.ServerSocketimport org.apache.spark.sql.{CarbonEnv, SparkSession}import org.apache.spark.sql.hive.CarbonRelationimport org.apache.spark.sql.streaming.{ProcessingTime, StreamingQuery}import org.apache.carbondata.core.constants.CarbonCommonConstantsimport org.apache.carbondata.core.util.CarbonPropertiesimport org.apache.carbondata.core.util.path.{CarbonStorePath, CarbonTablePath}CarbonProperties.getInstance().addProperty(CarbonCommonConstants.CARBON_TIMESTAMP_FORMAT, "yyyy/MM/dd")import org.apache.spark.sql.CarbonSession._val carbonSession = SparkSession.  builder().  appName("StreamExample").  getOrCreateCarbonSession("hdfs://hacluster/user/hive/warehouse/carbon.store")carbonSession.sparkContext.setLogLevel("INFO")def sql(sql: String) = carbonSession.sql(sql)def writeSocket(serverSocket: ServerSocket): Thread = {  val thread = new Thread() {    override def run(): Unit = {      // wait for client to connection request and accept      val clientSocket = serverSocket.accept()      val socketWriter = new PrintWriter(clientSocket.getOutputStream())      var index = 0      for (_ <- 1 to 1000) {        // write 5 records per iteration        for (_ <- 0 to 100) {          index = index + 1          socketWriter.println(index.toString + ",name_" + index                               + ",city_" + index + "," + (index * 10000.00).toString +                               ",school_" + index + ":school_" + index + index + "$" + index)        }        socketWriter.flush()        Thread.sleep(2000)      }      socketWriter.close()      System.out.println("Socket closed")    }  }  thread.start()  thread}def startStreaming(spark: SparkSession, tablePath: CarbonTablePath, tableName: String, port: Int): Thread = {    val thread = new Thread() {      override def run(): Unit = {        var qry: StreamingQuery = null        try {          val readSocketDF = spark.readStream            .format("socket")            .option("host", "10.18.98.34")            .option("port", port)            .load()          qry = readSocketDF.writeStream            .format("carbondata")            .trigger(ProcessingTime("5 seconds"))            .option("checkpointLocation", tablePath.getStreamingCheckpointDir)            .option("tablePath", tablePath.getPath).option("tableName", tableName)            .start()          qry.awaitTermination()        } catch {          case ex: Throwable =>            ex.printStackTrace()            println("Done reading and writing streaming data")        } finally {          qry.stop()        }      }    }    thread.start()    thread}val streamTableName = "all_datatypes_2048"sql(s"create table all_datatypes_2048 (imei string,deviceInformationId int,MAC string,deviceColor string,device_backColor string,modelId string,marketName string,AMSize string,ROMSize string,CUPAudit string,CPIClocked string,series string,productionDate timestamp,bomCode string,internalModels string, deliveryTime string, channelsId string, channelsName string , deliveryAreaId string, deliveryCountry string, deliveryProvince string, deliveryCity string,deliveryDistrict string, deliveryStreet string, oxSingleNumber string, ActiveCheckTime string, ActiveAreaId string, ActiveCountry string, ActiveProvince string, Activecity string, ActiveDistrict string, ActiveStreet string, ActiveOperatorId string, Active_releaseId string, Active_EMUIVersion string, Active_operaSysVersion string, Active_BacVerNumber string, Active_BacFlashVer string, Active_webUIVersion string, Active_webUITypeCarrVer string,Active_webTypeDataVerNumber string, Active_operatorsVersion string, Active_phonePADPartitionedVersions string, Latest_YEAR int, Latest_MONTH int, Latest_DAY Decimal(30,10), Latest_HOUR string, Latest_areaId string, Latest_country string, Latest_province string, Latest_city string, Latest_district string, Latest_street string, Latest_releaseId string, Latest_EMUIVersion string, Latest_operaSysVersion string, Latest_BacVerNumber string, Latest_BacFlashVer string, Latest_webUIVersion string, Latest_webUITypeCarrVer string, Latest_webTypeDataVerNumber string, Latest_operatorsVersion string, Latest_phonePADPartitionedVersions string, Latest_operatorId string, gamePointDescription string,gamePointId double,contractNumber BigInt) STORED BY 'org.apache.carbondata.format' TBLPROPERTIES('streaming'='true','table_blocksize'='2048')")sql(s"LOAD DATA INPATH 'hdfs://hacluster/chetan/100_olap_C20.csv' INTO table all_datatypes_2048 options ('DELIMITER'=',', 'BAD_RECORDS_ACTION'='FORCE','FILEHEADER'='imei,deviceInformationId,MAC,deviceColor,device_backColor,modelId,marketName,AMSize,ROMSize,CUPAudit,CPIClocked,series,productionDate,bomCode,internalModels,deliveryTime,channelsId,channelsName,deliveryAreaId,deliveryCountry,deliveryProvince,deliveryCity,deliveryDistrict,deliveryStreet,oxSingleNumber,contractNumber,ActiveCheckTime,ActiveAreaId,ActiveCountry,ActiveProvince,Activecity,ActiveDistrict,ActiveStreet,ActiveOperatorId,Active_releaseId,Active_EMUIVersion,Active_operaSysVersion,Active_BacVerNumber,Active_BacFlashVer,Active_webUIVersion,Active_webUITypeCarrVer,Active_webTypeDataVerNumber,Active_operatorsVersion,Active_phonePADPartitionedVersions,Latest_YEAR,Latest_MONTH,Latest_DAY,Latest_HOUR,Latest_areaId,Latest_country,Latest_province,Latest_city,Latest_district,Latest_street,Latest_releaseId,Latest_EMUIVersion,Latest_operaSysVersion,Latest_BacVerNumber,Latest_BacFlashVer,Latest_webUIVersion,Latest_webUITypeCarrVer,Latest_webTypeDataVerNumber,Latest_operatorsVersion,Latest_phonePADPartitionedVersions,Latest_operatorId,gamePointId,gamePointDescription')")val carbonTable = CarbonEnv.getInstance(carbonSession).carbonMetastore.  lookupRelation(Some("default"), streamTableName)(carbonSession).asInstanceOf&#91;CarbonRelation&#93;.carbonTableval tablePath = CarbonStorePath.getCarbonTablePath(carbonTable.getAbsoluteTableIdentifier)val port = 8007val serverSocket = new ServerSocket(port)val socketThread = writeSocket(serverSocket)val streamingThread = startStreaming(carbonSession, tablePath, streamTableName, port)While the streaming load is in progress from Beeline user executes the below select filter query  select imei,gamePointId, channelsId,series  from all_datatypes_2048 where channelsId >=10 OR channelsId <=1 and series='7Series';Issue : The select filter query fails with exception as shown below.0: jdbc:hive2://10.18.98.34:23040> select imei,gamePointId, channelsId,series  from all_datatypes_2048 where channelsId >=10 OR channelsId <=1 and series='7Series';Error: org.apache.spark.SparkException: Job aborted due to stage failure: Task 6 in stage 773.0 failed 4 times, most recent failure: Lost task 6.3 in stage 773.0 (TID 33727, BLR1000014269, executor 14): java.io.IOException: Failed to filter row in vector reader        at org.apache.carbondata.hadoop.streaming.CarbonStreamRecordReader.scanBlockletAndFillVector(CarbonStreamRecordReader.java:423)        at org.apache.carbondata.hadoop.streaming.CarbonStreamRecordReader.nextColumnarBatch(CarbonStreamRecordReader.java:317)        at org.apache.carbondata.hadoop.streaming.CarbonStreamRecordReader.nextKeyValue(CarbonStreamRecordReader.java:298)        at org.apache.carbondata.spark.rdd.CarbonScanRDD$$anon$1.hasNext(CarbonScanRDD.scala:298)        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.scan_nextBatch$(Unknown Source)        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)        at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)        at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)        at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:231)        at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:225)        at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:826)        at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:826)        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)        at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)        at org.apache.spark.scheduler.Task.run(Task.scala:99)        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)        at java.lang.Thread.run(Thread.java:745)Caused by: org.apache.carbondata.core.scan.expression.exception.FilterUnsupportedException: [B cannot be cast to org.apache.spark.unsafe.types.UTF8String        at org.apache.spark.sql.SparkUnknownExpression.evaluate(SparkUnknownExpression.scala:50)        at org.apache.carbondata.core.scan.expression.conditional.GreaterThanEqualToExpression.evaluate(GreaterThanEqualToExpression.java:38)        at org.apache.carbondata.core.scan.filter.executer.RowLevelFilterExecuterImpl.applyFilter(RowLevelFilterExecuterImpl.java:272)        at org.apache.carbondata.core.scan.filter.executer.OrFilterExecuterImpl.applyFilter(OrFilterExecuterImpl.java:49)        at org.apache.carbondata.hadoop.streaming.CarbonStreamRecordReader.scanBlockletAndFillVector(CarbonStreamRecordReader.java:418)        ... 20 moreDriver stacktrace: (state=,code=0)Expected : The select filter query should be success without error/exception.The issue also occurs with the below queriesselect imei,gamePointId, channelsId,series  from all_datatypes_2048 where channelsId >=10 OR channelsId <=1 or series='7Series';select imei,gamePointId, channelsId,series  from all_datatypes_2048 where channelsId >=10 OR (channelsId <=1 and series='1Series');select sum(gamePointId) a from all_datatypes_2048 where channelsId >=10 OR (channelsId <=1 and series='1Series');select * from (select imei,if(imei='1AA100060',NULL,imei) a from all_datatypes_2048) aa  where a IS NULL; select imei from all_datatypes_2048 where  (contractNumber == 5281803) and (gamePointId==2738.562); select deliveryCity from all_datatypes_2048 where  (deliveryCity == 'yichang') and ( deliveryStreet=='yichang'); select channelsId  from all_datatypes_2048 where  (channelsId == '4') and (gamePointId==2738.562); select imei from all_datatypes_2048 where  (contractNumber == 5281803) OR (gamePointId==2738.562) order by contractNumber ;  select channelsId from all_datatypes_2048 where  (channelsId == '4') OR (gamePointId==2738.562) order by channelsId ; select deliveryCity  from all_datatypes_2048 where  (deliveryCity == 'yichang') OR ( deliveryStreet=='yichang') order by deliveryCity;
issueID:CARBONDATA-1785
type:Improvement
changed files:
texts:Add Coveralls codecoverage badge to carbondata

issueID:CARBONDATA-1786
type:Bug
changed files:
texts:Getting null pointer exception while loading data into table and while fetching data getting NULL values
Getting null pointer exception while loading data into table and while fetching data getting NULL valuesSteps to reproduce:1)Create table: CREATE TABLE uniqdata (CUST_ID int,CUST_NAME String,ACTIVE_EMUI_VERSION string, DOB timestamp, DOJ timestamp, BIGINT_COLUMN1 bigint,BIGINT_COLUMN2 bigint,DECIMAL_COLUMN1 decimal(30,10), DECIMAL_COLUMN2 decimal(36,10),Double_COLUMN1 double, Double_COLUMN2 double,INTEGER_COLUMN1 int) STORED BY 'org.apache.carbondata.format' TBLPROPERTIES ("TABLE_BLOCKSIZE"= "256 MB");2)Load DataLOAD DATA INPATH 'hdfs://localhost:54310/Data/uniqdata/2000_UniqData.csv' into table uniqdata OPTIONS('DELIMITER'='/' , 'QUOTECHAR'='"','BAD_RECORDS_ACTION'='FORCE','FILEHEADER'='CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1,Double_COLUMN2,INTEGER_COLUMN1','TIMESTAMPFORMAT'='yyyy-mm-dd hh:mm:ss');3) Expected result: it should load data into table successfully.4) Actual Result: it throws an errorError: java.lang.NullPointerException (state=,code=0)logs:java.lang.NullPointerException at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:369) at org.apache.hadoop.fs.Path.getFileSystem(Path.java:295) at org.apache.carbondata.core.datastore.filesystem.AbstractDFSCarbonFile.delete(AbstractDFSCarbonFile.java:142) at org.apache.carbondata.processing.util.DeleteLoadFolders.physicalFactAndMeasureMetadataDeletion(DeleteLoadFolders.java:79) at org.apache.carbondata.processing.util.DeleteLoadFolders.deleteLoadFoldersFromFileSystem(DeleteLoadFolders.java:134) at org.apache.carbondata.spark.rdd.DataManagementFunc$.deleteLoadsAndUpdateMetadata(DataManagementFunc.scala:188) at org.apache.carbondata.spark.rdd.CarbonDataRDDFactory$.loadCarbonData(CarbonDataRDDFactory.scala:281) at org.apache.spark.sql.execution.command.management.LoadTableCommand.loadData(LoadTableCommand.scala:347) at org.apache.spark.sql.execution.command.management.LoadTableCommand.processData(LoadTableCommand.scala:183) at org.apache.spark.sql.execution.command.management.LoadTableCommand.run(LoadTableCommand.scala:64) at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58) at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56) at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74) at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114) at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114) at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:135) at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151) at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:132) at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:113) at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:87) at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:87) at org.apache.spark.sql.Dataset.<init>(Dataset.scala:185) at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:64) at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:592) at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:699) at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:220) at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1$$anon$2.run(SparkExecuteStatementOperation.scala:163) at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1$$anon$2.run(SparkExecuteStatementOperation.scala:160) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1698) at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1.run(SparkExecuteStatementOperation.scala:173) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745)17/11/21 12:12:21 ERROR SparkExecuteStatementOperation: Error running hive query: org.apache.hive.service.cli.HiveSQLException: java.lang.NullPointerException at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:258) at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1$$anon$2.run(SparkExecuteStatementOperation.scala:163) at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1$$anon$2.run(SparkExecuteStatementOperation.scala:160) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1698) at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1.run(SparkExecuteStatementOperation.scala:173) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745)5) Execute query:select * from uniqdata:6) Expected result: it should display zero rows as our load was not successful.7) Actual Result: it displays no of rows of pre-existing table with NULL values NULL      NULL        NULL                  NULL   NULL   NULL             NULL             NULL              NULL              NULL             NULL             NULL              NULL      NULL        NULL                  NULL   NULL   NULL             NULL             NULL              NULL              NULL             NULL             NULL              NULL      NULL        NULL                  NULL   NULL   NULL             NULL             NULL              NULL              NULL             NULL             NULL              NULL      NULL        NULL                  NULL   NULL   NULL             NULL             NULL              NULL              NULL             NULL             NULL              NULL      NULL        NULL                  NULL   NULL   NULL             NULL             NULL              NULL              NULL             NULL             NULL              NULL      NULL        NULL                  NULL   NULL   NULL             NULL             NULL              NULL              NULL             NULL             NULL             --------------------------------------------------------------------------------------------------------------------------------------------------------------45,117 rows selected (2.222 seconds)
issueID:CARBONDATA-1787
type:Bug
changed files:
texts:Carbon 1.3.0- Global Sort: Global_Sort_Partitions parameter doesn&#39;t work, if specified in the Tblproperties, while creating the table.
Steps:1. create table tstcust(c_custkey int, c_name string, c_address string, c_nationkey bigint, c_phone string,c_acctbal decimal, c_mktsegment string, c_comment string) STORED BY 'org.apache.carbondata.format' tblproperties('sort_scope'='global_sort','GLOBAL_SORT_PARTITIONS'='2');Issue: Global_Sort_Partitions when specified during creation of table, it doesn't work, whereas the same property works, if specified during the data load. Expected:Either it should throw error for the property if it is specified in the load like it throws for the sort_scope or the same thing should be updated in the document.
issueID:CARBONDATA-1789
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/statusmanager/SegmentStatusManager.java
processing/src/main/java/org/apache/carbondata/processing/util/CarbonLoaderUtil.java
texts:Carbon1.3.0 Concurrent Load-Drop: user is able to drop table even if insert/load job is running
Carbon1.3.0 Concurrent Load-Drop: user is able to drop table even if insert/load job is runningSteps:1:  Create a table2: Start a insert job3: Concurrently drop the table4: Observe that drop is success5: Observe that insert job is running and after some times job failsExpected behvaiour: drop job should wait for insert job to complete
issueID:CARBONDATA-179
type:Bug
changed files:
texts:describe new table show old table&#39;s schema
when do step like this:CREATE TABLE age2(period string,max int,min int) STORED BY 'org.apache.carbondata.format';desc age2;drop table age2;CREATE TABLE age2(city_id string,city_name string) STORED BY 'org.apache.carbondata.format';desc age2;the second desc will show  (period string,max int,min int), not (city_id string,city_name string)
issueID:CARBONDATA-1790
type:Bug
changed files:
texts:(Carbon1.3.0 - Streaming) Data load in Stream Segment fails if batch load is performed in between the streaming
Steps :1. Create a streaming table and do a batch load2. Set up the Streaming , so that it does streaming in chunk of 1000 records 20 times3. Do another batch load on the table4. Do one more time streaming--------------------------------------------------------------------------------------------- Segment Id     Status        Load Start Time            Load End Time        File Format   Merged To  --------------------------------------------------------------------------------------------- 2            Success     2017-11-21 21:42:36.77    2017-11-21 21:42:40.396   COLUMNAR_V3   NA          1            Streaming   2017-11-21 21:40:46.2     NULL                      ROW_V1        NA          0            Success     2017-11-21 21:40:39.782   2017-11-21 21:40:43.168   COLUMNAR_V3   NA         ---------------------------------------------------------------------------------------------Expected: Data should be loadedActual : Data load fiails1. One addition offset file is created(marked in bold)rw-rr-   2 root users         62 2017-11-21 21:40 /user/hive/warehouse/Ram/default/stream_table5/.streaming/checkpoint/offsets/0rw-rr-   2 root users         63 2017-11-21 21:40 /user/hive/warehouse/Ram/default/stream_table5/.streaming/checkpoint/offsets/1rw-rr-   2 root users         63 2017-11-21 21:42 /user/hive/warehouse/Ram/default/stream_table5/.streaming/checkpoint/offsets/10rw-rr-   2 root users         63 2017-11-21 21:40 /user/hive/warehouse/Ram/default/stream_table5/.streaming/checkpoint/offsets/2rw-rr-   2 root users         63 2017-11-21 21:41 /user/hive/warehouse/Ram/default/stream_table5/.streaming/checkpoint/offsets/3rw-rr-   2 root users         64 2017-11-21 21:41 /user/hive/warehouse/Ram/default/stream_table5/.streaming/checkpoint/offsets/4rw-rr-   2 root users         64 2017-11-21 21:41 /user/hive/warehouse/Ram/default/stream_table5/.streaming/checkpoint/offsets/5rw-rr-   2 root users         64 2017-11-21 21:41 /user/hive/warehouse/Ram/default/stream_table5/.streaming/checkpoint/offsets/6rw-rr-   2 root users         64 2017-11-21 21:41 /user/hive/warehouse/Ram/default/stream_table5/.streaming/checkpoint/offsets/7rw-rr-   2 root users         64 2017-11-21 21:41 /user/hive/warehouse/Ram/default/stream_table5/.streaming/checkpoint/offsets/8rw-rr-   2 root users         63 2017-11-21 21:42 /user/hive/warehouse/Ram/default/stream_table5/.streaming/checkpoint/offsets/92. Following error thrown:=== Streaming Query ===Identifier: &#91;id = 3a5334bc-d471-4676-b6ce-f21105d491d1, runId = b2be9f97-8141-46be-89db-9a0f98d13369&#93;Current Offsets: {org.apache.spark.sql.execution.streaming.TextSocketSource@14c45193: 1000}Current State: ACTIVEThread State: RUNNABLELogical Plan:org.apache.spark.sql.execution.streaming.TextSocketSource@14c45193        at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runBatches(StreamExecution.scala:284)        at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:177)Caused by: java.lang.RuntimeException: Offsets committed out of order: 20019 followed by 1000        at scala.sys.package$.error(package.scala:27)        at org.apache.spark.sql.execution.streaming.TextSocketSource.commit(socket.scala:151)        at org.apache.spark.sql.execution.streaming.StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$constructNextBatch$2$$anonfun$apply$mcV$sp$4.apply(StreamExecution.scala:421)        at org.apache.spark.sql.execution.streaming.StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$constructNextBatch$2$$anonfun$apply$mcV$sp$4.apply(StreamExecution.scala:420)        at scala.collection.Iterator$class.foreach(Iterator.scala:893)        at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)        at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)        at org.apache.spark.sql.execution.streaming.StreamProgress.foreach(StreamProgress.scala:25)        at org.apache.spark.sql.execution.streaming.StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$constructNextBatch$2.apply$mcV$sp(StreamExecution.scala:420)        at org.apache.spark.sql.execution.streaming.StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$constructNextBatch$2.apply(StreamExecution.scala:404)        at org.apache.spark.sql.execution.streaming.StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$constructNextBatch$2.apply(StreamExecution.scala:404)        at org.apache.spark.sql.execution.streaming.ProgressReporter$class.reportTimeTaken(ProgressReporter.scala:262)        at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:46)        at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$constructNextBatch(StreamExecution.scala:404)        at org.apache.spark.sql.execution.streaming.StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$runBatches$1$$anonfun$1.apply$mcV$sp(StreamExecution.scala:250)        at org.apache.spark.sql.execution.streaming.StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$runBatches$1$$anonfun$1.apply(StreamExecution.scala:244)        at org.apache.spark.sql.execution.streaming.StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$runBatches$1$$anonfun$1.apply(StreamExecution.scala:244)        at org.apache.spark.sql.execution.streaming.ProgressReporter$class.reportTimeTaken(ProgressReporter.scala:262)        at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:46)        at org.apache.spark.sql.execution.streaming.StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$runBatches$1.apply$mcZ$sp(StreamExecution.scala:244)        at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:43)        at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runBatches(StreamExecution.scala:239)        ... 1 moreDone reading and writing streaming dataSocket closed
issueID:CARBONDATA-1792
type:Task
changed files:
texts:Adding example of data management for Spark2.X
Adding example of data management for Spark2.X
issueID:CARBONDATA-1793
type:Bug
changed files:
texts:Insert / update is allowing more than 32000 characters for String column

issueID:CARBONDATA-1794
type:Bug
changed files:
texts:Table delete in Beeline does not work on stream table
Steps :1. Create a streaming table and do a batch load2. Set up the Streaming , so that it does streaming in chunk of 1000 records 20 times
issueID:CARBONDATA-1795
type:Bug
changed files:
texts:Fix code issue of all examples
Fix code issue of all examples:CarbonDataFrameExample,CarbonSortColumnsExample,HadoopFileExample
issueID:CARBONDATA-1796
type:Bug
changed files:
texts:While submitting new job to Hadoop, token should be generated for accessing paths
In hadoop secure mode cluster,while submitting job to hadoopRdd token should be generated for the path in JobConf, else Delegation Token exception will be thrown during load.
issueID:CARBONDATA-1797
type:Bug
changed files:
texts:Segment_Index compaction should take compaction lock to support concurrent scenarios better
SEGMENT_INDEX compaction is not taking compaction lock. While concurrent operation, this may be successful but the output may not be as expected.Scenario:Execute MINOR compaction and SEGMENT_INDEX compaction concurrently.As SEGMENT_INDEX compaction is not taking any lock it will do tasks in between, finally some segments index files will be merged, probably the newly created segments may be left out.Solution:To take compaction lock
issueID:CARBONDATA-1798
type:Bug
changed files:
texts:class cast exception when loading data from a hive table to carbondata table
1. i created the hive tpch table named lineitem with around 10 gb dataCREATE TABLE LINEITEM ( L_ORDERKEY INT ,L_PARTKEY INT ,L_SUPPKEY INT ,L_LINENUMBER INT ,L_QUANTITY DECIMAL(15,2)  ,L_EXTENDEDPRICE DECIMAL(15,2)  ,L_DISCOUNT DECIMAL(15,2)  ,L_TAX DECIMAL(15,2)  ,L_RETURNFLAG STRING ,L_LINESTATUS STRING ,L_SHIPDATE Date ,L_COMMITDATE Date ,L_RECEIPTDATE Date ,L_SHIPINSTRUCT STRING ,L_SHIPMODE STRING ,L_COMMENT STRING) stored as orc;2.tried to insert the data from that hive table into carbondata table CREATE TABLE LINEITEM ( L_ORDERKEY INT ,L_PARTKEY INT ,L_SUPPKEY INT ,L_LINENUMBER INT ,L_QUANTITY DECIMAL(15,2)  ,L_EXTENDEDPRICE DECIMAL(15,2)  ,L_DISCOUNT DECIMAL(15,2)  ,L_TAX DECIMAL(15,2)  ,L_RETURNFLAG STRING ,L_LINESTATUS STRING ,L_SHIPDATE Date ,L_COMMITDATE Date ,L_RECEIPTDATE Date ,L_SHIPINSTRUCT STRING ,L_SHIPMODE STRING ,L_COMMENT STRING) stored by 'carbondata';then i used INSERT INTO LINEITEM SELECT * FROM LINEITEM_ORC;here are error logs17/11/22 16:56:40 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on slave1:41401 (size: 25.2 KB, free: 3.0 GB)17/11/22 16:59:12 WARN TaskSetManager: Lost task 0.0 in stage 15.0 (TID 20, hadoop-master, executor 2): org.apache.carbondata.processing.loading.exception.CarbonDataLoadingException:  at org.apache.carbondata.processing.loading.sort.impl.UnsafeParallelReadMergeSorterImpl.sort(UnsafeParallelReadMergeSorterImpl.java:114) at org.apache.carbondata.processing.loading.steps.SortProcessorStepImpl.execute(SortProcessorStepImpl.java:62) at org.apache.carbondata.processing.loading.steps.DataWriterProcessorStepImpl.execute(DataWriterProcessorStepImpl.java:87) at org.apache.carbondata.processing.loading.DataLoadExecutor.execute(DataLoadExecutor.java:50) at org.apache.carbondata.spark.rdd.NewDataFrameLoaderRDD$$anon$2.<init>(NewCarbonDataLoadRDD.scala:391) at org.apache.carbondata.spark.rdd.NewDataFrameLoaderRDD.internalCompute(NewCarbonDataLoadRDD.scala:354) at org.apache.carbondata.spark.rdd.CarbonRDD.compute(CarbonRDD.scala:60) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) at org.apache.spark.rdd.RDD.iterator(RDD.scala:287) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87) at org.apache.spark.scheduler.Task.run(Task.scala:99) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:748)Caused by: org.apache.carbondata.processing.sort.exception.CarbonSortKeyAndGroupByException: org.apache.carbondata.processing.sort.exception.CarbonSortKeyAndGroupByException:  at org.apache.carbondata.processing.loading.sort.unsafe.merger.UnsafeIntermediateMerger.checkForFailure(UnsafeIntermediateMerger.java:188) at org.apache.carbondata.processing.loading.sort.unsafe.merger.UnsafeIntermediateMerger.finish(UnsafeIntermediateMerger.java:171) at org.apache.carbondata.processing.loading.sort.impl.UnsafeParallelReadMergeSorterImpl.sort(UnsafeParallelReadMergeSorterImpl.java:107) ... 14 moreCaused by: java.util.concurrent.ExecutionException: org.apache.carbondata.processing.sort.exception.CarbonSortKeyAndGroupByException:  at java.util.concurrent.FutureTask.report(FutureTask.java:122) at java.util.concurrent.FutureTask.get(FutureTask.java:192) at org.apache.carbondata.processing.loading.sort.unsafe.merger.UnsafeIntermediateMerger.checkForFailure(UnsafeIntermediateMerger.java:185) ... 16 moreCaused by: org.apache.carbondata.processing.sort.exception.CarbonSortKeyAndGroupByException:  at org.apache.carbondata.processing.loading.sort.unsafe.merger.UnsafeIntermediateFileMerger.call(UnsafeIntermediateFileMerger.java:142) at org.apache.carbondata.processing.loading.sort.unsafe.merger.UnsafeIntermediateFileMerger.call(UnsafeIntermediateFileMerger.java:45) at java.util.concurrent.FutureTask.run(FutureTask.java:266) ... 3 moreCaused by: java.lang.ClassCastException: java.math.BigDecimal cannot be cast to [B at org.apache.carbondata.processing.loading.sort.unsafe.merger.UnsafeIntermediateFileMerger.writeDataTofile(UnsafeIntermediateFileMerger.java:333) at org.apache.carbondata.processing.loading.sort.unsafe.merger.UnsafeIntermediateFileMerger.call(UnsafeIntermediateFileMerger.java:113) ... 5 more17/11/22 16:59:12 INFO TaskSetManager: Starting task 0.1 in stage 15.0 (TID 22, hadoop-master, executor 2, partition 0, NODE_LOCAL, 7696 bytes)
issueID:CARBONDATA-1799
type:Bug
changed files:
texts:CarbonInputMapperTest is failing

issueID:CARBONDATA-18
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/scan/expression/conditional/NotInExpression.java
core/src/main/java/org/apache/carbondata/core/cache/dictionary/ColumnDictionaryInfo.java
core/src/main/java/org/apache/carbondata/core/util/DataTypeUtil.java
core/src/main/java/org/apache/carbondata/core/scan/expression/conditional/EqualToExpression.java
core/src/main/java/org/apache/carbondata/core/util/DataFileFooterConverter.java
core/src/main/java/org/apache/carbondata/core/scan/expression/ExpressionResult.java
core/src/main/java/org/apache/carbondata/core/scan/expression/conditional/InExpression.java
core/src/main/java/org/apache/carbondata/core/scan/expression/conditional/LessThanExpression.java
core/src/main/java/org/apache/carbondata/core/scan/expression/conditional/GreaterThanExpression.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
core/src/main/java/org/apache/carbondata/core/scan/expression/conditional/GreaterThanEqualToExpression.java
core/src/main/java/org/apache/carbondata/core/scan/filter/FilterUtil.java
core/src/main/java/org/apache/carbondata/core/scan/expression/conditional/LessThanEqualToExpression.java
core/src/main/java/org/apache/carbondata/core/scan/expression/conditional/NotEqualsExpression.java
core/src/main/java/org/apache/carbondata/core/metadata/converter/ThriftWrapperSchemaConverterImpl.java
texts:DataType tinyint is not supported in carbondata
Currently datatype tinyint is not supported although hive supports this datatype. It would be nice to support tinyint in carbondata.
issueID:CARBONDATA-180
type:Bug
changed files:
texts:give proper error message when dataloading with wrong delimiter value
Reproduce:CREATE TABLE IF NOT EXISTS t3(ID Int, date Timestamp, country String, name String, phonetype String, serialname String, salary Int)STORED BY 'carbondata'LOAD DATA LOCAL INPATH 'data.csv' into table t3 options('DELIMITER'='#', 'FILEHEADER'='ID,date,country,name,phonetype,serialname,salary')File name:data.csv1,2015/7/23,china,aaa1,phone197,ASD69643,150002,2015/7/24,china,aaa2,phone756,ASD42892,150013,2015/7/25,china,aaa3,phone1904,ASD37014,15002Exception:ERROR 26-08 11:44:12,881 - generate global dictionary failedjava.lang.ArrayIndexOutOfBoundsException: 2         at org.apache.carbondata.spark.util.GlobalDictionaryUtil$$anonfun$pruneDimensions$1$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(GlobalDictionaryUtil.scala:79)         at org.apache.carbondata.spark.util.GlobalDictionaryUtil$$anonfun$pruneDimensions$1$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(GlobalDictionaryUtil.scala:76)         at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)         at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)         at org.apache.carbondata.spark.util.GlobalDictionaryUtil$$anonfun$pruneDimensions$1$$anonfun$apply$1.apply$mcV$sp(GlobalDictionaryUtil.scala:76)         at scala.util.control.Breaks.breakable(Breaks.scala:37)         at org.apache.carbondata.spark.util.GlobalDictionaryUtil$$anonfun$pruneDimensions$1.apply(GlobalDictionaryUtil.scala:75)         at org.apache.carbondata.spark.util.GlobalDictionaryUtil$$anonfun$pruneDimensions$1.apply(GlobalDictionaryUtil.scala:74)         at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)         at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)         at org.apache.carbondata.spark.util.GlobalDictionaryUtil$.pruneDimensions(GlobalDictionaryUtil.scala:74)         at org.apache.carbondata.spark.util.GlobalDictionaryUtil$.generateGlobalDictionary(GlobalDictionaryUtil.scala:697)         at org.apache.spark.sql.execution.command.LoadTable.run(carbonTableSchema.scala:1159)         at org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult$lzycompute(commands.scala:57)         at org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult(commands.scala:57)         at org.apache.spark.sql.execution.ExecutedCommand.doExecute(commands.scala:69)         at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$5.apply(SparkPlan.scala:140)         at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$5.apply(SparkPlan.scala:138)         at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)         at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:138)         at org.apache.spark.sql.SQLContext$QueryExecution.toRdd$lzycompute(SQLContext.scala:933)         at org.apache.spark.sql.SQLContext$QueryExecution.toRdd(SQLContext.scala:933)         at org.apache.spark.sql.DataFrame.<init>(DataFrame.scala:144)         at org.apache.spark.sql.DataFrame.<init>(DataFrame.scala:129)         at org.apache.carbondata.spark.rdd.CarbonDataFrameRDD.<init>(CarbonDataFrameRDD.scala:23)         at org.apache.spark.sql.CarbonContext.sql(CarbonContext.scala:130)         at org.apache.carbondata.examples.CarbonExample$.main(CarbonExample.scala:42)         at org.apache.carbondata.examples.CarbonExample.main(CarbonExample.scala)
issueID:CARBONDATA-1800
type:Bug
changed files:
texts:Mistyping in DataMapRow compare
In BlockletDMComparator  public int compare(DataMapRow first, DataMapRow second) {   ...    byte[][] firstBytes = splitKey(first.getByteArray(0));    byte[][] secondBytes = splitKey(first.getByteArray(0));    ...}line 65 and 66 use same input argument, the second line should be assign as "  splitKey(second.getByteArray(0)) "https://github.com/apache/carbondata/blob/79feac96ae789851c5ad7306a7acaaba25d8e6c9/core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockletDMComparator.java#L66
issueID:CARBONDATA-1801
type:Improvement
changed files:
texts:Remove unnecessary mdk computation code
In `org.apache.carbondata.core.datastore.page.key.TablePageKey#update`, argument `mdk` can be reused to avoid duplicate computation for mdk by replacing  `WriteStepRowUtil.getMdk(row, mdkGenerator)`Original Code:  /** update all keys based on the input row */  public void update(int rowId, CarbonRow row, byte[] mdk) throws KeyGenException {    if (hasNoDictionary) {      currentNoDictionaryKey = WriteStepRowUtil.getNoDictAndComplexDimension(row);    }    if (rowId == 0) {      startKey = WriteStepRowUtil.getMdk(row, mdkGenerator);      noDictStartKey = currentNoDictionaryKey;    }    noDictEndKey = currentNoDictionaryKey;    if (rowId == pageSize - 1) {      endKey = WriteStepRowUtil.getMdk(row, mdkGenerator);      finalizeKeys();    }  }https://github.com/apache/carbondata/blob/74226907990cdee41a6ccbd69e2a813077792f89/core/src/main/java/org/apache/carbondata/core/datastore/page/key/TablePageKey.java#L66
issueID:CARBONDATA-1802
type:Bug
changed files:
texts:Carbon1.3.0  Alter:Alter query fails if a column is dropped and there is no key column
Carbon1.3.0  Alter:Alter query fails if a column is dropped and there is no key column.Steps:1: create table ttt(c int,d int,e int) stored by 'carbondata';2: Alter table ttt drop columns(c);3: observe that below error is coming:Error: java.lang.RuntimeException: Alter table drop column operation failed: Alter drop operation failed. AtLeast one key column should exist after drop.Expected: Since user is able to create a table with all numeric columns, Same should be supported in Alter feature.
issueID:CARBONDATA-1803
type:Improvement
changed files:
texts:Changing format of Show segments

issueID:CARBONDATA-1804
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/datastore/impl/FileFactory.java
core/src/main/java/org/apache/carbondata/core/datastore/impl/DefaultFileTypeProvider.java
core/src/main/java/org/apache/carbondata/core/datastore/impl/DFSFileReaderImpl.java
core/src/main/java/org/apache/carbondata/core/datastore/filesystem/LocalCarbonFile.java
core/src/main/java/org/apache/carbondata/core/datastore/impl/FileTypeInterface.java
core/src/main/java/org/apache/carbondata/core/datastore/filesystem/AbstractDFSCarbonFile.java
core/src/main/java/org/apache/carbondata/core/datastore/filesystem/CarbonFile.java
texts:Make FileOperations Pluggable
1. Refactor FileFactory based on FileType to support plug-gable file handlers so that custom file handlers can have their specific logic.Example : User can provide his own implementations by extending existing FileTypes
issueID:CARBONDATA-1805
type:Improvement
changed files:
texts:Optimize pruning for dictionary loading
SCENARIORecently I tried dictionary feature in Carbondata and found its dictionary generating phase in data loading is quite slow. My scenario is as below:+ Input Data: 35.8GB CSV file with 199 columns and 126 Million lines+ Dictionary columns: 3 columns each containing 19213,4,9 distinct valuesThe whole data loading consumes about 2.9min for dictionary generating and 4.6min for fact data loading &#8211; about 39% of the time are spent on dictionary.Having observed the nmon result, Ifound the CPU usage were quite high during the dictionary generating phase and the Disk, Network were quite normal. ANALYZEAfter I went through the dictionary generating related code, I found Carbondata aleady prune non-dictionary columns before generating dictionary. But the problem is that `the pruning comes after data file reading`, this will cause some overhead, we can optimize it by `prune while reading data file`. RESOLVERefactor the `loadDataFrame` method in `GlobalDictionaryUtil`, only pruning the non-dictionary columns while reading the data file.After implementing the above optimization, the dictionary generating costs only `29s` &#8211; `about 6 times better than before`(2.9min), and the fact data loading costs the same as before(4.6min), about 10% of the time are spent on dictionary. NOTE+ Currently only `load data file` will benefit from this optimization, while `load data frame` will not.+ Before implementing this solution, I tried another solution &#8211; cache dataframe of the data file, the performance was even worse &#8211; the dictionary generating time was 5.6min.
issueID:CARBONDATA-1806
type:Bug
changed files:
texts:Carbon1.3.0 Load with global sort: Load fails If a table is created with sort scope as global sort
Carbon1.3.0 Load with global sort: Load fails If a table is created with sort scope as global sort.Steps: 1: create table dt1 (c1 string, c2 int) STORED BY 'org.apache.carbondata.format' tblproperties('sort_scope'='Global_sort');2: LOAD DATA INPATH 'hdfs://hacluster/user/test/dt1.txt' INTO TABLE dt1 OPTIONS('DELIMITER'=',', 'QUOTECHAR'= '\"');3: Observe that load fails with below error:Error: java.lang.Exception: DataLoad failure (state=,code=0)4: Check log:org.apache.carbondata.processing.loading.exception.CarbonDataLoadingException: There is an unexpected error: org.apache.carbondata.core.datastore.exception.CarbonDataWriterException at org.apache.carbondata.spark.load.DataLoadProcessorStepOnSpark$.writeFunc(DataLoadProcessorStepOnSpark.scala:198) at org.apache.carbondata.spark.load.DataLoadProcessBuilderOnSpark$$anonfun$loadDataUsingGlobalSort$1.apply(DataLoadProcessBuilderOnSpark.scala:130) at org.apache.carbondata.spark.load.DataLoadProcessBuilderOnSpark$$anonfun$loadDataUsingGlobalSort$1.apply(DataLoadProcessBuilderOnSpark.scala:129) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87) at org.apache.spark.scheduler.Task.run(Task.scala:99) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748) Suppressed: org.apache.spark.util.TaskCompletionListenerException: There is an unexpected error: org.apache.carbondata.core.datastore.exception.CarbonDataWriterExceptionPrevious exception in task: There is an unexpected error: org.apache.carbondata.core.datastore.exception.CarbonDataWriterException org.apache.carbondata.spark.load.DataLoadProcessorStepOnSpark$.writeFunc(DataLoadProcessorStepOnSpark.scala:198) org.apache.carbondata.spark.load.DataLoadProcessBuilderOnSpark$$anonfun$loadDataUsingGlobalSort$1.apply(DataLoadProcessBuilderOnSpark.scala:130) org.apache.carbondata.spark.load.DataLoadProcessBuilderOnSpark$$anonfun$loadDataUsingGlobalSort$1.apply(DataLoadProcessBuilderOnSpark.scala:129) org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87) org.apache.spark.scheduler.Task.run(Task.scala:99) org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322) java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) java.lang.Thread.run(Thread.java:748)  at org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:138)  at org.apache.spark.TaskContextImpl.markTaskFailed(TaskContextImpl.scala:106)  at org.apache.spark.scheduler.Task.run(Task.scala:104)  ... 4 moreCaused by: org.apache.carbondata.processing.loading.exception.CarbonDataLoadingException: org.apache.carbondata.core.datastore.exception.CarbonDataWriterException at org.apache.carbondata.processing.loading.steps.DataWriterProcessorStepImpl.processingComplete(DataWriterProcessorStepImpl.java:163) at org.apache.carbondata.processing.loading.steps.DataWriterProcessorStepImpl.finish(DataWriterProcessorStepImpl.java:149) at org.apache.carbondata.spark.load.DataLoadProcessorStepOnSpark$.writeFunc(DataLoadProcessorStepOnSpark.scala:189) ... 8 moreCaused by: org.apache.carbondata.core.datastore.exception.CarbonDataWriterException: org.apache.carbondata.core.datastore.exception.CarbonDataWriterException at org.apache.carbondata.processing.store.CarbonFactDataHandlerColumnar.processWriteTaskSubmitList(CarbonFactDataHandlerColumnar.java:326) at org.apache.carbondata.processing.store.CarbonFactDataHandlerColumnar.closeHandler(CarbonFactDataHandlerColumnar.java:350) at org.apache.carbondata.processing.loading.steps.DataWriterProcessorStepImpl.processingComplete(DataWriterProcessorStepImpl.java:160) ... 10 moreCaused by: java.util.concurrent.ExecutionException: org.apache.carbondata.core.datastore.exception.CarbonDataWriterException at java.util.concurrent.FutureTask.report(FutureTask.java:122) at java.util.concurrent.FutureTask.get(FutureTask.java:192) at org.apache.carbondata.processing.store.CarbonFactDataHandlerColumnar.processWriteTaskSubmitList(CarbonFactDataHandlerColumnar.java:323) ... 12 moreCaused by: org.apache.carbondata.core.datastore.exception.CarbonDataWriterException at org.apache.carbondata.processing.store.CarbonFactDataHandlerColumnar$Consumer.call(CarbonFactDataHandlerColumnar.java:630) at org.apache.carbondata.processing.store.CarbonFactDataHandlerColumnar$Consumer.call(CarbonFactDataHandlerColumnar.java:601) at java.util.concurrent.FutureTask.run(FutureTask.java:266) ... 3 more
issueID:CARBONDATA-1807
type:Bug
changed files:
texts:Carbon1.3.0-Pre-AggregateTable - Pre-aggregate creation not throwing error for wrong syntax and results in further query failures
Steps:Beeline:1. Create table and load with  data2. create a pre-aggregate table with incorrect syntax.3. Run select count on aggregate table4. Run aggregate select query on main tableExpected: Pre-aggregate table creation should have trhown syntax error Actual: Pre-aggregate table is shown successful, but aggregate query fails.Query:create table if not exists JL_r31(p_cap_time String,city String,product_code String,user_base_station String,user_belong_area_code String,user_num String,user_imsi String,user_id String,user_msisdn String,dim1 String,dim2 String,dim3 String,dim4 String,dim5 String,dim6 String,dim7 String,dim8 String,dim9 String,dim10 String,dim11 String,dim12 String,dim13 String,dim14 String,dim15 String,dim16 String,dim17 String,dim18 String,dim19 String,dim20 String,dim21 String,dim22 String,dim23 String,dim24 String,dim25 String,dim26 String,dim27 String,dim28 String,dim29 String,dim30 String,dim31 String,dim32 String,dim33 String,dim34 String,dim35 String,dim36 String,dim37 String,dim38 String,dim39 String,dim40 String,dim41 String,dim42 String,dim43 String,dim44 String,dim45 String,dim46 String,dim47 String,dim48 String,dim49 String,dim50 String,dim51 String,dim52 String,dim53 String,dim54 String,dim55 String,dim56 String,dim57 String,dim58 String,dim59 String,dim60 String,dim61 String,dim62 String,dim63 String,dim64 String,dim65 String,dim66 String,dim67 String,dim68 String,dim69 String,dim70 String,dim71 String,dim72 String,dim73 String,dim74 String,dim75 String,dim76 String,dim77 String,dim78 String,dim79 String,dim80 String,dim81 String,M1 double,M2 double,M3 double,M4 double,M5 double,M6 double,M7 double,M8 double,M9 double,M10 double )stored by 'org.apache.carbondata.format' TBLPROPERTIES('DICTIONARY_EXCLUDE'='user_num,user_imsi,user_ID,user_msisdn,user_base_station,user_belong_area_code','table_blocksize'='512');---------+ Result  ---------+---------+No rows selected (0.55 seconds)LOAD DATA inpath 'hdfs://hacluster/user/test/jin_test2.csv' into table JL_r31 options('DELIMITER'=',', 'QUOTECHAR'='"','BAD_RECORDS_ACTION'='FORCE','IS_EMPTY_DATA_BAD_RECORD'='TRUE','FILEHEADER'='p_cap_time,city,product_code,user_base_station,user_belong_area_code,user_num,user_imsi,user_id,user_msisdn,dim1,dim2,dim3,dim4,dim5,dim6,dim7,dim8,dim9,dim10,dim11,dim12,dim13,dim14,dim15,dim16,dim17,dim18,dim19,dim20,dim21,dim22,dim23,dim24,dim25,dim26,dim27,dim28,dim29,dim30,dim31,dim32,dim33,dim34,dim35,dim36,dim37,dim38,dim39,dim40,dim41,dim42,dim43,dim44,dim45,dim46,dim47,dim48,dim49,dim50,dim51,dim52,dim53,dim54,dim55,dim56,dim57,dim58,dim59,dim60,dim61,dim62,dim63,dim64,dim65,dim66,dim67,dim68,dim69,dim70,dim71,dim72,dim73,dim74,dim75,dim76,dim77,dim78,dim79,dim80,dim81,M1,M2,M3,M4,M5,M6,M7,M8,M9,M10');---------+ Result  ---------+---------+No rows selected (14.049 seconds)0: jdbc:hive2://10.18.98.136:23040> create datamap agr_JL_r31 ON TABLE JL_r31 USING 'org.apache.carbondta.datamap.AggregateDataMapHandler' as select user_num,user_imsi,sum(user_id),count(user_id) from JL_r31 group by  user_num, user_imsi;---------+ Result  ---------+---------+No rows selected (0.397 seconds)0: jdbc:hive2://10.18.98.136:23040> select count from JL_r31_agr_JL_r31;Error: org.apache.spark.sql.AnalysisException: Table or view not found: JL_r31_agr_JL_r31; line 1 pos 21 (state=,code=0)0: jdbc:hive2://10.18.98.136:23040> select user_num,user_imsi,sum(user_id),count(user_id) from JL_r31 group by  user_num, user_imsi;Error: java.lang.ClassCastException: org.apache.carbondata.core.metadata.schema.table.DataMapSchema cannot be cast to org.apache.carbondata.core.metadata.schema.table.AggregationDataMapSchema (state=,code=0)Driver Logs:2017-11-24 21:45:10,997 | INFO  | &#91;pool-23-thread-4&#93; | Parsing command: create datamap agr_JL_r31 ON TABLE JL_r31 USING 'org.apache.carbondta.datamap.AggregateDataMapHandler' as select user_num,user_imsi,sum(user_id),count(user_id) from JL_r31 group by  user_num, user_imsi | org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)2017-11-24 21:45:11,046 | INFO  | &#91;pool-23-thread-4&#93; | pool-23-thread-4 Skip CarbonOptimizer | org.apache.carbondata.common.logging.impl.StandardLogService.logInfoMessage(StandardLogService.java:150)2017-11-24 21:45:11,051 | INFO  | &#91;pool-23-thread-4&#93; | 5: get_table : db=default tbl=jl_r31 | org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.logInfo(HiveMetaStore.java:746)2017-11-24 21:45:11,051 | INFO  | &#91;pool-23-thread-4&#93; | ugi=anonymous ip=unknown-ip-addr cmd=get_table : db=default tbl=jl_r31  | org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.logAuditEvent(HiveMetaStore.java:371)2017-11-24 21:45:11,052 | INFO  | &#91;pool-23-thread-4&#93; | 5: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore | org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:589)2017-11-24 21:45:11,055 | INFO  | &#91;pool-23-thread-4&#93; | ObjectStore, initialize called | org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:289)2017-11-24 21:45:11,060 | INFO  | &#91;pool-23-thread-4&#93; | Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing | org.datanucleus.util.Log4JLogger.info(Log4JLogger.java:77)2017-11-24 21:45:11,061 | INFO  | &#91;pool-23-thread-4&#93; | Using direct SQL, underlying DB is MYSQL | org.apache.hadoop.hive.metastore.MetaStoreDirectSql.<init>(MetaStoreDirectSql.java:139)2017-11-24 21:45:11,062 | INFO  | &#91;pool-23-thread-4&#93; | Initialized ObjectStore | org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:272)2017-11-24 21:45:11,084 | INFO  | &#91;pool-23-thread-4&#93; | Parsing command: array<string> | org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)2017-11-24 21:45:11,094 | INFO  | &#91;pool-23-thread-4&#93; | pool-23-thread-4 HDFS lock path:hdfs://hacluster/carbonstore/default/jl_r31/meta.lock | org.apache.carbondata.common.logging.impl.StandardLogService.logInfoMessage(StandardLogService.java:150)2017-11-24 21:45:11,095 | INFO  | &#91;pool-23-thread-4&#93; | pool-23-thread-4 Trying to acquire lock: org.apache.carbondata.core.locks.HdfsFileLock@7abafd48 | org.apache.carbondata.common.logging.impl.StandardLogService.logInfoMessage(StandardLogService.java:150)2017-11-24 21:45:11,129 | INFO  | &#91;pool-23-thread-4&#93; | pool-23-thread-4 Successfully acquired the lock org.apache.carbondata.core.locks.HdfsFileLock@7abafd48 | org.apache.carbondata.common.logging.impl.StandardLogService.logInfoMessage(StandardLogService.java:150)2017-11-24 21:45:11,129 | INFO  | &#91;pool-23-thread-4&#93; | pool-23-thread-4 HDFS lock path:hdfs://hacluster/carbonstore/default/jl_r31/droptable.lock | org.apache.carbondata.common.logging.impl.StandardLogService.logInfoMessage(StandardLogService.java:150)2017-11-24 21:45:11,129 | INFO  | &#91;pool-23-thread-4&#93; | pool-23-thread-4 Trying to acquire lock: org.apache.carbondata.core.locks.HdfsFileLock@650ff0aa | org.apache.carbondata.common.logging.impl.StandardLogService.logInfoMessage(StandardLogService.java:150)2017-11-24 21:45:11,160 | INFO  | &#91;pool-23-thread-4&#93; | pool-23-thread-4 Successfully acquired the lock org.apache.carbondata.core.locks.HdfsFileLock@650ff0aa | org.apache.carbondata.common.logging.impl.StandardLogService.logInfoMessage(StandardLogService.java:150)2017-11-24 21:45:11,254 | INFO  | &#91;pool-23-thread-4&#93; | Parsing command: `default`.`jl_r31` | org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)2017-11-24 21:45:11,269 | INFO  | &#91;pool-23-thread-4&#93; | 5: get_table : db=default tbl=jl_r31 | org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.logInfo(HiveMetaStore.java:746)2017-11-24 21:45:11,270 | INFO  | &#91;pool-23-thread-4&#93; | ugi=anonymous ip=unknown-ip-addr cmd=get_table : db=default tbl=jl_r31  | org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.logAuditEvent(HiveMetaStore.java:371)2017-11-24 21:45:11,288 | INFO  | &#91;pool-23-thread-4&#93; | Parsing command: array<string> | org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)2017-11-24 21:45:11,293 | INFO  | &#91;pool-23-thread-4&#93; | 5: get_table : db=default tbl=jl_r31 | org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.logInfo(HiveMetaStore.java:746)2017-11-24 21:45:11,294 | INFO  | &#91;pool-23-thread-4&#93; | ugi=anonymous ip=unknown-ip-addr cmd=get_table : db=default tbl=jl_r31  | org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.logAuditEvent(HiveMetaStore.java:371)2017-11-24 21:45:11,311 | INFO  | &#91;pool-23-thread-4&#93; | Parsing command: array<string> | org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)2017-11-24 21:45:11,319 | INFO  | &#91;pool-23-thread-4&#93; | 5: get_database: default | org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.logInfo(HiveMetaStore.java:746)2017-11-24 21:45:11,320 | INFO  | &#91;pool-23-thread-4&#93; | ugi=anonymous ip=unknown-ip-addr cmd=get_database: default  | org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.logAuditEvent(HiveMetaStore.java:371)2017-11-24 21:45:11,325 | INFO  | &#91;pool-23-thread-4&#93; | 5: get_database: default | org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.logInfo(HiveMetaStore.java:746)2017-11-24 21:45:11,325 | INFO  | &#91;pool-23-thread-4&#93; | ugi=anonymous ip=unknown-ip-addr cmd=get_database: default  | org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.logAuditEvent(HiveMetaStore.java:371)2017-11-24 21:45:11,330 | INFO  | &#91;pool-23-thread-4&#93; | 5: get_tables: db=default pat=* | org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.logInfo(HiveMetaStore.java:746)2017-11-24 21:45:11,330 | INFO  | &#91;pool-23-thread-4&#93; | ugi=anonymous ip=unknown-ip-addr cmd=get_tables: db=default pat=*  | org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.logAuditEvent(HiveMetaStore.java:371)2017-11-24 21:45:11,345 | INFO  | &#91;pool-23-thread-4&#93; | pool-23-thread-4 Parent table updated is successful for table default.JL_r31 | org.apache.carbondata.common.logging.impl.StandardLogService.logInfoMessage(StandardLogService.java:150)2017-11-24 21:45:11,360 | INFO  | &#91;pool-23-thread-4&#93; | pool-23-thread-4 Deleted the lock file hdfs://hacluster/carbonstore/default/jl_r31/meta.lock | org.apache.carbondata.common.logging.impl.StandardLogService.logInfoMessage(StandardLogService.java:150)2017-11-24 21:45:11,360 | INFO  | &#91;pool-23-thread-4&#93; | pool-23-thread-4 Pre agg table lock released successfully | org.apache.carbondata.common.logging.impl.StandardLogService.logInfoMessage(StandardLogService.java:150)2017-11-24 21:45:11,372 | INFO  | &#91;pool-23-thread-4&#93; | pool-23-thread-4 Deleted the lock file hdfs://hacluster/carbonstore/default/jl_r31/droptable.lock | org.apache.carbondata.common.logging.impl.StandardLogService.logInfoMessage(StandardLogService.java:150)2017-11-24 21:45:11,372 | INFO  | &#91;pool-23-thread-4&#93; | pool-23-thread-4 Pre agg table lock released successfully | org.apache.carbondata.common.logging.impl.StandardLogService.logInfoMessage(StandardLogService.java:150)2017-11-24 21:45:11,373 | AUDIT | &#91;pool-23-thread-4&#93; | &#91;BLR1000014290&#93;&#91;anonymous&#93;&#91;Thread-171&#93;DataMap agr_JL_r31 successfully added to Table JL_r31 | org.apache.carbondata.common.logging.impl.StandardLogService.audit(StandardLogService.java:207)2017-11-24 21:46:06,670 | INFO  | &#91;spark-dynamic-executor-allocation&#93; | Request to remove executorIds: 5 | org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)2017-11-24 21:46:06,670 | INFO  | &#91;spark-dynamic-executor-allocation&#93; | Requesting to kill executor(s) 5 | org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)2017-11-24 21:46:06,670 | INFO  | &#91;spark-dynamic-executor-allocation&#93; | Actual list of executor(s) to be killed is 5 | org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)2017-11-24 21:46:06,679 | INFO  | &#91;spark-dynamic-executor-allocation&#93; | Removing executor 5 because it has been idle for 60 seconds (new desired total will be 1) | org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)2017-11-24 21:46:07,682 | INFO  | &#91;spark-dynamic-executor-allocation&#93; | Request to remove executorIds: 4 | org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)2017-11-24 21:46:07,682 | INFO  | &#91;spark-dynamic-executor-allocation&#93; | Requesting to kill executor(s) 4 | org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)2017-11-24 21:46:07,683 | INFO  | &#91;spark-dynamic-executor-allocation&#93; | Actual list of executor(s) to be killed is 4 | org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)2017-11-24 21:46:07,691 | INFO  | &#91;spark-dynamic-executor-allocation&#93; | Removing executor 4 because it has been idle for 60 seconds (new desired total will be 0) | org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)2017-11-24 21:46:08,221 | INFO  | &#91;dispatcher-event-loop-1&#93; | Disabling executor 5. | org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)2017-11-24 21:46:08,221 | INFO  | &#91;dag-scheduler-event-loop&#93; | Executor lost: 5 (epoch 0) | org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)2017-11-24 21:46:08,222 | INFO  | &#91;dispatcher-event-loop-5&#93; | Trying to remove executor 5 from BlockManagerMaster. | org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)2017-11-24 21:46:08,222 | INFO  | &#91;dispatcher-event-loop-5&#93; | Removing block manager BlockManagerId(5, BLR1000014291, 38929, None) | org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)2017-11-24 21:46:08,222 | INFO  | &#91;dag-scheduler-event-loop&#93; | Removed 5 successfully in removeExecutor | org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)2017-11-24 21:46:08,225 | INFO  | &#91;dispatcher-event-loop-0&#93; | Executor 5 on BLR1000014291 killed by driver. | org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)2017-11-24 21:46:08,227 | INFO  | &#91;SparkListenerBus&#93; | Existing executor 5 has been removed (new total is 1) | org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)2017-11-24 21:46:08,670 | INFO  | &#91;dispatcher-event-loop-3&#93; | Disabling executor 4. | org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)2017-11-24 21:46:08,671 | INFO  | &#91;dag-scheduler-event-loop&#93; | Executor lost: 4 (epoch 0) | org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)2017-11-24 21:46:08,671 | INFO  | &#91;dispatcher-event-loop-4&#93; | Trying to remove executor 4 from BlockManagerMaster. | org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)2017-11-24 21:46:08,671 | INFO  | &#91;dispatcher-event-loop-4&#93; | Removing block manager BlockManagerId(4, BLR1000014290, 55432, None) | org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)2017-11-24 21:46:08,672 | INFO  | &#91;dag-scheduler-event-loop&#93; | Removed 4 successfully in removeExecutor | org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)2017-11-24 21:46:08,675 | INFO  | &#91;dispatcher-event-loop-2&#93; | Executor 4 on BLR1000014290 killed by driver. | org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)2017-11-24 21:46:08,677 | INFO  | &#91;SparkListenerBus&#93; | Existing executor 4 has been removed (new total is 0) | org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)2017-11-24 21:49:03,912 | INFO  | &#91;pool-23-thread-5&#93; | Running query 'select count from JL_r31_agr_JL_r31' with 6d7abd93-6d45-4a6b-bcda-d31441763de5 | org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)2017-11-24 21:49:03,913 | INFO  | &#91;pool-23-thread-5&#93; | Parsing command: select count from JL_r31_agr_JL_r31 | org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)2017-11-24 21:49:03,962 | INFO  | &#91;pool-23-thread-5&#93; | 6: get_table : db=default tbl=jl_r31_agr_jl_r31 | org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.logInfo(HiveMetaStore.java:746)2017-11-24 21:49:03,963 | INFO  | &#91;pool-23-thread-5&#93; | ugi=anonymous ip=unknown-ip-addr cmd=get_table : db=default tbl=jl_r31_agr_jl_r31  | org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.logAuditEvent(HiveMetaStore.java:371)2017-11-24 21:49:03,963 | INFO  | &#91;pool-23-thread-5&#93; | 6: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore | org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:589)2017-11-24 21:49:03,967 | INFO  | &#91;pool-23-thread-5&#93; | ObjectStore, initialize called | org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:289)2017-11-24 21:49:03,972 | INFO  | &#91;pool-23-thread-5&#93; | Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing | org.datanucleus.util.Log4JLogger.info(Log4JLogger.java:77)2017-11-24 21:49:03,973 | INFO  | &#91;pool-23-thread-5&#93; | Using direct SQL, underlying DB is MYSQL | org.apache.hadoop.hive.metastore.MetaStoreDirectSql.<init>(MetaStoreDirectSql.java:139)2017-11-24 21:49:03,974 | INFO  | &#91;pool-23-thread-5&#93; | Initialized ObjectStore | org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:272)2017-11-24 21:49:03,982 | INFO  | &#91;pool-23-thread-5&#93; | 6: get_table : db=default tbl=jl_r31_agr_jl_r31 | org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.logInfo(HiveMetaStore.java:746)2017-11-24 21:49:03,982 | INFO  | &#91;pool-23-thread-5&#93; | ugi=anonymous ip=unknown-ip-addr cmd=get_table : db=default tbl=jl_r31_agr_jl_r31  | org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.logAuditEvent(HiveMetaStore.java:371)2017-11-24 21:49:03,987 | ERROR | &#91;pool-23-thread-5&#93; | Error executing query, currentState RUNNING,  | org.apache.spark.internal.Logging$class.logError(Logging.scala:91)org.apache.spark.sql.AnalysisException: Table or view not found: JL_r31_agr_JL_r31; line 1 pos 21 at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42) at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableFromCatalog(Analyzer.scala:459) at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$8.applyOrElse(Analyzer.scala:478) at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$8.apply2017-11-24 21:49:11,895 | INFO  | &#91;pool-23-thread-6&#93; | Running query 'select user_num,user_imsi,sum(user_id),count(user_id) from JL_r31 group by  user_num, user_imsi' with 95a0b546-381a-4300-9984-5ad53553036e | org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)2017-11-24 21:49:11,896 | INFO  | &#91;pool-23-thread-6&#93; | Parsing command: select user_num,user_imsi,sum(user_id),count(user_id) from JL_r31 group by  user_num, user_imsi | org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)2017-11-24 21:49:11,951 | INFO  | &#91;pool-23-thread-6&#93; | 7: get_table : db=default tbl=jl_r31 | org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.logInfo(HiveMetaStore.java:746)2017-11-24 21:49:11,952 | INFO  | &#91;pool-23-thread-6&#93; | ugi=anonymous ip=unknown-ip-addr cmd=get_table : db=default tbl=jl_r31  | org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.logAuditEvent(HiveMetaStore.java:371)2017-11-24 21:49:11,953 | INFO  | &#91;pool-23-thread-6&#93; | 7: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore | org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:589)2017-11-24 21:49:11,956 | INFO  | &#91;pool-23-thread-6&#93; | ObjectStore, initialize called | org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:289)2017-11-24 21:49:11,962 | INFO  | &#91;pool-23-thread-6&#93; | Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing | org.datanucleus.util.Log4JLogger.info(Log4JLogger.java:77)2017-11-24 21:49:11,964 | INFO  | &#91;pool-23-thread-6&#93; | Using direct SQL, underlying DB is MYSQL | org.apache.hadoop.hive.metastore.MetaStoreDirectSql.<init>(MetaStoreDirectSql.java:139)2017-11-24 21:49:11,964 | INFO  | &#91;pool-23-thread-6&#93; | Initialized ObjectStore | org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:272)2017-11-24 21:49:11,985 | INFO  | &#91;pool-23-thread-6&#93; | Parsing command: array<string> | org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)2017-11-24 21:49:12,129 | ERROR | &#91;pool-23-thread-6&#93; | Error executing query, currentState RUNNING,  | org.apache.spark.internal.Logging$class.logError(Logging.scala:91)java.lang.ClassCastException: org.apache.carbondata.core.metadata.schema.table.DataMapSchema cannot be cast to org.apache.carbondata.core.metadata.schema.table.AggregationDataMapSchema at org.apache.carbondata.core.preagg.AggregateTableSelector.selectPreAggDataMapSchema(AggregateTableSelector.java:70) at org.apache.spark.sql.hive.CarbonPreAggregateQueryRules.apply(CarbonPreAggregateRules.scala:185) at org.apache.spark.sql.hive.CarbonPreAggregateQueryRules.apply(CarbonPreAggregateRules.scala:67) at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:85) at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:82) at scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:124) at scala.collection.immutable.List.foldLeft(List.scala:84) at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:82) at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:74) at scala.collection.immutable.List.foreach(List.scala:381) at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:74) at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:64) at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:62) at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:48) at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:63) at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:592) at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:699) at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:220) at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1$$anon$2.run(SparkExecuteStatementOperation.scala:163) at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1$$anon$2.run(SparkExecuteStatementOperation.scala:160) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1698) at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1.run(SparkExecuteStatementOperation.scala:173) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
issueID:CARBONDATA-1808
type:Bug
changed files:
texts:(Carbon1.3.0 - Alter Table) Inconsistency in create table and alter table usage for char and varchar column
Steps:User creates a table with char datatype --> Create table is success.0: jdbc:hive2://10.18.98.34:23040> CREATE TABLE sensor_reading_blockblank_false(id char)  STORED BY 'carbondata';---------+ Result  ---------+---------+No rows selected (0.688 seconds)User tries to alter the table using column name for char datatype in the same way.alter table sensor_reading_blockblank_false add columns(id1 char);Issue : Alter table fails with parsing error as shown below0: jdbc:hive2://10.18.98.34:23040> alter table sensor_reading_blockblank_false add columns(id1 char);Error: java.lang.RuntimeException:BaseSqlParser>>>>Operation not allowed: alter table add columns(line 1, pos 0)== SQL ==alter table sensor_reading_blockblank_false add columns(id1 char)^^^CarbonSqlParser>>>> &#91;1.65&#93; failure: ``('' expected but `)' foundalter table sensor_reading_blockblank_false add columns(id1 char)                                                                ^ (state=,code=0)Similar consistency issue is observed for varchar data type create table and alter table usage.0: jdbc:hive2://10.18.98.34:23040> CREATE TABLE sensor_reading_blockblank_false(id varchar)  STORED BY 'carbondata';---------+ Result  ---------+---------+No rows selected (0.244 seconds)0: jdbc:hive2://10.18.98.34:23040> alter table sensor_reading_blockblank_false add columns(id1 varchar);Error: java.lang.RuntimeException:BaseSqlParser>>>>Operation not allowed: alter table add columns(line 1, pos 0)== SQL ==alter table sensor_reading_blockblank_false add columns(id1 varchar)^^^CarbonSqlParser>>>> &#91;1.68&#93; failure: ``('' expected but `)' foundalter table sensor_reading_blockblank_false add columns(id1 varchar)                                                                   ^ (state=,code=0)Expected : The create table and alter table output should be consistent for char and varchar types for similar syntax usage.
issueID:CARBONDATA-181
type:Bug
changed files:
texts:Query result shows wrong
Following query result shows wrong in carbondata.SELECT            *        FROM            (    SELECT                    ROW_NUMBER() over(                    ORDER BY                        SUBS_COUNT_ID DESC                    )                   AS ROWID,                    COUNT(1) over()                              AS DATACNT,                    DEVICE_BRAND,                    DEVICE_BRAND_EN,                    CAST(SUBS_COUNT_ID AS DECIMAL(20)) AS SUBS_COUNT_ID,                    CAST(SUBS_COUNT_TOTAL_ID AS DECIMAL(20,2))  AS PENETRATION_RATE_KPIID                FROM                    (    SELECT                            SDR_FLOW_BASE_USER_1DAY.DEVICE_BRAND,                            SDR_FLOW_BASE_USER_1DAY.DEVICE_BRAND_EN,                            SUM(1)             AS SUBS_COUNT_ID,                            SUM(SUM(1)) over() AS SUBS_COUNT_TOTAL_ID                        FROM                            (    SELECT                                    DIM_TERMINAL.DEVICE_BRAND  AS DEVICE_BRAND,                                    DIM_TERMINAL.DEVICE_BRAND_EN AS DEVICE_BRAND_EN,                                    SDR_FLOW_BASE_USER_1DAY.SUBS_COUNT_ID        AS SUBS_COUNT_ID,                                    SDR_FLOW_BASE_USER_1DAY.SUBS_COUNT_TOTAL_ID AS SUBS_COUNT_TOTAL_ID                                FROM                                    (    SELECT                                            MSISDN AS SUBS_COUNT_ID,                                            MSISDN AS SUBS_COUNT_TOTAL_ID,                                            SGSN_ID,                                            TAC                                        FROM                                            devload AS SDR_FLOW_BASE_USER_1DAY                                        WHERE                            ((STARTTIME >= 1                                            AND STARTTIME < 14598720000))                                        GROUP BY                                            SGSN_ID,                                            TAC,                                            MSISDN) AS SDR_FLOW_BASE_USER_1DAY                                    LEFT OUTER JOIN(SELECT                                                        DIM_LOC_SGSN.SGSN_ID   AS SGSN_SGSN,                                                        DIM_LOC_SGSN.SGSN_NAME AS SGSN_SGSN_EN,                                                        DIM_LOC_SGSN.SGSN_ID   AS NE_ID                                                    FROM                                                        (    SELECT                                                                *,                                                                -9999 AS DUMMY_ID                                                            FROM                                                                DIM_LOC_SGSN) AS DIM_LOC_SGSN) AS DIM_LOC_SGSN                                    ON(((DIM_LOC_SGSN.NE_ID = SDR_FLOW_BASE_USER_1DAY.SGSN_ID                                    )))                                    LEFT OUTER JOIN(SELECT                                                        DIM_TERMINAL.TER_BRAND_NAME AS DEVICE_BRAND,                                                        DIM_TERMINAL.TER_BRAND_NAME AS DEVICE_BRAND_EN,                                                        DIM_TERMINAL.TAC            AS BRAND                                                    FROM                                                        DIM_TERMINAL AS DIM_TERMINAL) AS DIM_TERMINAL                                    ON(((DIM_TERMINAL.BRAND = SDR_FLOW_BASE_USER_1DAY.TAC                                    )))                                GROUP BY                                    SDR_FLOW_BASE_USER_1DAY.SUBS_COUNT_ID,                                    SDR_FLOW_BASE_USER_1DAY.SUBS_COUNT_TOTAL_ID,                                    DEVICE_BRAND,                                    DEVICE_BRAND_EN,                                    TAC) AS SDR_FLOW_BASE_USER_1DAY                        GROUP BY                            DEVICE_BRAND,                            DEVICE_BRAND_EN) AS SDR_FLOW_BASE_USER_1DAY) AS SDR_FLOW_BASE_USER_1DAY        WHERE            ROWID > 0            AND ROWID < 10
issueID:CARBONDATA-1810
type:Bug
changed files:
texts:Bad record path is not correct for UT
Bad record path is not correct for UT
issueID:CARBONDATA-1812
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/metadata/schema/table/CarbonTable.java
texts:provide API to get table dynamic information(table size and last modified time)
provide API to get table dynamic information(table size and last modified time)
issueID:CARBONDATA-1813
type:Bug
changed files:
texts:Nullpointereception in spark shell when the streaming started with table streaming altered from default(false) to true
Steps :Spark submit thrift server is started.User starts spark shell using the command - bin/spark-shell --master yarn-client --executor-memory 10G --executor-cores 5 --driver-memory 5G --num-executors 3 --jars /srv/spark2.2Bigdata/install/spark/sparkJdbc/carbonlib/carbondata_2.11-1.3.0-SNAPSHOT-shade-hadoop2.7.2.jarIn spark shell User tries to start streaming with table streaming property altered from default(false) to true.scala> import java.io.{File, PrintWriter}import java.io.{File, PrintWriter}scala> import java.net.ServerSocketimport java.net.ServerSocketscala>scala> import org.apache.spark.sql.{CarbonEnv, SparkSession}import org.apache.spark.sql.{CarbonEnv, SparkSession}scala> import org.apache.spark.sql.hive.CarbonRelationimport org.apache.spark.sql.hive.CarbonRelationscala> import org.apache.spark.sql.streaming.{ProcessingTime, StreamingQuery}import org.apache.spark.sql.streaming.{ProcessingTime, StreamingQuery}scala>scala> import org.apache.carbondata.core.constants.CarbonCommonConstantsimport org.apache.carbondata.core.constants.CarbonCommonConstantsscala> import org.apache.carbondata.core.util.CarbonPropertiesimport org.apache.carbondata.core.util.CarbonPropertiesscala> import org.apache.carbondata.core.util.path.{CarbonStorePath, CarbonTablePath}import org.apache.carbondata.core.util.path.{CarbonStorePath, CarbonTablePath}scala>scala> CarbonProperties.getInstance().addProperty(CarbonCommonConstants.CARBON_TIMESTAMP_FORMAT, "yyyy/MM/dd")res0: org.apache.carbondata.core.util.CarbonProperties = org.apache.carbondata.core.util.CarbonProperties@69ee0861scala>scala> import org.apache.spark.sql.CarbonSession._import org.apache.spark.sql.CarbonSession._scala>scala> val carbonSession = SparkSession.   builder().   appName("StreamExample").   getOrCreateCarbonSession("hdfs://hacluster/user/hive/warehouse/carbon.store")carbonSession: org.apache.spark.sql.SparkSession = org.apache.spark.sql.CarbonSession@6ce365b7scala> carbonSession.sparkContext.setLogLevel("INFO")scala>scala> def sql(sql: String) = carbonSession.sql(sql)sql: (sql: String)org.apache.spark.sql.DataFramescala>scala> def writeSocket(serverSocket: ServerSocket): Thread = {   val thread = new Thread() {     override def run(): Unit = {       // wait for client to connection request and accept       val clientSocket = serverSocket.accept()       val socketWriter = new PrintWriter(clientSocket.getOutputStream())       var index = 0       for (_ <- 1 to 1000) {         // write 5 records per iteration         for (_ <- 0 to 100) {     |           index = index + 1     |           socketWriter.println(index.toString + ",name_" + index     |                                + ",city_" + index + "," + (index * 10000.00).toString +     |                                ",school_" + index + ":school_" + index + index + "$" + index)     |         }         socketWriter.flush()         Thread.sleep(2000)       }       socketWriter.close()       System.out.println("Socket closed")     }   }   thread.start()   thread }writeSocket: (serverSocket: java.net.ServerSocket)Threadscala> def startStreaming(spark: SparkSession, tablePath: CarbonTablePath, tableName: String, port: Int): Thread = {     val thread = new Thread() {       override def run(): Unit = {         var qry: StreamingQuery = null         try {     |           val readSocketDF = spark.readStream     |             .format("socket")     |             .option("host", "10.18.98.34")     |             .option("port", port)     |             .load()     |     |           qry = readSocketDF.writeStream     |             .format("carbondata")     |             .trigger(ProcessingTime("5 seconds"))     |             .option("checkpointLocation", tablePath.getStreamingCheckpointDir)     |             .option("tablePath", tablePath.getPath).option("tableName", tableName)     |             .start()     |     |           qry.awaitTermination()     |         } catch {     |           case ex: Throwable =>     |             ex.printStackTrace()     |             println("Done reading and writing streaming data")     |         } finally {     |           qry.stop()     |         }       }     }     thread.start()     thread }startStreaming: (spark: org.apache.spark.sql.SparkSession, tablePath: org.apache.carbondata.core.util.path.CarbonTablePath, tableName: String, port: Int)Threadscala>scala> val streamTableName = "all_datatypes_2048"streamTableName: String = all_datatypes_2048scala>scala>scala> sql(s"create table all_datatypes_2048 (imei string,deviceInformationId int,MAC string,deviceColor string,device_backColor string,modelId string,marketName string,AMSize string,ROMSize string,CUPAudit string,CPIClocked string,series string,productionDate timestamp,bomCode string,internalModels string, deliveryTime string, channelsId string, channelsName string , deliveryAreaId string, deliveryCountry string, deliveryProvince string, deliveryCity string,deliveryDistrict string, deliveryStreet string, oxSingleNumber string, ActiveCheckTime string, ActiveAreaId string, ActiveCountry string, ActiveProvince string, Activecity string, ActiveDistrict string, ActiveStreet string, ActiveOperatorId string, Active_releaseId string, Active_EMUIVersion string, Active_operaSysVersion string, Active_BacVerNumber string, Active_BacFlashVer string, Active_webUIVersion string, Active_webUITypeCarrVer string,Active_webTypeDataVerNumber string, Active_operatorsVersion string, Active_phonePADPartitionedVersions string, Latest_YEAR int, Latest_MONTH int, Latest_DAY Decimal(30,10), Latest_HOUR string, Latest_areaId string, Latest_country string, Latest_province string, Latest_city string, Latest_district string, Latest_street string, Latest_releaseId string, Latest_EMUIVersion string, Latest_operaSysVersion string, Latest_BacVerNumber string, Latest_BacFlashVer string, Latest_webUIVersion string, Latest_webUITypeCarrVer string, Latest_webTypeDataVerNumber string, Latest_operatorsVersion string, Latest_phonePADPartitionedVersions string, Latest_operatorId string, gamePointDescription string,gamePointId double,contractNumber BigInt) STORED BY 'org.apache.carbondata.format' TBLPROPERTIES('table_blocksize'='2048')")res4: org.apache.spark.sql.DataFrame = []scala>scala> sql(s"LOAD DATA INPATH 'hdfs://hacluster/chetan/100_olap_C20.csv' INTO table all_datatypes_2048 options ('DELIMITER'=',', 'BAD_RECORDS_ACTION'='FORCE','FILEHEADER'='imei,deviceInformationId,MAC,deviceColor,device_backColor,modelId,marketName,AMSize,ROMSize,CUPAudit,CPIClocked,series,productionDate,bomCode,internalModels,deliveryTime,channelsId,channelsName,deliveryAreaId,deliveryCountry,deliveryProvince,deliveryCity,deliveryDistrict,deliveryStreet,oxSingleNumber,contractNumber,ActiveCheckTime,ActiveAreaId,ActiveCountry,ActiveProvince,Activecity,ActiveDistrict,ActiveStreet,ActiveOperatorId,Active_releaseId,Active_EMUIVersion,Active_operaSysVersion,Active_BacVerNumber,Active_BacFlashVer,Active_webUIVersion,Active_webUITypeCarrVer,Active_webTypeDataVerNumber,Active_operatorsVersion,Active_phonePADPartitionedVersions,Latest_YEAR,Latest_MONTH,Latest_DAY,Latest_HOUR,Latest_areaId,Latest_country,Latest_province,Latest_city,Latest_district,Latest_street,Latest_releaseId,Latest_EMUIVersion,Latest_operaSysVersion,Latest_BacVerNumber,Latest_BacFlashVer,Latest_webUIVersion,Latest_webUITypeCarrVer,Latest_webTypeDataVerNumber,Latest_operatorsVersion,Latest_phonePADPartitionedVersions,Latest_operatorId,gamePointId,gamePointDescription')")res5: org.apache.spark.sql.DataFrame = []scala>scala> sql(s"ALTER TABLE all_datatypes_2048 SET TBLPROPERTIES('streaming'='true')")res6: org.apache.spark.sql.DataFrame = []scala>scala>scala>scala> val carbonTable = CarbonEnv.getInstance(carbonSession).carbonMetastore.   lookupRelation(Some("default"), streamTableName)(carbonSession).asInstanceOf&#91;CarbonRelation&#93;.carbonTablecarbonTable: org.apache.carbondata.core.metadata.schema.table.CarbonTable = org.apache.carbondata.core.metadata.schema.table.CarbonTable@77648a90scala>scala> val tablePath = CarbonStorePath.getCarbonTablePath(carbonTable.getAbsoluteTableIdentifier)tablePath: org.apache.carbondata.core.util.path.CarbonTablePath = hdfs://hacluster/user/hive/warehouse/carbon.store/default/all_datatypes_2048scala>scala> val port = 8010port: Int = 8010scala> val serverSocket = new ServerSocket(port)serverSocket: java.net.ServerSocket = ServerSocket&#91;addr=0.0.0.0/0.0.0.0,localport=8010&#93;scala> val socketThread = writeSocket(serverSocket)socketThread: Thread = Thread&#91;Thread-81,5,main&#93;scala> val streamingThread = startStreaming(carbonSession, tablePath, streamTableName, port)Issue : Nullpointereception in spark shell when the streaming started with table streaming altered from default(false) to true. Streaming fails.scala> org.apache.carbondata.streaming.CarbonStreamException: Table default.all_datatypes_2048 is not a streaming table        at org.apache.spark.sql.CarbonSource.createSink(CarbonSource.scala:242)        at org.apache.spark.sql.execution.datasources.DataSource.createSink(DataSource.scala:274)        at org.apache.spark.sql.streaming.DataStreamWriter.start(DataStreamWriter.scala:266)        at $line28.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$anon$1.run(<console>:51)Done reading and writing streaming dataException in thread "Thread-82" java.lang.NullPointerException        at $line28.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$anon$1.run(<console>:59)Expected : Streaming should be continued successfully without any failure or exception after table streaming property altered from default(false) to true.
issueID:CARBONDATA-1814
type:Bug
changed files:
texts:(Carbon1.3.0 - Streaming) Nullpointereception in spark shell when the streaming started with table streaming altered from default(false) to true
Steps :Spark submit thrift server is started.User starts spark shell using the command - bin/spark-shell --master yarn-client --executor-memory 10G --executor-cores 5 --driver-memory 5G --num-executors 3 --jars /srv/spark2.2Bigdata/install/spark/sparkJdbc/carbonlib/carbondata_2.11-1.3.0-SNAPSHOT-shade-hadoop2.7.2.jarIn spark shell User tries to start streaming with table streaming property altered from default(false) to true.scala> import java.io.{File, PrintWriter}import java.io.{File, PrintWriter}scala> import java.net.ServerSocketimport java.net.ServerSocketscala>scala> import org.apache.spark.sql.{CarbonEnv, SparkSession}import org.apache.spark.sql.{CarbonEnv, SparkSession}scala> import org.apache.spark.sql.hive.CarbonRelationimport org.apache.spark.sql.hive.CarbonRelationscala> import org.apache.spark.sql.streaming.{ProcessingTime, StreamingQuery}import org.apache.spark.sql.streaming.{ProcessingTime, StreamingQuery}scala>scala> import org.apache.carbondata.core.constants.CarbonCommonConstantsimport org.apache.carbondata.core.constants.CarbonCommonConstantsscala> import org.apache.carbondata.core.util.CarbonPropertiesimport org.apache.carbondata.core.util.CarbonPropertiesscala> import org.apache.carbondata.core.util.path.{CarbonStorePath, CarbonTablePath}import org.apache.carbondata.core.util.path.{CarbonStorePath, CarbonTablePath}scala>scala> CarbonProperties.getInstance().addProperty(CarbonCommonConstants.CARBON_TIMESTAMP_FORMAT, "yyyy/MM/dd")res0: org.apache.carbondata.core.util.CarbonProperties = org.apache.carbondata.core.util.CarbonProperties@69ee0861scala>scala> import org.apache.spark.sql.CarbonSession._import org.apache.spark.sql.CarbonSession._scala>scala> val carbonSession = SparkSession.   builder().   appName("StreamExample").   getOrCreateCarbonSession("hdfs://hacluster/user/hive/warehouse/carbon.store")carbonSession: org.apache.spark.sql.SparkSession = org.apache.spark.sql.CarbonSession@6ce365b7scala> carbonSession.sparkContext.setLogLevel("INFO")scala>scala> def sql(sql: String) = carbonSession.sql(sql)sql: (sql: String)org.apache.spark.sql.DataFramescala>scala> def writeSocket(serverSocket: ServerSocket): Thread = {   val thread = new Thread() {     override def run(): Unit = {       // wait for client to connection request and accept       val clientSocket = serverSocket.accept()       val socketWriter = new PrintWriter(clientSocket.getOutputStream())       var index = 0       for (_ <- 1 to 1000) {         // write 5 records per iteration         for (_ <- 0 to 100) {     |           index = index + 1     |           socketWriter.println(index.toString + ",name_" + index     |                                + ",city_" + index + "," + (index * 10000.00).toString +     |                                ",school_" + index + ":school_" + index + index + "$" + index)     |         }         socketWriter.flush()         Thread.sleep(2000)       }       socketWriter.close()       System.out.println("Socket closed")     }   }   thread.start()   thread }writeSocket: (serverSocket: java.net.ServerSocket)Threadscala> def startStreaming(spark: SparkSession, tablePath: CarbonTablePath, tableName: String, port: Int): Thread = {     val thread = new Thread() {       override def run(): Unit = {         var qry: StreamingQuery = null         try {     |           val readSocketDF = spark.readStream     |             .format("socket")     |             .option("host", "10.18.98.34")     |             .option("port", port)     |             .load()     |     |           qry = readSocketDF.writeStream     |             .format("carbondata")     |             .trigger(ProcessingTime("5 seconds"))     |             .option("checkpointLocation", tablePath.getStreamingCheckpointDir)     |             .option("tablePath", tablePath.getPath).option("tableName", tableName)     |             .start()     |     |           qry.awaitTermination()     |         } catch {     |           case ex: Throwable =>     |             ex.printStackTrace()     |             println("Done reading and writing streaming data")     |         } finally {     |           qry.stop()     |         }       }     }     thread.start()     thread }startStreaming: (spark: org.apache.spark.sql.SparkSession, tablePath: org.apache.carbondata.core.util.path.CarbonTablePath, tableName: String, port: Int)Threadscala>scala> val streamTableName = "all_datatypes_2048"streamTableName: String = all_datatypes_2048scala>scala>scala> sql(s"create table all_datatypes_2048 (imei string,deviceInformationId int,MAC string,deviceColor string,device_backColor string,modelId string,marketName string,AMSize string,ROMSize string,CUPAudit string,CPIClocked string,series string,productionDate timestamp,bomCode string,internalModels string, deliveryTime string, channelsId string, channelsName string , deliveryAreaId string, deliveryCountry string, deliveryProvince string, deliveryCity string,deliveryDistrict string, deliveryStreet string, oxSingleNumber string, ActiveCheckTime string, ActiveAreaId string, ActiveCountry string, ActiveProvince string, Activecity string, ActiveDistrict string, ActiveStreet string, ActiveOperatorId string, Active_releaseId string, Active_EMUIVersion string, Active_operaSysVersion string, Active_BacVerNumber string, Active_BacFlashVer string, Active_webUIVersion string, Active_webUITypeCarrVer string,Active_webTypeDataVerNumber string, Active_operatorsVersion string, Active_phonePADPartitionedVersions string, Latest_YEAR int, Latest_MONTH int, Latest_DAY Decimal(30,10), Latest_HOUR string, Latest_areaId string, Latest_country string, Latest_province string, Latest_city string, Latest_district string, Latest_street string, Latest_releaseId string, Latest_EMUIVersion string, Latest_operaSysVersion string, Latest_BacVerNumber string, Latest_BacFlashVer string, Latest_webUIVersion string, Latest_webUITypeCarrVer string, Latest_webTypeDataVerNumber string, Latest_operatorsVersion string, Latest_phonePADPartitionedVersions string, Latest_operatorId string, gamePointDescription string,gamePointId double,contractNumber BigInt) STORED BY 'org.apache.carbondata.format' TBLPROPERTIES('table_blocksize'='2048')")res4: org.apache.spark.sql.DataFrame = []scala>scala> sql(s"LOAD DATA INPATH 'hdfs://hacluster/chetan/100_olap_C20.csv' INTO table all_datatypes_2048 options ('DELIMITER'=',', 'BAD_RECORDS_ACTION'='FORCE','FILEHEADER'='imei,deviceInformationId,MAC,deviceColor,device_backColor,modelId,marketName,AMSize,ROMSize,CUPAudit,CPIClocked,series,productionDate,bomCode,internalModels,deliveryTime,channelsId,channelsName,deliveryAreaId,deliveryCountry,deliveryProvince,deliveryCity,deliveryDistrict,deliveryStreet,oxSingleNumber,contractNumber,ActiveCheckTime,ActiveAreaId,ActiveCountry,ActiveProvince,Activecity,ActiveDistrict,ActiveStreet,ActiveOperatorId,Active_releaseId,Active_EMUIVersion,Active_operaSysVersion,Active_BacVerNumber,Active_BacFlashVer,Active_webUIVersion,Active_webUITypeCarrVer,Active_webTypeDataVerNumber,Active_operatorsVersion,Active_phonePADPartitionedVersions,Latest_YEAR,Latest_MONTH,Latest_DAY,Latest_HOUR,Latest_areaId,Latest_country,Latest_province,Latest_city,Latest_district,Latest_street,Latest_releaseId,Latest_EMUIVersion,Latest_operaSysVersion,Latest_BacVerNumber,Latest_BacFlashVer,Latest_webUIVersion,Latest_webUITypeCarrVer,Latest_webTypeDataVerNumber,Latest_operatorsVersion,Latest_phonePADPartitionedVersions,Latest_operatorId,gamePointId,gamePointDescription')")res5: org.apache.spark.sql.DataFrame = []scala>scala> sql(s"ALTER TABLE all_datatypes_2048 SET TBLPROPERTIES('streaming'='true')")res6: org.apache.spark.sql.DataFrame = []scala>scala>scala>scala> val carbonTable = CarbonEnv.getInstance(carbonSession).carbonMetastore.   lookupRelation(Some("default"), streamTableName)(carbonSession).asInstanceOf&#91;CarbonRelation&#93;.carbonTablecarbonTable: org.apache.carbondata.core.metadata.schema.table.CarbonTable = org.apache.carbondata.core.metadata.schema.table.CarbonTable@77648a90scala>scala> val tablePath = CarbonStorePath.getCarbonTablePath(carbonTable.getAbsoluteTableIdentifier)tablePath: org.apache.carbondata.core.util.path.CarbonTablePath = hdfs://hacluster/user/hive/warehouse/carbon.store/default/all_datatypes_2048scala>scala> val port = 8010port: Int = 8010scala> val serverSocket = new ServerSocket(port)serverSocket: java.net.ServerSocket = ServerSocket&#91;addr=0.0.0.0/0.0.0.0,localport=8010&#93;scala> val socketThread = writeSocket(serverSocket)socketThread: Thread = Thread&#91;Thread-81,5,main&#93;scala> val streamingThread = startStreaming(carbonSession, tablePath, streamTableName, port)Issue : Nullpointereception in spark shell when the streaming started with table streaming altered from default(false) to true. Streaming fails.scala> org.apache.carbondata.streaming.CarbonStreamException: Table default.all_datatypes_2048 is not a streaming table        at org.apache.spark.sql.CarbonSource.createSink(CarbonSource.scala:242)        at org.apache.spark.sql.execution.datasources.DataSource.createSink(DataSource.scala:274)        at org.apache.spark.sql.streaming.DataStreamWriter.start(DataStreamWriter.scala:266)        at $line28.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$anon$1.run(<console>:51)Done reading and writing streaming dataException in thread "Thread-82" java.lang.NullPointerException        at $line28.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$anon$1.run(<console>:59)Expected : Streaming should be continued successfully without any failure or exception after table streaming property altered from default(false) to true.
issueID:CARBONDATA-1815
type:Improvement
changed files:
texts:Add AtomicRunnableCommand abstraction
Some CarbonData command need to process both metadata and data which should be in an atomic fashion. These commands need to support undo if any failure.
issueID:CARBONDATA-1816
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
texts:Changing BAD_RECORDS_ACTION default action to FAIL
Changing BAD_RECORDS_ACTION default action to FAILCurrently, the default action is FORCE, this may allow the user by default to load bad records also. So, changing the default action of BAD_RECORDS_ACTION to FAIL will alert the user if any bad_records are there in the loading data.
issueID:CARBONDATA-1817
type:Sub-task
changed files:
texts:Reject create datamap on streaming table
Since streaming segment does not support building index and pre-aggregate yet, so streaming table should not support create datamap operation
issueID:CARBONDATA-1818
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
streaming/src/main/java/org/apache/carbondata/streaming/CarbonStreamOutputFormat.java
core/src/main/java/org/apache/carbondata/core/util/CarbonProperties.java
texts:Should make carbon.streaming.segment.max.size as configurable
Should make carbon.streaming.segment.max.size as configurable
issueID:CARBONDATA-1819
type:Improvement
changed files:
texts:Remove profiles for Spark-2.1 and Spark-2.2 in module assembly and spark-common-test

issueID:CARBONDATA-182
type:Bug
changed files:
texts:save to parquet file is not working
The following query is not working.cc.sql("select * from DIM_LOC_SGSN").write.parquet("/home/root1/DIM_LOC_SGSN1")
issueID:CARBONDATA-1820
type:Improvement
changed files:processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactDataHandlerModel.java
processing/src/main/java/org/apache/carbondata/processing/merger/RowResultMergerProcessor.java
processing/src/main/java/org/apache/carbondata/processing/util/CarbonDataProcessorUtil.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/CarbonTable.java
core/src/main/java/org/apache/carbondata/core/metadata/CarbonMetadata.java
core/src/main/java/org/apache/carbondata/core/metadata/converter/ThriftWrapperSchemaConverterImpl.java
texts:Should extract CarbonTable.buildUniqueName method and re-factory code to invoke this method
Should  extract CarbonTable.buildUniqueName method and re-factory code to invoke this method
issueID:CARBONDATA-1821
type:Improvement
changed files:
texts:Incorrect headings in documentation
Incorrect headings in documentation
issueID:CARBONDATA-1822
type:New Feature
changed files:core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/SchemaReader.java
core/src/main/java/org/apache/carbondata/core/metadata/converter/ThriftWrapperSchemaConverterImpl.java
texts:Support DDL to register the CarbonData table from existing carbon table data
For details please refer to the below linkhttp://apache-carbondata-dev-mailing-list-archive.1130556.n5.nabble.com/DDL-for-CarbonData-table-backup-and-recovery-new-feature-td27854.html
issueID:CARBONDATA-1824
type:Bug
changed files:
texts:Carbon 1.3.0 - Spark 2.2-Residual segment files left over when load failure happens
Steps:Beeline:1. Create a table with batch sort as sort type, keep block size small2. Run Load/Insert/Compaction the table3. Bring down thrift server when carbon data is being written to the segment4. Do show segments on the tableExpected: It should not show the residual segments  Actual: The segment intended for load is shown as marked for delete and it does not get deleted with clean file. No impact on the table as such.Query:create table if not exists lineitem1(L_SHIPDATE string,L_SHIPMODE string,L_SHIPINSTRUCT string,L_RETURNFLAG string,L_RECEIPTDATE string,L_ORDERKEY string,L_PARTKEY string,L_SUPPKEY   string,L_LINENUMBER int,L_QUANTITY double,L_EXTENDEDPRICE double,L_DISCOUNT double,L_TAX double,L_LINESTATUS string,L_COMMITDATE string,L_COMMENT  string) STORED BY 'org.apache.carbondata.format' TBLPROPERTIES ('table_blocksize'='1','sort_scope'='BATCH_SORT','batch_sort_size_inmb'='5000');load data inpath "hdfs://hacluster/user/test/lineitem.tbl.1" into table lineitem options('DELIMITER'='|','FILEHEADER'='L_ORDERKEY,L_PARTKEY,L_SUPPKEY,L_LINENUMBER,L_QUANTITY,L_EXTENDEDPRICE,L_DISCOUNT,L_TAX,L_RETURNFLAG,L_LINESTATUS,L_SHIPDATE,L_COMMITDATE,L_RECEIPTDATE,L_SHIPINSTRUCT,L_SHIPMODE,L_COMMENT');0: jdbc:hive2://10.18.98.34:23040> select count from t_carbn0161;-----------+ count(1)  -----------+ 0         -----------+1 row selected (13.011 seconds)0: jdbc:hive2://10.18.98.34:23040> show segments for table lineitem1;------------------------------------------------------------------------------------------------------------ SegmentSequenceId         Status            Load Start Time            Load End Time        Merged To   File Format  ------------------------------------------------------------------------------------------------------------ 1                   Marked for Delete   2017-11-28 19:14:46.265   2017-11-28 19:15:28.396   NA          COLUMNAR_V3   0                   Marked for Delete   2017-11-28 19:12:58.269   2017-11-28 19:13:37.26    NA          COLUMNAR_V3  ------------------------------------------------------------------------------------------------------------0: jdbc:hive2://10.18.98.34:23040> clean files for table t_carbn0161;---------+ Result  ---------+---------+No rows selected (7.473 seconds)0: jdbc:hive2://10.18.98.34:23040> show segments for table lineitem1;------------------------------------------------------------------------------------------------------------ SegmentSequenceId         Status            Load Start Time            Load End Time        Merged To   File Format  ------------------------------------------------------------------------------------------------------------ 1                   Marked for Delete   2017-11-28 19:14:46.265   2017-11-28 19:15:28.396   NA          COLUMNAR_V3   0                   Marked for Delete   2017-11-28 19:12:58.269   2017-11-28 19:13:37.26    NA          COLUMNAR_V3  ------------------------------------------------------------------------------------------------------------
issueID:CARBONDATA-1826
type:Bug
changed files:
texts:Carbon 1.3.0 - Spark 2.2: Describe table & Describe Formatted shows the same result
1. CREATE table NoTblproperties (CUST_ID bigint,CUST_NAME String,INT_COLUMN1 bigint) STORED BY 'org.apache.carbondata.format'2. Desc NoTblproperties 3.  Desc formatted NoTblproperties
issueID:CARBONDATA-1827
type:Task
changed files:core/src/main/java/org/apache/carbondata/core/datastore/impl/FileFactory.java
hadoop/src/main/java/org/apache/carbondata/hadoop/util/CarbonInputFormatUtil.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
core/src/main/java/org/apache/carbondata/core/locks/S3FileLock.java
core/src/main/java/org/apache/carbondata/core/locks/CarbonLockFactory.java
core/src/main/java/org/apache/carbondata/core/datastore/filesystem/AbstractDFSCarbonFile.java
core/src/main/java/org/apache/carbondata/core/util/CarbonProperties.java
core/src/main/java/org/apache/carbondata/core/datastore/filesystem/HDFSCarbonFile.java
texts:Add Support to provide S3 Functionality in Carbondata
Added Support to provide S3 Functionality in Carbondata.
issueID:CARBONDATA-1828
type:Bug
changed files:
texts:Carbon 1.3.0 - Spark 2.2 Empty CSV is being loaded successfully.
1. CREATE TABLE test3 (ID int,CUST_ID int,cust_name string) STORED BY 'org.apache.carbondata.format'2. LOAD DATA INPATH 'hdfs://hacluster/BabuStore/Data/InsertData/test3.csv' into table test3 OPTIONS('DELIMITER'=',' , 'QUOTECHAR'='"','FILEHEADER'='ID,CUST_ID,Cust_name')The test case should have been failed, since the CSV is empty, but the data load was successful.
issueID:CARBONDATA-1829
type:Bug
changed files:
texts:Carbon 1.3.0 - Spark 2.2: Insert is passing when Hive is having Float and Carbon is having INT value and load file is having single precision decimal value
Steps:1. create table Hive3(Sell_price FLOAT, Item_code STRING, Qty_total Double,Profit Decimal(4,3), Update_time TIMESTAMP )row format delimited fields terminated by ',' collection items terminated by '$'2. create table Carbon3(Sell_price INT, Item_code STRING, Qty_total DECIMAL(3,1),Profit  DECIMAL(3,2), Update_time TIMESTAMP ) STORED BY 'org.apache.carbondata.format'3. load data LOCAL INPATH '/opt/csv/Data/InsertData/Hive3.csv' overwrite into table Hive3Issue:Insert is passing when Hive is having Float and Carbon is having INT value and load file is having single precision decimal value. This should be failed.Expected:It should be failed.
issueID:CARBONDATA-183
type:Bug
changed files:integration/spark-common/src/main/java/org/apache/carbondata/spark/load/CarbonLoaderUtil.java
texts:Blocks are allocated to single node when Executors configured is based on the ip address.

issueID:CARBONDATA-1830
type:Bug
changed files:
texts:Carbon 1.3.0 - Spark 2.2: Data Load with invalid File-header is being executed successfully.
Steps:1. create table DL_Invalid_FILEHEADER(Active_status String,Item_type_cd INT,Qty_day_avg INT,Qty_total INT,Sell_price BIGINT,Sell_pricep DOUBLE,Discount_price DOUBLE,Profit DECIMAL(3,2),Item_code String,Item_name String,Outlet_name String,Update_time TIMESTAMP,Create_date String)STORED BY 'org.apache.carbondata.format'2. LOAD DATA INPATH 'HDFS_URL/BabuStore/Data/InsertData/T_Hive1.csv' INTO table DL_Invalid_FILEHEADER options ('DELIMITER'=',', 'QUOTECHAR'='\', 'FILEHEADER'='Item_type_cd,Active_status,Qty_day_avg,Qty_total,Sell_price,Sell_pricep,Discount_price,Profit,Item_code,Item_name,Outlet_name,Update_time,Create_date')Issue:Data Load with invalid File-header is being executed successfully.Expected:Data Loading should be failed.
issueID:CARBONDATA-1831
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/datastore/filesystem/LocalCarbonFile.java
texts:Carbon 1.3.0 - BAD_RECORDS: Data Loading with Action as Redirect & logger enable is not logging the logs in the defined path.
Steps:1. CREATE TABLE uniqdata (CUST_ID int,CUST_NAME String,ACTIVE_EMUI_VERSION string, DOB timestamp, DOJ timestamp, BIGINT_COLUMN1 bigint,BIGINT_COLUMN2 bigint,DECIMAL_COLUMN1 decimal(30,10), DECIMAL_COLUMN2 decimal(36,10),Double_COLUMN1 double, Double_COLUMN2 double,INTEGER_COLUMN1 int) STORED BY 'org.apache.carbondata.format'2. LOAD DATA INPATH 'HDFS_URL/BabuStore/Data/InsertData/2000_UniqData.csv' into table uniqdata OPTIONS('DELIMITER'=',' , 'QUOTECHAR'='"','BAD_RECORDS_LOGGER_ENABLE'='TRUE', 'BAD_RECORDS_ACTION'='REDIRECT','FILEHEADER'='CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1,Double_COLUMN2,INTEGER_COLUMN1')Issue:Data Load is not creating the logs when bad_records location is specified in carbon.properties.Expected:Bad_Records log should be created in the specified path.
issueID:CARBONDATA-1832
type:Bug
changed files:
texts:Table cache should be cleared when dropping table

issueID:CARBONDATA-1833
type:Bug
changed files:
texts:Should fix BindException in TestStreamingTableOperation
Test case TestStreamingTableOperation throwing BindException: Address already in use
issueID:CARBONDATA-1834
type:Improvement
changed files:integration/spark-datasource/src/main/scala/org/apache/carbondata/spark/vectorreader/VectorizedCarbonRecordReader.java
texts:Multi-user concurrent scene: when running "insert overwrite" task,and parallely executing select query task
【test steps】：1. create table：create table obs (name string , age int, gender string, class int , comment string) using csv options ( path 's3a://obs-js/namelist.csv, header 'true' );2. create table:   create table uquery (name string , age int, gender string, class int , comment string);3. run command:   insert overwrite table uquery select * from obs3. when insert command is running,  run some  select command【expect result】 ： if insert overwrite  finish，can  get new result;   if not finish, can get old result【actual result】：Multi-user concurrent scene: when running "insert overwrite" task,  another user will scan table failed sometimes
issueID:CARBONDATA-1836
type:Bug
changed files:
texts:[Spark-2.2 Integration] Resolving CatalogRelation
CatalogRelation Class is Renamed to HiveTableRelation in post spark 2.2 releases. Handling the case class name change for future compatibility.
issueID:CARBONDATA-1837
type:Improvement
changed files:processing/src/main/java/org/apache/carbondata/processing/loading/steps/DataConverterProcessorStepImpl.java
processing/src/main/java/org/apache/carbondata/processing/loading/row/CarbonRowBatch.java
texts:Reusing old row to reduce memory consumption
In data converting process of data loading, Carbondata will convert each row to another row by batch. Currently, it will create a new batch to store the converted rows, which I think can be optimized to reuse the old row batch's space, thus will reduce memory consumption and GC overhead.
issueID:CARBONDATA-1838
type:Improvement
changed files:processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/merger/UnsafeSingleThreadFinalSortFilesMerger.java
texts:Refactor SortStepRowUtil to make it more readable
Refactor and optimize `SortRowStepUtil` to make it efficient and more readable.
issueID:CARBONDATA-1839
type:Bug
changed files:processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/SortDataRows.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/SortParameters.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/merger/UnsafeIntermediateFileMerger.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/IntermediateFileMerger.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/SingleThreadFinalSortFilesMerger.java
core/src/main/java/org/apache/carbondata/core/datastore/filesystem/AbstractDFSCarbonFile.java
core/src/main/java/org/apache/carbondata/core/datastore/filesystem/CarbonFile.java
core/src/main/java/org/apache/carbondata/core/util/CarbonProperties.java
core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
core/src/main/java/org/apache/carbondata/core/datastore/impl/FileFactory.java
core/src/main/java/org/apache/carbondata/core/datastore/filesystem/LocalCarbonFile.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/impl/ParallelReadMergeSorterWithColumnRangeImpl.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/impl/ParallelReadMergeSorterImpl.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/holder/UnsafeSortTempFileChunkHolder.java
processing/src/main/java/org/apache/carbondata/processing/merger/CompactionResultSortProcessor.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/SortTempFileChunkHolder.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/UnsafeSortDataRows.java
texts:Data load failed when using compressed sort temp file
Carbondata provide an option to optimize data load process by compressing the intermediate sort temp files.The option is `carbon.is.sort.temp.file.compression.enabled` and its default value is `false`. In some disk tense scenario, user can turn on this feature by setting the option `true`, it will compress the file content before writing it to disk.How ever I have found bugs in the related code and the data load was failed after turning on this feature.This bug can be reproduced easily. I used the example from `TestLoadDataFrame` Line98.1. create a dataframe (e.g. 320000 rows with 3 columns)2. set carbon.is.sort.temp.file.compression.enabled=true in CarbonProperities3. write the dataframe to a carbontable through dataframewriterError messages are shown as below:```17/11/29 18:04:12 ERROR SortDataRows: SortDataRowPool:test1 java.lang.ClassCastException: [B cannot be cast to [Ljava.lang.Integer;    at org.apache.carbondata.core.util.NonDictionaryUtil.getDimension(NonDictionaryUtil.java:93)    at org.apache.carbondata.processing.sort.sortdata.UnCompressedTempSortFileWriter.writeDataOutputStream(UnCompressedTempSortFileWriter.java:52)    at org.apache.carbondata.processing.sort.sortdata.CompressedTempSortFileWriter.writeSortTempFile(CompressedTempSortFileWriter.java:65)    at org.apache.carbondata.processing.sort.sortdata.SortTempFileChunkWriter.writeSortTempFile(SortTempFileChunkWriter.java:72)    at org.apache.carbondata.processing.sort.sortdata.SortDataRows.writeSortTempFile(SortDataRows.java:245)    at org.apache.carbondata.processing.sort.sortdata.SortDataRows.writeDataTofile(SortDataRows.java:232)    at org.apache.carbondata.processing.sort.sortdata.SortDataRows.access$300(SortDataRows.java:45)    at org.apache.carbondata.processing.sort.sortdata.SortDataRows$DataSorterAndWriter.run(SortDataRows.java:426)    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)    at java.lang.Thread.run(Thread.java:745)``````17/11/29 18:04:13 ERROR SortDataRows: SafeParallelSorterPool:test1 exception occurred while trying to acquire a semaphore lock: Task org.apache.carbondata.processing.sort.sortdata.SortDataRows$DataSorterAndWriter@3d413b40 rejected from java.util.concurrent.ThreadPoolExecutor@cb56011&#91;Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 1&#93;17/11/29 18:04:13 ERROR ParallelReadMergeSorterImpl: SafeParallelSorterPool:test1 org.apache.carbondata.processing.sort.exception.CarbonSortKeyAndGroupByException:     at org.apache.carbondata.processing.sort.sortdata.SortDataRows.addRowBatch(SortDataRows.java:173)    at org.apache.carbondata.processing.loading.sort.impl.ParallelReadMergeSorterImpl$SortIteratorThread.run(ParallelReadMergeSorterImpl.java:227)    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)    at java.lang.Thread.run(Thread.java:745)Caused by: java.util.concurrent.RejectedExecutionException: Task org.apache.carbondata.processing.sort.sortdata.SortDataRows$DataSorterAndWriter@3d413b40 rejected from java.util.concurrent.ThreadPoolExecutor@cb56011&#91;Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 1&#93;    at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2047)    at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:823)    at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1369)    at org.apache.carbondata.processing.sort.sortdata.SortDataRows.addRowBatch(SortDataRows.java:169)    ... 4 more``````    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)    at java.lang.Thread.run(Thread.java:745)Caused by: org.apache.carbondata.processing.sort.exception.CarbonSortKeyAndGroupByException:     at org.apache.carbondata.processing.sort.sortdata.SortDataRows.addRowBatch(SortDataRows.java:173)    at org.apache.carbondata.processing.loading.sort.impl.ParallelReadMergeSorterImpl$SortIteratorThread.run(ParallelReadMergeSorterImpl.java:227)    ... 3 moreCaused by: java.util.concurrent.RejectedExecutionException: Task org.apache.carbondata.processing.sort.sortdata.SortDataRows$DataSorterAndWriter@3d413b40 rejected from java.util.concurrent.ThreadPoolExecutor@cb56011&#91;Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 1&#93;    at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2047)    at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:823)    at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1369)    at org.apache.carbondata.processing.sort.sortdata.SortDataRows.addRowBatch(SortDataRows.java:169)    ... 4 more```
issueID:CARBONDATA-184
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
texts:Complex types data load is not loading the data with special character delimiters like " ^ * - .
When complex types data is having ^ delimiter, data is not getting loaded properly. When the data is queried, output is NULLeg; array<string> and complex data level1 delimiter is '^' actual data is john^carter^justindata is not getting split and not getting loaded & when queried, its give NULL value.
issueID:CARBONDATA-1840
type:Bug
changed files:
texts:carbon.data.file.version default value is not correct in http://carbondata.apache.org/configuration-parameters.html
Steps:User checks the http://carbondata.apache.org/configuration-parameters.html link for the default value of carbon.data.file.version.Issue : carbon.data.file.version default value is mentioned as 2. Old format value is mentioned as 1.Expected : carbon.data.file.version default value should be 3. Old format value should be 1 and 2.
issueID:CARBONDATA-1841
type:Bug
changed files:
texts:Data is not being loaded into pre-aggregation table after creation

issueID:CARBONDATA-1842
type:Bug
changed files:
texts:Fix &#39;wrong argument number&#39; error of class Cast for Spark 2.2 when pattern matching
The constructor of class org.apache.spark.sql.catalyst.expressions.Cast has two arguments in Spark 2.1, but it needs three arguments in Spark 2.2.
issueID:CARBONDATA-1843
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/util/CarbonProperties.java
texts:Block CTAS and external table syntax
1. Block 'external' syntax2. Block 'CTAS' syntax
issueID:CARBONDATA-1844
type:New Feature
changed files:processing/src/main/java/org/apache/carbondata/processing/loading/converter/impl/FieldEncoderFactory.java
core/src/main/java/org/apache/carbondata/core/service/impl/ColumnUniqueIdGenerator.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/TableInfo.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/SchemaReader.java
core/src/main/java/org/apache/carbondata/core/util/path/CarbonTablePath.java
core/src/main/java/org/apache/carbondata/core/service/ColumnUniqueIdService.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/CarbonTable.java
core/src/main/java/org/apache/carbondata/core/metadata/CarbonMetadata.java
integration/presto/src/main/java/org/apache/carbondata/presto/impl/CarbonTableReader.java
processing/src/main/java/org/apache/carbondata/processing/util/CarbonLoaderUtil.java
core/src/main/java/org/apache/carbondata/core/metadata/converter/SchemaConverter.java
core/src/main/java/org/apache/carbondata/core/scan/executor/util/QueryUtil.java
core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
core/src/main/java/org/apache/carbondata/core/metadata/converter/ThriftWrapperSchemaConverterImpl.java
core/src/main/java/org/apache/carbondata/core/metadata/AbsoluteTableIdentifier.java
texts:Support specify tablePath when creating table
User should be able to specify table path when creating table
issueID:CARBONDATA-1845
type:Bug
changed files:
texts:Incorrect output on presto CLI while executing OR operator with multiple load
Incorrect output on presto CLI while executing OR operator with multiple loadSteps to reproduce:On beeline:1) Create a table:CREATE TABLE uniqdata_time (CUST_ID int,CUST_NAME String,ACTIVE_EMUI_VERSION string, DOB timestamp, DOJ timestamp, BIGINT_COLUMN1 bigint,BIGINT_COLUMN2 bigint,DECIMAL_COLUMN1 decimal(30,10), DECIMAL_COLUMN2 decimal(36,10),Double_COLUMN1 double, Double_COLUMN2 double,INTEGER_COLUMN1 int) STORED BY 'org.apache.carbondata.format' TBLPROPERTIES ("TABLE_BLOCKSIZE"= "256 MB")2)Load Data with multiple segments in it: a) LOAD DATA INPATH 'hdfs://localhost:54310/Data/uniqdata/2000_UniqData.csv' into table uniqdata_time OPTIONS('DELIMITER'=',' , 'QUOTECHAR'='"','BAD_RECORDS_ACTION'='FORCE','FILEHEADER'='CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1,Double_COLUMN2,INTEGER_COLUMN1')b) LOAD DATA INPATH 'hdfs://localhost:54310/Data/uniqdata/3000_UniqData.csv' into table uniqdata_time OPTIONS('DELIMITER'=',' , 'QUOTECHAR'='"','BAD_RECORDS_ACTION'='FORCE','FILEHEADER'='CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1,Double_COLUMN2,INTEGER_COLUMN1')c) LOAD DATA INPATH 'hdfs://localhost:54310/Data/uniqdata/4000_UniqData.csv' into table uniqdata_time OPTIONS('DELIMITER'=',' , 'QUOTECHAR'='"','BAD_RECORDS_ACTION'='FORCE','FILEHEADER'='CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1,Double_COLUMN2,INTEGER_COLUMN1')d) LOAD DATA INPATH 'hdfs://localhost:54310/Data/uniqdata/5000_UniqData.csv' into table uniqdata_time OPTIONS('DELIMITER'=',' , 'QUOTECHAR'='"','BAD_RECORDS_ACTION'='FORCE','FILEHEADER'='CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1,Double_COLUMN2,INTEGER_COLUMN1')e) LOAD DATA INPATH 'hdfs://localhost:54310/Data/uniqdata/6000_UniqData.csv' into table uniqdata_time OPTIONS('DELIMITER'=',' , 'QUOTECHAR'='"','BAD_RECORDS_ACTION'='FORCE','FILEHEADER'='CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1,Double_COLUMN2,INTEGER_COLUMN1')f) LOAD DATA INPATH 'hdfs://localhost:54310/Data/uniqdata/7000_UniqData.csv' into table uniqdata_time OPTIONS('DELIMITER'=',' , 'QUOTECHAR'='"','BAD_RECORDS_ACTION'='FORCE','FILEHEADER'='CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1,Double_COLUMN2,INTEGER_COLUMN1')3) Execute Query:select * from uniqdata_time where DOB = cast('2015-10-04 01:00:03' as timestamp) or DOB = cast('2015-10-07 01:00:03' as timestamp)Output on beeline:---------------------------------------------------------------------------------------------------------+ CUST_ID      CUST_NAME         ACTIVE_EMUI_VERSION               DOB                     DOJ            BIGINT_COLUMN1   BIGINT_COLUMN2       DECIMAL_COLUMN1          DECIMAL_COLUMN2         Double_COLUMN1        Double_COLUMN2      INTEGER_COLUMN1  ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- 25712     CUST_NAME_16712   ACTIVE_EMUI_VERSION_16712   2015-10-04 01:00:03.0   2015-10-04 02:00:03.0   123372053566     -223372020142    12345695613.1234000000   22345695613.1234000000   1.12345674897976E10   -1.12345674897976E10   16713             25715     CUST_NAME_16715   ACTIVE_EMUI_VERSION_16715   2015-10-07 01:00:03.0   2015-10-07 02:00:03.0   123372053569     -223372020139    12345695616.1234000000   22345695616.1234000000   1.12345674897976E10   -1.12345674897976E10   167164) Output On Presto CLI:a)Expected Output on Presto: Similar to the output on beelineb) Actual output on Presto: cust_id | cust_name | active_emui_version | dob | doj | bigint_column1 | bigint_column2 | decimal_column1 | decimal_column2 | double_column1 |-------------------------------------------------------------------------------------------------------------------(0 rows)(END)
issueID:CARBONDATA-1846
type:Bug
changed files:integration/presto/src/main/java/org/apache/carbondata/presto/PrestoFilterUtil.java
texts:Incorrect output on presto CLI while executing IN operator with multiple load
Incorrect output on presto CLI while executing IN operator with multiple loadSteps to reproduce:On beeline:1) Create a table:CREATE TABLE uniqdata_time (CUST_ID int,CUST_NAME String,ACTIVE_EMUI_VERSION string, DOB timestamp, DOJ timestamp, BIGINT_COLUMN1 bigint,BIGINT_COLUMN2 bigint,DECIMAL_COLUMN1 decimal(30,10), DECIMAL_COLUMN2 decimal(36,10),Double_COLUMN1 double, Double_COLUMN2 double,INTEGER_COLUMN1 int) STORED BY 'org.apache.carbondata.format' TBLPROPERTIES ("TABLE_BLOCKSIZE"= "256 MB")2)Load Data with multiple segments in it: a) LOAD DATA INPATH 'hdfs://localhost:54310/Data/uniqdata/2000_UniqData.csv' into table uniqdata_time OPTIONS('DELIMITER'=',' , 'QUOTECHAR'='"','BAD_RECORDS_ACTION'='FORCE','FILEHEADER'='CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1,Double_COLUMN2,INTEGER_COLUMN1')b) LOAD DATA INPATH 'hdfs://localhost:54310/Data/uniqdata/3000_UniqData.csv' into table uniqdata_time OPTIONS('DELIMITER'=',' , 'QUOTECHAR'='"','BAD_RECORDS_ACTION'='FORCE','FILEHEADER'='CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1,Double_COLUMN2,INTEGER_COLUMN1')c) LOAD DATA INPATH 'hdfs://localhost:54310/Data/uniqdata/4000_UniqData.csv' into table uniqdata_time OPTIONS('DELIMITER'=',' , 'QUOTECHAR'='"','BAD_RECORDS_ACTION'='FORCE','FILEHEADER'='CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1,Double_COLUMN2,INTEGER_COLUMN1')d) LOAD DATA INPATH 'hdfs://localhost:54310/Data/uniqdata/5000_UniqData.csv' into table uniqdata_time OPTIONS('DELIMITER'=',' , 'QUOTECHAR'='"','BAD_RECORDS_ACTION'='FORCE','FILEHEADER'='CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1,Double_COLUMN2,INTEGER_COLUMN1')e) LOAD DATA INPATH 'hdfs://localhost:54310/Data/uniqdata/6000_UniqData.csv' into table uniqdata_time OPTIONS('DELIMITER'=',' , 'QUOTECHAR'='"','BAD_RECORDS_ACTION'='FORCE','FILEHEADER'='CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1,Double_COLUMN2,INTEGER_COLUMN1')f) LOAD DATA INPATH 'hdfs://localhost:54310/Data/uniqdata/7000_UniqData.csv' into table uniqdata_time OPTIONS('DELIMITER'=',' , 'QUOTECHAR'='"','BAD_RECORDS_ACTION'='FORCE','FILEHEADER'='CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1,Double_COLUMN2,INTEGER_COLUMN1')3) Execute Query:select DOB from UNIQDATA_time where DOB in (cast('2015-10-04 01:00:03' as timestamp),cast('2015-10-07' as timestamp),cast('2015-10-07 01:00:03' as timestamp));Output on Beeline:------------------------+          DOB           ------------------------+ 2015-10-04 01:00:03.0   2015-10-07 01:00:03.0  ------------------------+2 rows selected (0.351 seconds)4) Output on Presto CLI:a) Expected Output: similar to the result on Beeline.b) Actual Output : DOB (0 rows)
issueID:CARBONDATA-1847
type:Improvement
changed files:integration/spark-datasource/src/main/scala/org/apache/carbondata/spark/vectorreader/VectorizedCarbonRecordReader.java
hadoop/src/main/java/org/apache/carbondata/hadoop/InputMetricsStats.java
texts:Add inputSize for row

issueID:CARBONDATA-1848
type:Bug
changed files:
texts:Streaming sink should adapt spark 2.2
Streaming sink should adapt spark 2.2
issueID:CARBONDATA-185
type:Bug
changed files:
texts:"DROP CUBE" need change to "DROP TABLE" in CarbonDatasourceRelation.scala
"DROP CUBE" need change to "DROP TABLE" in CarbonDatasourceRelation.scala
issueID:CARBONDATA-1854
type:Sub-task
changed files:core/src/main/java/org/apache/carbondata/core/metadata/blocklet/BlockletInfo.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/ImplicitIncludeFilterExecutorImpl.java
core/src/main/java/org/apache/carbondata/core/datastore/DataRefNode.java
core/src/main/java/org/apache/carbondata/core/scan/filter/FilterUtil.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/ImplicitColumnFilterExecutor.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/AndFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockletDataMap.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/resolverinfo/visitor/FilterInfoTypeVisitorFactory.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/resolverinfo/visitor/ImplicitColumnVisitor.java
core/src/main/java/org/apache/carbondata/core/scan/filter/FilterExpressionProcessor.java
core/src/main/java/org/apache/carbondata/core/scan/filter/ColumnFilterInfo.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockletDataRefNode.java
texts:Add support for implicit column filter
Whenever a filter is applied on implicit column, filter is applied at blocklet level to scan/read only the valid blocklets. This will ensure that only blocklets that contain the required data are read.
issueID:CARBONDATA-1855
type:Sub-task
changed files:processing/src/main/java/org/apache/carbondata/processing/merger/CarbonDataMergerUtil.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonOutputCommitter.java
processing/src/main/java/org/apache/carbondata/processing/loading/model/CarbonLoadModel.java
core/src/main/java/org/apache/carbondata/core/metadata/datatype/StructType.java
processing/src/main/java/org/apache/carbondata/processing/util/CarbonLoaderUtil.java
processing/src/main/java/org/apache/carbondata/processing/loading/iterator/CarbonOutputIteratorWrapper.java
core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonTableOutputFormat.java
texts:Add outputformat in carbon.
Support standard Hadoop outputformat interface for carbon. It will be helpful for integrations to execution engines like the spark, hive, and presto.It should maintain segment management as well while writing the data to support incremental loading feature.
issueID:CARBONDATA-1856
type:Sub-task
changed files:processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactDataHandlerModel.java
core/src/main/java/org/apache/carbondata/core/util/path/CarbonTablePath.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonTableInputFormat.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/CarbonTable.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonOutputCommitter.java
core/src/main/java/org/apache/carbondata/core/mutate/CarbonUpdateUtil.java
processing/src/main/java/org/apache/carbondata/processing/store/CarbonDataFileAttributes.java
streaming/src/main/java/org/apache/carbondata/streaming/CarbonStreamRecordWriter.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/partition/PartitionType.java
processing/src/main/java/org/apache/carbondata/processing/loading/iterator/CarbonOutputIteratorWrapper.java
processing/src/main/java/org/apache/carbondata/processing/merger/AbstractResultProcessor.java
core/src/main/java/org/apache/carbondata/core/metadata/converter/ThriftWrapperSchemaConverterImpl.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonTableOutputFormat.java
texts:Support  insert/load data for partition table.
Change carbonrelation to HadoopFSRelation inside optimizer for insert statement in case of the partition table. And also update to HadoopFSRelation even for Load command in case of the partition table.Implement sparks Fileformat interface for carbon and use carbonoutputformat inside.Create partition.map file inside each segment for mapping between partition and index file.
issueID:CARBONDATA-1857
type:Sub-task
changed files:
texts:Create a system level switch for supporting standard partition or carbon custom partition.

issueID:CARBONDATA-1858
type:Sub-task
changed files:core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockletDataMapModel.java
core/src/main/java/org/apache/carbondata/core/util/path/CarbonTablePath.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonTableInputFormat.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockletDataMap.java
core/src/main/java/org/apache/carbondata/core/indexstore/BlockletDataMapIndexStore.java
core/src/main/java/org/apache/carbondata/core/datamap/dev/DataMap.java
core/src/main/java/org/apache/carbondata/core/datamap/TableDataMap.java
core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
core/src/main/java/org/apache/carbondata/core/datamap/DistributableDataMapFormat.java
texts:Support querying data from partition table.
In case of partition table first, use sessioncatalog to prune the partitions. With the partition information, datamap should read partition.map file to get the index file and corresponding blocklets to prune
issueID:CARBONDATA-1859
type:Sub-task
changed files:core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockletDataMapModel.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonTableInputFormat.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockletDataMap.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonOutputCommitter.java
core/src/main/java/org/apache/carbondata/core/indexstore/BlockletDataMapIndexStore.java
processing/src/main/java/org/apache/carbondata/processing/loading/iterator/CarbonOutputIteratorWrapper.java
texts:Support drop partition in carbon
First, drop the partitions from sessioncatalog and then drop from carbon store as per the partition.map file.
issueID:CARBONDATA-186
type:Bug
changed files:
texts:Except Compaction all other alter operations on carbon table should not be performed.
As Carbon table will not support alter operations except compaction, all the alter operations on carbon table should be skipped and error message should be displayed as "Unsupported alter operation on carbon table"Whereas if the alter operation is on hive table, it should be transferred to hive for performing the operation.
issueID:CARBONDATA-1860
type:Sub-task
changed files:
texts:Support insertoverwrite for a specific partition.
User should able to overwrite partition for a specific partition. LikeINSERT OVERWRITE TABLE partitioned_user      PARTITION (country = 'US')      SELECT * FROM another_user au       WHERE au.country = 'US';In the above example, the user can overwrite only the partition(country = 'US') data. So remaining partitions data would be intact.  While overwriting a specific partition carbon should first load data to the new segment and drop that partition from all remaining segments using partition.map file.
issueID:CARBONDATA-1861
type:Sub-task
changed files:core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockletDataMapModel.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonTableInputFormat.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockletDataMap.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonOutputCommitter.java
core/src/main/java/org/apache/carbondata/core/indexstore/BlockletDataMapIndexStore.java
processing/src/main/java/org/apache/carbondata/processing/loading/iterator/CarbonOutputIteratorWrapper.java
texts:Support show partitions
Show partition information directly from sessioncatalog
issueID:CARBONDATA-1862
type:Sub-task
changed files:core/src/main/java/org/apache/carbondata/core/metadata/schema/table/CarbonTable.java
processing/src/main/java/org/apache/carbondata/processing/merger/CompactionResultSortProcessor.java
processing/src/main/java/org/apache/carbondata/processing/merger/RowResultMergerProcessor.java
texts:Support compaction for partition table .
There is a change in compaction during the block identification and grouping. As all blocks which are related same partition always needs to group to same set for compaction.So compactor needs to get the partition information from partition map file during compaction of partition table
issueID:CARBONDATA-1863
type:Sub-task
changed files:processing/src/main/java/org/apache/carbondata/processing/loading/model/CarbonLoadModel.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/SegmentIndexFileStore.java
processing/src/main/java/org/apache/carbondata/processing/util/CarbonLoaderUtil.java
core/src/main/java/org/apache/carbondata/core/writer/CarbonIndexFileMergeWriter.java
texts:Clean segment information while using clean table command
Clean should update the partition.map file also upon cleaning any carbonindex file.
issueID:CARBONDATA-1864
type:Improvement
changed files:
texts:Using org.apache.spark.SPARK_VERSION instead of sparkSession.version
Using org.apache.spark.SPARK_VERSION instead of sparkSession.version
issueID:CARBONDATA-1865
type:Task
changed files:
texts:Skip Single Pass for first data load.

issueID:CARBONDATA-1866
type:Improvement
changed files:
texts:refactor CarbonLateDecodeRule to split different rule for better usablity

issueID:CARBONDATA-1867
type:Improvement
changed files:hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonTableInputFormat.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockletDataMap.java
core/src/main/java/org/apache/carbondata/core/datamap/dev/DataMap.java
core/src/main/java/org/apache/carbondata/core/scan/filter/FilterExpressionProcessor.java
core/src/main/java/org/apache/carbondata/core/datamap/TableDataMap.java
texts:Add support for task/segment level pruning
Add support for task/segment level pruning. Add code to compute task level min/max which can be helpful for task/segment level pruning
issueID:CARBONDATA-1868
type:Bug
changed files:
texts:Carbon Spark-2.2 Integration Phase 2
Enable Spark2.2 Integration for feature like 1. Alter Table Add and Modify Columns2. Update | Delete 3. SubQuery Handling 4. SubqueryAlias Handling 5. Preaggregate handling for Spark2.26. Small Bug Fixes.
issueID:CARBONDATA-1869
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/datastore/filesystem/AbstractDFSCarbonFile.java
texts:(Carbon1.3.0 - Spark 2.2) Null pointer exception thrown when concurrent load and select queries executed for table with dictionary exclude or NO_INVERTED_INDEX
Steps -From beeline terminal a table is created with with table properties having dictionary exclude or NO_INVERTED_INDEX- create table test(a1 int) STORED BY 'org.apache.carbondata.format' TBLPROPERTIES ('DICTIONARY_EXCLUDE'='a1');   or create table test(a1 int) STORED BY 'org.apache.carbondata.format' TBLPROPERTIES ('NO_INVERTED_INDEX'='a1');  From 3 concurrent beeline terminals the below sequence of insert into select and select queries are executed 120 times.  insert into test select 2147483647;  select * from test;  select count from test;  select a1 from test;  select round(a1),bround(a1),floor(a1),ceil(a1),rand(),exp(a1),ln(a1),log10(a1),log2(1),log(a1),pow(a1,a1),sqrt(a1),bin(a1),pmod(a1,a1),sin(a1),asin(a1),cos(a1),tan(a1),atan(a1),degrees(a1),radians(a1),positive(a1),negative(a1),sign(a1),factorial(a1),cbrt(a1) from test;【Expected Output】：The insert into select query should be successful and the null pointer exception should not be thrown when concurrent load and select queries executed for table with dictionary exclude or NO_INVERTED_INDEX.【Actual Output】：Null pointer exception thrown when concurrent load and select queries executed with table properties having dictionary exclude or NO_INVERTED_INDEX  0: jdbc:hive2://10.18.98.136:23040> insert into test select 2147483647;Error: java.lang.NullPointerException (state=,code=0)Stacktrace:java.lang.NullPointerException at org.apache.carbondata.core.datastore.filesystem.AbstractDFSCarbonFile.delete(AbstractDFSCarbonFile.java:152) at org.apache.carbondata.processing.util.DeleteLoadFolders.physicalFactAndMeasureMetadataDeletion(DeleteLoadFolders.java:90) at org.apache.carbondata.processing.util.DeleteLoadFolders.deleteLoadFoldersFromFileSystem(DeleteLoadFolders.java:134) at org.apache.carbondata.spark.rdd.DataManagementFunc$.deleteLoadsAndUpdateMetadata(DataManagementFunc.scala:187) at org.apache.carbondata.spark.rdd.CarbonDataRDDFactory$.loadCarbonData(CarbonDataRDDFactory.scala:296) at org.apache.spark.sql.execution.command.management.CarbonLoadDataCommand.loadData(CarbonLoadDataCommand.scala:362) at org.apache.spark.sql.execution.command.management.CarbonLoadDataCommand.processData(CarbonLoadDataCommand.scala:193) at org.apache.spark.sql.execution.command.management.CarbonLoadDataCommand.run(CarbonLoadDataCommand.scala:65) at org.apache.spark.sql.execution.command.management.CarbonInsertIntoCommand.processData(CarbonInsertIntoCommand.scala:43) at org.apache.spark.sql.execution.command.DataCommand.run(package.scala:71) at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58) at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56) at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:67) at org.apache.spark.sql.Dataset.<init>(Dataset.scala:182) at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:67) at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:623) at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:691)
issueID:CARBONDATA-187
type:Bug
changed files:
texts:when using Decimal type as dictionary the generated surrogate key would mismatch for the same values during increment load
Fix bug: when using Decimal type as dictionary gen surrogate key will mismatch for the same values during increment load.For example, when we specify Decimal type column using dictionary, as the using of DataTypeUtil.normalizeColumnValueForItsDataType, deciaml data for example 45, if we specify the precision of this column as 3, parsedValue would be 45.000, and this 45.000 would be written into dic file by writer.write(parsedValue). As a result, the second time we load the same data 45, dictionary.getSurrogateKey(value) would compare the value with dic value, but here the value is 45, our dic value is 45.000 stored as string, so dic would think that i don not have 45, this would lead to repeated values in dic.The dic would be like this: @NU#LL$!100.050 100.055 �45.000� 100.050 �45.000, this is a bug.
issueID:CARBONDATA-1870
type:Improvement
changed files:processing/src/main/java/org/apache/carbondata/processing/loading/converter/impl/FieldEncoderFactory.java
processing/src/main/java/org/apache/carbondata/processing/datatypes/PrimitiveDataType.java
hadoop/src/main/java/org/apache/carbondata/hadoop/readsupport/impl/DictionaryDecodeReadSupport.java
processing/src/main/java/org/apache/carbondata/processing/util/CarbonLoaderUtil.java
integration/hive/src/main/java/org/apache/carbondata/hive/CarbonDictionaryDecodeReadSupport.java
integration/hive/src/main/java/org/apache/carbondata/hive/CarbonHiveRecordReader.java
streaming/src/main/java/org/apache/carbondata/streaming/CarbonStreamInputFormat.java
core/src/main/java/org/apache/carbondata/core/scan/executor/util/QueryUtil.java
core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
core/src/main/java/org/apache/carbondata/core/util/path/CarbonTablePath.java
hadoop/src/main/java/org/apache/carbondata/hadoop/readsupport/CarbonReadSupport.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
integration/spark2/src/main/java/org/apache/carbondata/spark/readsupport/SparkRowReadSupportImpl.java
hadoop/src/main/java/org/apache/carbondata/hadoop/CarbonRecordReader.java
core/src/main/java/org/apache/carbondata/core/scan/filter/FilterUtil.java
core/src/main/java/org/apache/carbondata/core/cache/dictionary/DictionaryColumnUniqueIdentifier.java
texts:Add dictionary path support to carbondata
Add dictionary path support. Carbondata dictionary framework need to be further enhanced to support reading dictionary files from a given dictionary file location.Support new table property TABLE_PROPERTIES(''dictionary_path',"/XXX"). This will allow providing external dictionary path, to a table.Ex: pre-aggregate tables can share dictionary path with main table, so that dictionary need not be created again.
issueID:CARBONDATA-1872
type:Sub-task
changed files:
texts:Clean up unused constant in CarbonCommonConstant

issueID:CARBONDATA-1876
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
texts:clean all the InProgress segments for all databases during session initialization
clean all the InProgress segments for all databases during session initialization. when carbon session initialize , clean all the in progress segments for all the databases created by user.
issueID:CARBONDATA-1878
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/indexstore/UnsafeMemoryDMStore.java
texts:JVM crash after off-heap-sort disabled
SCENARIORecently I have fixed some issues in Carbondata. To perform a full test to cover all the code that has been modified by me, I performed some iteration of the whole test case. Each iteration is started with different  key configurations that will affect the flow in the code.After I set `enable.offheap.sort=false` (default value is true) in the configuration, running tests will always end up with JVM crash error. The error messages are shown as below:```# A fatal error has been detected by the Java Runtime Environment:# SIGSEGV (0xb) at pc=0x00007f346b207ff1, pid=144619, tid=0x00007f346c2fc700# JRE version: Java(TM) SE Runtime Environment (8.0_111-b14) (build 1.8.0_111-b14) Java VM: Java HotSpot(TM) 64-Bit Server VM (25.111-b14 mixed mode linux-amd64 compressed oops) Problematic frame: V  &#91;libjvm.so+0xa90ff1&#93;  Unsafe_SetNativeShort+0x51# Failed to write core dump. Core dumps have been disabled. To enable core dumping, try "ulimit -c unlimited" before starting Java again# An error report file with more information is saved as: /home/xu/ws/carbondata/hs_err_pid144619.log# If you would like to submit a bug report, please visit: http://bugreport.java.com/bugreport/crash.jsp#Process finished with exit code 134 (interrupted by signal 6: SIGABRT)``` STEPS TOREPRODUCEThe error can be easily reproduced in different ways. Here I will provide a simple way to reproduce it:1. Find the test case `DateDataTypeDirectDictionaryTest`.2. Add the following code in the method `beforeAll`.```CarbonProperties.getInstance()    .addProperty(CarbonCommonConstants.ENABLE_OFFHEAP_SORT, "false")```3. Run this test case.4. You will find the test failed with the above error.5. Replace the code in Step2 with the following code:```CarbonProperties.getInstance()    .addProperty(CarbonCommonConstants.ENABLE_OFFHEAP_SORT, "true")```6. Run this test case.7. The test is success without error. ANALYZE & RESOLVEI have reproduced this error and analyzed the core dump file. The final stack message in core dump looks like below:```Thread 73303: (state = IN_VM) sun.misc.Unsafe.putShort(long, short) @bci=0 (Interpreted frame) org.apache.carbondata.core.indexstore.UnsafeMemoryDMStore.addToUnsafe(org.apache.carbondata.core.indexstore.schema.CarbonRowSchema, org.apache.carbondata.core.indexstore.row.DataMapRow, int) @bci=781, line=150 (Interpreted frame) org.apache.carbondata.core.indexstore.UnsafeMemoryDMStore.addIndexRowToUnsafe(org.apache.carbondata.core.indexstore.row.DataMapRow) @bci=59, line=99 (Interpreted frame) ...```After inspecting the code, I found there lies bug in `UnsafeMemoryDMStore line=150` while writing length to unsafe memory &#8211; It writes with wrong base object.
issueID:CARBONDATA-1879
type:Sub-task
changed files:core/src/main/java/org/apache/carbondata/core/metadata/converter/ThriftWrapperSchemaConverterImpl.java
streaming/src/main/java/org/apache/carbondata/streaming/segment/StreamSegment.java
texts:Support alter table to change the status of the streaming segment
Support alter table finish streaming:Alter table <dbname.tablename>  finish streamingThis command will try to change the status of the segment from "streaming" to "streaming finish"
issueID:CARBONDATA-188
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/datastore/impl/FileFactory.java
texts:Compress CSV file while loading
Currently when loading CarbonData file using Spark Dataframe API, it will firstly save as CSV file then load to CarbonData file. Sometimes CSV requires a lot of disk space,  suggest to compress the CSV before loading, thus disk space required is less.
issueID:CARBONDATA-1880
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
hadoop/src/main/java/org/apache/carbondata/hadoop/CarbonMultiBlockSplit.java
core/src/main/java/org/apache/carbondata/core/util/CarbonProperties.java
texts:Global Sort maybe generates many small files
Global Sort maybe generates many small files without option "carbon.load.global.sort.partitions", It makes the select query be slower, Global_Sort_Small_Files_final.pdf
issueID:CARBONDATA-1881
type:Bug
changed files:
texts:insert overwrite not working properly for pre-aggregate tables
when insert overwrite if fired on the main table then the pre-aggregate tables are not overwritten with the new values instead the values are appended to the table like a normal insert
issueID:CARBONDATA-1882
type:Bug
changed files:processing/src/main/java/org/apache/carbondata/processing/util/CarbonLoaderUtil.java
texts:select a table with &#39;group by&#39; and perform insert overwrite to another carbon table it fails

issueID:CARBONDATA-1883
type:Improvement
changed files:
texts:Improvement in merge index code

issueID:CARBONDATA-1884
type:New Feature
changed files:
texts:Add CTAS support to carbondata
Implement  create table as select (CTAS) feature in carbondata
issueID:CARBONDATA-1885
type:Bug
changed files:
texts:Test error in AlterTableValidationTestCase
Running alone the test class fail:17/12/12 01:51:41 AUDIT CarbonAlterTableAddColumnCommand: [hadoop][root][Thread-1]Alter table for add columns is successful for table default.restructure17/12/12 01:51:41 AUDIT CarbonAlterTableDropColumnCommand: [hadoop][root][Thread-1]Alter table drop columns request has been received for default.restructure17/12/12 01:51:41 AUDIT CarbonAlterTableDropColumnCommand: [hadoop][root][Thread-1]Alter table for drop columns is successful for table default.restructure17/12/12 01:51:41 AUDIT CarbonAlterTableAddColumnCommand: [hadoop][root][Thread-1]Alter table add columns request has been received for default.restructure17/12/12 01:51:41 AUDIT CarbonAlterTableAddColumnCommand: [hadoop][root][Thread-1]Alter table for add columns is successful for table default.restructure17/12/12 01:51:41 AUDIT CarbonAlterTableDropColumnCommand: [hadoop][root][Thread-1]Alter table drop columns request has been received for default.restructure17/12/12 01:51:41 AUDIT CarbonAlterTableDropColumnCommand: [hadoop][root][Thread-1]Alter table for drop columns is successful for table default.restructure17/12/12 01:51:41 AUDIT CarbonAlterTableAddColumnCommand: [hadoop][root][Thread-1]Alter table add columns request has been received for default.restructure17/12/12 01:51:41 AUDIT CarbonAlterTableAddColumnCommand: [hadoop][root][Thread-1]Alter table for add columns is successful for table default.restructure17/12/12 01:51:41 AUDIT CarbonAlterTableDropColumnCommand: [hadoop][root][Thread-1]Alter table drop columns request has been received for default.restructure17/12/12 01:51:42 AUDIT CarbonAlterTableDropColumnCommand: [hadoop][root][Thread-1]Alter table for drop columns is successful for table default.restructure17/12/12 01:51:42 AUDIT CarbonAlterTableAddColumnCommand: [hadoop][root][Thread-1]Alter table add columns request has been received for default.restructure17/12/12 01:51:42 ERROR DataTypeUtil: ScalaTest-run-running-AlterTableValidationTestCase Cannot convert value to Time/Long type value. Value is considered as nullUnparseable date: "17-01-2007"17/12/12 01:51:42 ERROR AlterTableColumnSchemaGenerator: ScalaTest-run-running-AlterTableValidationTestCase Invalid default value for new column default.restructure.designation : 17-01-200717/12/12 01:51:42 AUDIT CarbonAlterTableAddColumnCommand: [hadoop][root][Thread-1]Alter table for add columns is successful for table default.restructureResults do not match for query:== Parsed Logical Plan =='Distinct+- 'Project ['designation]   +- 'UnresolvedRelation `restructure`== Analyzed Logical Plan ==designation: timestampDistinct+- Project [designation#3679]   +- SubqueryAlias restructure      +- Relation[workgroupcategory#3650,workgroupcategoryname#3651,deptno#3652,deptname#3653,projectcode#3654,projectjoindate#3655,projectenddate#3656,attendance#3657,utilization#3658,salary#3659,dict#3660,nodict#3661,tmpstmp1#3662,msrfield#3663,strfld#3664,datefld#3665,tptfld#3666,shortfld#3667,intfld#3668,longfld#3669L,dblfld#3670,dcml#3671,dcmlfld#3672,dimfld#3673,... 6 more fields] CarbonDatasourceHadoopRelation [ Database name :default, Table name :restructure, Schema :Some(StructType(StructField(workgroupcategory,IntegerType,true), StructField(workgroupcategoryname,StringType,true), StructField(deptno,IntegerType,true), StructField(deptname,StringType,true), StructField(projectcode,IntegerType,true), StructField(projectjoindate,TimestampType,true), StructField(projectenddate,TimestampType,true), StructField(attendance,IntegerType,true), StructField(utilization,IntegerType,true), StructField(salary,IntegerType,true), StructField(dict,IntegerType,true), StructField(nodict,StringType,true), StructField(tmpstmp1,TimestampType,true), StructField(msrfield,DecimalType(5,2),true), StructField(strfld,StringType,true), StructField(datefld,DateType,true), StructField(tptfld,TimestampType,true), StructField(shortfld,ShortType,true), StructField(intfld,IntegerType,true), StructField(longfld,LongType,true), StructField(dblfld,DoubleType,true), StructField(dcml,DecimalType(5,4),true), StructField(dcmlfld,DecimalType(5,4),true), StructField(dimfld,StringType,true), StructField(dimfld1,StringType,true), StructField(msrcol,DoubleType,true), StructField(empname,StringType,true), StructField(empno,IntegerType,true), StructField(doj,TimestampType,true), StructField(designation,TimestampType,true))) ]== Optimized Logical Plan ==CarbonDictionaryCatalystDecoder [CarbonDecoderRelation(Map(dcml#3671 -> dcml#3671, workgroupcategoryname#3651 -> workgroupcategoryname#3651, projectjoindate#3655 -> projectjoindate#3655, dict#3660 -> dict#3660, tmpstmp1#3662 -> tmpstmp1#3662, projectcode#3654 -> projectcode#3654, dimfld1#3674 -> dimfld1#3674, msrcol#3675 -> msrcol#3675, deptno#3652 -> deptno#3652, dcmlfld#3672 -> dcmlfld#3672, tptfld#3666 -> tptfld#3666, dblfld#3670 -> dblfld#3670, workgroupcategory#3650 -> workgroupcategory#3650, empno#3677 -> empno#3677, msrfield#3663 -> msrfield#3663, longfld#3669L -> longfld#3669L, intfld#3668 -> intfld#3668, shortfld#3667 -> shortfld#3667, dimfld#3673 -> dimfld#3673, datefld#3665 -> datefld#3665, attendance#3657 -> attendance#3657, deptname#3653 -> deptname#3653, projectenddate#3656 -> projectenddate#3656, salary#3659 -> salary#3659, doj#3678 -> doj#3678, nodict#3661 -> nodict#3661, strfld#3664 -> strfld#3664, utilization#3658 -> utilization#3658, empname#3676 -> empname#3676, designation#3679 -> designation#3679),CarbonDatasourceHadoopRelation [ Database name :default, Table name :restructure, Schema :Some(StructType(StructField(workgroupcategory,IntegerType,true), StructField(workgroupcategoryname,StringType,true), StructField(deptno,IntegerType,true), StructField(deptname,StringType,true), StructField(projectcode,IntegerType,true), StructField(projectjoindate,TimestampType,true), StructField(projectenddate,TimestampType,true), StructField(attendance,IntegerType,true), StructField(utilization,IntegerType,true), StructField(salary,IntegerType,true), StructField(dict,IntegerType,true), StructField(nodict,StringType,true), StructField(tmpstmp1,TimestampType,true), StructField(msrfield,DecimalType(5,2),true), StructField(strfld,StringType,true), StructField(datefld,DateType,true), StructField(tptfld,TimestampType,true), StructField(shortfld,ShortType,true), StructField(intfld,IntegerType,true), StructField(longfld,LongType,true), StructField(dblfld,DoubleType,true), StructField(dcml,DecimalType(5,4),true), StructField(dcmlfld,DecimalType(5,4),true), StructField(dimfld,StringType,true), StructField(dimfld1,StringType,true), StructField(msrcol,DoubleType,true), StructField(empname,StringType,true), StructField(empno,IntegerType,true), StructField(doj,TimestampType,true), StructField(designation,TimestampType,true))) ])], ExcludeProfile(ArrayBuffer()), CarbonAliasDecoderRelation(), true+- Aggregate [designation#3679], [designation#3679]   +- Project [designation#3679]      +- Relation[workgroupcategory#3650,workgroupcategoryname#3651,deptno#3652,deptname#3653,projectcode#3654,projectjoindate#3655,projectenddate#3656,attendance#3657,utilization#3658,salary#3659,dict#3660,nodict#3661,tmpstmp1#3662,msrfield#3663,strfld#3664,datefld#3665,tptfld#3666,shortfld#3667,intfld#3668,longfld#3669L,dblfld#3670,dcml#3671,dcmlfld#3672,dimfld#3673,... 6 more fields] CarbonDatasourceHadoopRelation [ Database name :default, Table name :restructure, Schema :Some(StructType(StructField(workgroupcategory,IntegerType,true), StructField(workgroupcategoryname,StringType,true), StructField(deptno,IntegerType,true), StructField(deptname,StringType,true), StructField(projectcode,IntegerType,true), StructField(projectjoindate,TimestampType,true), StructField(projectenddate,TimestampType,true), StructField(attendance,IntegerType,true), StructField(utilization,IntegerType,true), StructField(salary,IntegerType,true), StructField(dict,IntegerType,true), StructField(nodict,StringType,true), StructField(tmpstmp1,TimestampType,true), StructField(msrfield,DecimalType(5,2),true), StructField(strfld,StringType,true), StructField(datefld,DateType,true), StructField(tptfld,TimestampType,true), StructField(shortfld,ShortType,true), StructField(intfld,IntegerType,true), StructField(longfld,LongType,true), StructField(dblfld,DoubleType,true), StructField(dcml,DecimalType(5,4),true), StructField(dcmlfld,DecimalType(5,4),true), StructField(dimfld,StringType,true), StructField(dimfld1,StringType,true), StructField(msrcol,DoubleType,true), StructField(empname,StringType,true), StructField(empno,IntegerType,true), StructField(doj,TimestampType,true), StructField(designation,TimestampType,true))) ]== Physical Plan ==*HashAggregate(keys=[designation#3679], functions=[], output=[designation#3679])+- Exchange hashpartitioning(designation#3679, 200)   +- *HashAggregate(keys=[designation#3679], functions=[], output=[designation#3679])      +- *BatchedScan CarbonDatasourceHadoopRelation [ Database name :default, Table name :restructure, Schema :Some(StructType(StructField(workgroupcategory,IntegerType,true), StructField(workgroupcategoryname,StringType,true), StructField(deptno,IntegerType,true), StructField(deptname,StringType,true), StructField(projectcode,IntegerType,true), StructField(projectjoindate,TimestampType,true), StructField(projectenddate,TimestampType,true), StructField(attendance,IntegerType,true), StructField(utilization,IntegerType,true), StructField(salary,IntegerType,true), StructField(dict,IntegerType,true), StructField(nodict,StringType,true), StructField(tmpstmp1,TimestampType,true), StructField(msrfield,DecimalType(5,2),true), StructField(strfld,StringType,true), StructField(datefld,DateType,true), StructField(tptfld,TimestampType,true), StructField(shortfld,ShortType,true), StructField(intfld,IntegerType,true), StructField(longfld,LongType,true), StructField(dblfld,DoubleType,true), StructField(dcml,DecimalType(5,4),true), StructField(dcmlfld,DecimalType(5,4),true), StructField(dimfld,StringType,true), StructField(dimfld1,StringType,true), StructField(msrcol,DoubleType,true), StructField(empname,StringType,true), StructField(empno,IntegerType,true), StructField(doj,TimestampType,true), StructField(designation,TimestampType,true))) ] default.restructure[designation#3679]== Results ==!== Correct Answer - 1 ==   == Spark Answer - 1 ==![2007-01-17 00:00:00.0]    [null]      ScalaTestFailureLocation: org.apache.spark.sql.test.util.QueryTest at (QueryTest.scala:87)org.scalatest.exceptions.TestFailedException: Results do not match for query:== Parsed Logical Plan =='Distinct+- 'Project ['designation]   +- 'UnresolvedRelation `restructure`== Analyzed Logical Plan ==designation: timestampDistinct+- Project [designation#3679]   +- SubqueryAlias restructure      +- Relation[workgroupcategory#3650,workgroupcategoryname#3651,deptno#3652,deptname#3653,projectcode#3654,projectjoindate#3655,projectenddate#3656,attendance#3657,utilization#3658,salary#3659,dict#3660,nodict#3661,tmpstmp1#3662,msrfield#3663,strfld#3664,datefld#3665,tptfld#3666,shortfld#3667,intfld#3668,longfld#3669L,dblfld#3670,dcml#3671,dcmlfld#3672,dimfld#3673,... 6 more fields] CarbonDatasourceHadoopRelation [ Database name :default, Table name :restructure, Schema :Some(StructType(StructField(workgroupcategory,IntegerType,true), StructField(workgroupcategoryname,StringType,true), StructField(deptno,IntegerType,true), StructField(deptname,StringType,true), StructField(projectcode,IntegerType,true), StructField(projectjoindate,TimestampType,true), StructField(projectenddate,TimestampType,true), StructField(attendance,IntegerType,true), StructField(utilization,IntegerType,true), StructField(salary,IntegerType,true), StructField(dict,IntegerType,true), StructField(nodict,StringType,true), StructField(tmpstmp1,TimestampType,true), StructField(msrfield,DecimalType(5,2),true), StructField(strfld,StringType,true), StructField(datefld,DateType,true), StructField(tptfld,TimestampType,true), StructField(shortfld,ShortType,true), StructField(intfld,IntegerType,true), StructField(longfld,LongType,true), StructField(dblfld,DoubleType,true), StructField(dcml,DecimalType(5,4),true), StructField(dcmlfld,DecimalType(5,4),true), StructField(dimfld,StringType,true), StructField(dimfld1,StringType,true), StructField(msrcol,DoubleType,true), StructField(empname,StringType,true), StructField(empno,IntegerType,true), StructField(doj,TimestampType,true), StructField(designation,TimestampType,true))) ]== Optimized Logical Plan ==CarbonDictionaryCatalystDecoder [CarbonDecoderRelation(Map(dcml#3671 -> dcml#3671, workgroupcategoryname#3651 -> workgroupcategoryname#3651, projectjoindate#3655 -> projectjoindate#3655, dict#3660 -> dict#3660, tmpstmp1#3662 -> tmpstmp1#3662, projectcode#3654 -> projectcode#3654, dimfld1#3674 -> dimfld1#3674, msrcol#3675 -> msrcol#3675, deptno#3652 -> deptno#3652, dcmlfld#3672 -> dcmlfld#3672, tptfld#3666 -> tptfld#3666, dblfld#3670 -> dblfld#3670, workgroupcategory#3650 -> workgroupcategory#3650, empno#3677 -> empno#3677, msrfield#3663 -> msrfield#3663, longfld#3669L -> longfld#3669L, intfld#3668 -> intfld#3668, shortfld#3667 -> shortfld#3667, dimfld#3673 -> dimfld#3673, datefld#3665 -> datefld#3665, attendance#3657 -> attendance#3657, deptname#3653 -> deptname#3653, projectenddate#3656 -> projectenddate#3656, salary#3659 -> salary#3659, doj#3678 -> doj#3678, nodict#3661 -> nodict#3661, strfld#3664 -> strfld#3664, utilization#3658 -> utilization#3658, empname#3676 -> empname#3676, designation#3679 -> designation#3679),CarbonDatasourceHadoopRelation [ Database name :default, Table name :restructure, Schema :Some(StructType(StructField(workgroupcategory,IntegerType,true), StructField(workgroupcategoryname,StringType,true), StructField(deptno,IntegerType,true), StructField(deptname,StringType,true), StructField(projectcode,IntegerType,true), StructField(projectjoindate,TimestampType,true), StructField(projectenddate,TimestampType,true), StructField(attendance,IntegerType,true), StructField(utilization,IntegerType,true), StructField(salary,IntegerType,true), StructField(dict,IntegerType,true), StructField(nodict,StringType,true), StructField(tmpstmp1,TimestampType,true), StructField(msrfield,DecimalType(5,2),true), StructField(strfld,StringType,true), StructField(datefld,DateType,true), StructField(tptfld,TimestampType,true), StructField(shortfld,ShortType,true), StructField(intfld,IntegerType,true), StructField(longfld,LongType,true), StructField(dblfld,DoubleType,true), StructField(dcml,DecimalType(5,4),true), StructField(dcmlfld,DecimalType(5,4),true), StructField(dimfld,StringType,true), StructField(dimfld1,StringType,true), StructField(msrcol,DoubleType,true), StructField(empname,StringType,true), StructField(empno,IntegerType,true), StructField(doj,TimestampType,true), StructField(designation,TimestampType,true))) ])], ExcludeProfile(ArrayBuffer()), CarbonAliasDecoderRelation(), true+- Aggregate [designation#3679], [designation#3679]   +- Project [designation#3679]      +- Relation[workgroupcategory#3650,workgroupcategoryname#3651,deptno#3652,deptname#3653,projectcode#3654,projectjoindate#3655,projectenddate#3656,attendance#3657,utilization#3658,salary#3659,dict#3660,nodict#3661,tmpstmp1#3662,msrfield#3663,strfld#3664,datefld#3665,tptfld#3666,shortfld#3667,intfld#3668,longfld#3669L,dblfld#3670,dcml#3671,dcmlfld#3672,dimfld#3673,... 6 more fields] CarbonDatasourceHadoopRelation [ Database name :default, Table name :restructure, Schema :Some(StructType(StructField(workgroupcategory,IntegerType,true), StructField(workgroupcategoryname,StringType,true), StructField(deptno,IntegerType,true), StructField(deptname,StringType,true), StructField(projectcode,IntegerType,true), StructField(projectjoindate,TimestampType,true), StructField(projectenddate,TimestampType,true), StructField(attendance,IntegerType,true), StructField(utilization,IntegerType,true), StructField(salary,IntegerType,true), StructField(dict,IntegerType,true), StructField(nodict,StringType,true), StructField(tmpstmp1,TimestampType,true), StructField(msrfield,DecimalType(5,2),true), StructField(strfld,StringType,true), StructField(datefld,DateType,true), StructField(tptfld,TimestampType,true), StructField(shortfld,ShortType,true), StructField(intfld,IntegerType,true), StructField(longfld,LongType,true), StructField(dblfld,DoubleType,true), StructField(dcml,DecimalType(5,4),true), StructField(dcmlfld,DecimalType(5,4),true), StructField(dimfld,StringType,true), StructField(dimfld1,StringType,true), StructField(msrcol,DoubleType,true), StructField(empname,StringType,true), StructField(empno,IntegerType,true), StructField(doj,TimestampType,true), StructField(designation,TimestampType,true))) ]== Physical Plan ==*HashAggregate(keys=[designation#3679], functions=[], output=[designation#3679])+- Exchange hashpartitioning(designation#3679, 200)   +- *HashAggregate(keys=[designation#3679], functions=[], output=[designation#3679])      +- *BatchedScan CarbonDatasourceHadoopRelation [ Database name :default, Table name :restructure, Schema :Some(StructType(StructField(workgroupcategory,IntegerType,true), StructField(workgroupcategoryname,StringType,true), StructField(deptno,IntegerType,true), StructField(deptname,StringType,true), StructField(projectcode,IntegerType,true), StructField(projectjoindate,TimestampType,true), StructField(projectenddate,TimestampType,true), StructField(attendance,IntegerType,true), StructField(utilization,IntegerType,true), StructField(salary,IntegerType,true), StructField(dict,IntegerType,true), StructField(nodict,StringType,true), StructField(tmpstmp1,TimestampType,true), StructField(msrfield,DecimalType(5,2),true), StructField(strfld,StringType,true), StructField(datefld,DateType,true), StructField(tptfld,TimestampType,true), StructField(shortfld,ShortType,true), StructField(intfld,IntegerType,true), StructField(longfld,LongType,true), StructField(dblfld,DoubleType,true), StructField(dcml,DecimalType(5,4),true), StructField(dcmlfld,DecimalType(5,4),true), StructField(dimfld,StringType,true), StructField(dimfld1,StringType,true), StructField(msrcol,DoubleType,true), StructField(empname,StringType,true), StructField(empno,IntegerType,true), StructField(doj,TimestampType,true), StructField(designation,TimestampType,true))) ] default.restructure[designation#3679]== Results ==!== Correct Answer - 1 ==   == Spark Answer - 1 ==![2007-01-17 00:00:00.0]    [null]       at org.scalatest.Assertions$class.newAssertionFailedException(Assertions.scala:495) at org.scalatest.FunSuite.newAssertionFailedException(FunSuite.scala:1555) at org.scalatest.Assertions$class.fail(Assertions.scala:1328) at org.scalatest.FunSuite.fail(FunSuite.scala:1555) at org.apache.spark.sql.test.util.QueryTest.checkAnswer(QueryTest.scala:87) at org.apache.spark.sql.test.util.QueryTest.checkAnswer(QueryTest.scala:93) at org.apache.spark.carbondata.restructure.AlterTableValidationTestCase$$anonfun$21.apply$mcV$sp(AlterTableValidationTestCase.scala:325) at org.apache.spark.carbondata.restructure.AlterTableValidationTestCase$$anonfun$21.apply(AlterTableValidationTestCase.scala:307) at org.apache.spark.carbondata.restructure.AlterTableValidationTestCase$$anonfun$21.apply(AlterTableValidationTestCase.scala:307) at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22) at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85) at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104) at org.scalatest.Transformer.apply(Transformer.scala:22) at org.scalatest.Transformer.apply(Transformer.scala:20) at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166) at org.apache.spark.sql.test.util.CarbonFunSuite.withFixture(CarbonFunSuite.scala:41) at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:163) at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175) at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175) at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306) at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:175) at org.scalatest.FunSuite.runTest(FunSuite.scala:1555) at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208) at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208) at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413) at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401) at scala.collection.immutable.List.foreach(List.scala:381) at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401) at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396) at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483) at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:208) at org.scalatest.FunSuite.runTests(FunSuite.scala:1555) at org.scalatest.Suite$class.run(Suite.scala:1424) at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1555) at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212) at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212) at org.scalatest.SuperEngine.runImpl(Engine.scala:545) at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:212) at org.apache.spark.carbondata.restructure.AlterTableValidationTestCase.org$scalatest$BeforeAndAfterAll$$super$run(AlterTableValidationTestCase.scala:34) at org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:257) at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:256) at org.apache.spark.carbondata.restructure.AlterTableValidationTestCase.run(AlterTableValidationTestCase.scala:34) at org.scalatest.tools.SuiteRunner.run(SuiteRunner.scala:55) at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2563) at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2557) at scala.collection.immutable.List.foreach(List.scala:381) at org.scalatest.tools.Runner$.doRunRunRunDaDoRunRun(Runner.scala:2557) at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1044) at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1043) at org.scalatest.tools.Runner$.withClassLoaderAndDispatchReporter(Runner.scala:2722) at org.scalatest.tools.Runner$.runOptionallyWithPassFailReporter(Runner.scala:1043) at org.scalatest.tools.Runner$.run(Runner.scala:883) at org.scalatest.tools.Runner.run(Runner.scala) at org.jetbrains.plugins.scala.testingSupport.scalaTest.ScalaTestRunner.runScalaTest2(ScalaTestRunner.java:138) at org.jetbrains.plugins.scala.testingSupport.scalaTest.ScalaTestRunner.main(ScalaTestRunner.java:28)
issueID:CARBONDATA-1886
type:Bug
changed files:processing/src/main/java/org/apache/carbondata/processing/loading/TableProcessingOperations.java
texts:Stale folders are not getting deleted on deletion on table status file
Stale segment folder is not getting deleted when its entry is not present in table status file. Therefore wrong results are retrieved from the table Steps to reproduce1. Create table2. insert into table3. delete table status file4. insert into table5. select * from table.
issueID:CARBONDATA-1887
type:Bug
changed files:
texts:block pruning not happening is carbon for ShortType and SmallIntType columns
spark.sql(      s""" create table test_numeric_type(c1 int, c2 long, c3 smallint, c4 bigint, c5 short) stored by 'carbondata'       """.stripMargin).show()    spark.sql(      s""" insert into test_numeric_type select 1,1111111,111,1111111111,'2019-01-03 12:12:12'       """.stripMargin).show()    spark.sql(      s""" insert into test_numeric_type select 2,2222222,222,2222222222,'2020-01-03 12:12:12'       """.stripMargin).show()    spark.sql(      s""" insert into test_numeric_type select 3,3333333,333,3333333333,'2021-01-03 12:12:12'       """.stripMargin).show()    spark.sql(      s""" select * from test_numeric_type where c5>1111       """.stripMargin).show()Only two blocks should be selected but all blocks are selected during query execution.
issueID:CARBONDATA-1888
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/util/AbstractDataFileFooterConverter.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/column/ColumnSchema.java
core/src/main/java/org/apache/carbondata/core/metadata/converter/ThriftWrapperSchemaConverterImpl.java
texts:Compaction is failing in case of timeseries
Compaction is failing in case of timeseries
issueID:CARBONDATA-1889
type:Bug
changed files:
texts:Block pruning not working for date type column
spark.sql(s"""create table test_dateType(c1 int, c2 string, c3 smallint, c4 bigint, c5 short, c6 date) stored by 'carbondata'""".stripMargin).show()    spark.sql(s"""insert into test_dateType select 1,'1111111',111,1111111111,1111,'2017-12-12'""".stripMargin)    spark.sql(s"""insert into test_dateType select 2,'2222222',222,2222222222,2222,'2018-11-11'""".stripMargin)    spark.sql(s"""insert into test_dateType select 3,'3333333',333,3333333333,3333,'2019-10-10'""".stripMargin)    spark.sql(s"""select * from test_dateType where c6 = '2018-11-11' """.stripMargin).show(200,false)Check for "Identified no.of.blocks" in logsIt shows :  Identified no.of.blocks: 3Only 1 block should be selected but all the 3 blocks are getting selected
issueID:CARBONDATA-189
type:Bug
changed files:
texts:Drop database dbname cascade should be restricted in carbondata
Carbondata will not support "Drop database dbname cascade"When user provides this command, carbon will validate and give below error message "Unsupported cascade operation in drop database command"
issueID:CARBONDATA-1891
type:Bug
changed files:
texts:None.get when creating timeseries table after loading data into main table
Steps to reproduce1. CREATE TABLE mainTable(mytime timestamp, name string, age int) STORED BY 'org.apache.carbondata.format'2. LOAD DATA LOCAL INPATH 'timeseriestest.csv' into table mainTable3. create datamap agg0 on table mainTable using 'preaggregate' DMPROPERTIES ('timeseries.eventTime'='mytime', 'timeseries.hierarchy'='second=1,minute=1,hour=1,day=1,month=1,year=1') as select mytime, sum(age) from mainTable group by mytime
issueID:CARBONDATA-1892
type:Improvement
changed files:
texts:Documentation update for disability of single_pass on first load
Correction In the documentation of single_pass is required as now single_pass is not disabled for the first load.
issueID:CARBONDATA-1893
type:Bug
changed files:processing/src/main/java/org/apache/carbondata/processing/util/CarbonLoaderUtil.java
core/src/main/java/org/apache/carbondata/core/enums/EscapeSequences.java
texts:Data load with multiple QUOTECHAR characters in syntax should fail

issueID:CARBONDATA-1894
type:Improvement
changed files:
texts:Add compactionType Parameter to compaction event

issueID:CARBONDATA-1895
type:Bug
changed files:
texts:Fix issue of create table if not exits

issueID:CARBONDATA-1896
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/util/DeleteLoadFolders.java
texts:Clean files operation improvement
Problem:When bringing up the session, clean operation is handled in a way to mark all the INSERT_OVERWRITE_IN_PROGRESS or INSERT_IN_PROGRESS segments to MARKED_FOR_DELETE in tablestatus file. This clean operation is not considering the other parallel sessions. If any other session's data load is IN_PROGRESS at the time of bringing up one session, then the executing load also will be changed to MARKED_FOR_DELETE irrespective of the actual load status. Handling stale segments cleaning while session bring up also increases the time of bringing up a session.Solution:SEGMENT_LOCK should be taken on the new segment while loading.While cleaning segments tablestatus file and SEGMENT_LOCK should be considered.Cleaning stale files while bringing up the session should be removed and this can be either manually done on the needed tables through already existing CLEAN FILES DDL or the next  load will automatically clean the same.
issueID:CARBONDATA-1897
type:Improvement
changed files:
texts:Remove column group information in DESC TABLE command
column group information is not valid, remove it from DESC TABLE output
issueID:CARBONDATA-1898
type:Improvement
changed files:
texts:Like, Contains, Ends With query optimization in case of or filter
Problem: In case of like, contains, ends with filter With all or condition query is taking more time in carbonSolution: This type of query avoid filter push down and let spark handle those filters
issueID:CARBONDATA-1899
type:Bug
changed files:
texts:Add CarbonData concurrency test case
Add CarbonData concurrency test case
issueID:CARBONDATA-19
type:Improvement
changed files:
texts:Column Group Filter (Rowlevel and Exclude)
Below query scenario is not working with column group columnsselect * from comb01 where num not in (145);select * from comb01 where num like '14%';select * from comb01 where num not like '14%'; select * from comb01 where num not between 146 and 150;select * from comb01 where num regexp 2;select * from comb01 where num not regexp 4;
issueID:CARBONDATA-190
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/metadata/schema/table/column/ColumnSchema.java
texts:Data mismatch issue
Issue steps:1. create table , then restart the server and then do data load, in that case filter query record count is not matching.Problem: When user is creating any table and if user has not disabled inverted index false for any key column we are setting the inverted index true in column schema object. As we are not persisting this information in schema file, so after restarting the server useInvertedIndex property is false in columnschema object and in data loading column data is not sorted and in filter execution we are doing binary search, as data is not sorted binary search is failing and it is skipping some of the record.Solution : In this pr default value is set to true. One more PR will be raised to handle inverted index disabled scneario. By default Inverted index will be enabled for all the column for better query performance
issueID:CARBONDATA-1900
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/statusmanager/LoadMetadataDetails.java
texts:Modify loadmetadata to store timestamp long value(in ms) instead of formated date string for fields "loadStartTime" and "timestamp"
For detail refer to mailing link http://apache-carbondata-dev-mailing-list-archive.1130556.n5.nabble.com/Problem-with-with-writing-the-loadStartTime-in-quot-dd-MM-yyyy-HH-mm-ss-SSS-quot-format-td30971.html
issueID:CARBONDATA-1901
type:Improvement
changed files:
texts:Fixed Pre aggregate data map creation and query parsing
*Problem:*Fixed below issues in case of pre aggregate1. Pre aggregate data map table column order is not as per query given  by user because of which while data is loaded to wrong column2. when aggregate function contains any expression query is failing with match error3. pre aggregate data map columns and parent tables columns encoder is not matchingSolution:1. Do not consider group columns in pre aggregate2. when aggregate function contains any expression hit the maintable3. Get encoder from main table and add in pre aggregate table columnWhen aggregation type is sum or avg create measure column
issueID:CARBONDATA-1903
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/SegmentIndexFileStore.java
processing/src/main/java/org/apache/carbondata/processing/loading/steps/DataWriterProcessorStepImpl.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/CarbonTable.java
core/src/main/java/org/apache/carbondata/core/indexstore/BlockletDataMapIndexStore.java
core/src/main/java/org/apache/carbondata/core/scan/scanner/impl/BlockletFilterScanner.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockletDataRefNode.java
core/src/main/java/org/apache/carbondata/core/util/path/HDFSLeaseUtils.java
core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
core/src/main/java/org/apache/carbondata/core/locks/ZooKeeperLocking.java
core/src/main/java/org/apache/carbondata/core/util/DataTypeConverterImpl.java
integration/hive/src/main/java/org/apache/carbondata/hive/CarbonArrayInspector.java
core/src/main/java/org/apache/carbondata/core/keygenerator/columnar/impl/MultiDimKeyVarLengthEquiSplitGenerator.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/SchemaReader.java
processing/src/main/java/org/apache/carbondata/processing/merger/CarbonDataMergerUtil.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonTableInputFormat.java
core/src/main/java/org/apache/carbondata/core/statusmanager/SegmentStatusManager.java
core/src/main/java/org/apache/carbondata/core/keygenerator/columnar/impl/MultiDimKeyVarLengthVariableSplitGenerator.java
processing/src/main/java/org/apache/carbondata/processing/loading/DataLoadProcessBuilder.java
processing/src/main/java/org/apache/carbondata/processing/merger/CompactionResultSortProcessor.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/DefaultEncodingFactory.java
integration/presto/src/main/java/org/apache/carbondata/presto/impl/CarbonTableReader.java
texts:Fix some code issues in carbondata
Remove useless code in carbondata
issueID:CARBONDATA-1904
type:Sub-task
changed files:core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
core/src/main/java/org/apache/carbondata/core/util/CarbonProperties.java
streaming/src/main/java/org/apache/carbondata/streaming/segment/StreamSegment.java
processing/src/main/java/org/apache/carbondata/processing/merger/CompactionType.java
texts:auto handoff streaming segment

issueID:CARBONDATA-1905
type:Sub-task
changed files:core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
core/src/main/java/org/apache/carbondata/core/util/CarbonProperties.java
streaming/src/main/java/org/apache/carbondata/streaming/segment/StreamSegment.java
processing/src/main/java/org/apache/carbondata/processing/merger/CompactionType.java
texts:alter streaming table to normal table

issueID:CARBONDATA-1906
type:Improvement
changed files:
texts:Update registerTempTable method because it was marked deprecated
Update registerTempTable method because it was marked deprecated
issueID:CARBONDATA-1907
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/scan/executor/impl/AbstractQueryExecutor.java
core/src/main/java/org/apache/carbondata/core/util/DataTypeUtil.java
texts:Avoid unnecessary logging to improve query performance for no dictionary non string columns
In case of no dictionary column for non string data types exception is thrown while parsing when data is empty. Due to this  excessive logging is happening which is impacting the query performance.Log printed in the logs:"Problem while converting data type"
issueID:CARBONDATA-1908
type:Sub-task
changed files:processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactDataHandlerModel.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonTableInputFormat.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonOutputCommitter.java
processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactDataHandlerColumnar.java
processing/src/main/java/org/apache/carbondata/processing/loading/DataLoadProcessBuilder.java
processing/src/main/java/org/apache/carbondata/processing/util/CarbonLoaderUtil.java
processing/src/main/java/org/apache/carbondata/processing/loading/CarbonDataLoadConfiguration.java
processing/src/main/java/org/apache/carbondata/processing/loading/iterator/CarbonOutputIteratorWrapper.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonTableOutputFormat.java
texts:Support Update/Delete on partition tables
As partition tables loading flow is different we need to handle the update operations specially on partition tables.
issueID:CARBONDATA-1909
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
texts:Load is failing during insert into operation when load is concurrently done to source table
Load is failing during insert into operation when load is concurrently done to source tableScenario:Client1:1. Create source table2. load big dataClient2:1. create table t12. insert into t1 select * from  source table   - load fails
issueID:CARBONDATA-191
type:Bug
changed files:
texts:load data is null when quote char is single and no &#39;\n&#39; being end.
when load data just like below,CREATE TABLE Priyal11 (id int,name string) STORED BY 'org.apache.carbondata.format';LOAD DATA inpath 'hdfs://hacluster/Priyal1/test34.csv' INTO table Priyal11  options ('DELIMITER'=',', 'QUOTECHAR'='\"', 'FILEHEADER'='id,name');and test34.csv is like below(note: there is no new line in the end of file.):1,"priyal\"2,"hello\"then query  name's result is null. Actually, because of the existence of quote char. the expected result should be prival"2,"hello"and if we add new line in the end of file, then query is right.
issueID:CARBONDATA-1910
type:Bug
changed files:
texts:do not allow tupleid, referenceid and positionReference as columns names
do not allow tupleid, referenceid and positionReference as columns names, when it is created with these keywords and if those columns are tried to delete, then error is thrown
issueID:CARBONDATA-1911
type:Bug
changed files:
texts:Enhance Carbon Test cases
Enhance Carbon Test cases
issueID:CARBONDATA-1912
type:Bug
changed files:
texts:Getting error trace on spark-sql console while executing compaction and alter table rename commands
Getting error trace in spark-sql console while executing compaction and alter table rename commands.Scenario:execute compaction and alter table rename queries in spark - sqlEven though operation is success , getting error trace on the console
issueID:CARBONDATA-1913
type:Bug
changed files:
texts:Global Sort data dataload fails for big with RPC timeout exception
When gloabl sort option is used for big data then for some times it fails withRPC timeout after 120s. This is happening because the driver is not able to unpersist rdd cache with in 120s.The issue is happening due to rdd unpersist blocking call. Sometimes spark is not able to unppersist the rdd in default "spark.rpc.askTimeout" or "spark.network.timeout" time.
issueID:CARBONDATA-1914
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/cache/dictionary/Dictionary.java
processing/src/main/java/org/apache/carbondata/processing/loading/converter/impl/DirectDictionaryFieldConverterImpl.java
core/src/main/java/org/apache/carbondata/core/cache/dictionary/ForwardDictionary.java
processing/src/main/java/org/apache/carbondata/processing/loading/converter/impl/RowConverterImpl.java
processing/src/main/java/org/apache/carbondata/processing/loading/converter/FieldConverter.java
processing/src/main/java/org/apache/carbondata/processing/loading/converter/impl/MeasureFieldConverterImpl.java
processing/src/main/java/org/apache/carbondata/processing/loading/converter/impl/ComplexFieldConverterImpl.java
processing/src/main/java/org/apache/carbondata/processing/loading/converter/impl/NonDictionaryFieldConverterImpl.java
texts:Dictionary Cache Access Count Maintenance
When dictionary cache is being accessed the access count is being atomicaly increased and after the access it has to be decremented. This access count prevent it from removal from cache when cache overflow scenario comes into picture. There are some code places where access is incremented but not decremented after usage is completed. This is a resource leak. Test Case=======Precondition ----------------(CarbonCommonConstants.CARBON_MAX_DRIVER_LRU_CACHE_SIZE, "1") (CarbonCommonConstants.CARBON_MAX_EXECUTOR_LRU_CACHE_SIZE, "1")spark.sql("drop table if exists carbon_new6").show(200,false)spark.sql("drop table if exists carbon_new7").show(200,false)spark.sql("drop table if exists carbon_new8").show(200,false)//    spark.sql("CREATE TABLE carbon_new6 (CUST_ID INT,CUST_NAME STRING,ACTIVE_EMUI_VERSION STRING, DOB TIMESTAMP, DOJ TIMESTAMP, BIGINT_COLUMN1 BIGINT,BIGINT_COLUMN2 BIGINT,DECIMAL_COLUMN1 decimal(30,10), DECIMAL_COLUMN2 DECIMAL(36,10),Double_COLUMN1 double, Double_COLUMN2 double,INTEGER_COLUMN1 INT) STORED BY 'org.apache.carbondata.format' TBLPROPERTIES ('dictionary_include'='CUST_NAME')").show(200,false)//    spark.sql("LOAD DATA INPATH '/home/root1/data_2000.csv' INTO TABLE carbon_new6 OPTIONS('DELIMITER'=',' , 'QUOTECHAR'='\"','BAD_RECORDS_ACTION'='FORCE','SINGLE_PASS'='TRUE','FILEHEADER'='CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1,Double_COLUMN2, INTEGER_COLUMN1')").show(200,false)spark.sql("CREATE TABLE carbon_new7 (CUST_ID INT,CUST_NAME STRING,ACTIVE_EMUI_VERSION STRING, DOB TIMESTAMP, DOJ TIMESTAMP, BIGINT_COLUMN1 BIGINT,BIGINT_COLUMN2 BIGINT,DECIMAL_COLUMN1 decimal(30,10), DECIMAL_COLUMN2 DECIMAL(36,10),Double_COLUMN1 double, Double_COLUMN2 double,INTEGER_COLUMN1 INT) STORED BY 'org.apache.carbondata.format' TBLPROPERTIES ('dictionary_include'='CUST_NAME, ACTIVE_EMUI_VERSION,BIGINT_COLUMN1,Double_COLUMN1, Double_COLUMN2')").show(200,false)spark.sql("LOAD DATA INPATH '/home/root1/data_2000.csv' INTO TABLE carbon_new7 OPTIONS('DELIMITER'=',' , 'QUOTECHAR'='\"','BAD_RECORDS_ACTION'='FORCE','SINGLE_PASS'='TRUE','FILEHEADER'='CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1,Double_COLUMN2, INTEGER_COLUMN1')").show(200,false)spark.sql("LOAD DATA INPATH '/home/root1/data_2000.csv' INTO TABLE carbon_new7 OPTIONS('DELIMITER'=',' , 'QUOTECHAR'='\"','BAD_RECORDS_ACTION'='FORCE','SINGLE_PASS'='TRUE','FILEHEADER'='CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1,Double_COLUMN2, INTEGER_COLUMN1')").show(200,false)spark.sql("CREATE TABLE carbon_new8 (CUST_ID INT,CUST_NAME STRING,ACTIVE_EMUI_VERSION STRING, DOB TIMESTAMP, DOJ TIMESTAMP, BIGINT_COLUMN1 BIGINT,BIGINT_COLUMN2 BIGINT,DECIMAL_COLUMN1 decimal(30,10), DECIMAL_COLUMN2 DECIMAL(36,10),Double_COLUMN1 double, Double_COLUMN2 double,INTEGER_COLUMN1 INT) STORED BY 'org.apache.carbondata.format' TBLPROPERTIES ('dictionary_include'='CUST_NAME, ACTIVE_EMUI_VERSION,BIGINT_COLUMN1,Double_COLUMN1, Double_COLUMN2')").show(200,false)spark.sql("LOAD DATA INPATH '/home/root1/data_2000.csv' INTO TABLE carbon_new8 OPTIONS('DELIMITER'=',' , 'QUOTECHAR'='\"','BAD_RECORDS_ACTION'='FORCE','SINGLE_PASS'='TRUE','FILEHEADER'='CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1,Double_COLUMN2, INTEGER_COLUMN1')").show(200,false)spark.sql("LOAD DATA INPATH '/home/root1/data_2000.csv' INTO TABLE carbon_new8 OPTIONS('DELIMITER'=',' , 'QUOTECHAR'='\"','BAD_RECORDS_ACTION'='FORCE','SINGLE_PASS'='TRUE','FILEHEADER'='CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1,Double_COLUMN2, INTEGER_COLUMN1')").show(200,false)
issueID:CARBONDATA-1915
type:Bug
changed files:
texts:In the insert into and the update flow when static values are inserted then the preferred locations are coming empty
【Test step】：CREATE TABLE carbon_01(imei string,age int,task bigint,num double,level decimal(10,3),productdate timestamp)STORED BY 'org.apache.carbondata.format';CREATE TABLE carbon_02(imei string,age int,task bigint,num double,level decimal(10,3),productdate timestamp,name string,point int)STORED BY 'org.apache.carbondata.format';LOAD DATA INPATH 'hdfs://hacluster/mytest/moredata01.csv'  INTO TABLE carbon_02 options ('DELIMITER'=',', 'QUOTECHAR'='"','FILEHEADER' = 'imei,age,task,num,level,productdate,name,point');insert into carbon_01 select imei,age,task,num,level,productdate from carbon_02 where age is not NULL;show segments for table carbon_01;select * from carbon_01;update carbon_01 set (imei) = ("RNG") where age <=0;select * from carbon_01;update carbon_01 set (imei) = ("SSG") where num in (15.5);select * from carbon_01;delete from carbon_01 where imei IN ('RNG','SSG');select * from carbon_01;result:update the table,then query table sometimes need long time to output result,or sometimes failed,if query failed,the JDBCServer master/standby will be changedupdate table then query table should working
issueID:CARBONDATA-1916
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
texts:Correct the database location path during carbon drop databsae
when drop database is called, to delete the databsae directory, the path formed is wrong, so when drop datasbe is executed, operation is successful , but the database directory is still present in hdfs
issueID:CARBONDATA-1917
type:Improvement
changed files:
texts:While loading, check for stale dictionary files

issueID:CARBONDATA-1918
type:Bug
changed files:
texts:Incorrect data is displayed when String is updated using Sentences
update t_carbn01 set (active_status)= (sentences('Hello there! How are you?'));---------+ Result  ---------+---------+No rows selected (2.784 seconds)select active_status from t_carbn01;---------------------------------+          active_status          ---------------------------------+| Hello\:there\\$How\:are\:you  || Hello\:there\\$How\:are\:you  || Hello\:there\\$How\:are\:you  || Hello\:there\\$How\:are\:you  || Hello\:there\\$How\:are\:you  || Hello\:there\\$How\:are\:you  || Hello\:there\\$How\:are\:you  || Hello\:there\\$How\:are\:you  || Hello\:there\\$How\:are\:you  || Hello\:there\\$How\:are\:you  |---------------------------------–+ The issue for sentences function also occurs when the below update is performed.  update t_carbn01 set (active_status)= (split('ab', 'a'));
issueID:CARBONDATA-192
type:Bug
changed files:
texts:Invalidate table from hive context while dropping the table
Create  table table1load table1select * from table1drop table1create same table table1load table1select * from table1throws exceptionInvalidate table from hive context while dropping the table
issueID:CARBONDATA-1920
type:Bug
changed files:integration/presto/src/main/java/org/apache/carbondata/presto/PrestoFilterUtil.java
integration/presto/src/main/java/org/apache/carbondata/presto/readers/IntegerStreamReader.java
integration/presto/src/main/java/org/apache/carbondata/presto/readers/DecimalSliceStreamReader.java
integration/presto/src/main/java/org/apache/carbondata/presto/readers/DoubleStreamReader.java
integration/presto/src/main/java/org/apache/carbondata/presto/readers/LongStreamReader.java
integration/presto/src/main/java/org/apache/carbondata/presto/readers/ShortStreamReader.java
integration/presto/src/main/java/org/apache/carbondata/presto/readers/SliceStreamReader.java
integration/presto/src/main/java/org/apache/carbondata/presto/CarbondataPageSource.java
texts:Sparksql query result is not same as presto on same sql
i  use carbondata version is 1.2.0 and spark version is 1.6.0.     in my test case   1.Creating a Table  cc.sql("create table IF NOT EXISTS  test.table5(id string,nameString,city String,age int) stored by 'carbondata' *tblproperties('DICTIONARY_INCLUDE' = 'age')* ")  2.load csv data into table,data like this:    id,name,city,age    1,david,shenzhen,31    88,eason,shenzhen,27    3,jarry,wuhan,35   3.select from sparksql,result is :  ------------------------ id    name     city     age  ------------------------ 1    david   shenzhen   31    3    jarry   wuhan      35    88   eason   shenzhen   27   ------------------------   this result is correct 4.select from presto,result is: id | name  |   city   | age----------------+---- 1  | david | shenzhen |   3 3  | jarry | wuhan    |   4 88 | eason | shenzhen |   2(3 rows)  look at the age filed,is wrongI know why this happens because I used dictionary encoding in the age field。
issueID:CARBONDATA-1921
type:New Feature
changed files:
texts:Update product document with Merge Index Feature

issueID:CARBONDATA-1922
type:New Feature
changed files:
texts:Update product document with Ignoring empty line OPTION

issueID:CARBONDATA-1923
type:Improvement
changed files:
texts:Remove file after running test class
Remove file after running test class
issueID:CARBONDATA-1924
type:Sub-task
changed files:
texts:Add restriction for creating streaming table as partition table.And support PARTITION syntax to LOAD command

issueID:CARBONDATA-1925
type:Sub-task
changed files:
texts:Support expression inside aggregate expression in create and load data on Pre aggregate table
Support expression inside aggregate expression in create and load data on Pre aggregate table
issueID:CARBONDATA-1926
type:Sub-task
changed files:
texts:Support expression inside aggregate expression during query on Pre Aggregate table
Support expression inside aggregate expression during query on Pre Aggregate table
issueID:CARBONDATA-1927
type:Sub-task
changed files:
texts:Support sub query on Pre Aggregate table
Currently sub query is not hitting the pre aggregate table. This Jira is to handle the sub query on pre Aggregate table
issueID:CARBONDATA-1928
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
core/src/main/java/org/apache/carbondata/core/locks/ICarbonLock.java
processing/src/main/java/org/apache/carbondata/processing/util/CarbonLoaderUtil.java
core/src/main/java/org/apache/carbondata/core/locks/CarbonLockUtil.java
core/src/main/java/org/apache/carbondata/core/locks/AbstractCarbonLock.java
texts:Separate the lock property for concurrent load and others
Currently the property that is used to configure the lock retry count and the interval between retries is common for all the locks.This will be problematic when the user has configured the retries to 10/20 for concurrent loading. This property will be affecting other lock behaviours also, all other locks would have to retry for 10 times too.
issueID:CARBONDATA-1929
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
core/src/main/java/org/apache/carbondata/core/util/CarbonProperties.java
texts:carbon property configuration validation
Issue 1 :  In carbon.properties carbon.timestamp.format=testserwerwerwer12312312312312 is configured and spark submit server is started using the command - bin/spark-submit --master yarn-client  --executor-memory 10G --executor-cores 5 --driver-memory 5G --num-executors 3 --class org.apache.carbondata.spark.thriftserver.CarbonThriftServer --jars /srv/spark2.2Bigdata/install/spark/sparkJdbc/carbonlib/carbondata-common-1.3.0.0100.jar,/srv/spark2.2Bigdata/install/spark/sparkJdbc/carbonlib/carbondata-core-1.3.0.0100.jar,/srv/spark2.2Bigdata/install/spark/sparkJdbc/carbonlib/carbondata-format-1.3.0.0100.jar,/srv/spark2.2Bigdata/install/spark/sparkJdbc/carbonlib/carbondata-hadoop-1.3.0.0100.jar,/srv/spark2.2Bigdata/install/spark/sparkJdbc/carbonlib/carbondata-processing-1.3.0.0100.jar,/srv/spark2.2Bigdata/install/spark/sparkJdbc/carbonlib/carbondata-spark-common-1.3.0.0100.jar,/srv/spark2.2Bigdata/install/spark/sparkJdbc/carbonlib/carbondata-streaming-1.3.0.0100.jar /srv/spark2.2Bigdata/install/spark/sparkJdbc/carbonlib/carbondata-spark2-1.3.0.0100.jar "hdfs://hacluster/carbonstore"  From Beeline User tries to create a table Issue 2 : In carbon.properties the below properties is configured and spark submit server is started using the command  carbon.sort.file.write.buffer.size=test  carbon.sort.intermediate.files.limit=testFrom Beeline User creates a table User tries to load data.Issue 3:In carbon.properties the below properties is configured and spark submit server is started using the command carbon.block.meta.size.reserved.percentage=test From beeline user creates table and tries to load data.
issueID:CARBONDATA-193
type:Bug
changed files:
texts:Data is not loading properly when double data type is having negative values
For example:-7489.7976000000-11234567489.797-11234567489.7-1.2-2-11234567489.7976000000-11234567489.7976000000-11234567489.7976000000-11234567489.7976000000would be all 0 after query
issueID:CARBONDATA-1930
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/scan/filter/FilterUtil.java
texts:Dictionary not found exception is thrown when filter expression is given in aggergate table query
Steps to reproduce;1. CREATE TABLE filtertable(id int, name string, city string, age string) STORED BY  'org.apache.carbondata.format' TBLPROPERTIES('dictionary_include'='name,age')2. LOAD DATA LOCAL INPATH 3. create datamap agg9 on table filtertable using 'preaggregate' as select name, age, sum(age) from filtertable group by name, age4. select name, sum(age) from filtertable where age = '29' group by name, age
issueID:CARBONDATA-1931
type:Bug
changed files:
texts:DataLoad failed for Aggregate table when measure is used for groupby
Run commands in sequence spark.sql(      "create table y(year int,month int,name string,salary int) stored by 'carbondata'"    )    spark.sql(      s"insert into y select 10,11,'x',12"    )spark.sql("create datamap y1_sum1 on table y using 'preaggregate' as select year,name,sum(salary) from y group by year,name")Result :- Aggregate creation is failed.
issueID:CARBONDATA-1932
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/constants/CarbonVersionConstants.java
texts:Add version for CarbonData
Add version for CarbonData
issueID:CARBONDATA-1933
type:Sub-task
changed files:
texts:Support Spark 2.2.1 for carbon partition tables
Currently carbon partition tables cannot work on spark 2.2.1 as CataLogRelation is removed in Spark 2.2.1
issueID:CARBONDATA-1934
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockletDataRefNode.java
core/src/main/java/org/apache/carbondata/core/scan/executor/impl/AbstractQueryExecutor.java
texts:Incorrect results are returned by select query in case when the number of blocklets for one part file are > 1 in the same task
When a select query is triggered, driver will prune the segments and give a list of blocklets that need to be scanned. The number of tasks from spark will be equal to the number of blocklets identified.In case where one task has more than one blocklet for same file, then BlockExecution getting formed is incorrect. Due to this the query results are incorrect.
issueID:CARBONDATA-1935
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/metadata/datatype/DataTypeAdapter.java
core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
core/src/main/java/org/apache/carbondata/core/metadata/datatype/DataTypes.java
core/src/main/java/org/apache/carbondata/core/util/DataTypeUtil.java
texts:Fix the backword compatibility issue for tableInfo deserialization
in carbon old version datatype is string in tableInfo, now new version it is object with extra field, fix the issue to fix the compatibility issue
issueID:CARBONDATA-1936
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/util/SessionParams.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonLoadOptionConstants.java
texts:Bad Record logger is not working properly in Carbon Partition
Bad records are not logging and the load is always success irrespective of bad records are present. : CREATE  TABLE IF NOT EXISTS emp1 (emp_no int,ename string,job string,mgr_id int,date_of_joining string,salary int,bonus int) partitioned by (dept_no int) STORED BY 'org.apache.carbondata.format' ;2: LOAD DATA INPATH 'hdfs://hacluster/user/test/emp.csv' overwrite INTO TABLE emp1 OPTIONS('DELIMITER'=',', 'QUOTECHAR'= '\');3: Observe that load is failing4: LOAD DATA INPATH 'hdfs://hacluster/user/test/emp.csv' overwrite INTO TABLE emp1 OPTIONS('DELIMITER'=',', 'QUOTECHAR'= '\','BAD_RECORDS_ACTION'='FORCE');5: Observe that load is success when 'BAD_RECORDS_ACTION'='FORCE' is given in load command6: Also observe that same load is suucess in a table without partition.
issueID:CARBONDATA-1937
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
texts:NULL values on Non string partition columns throws exception
If any null values of non string partition columns throws error while doing filter query. It seems lile restrioction from sparkCaused by: MetaException(message:Filtering is supported only on partition keys of type string) at org.apache.hadoop.hive.metastore.parser.ExpressionTree$FilterBuilder.setError(ExpressionTree.java:185) at org.apache.hadoop.hive.metastore.parser.ExpressionTree$LeafNode.getJdoFilterPushdownParam(ExpressionTree.java:440) at org.apache.hadoop.hive.metastore.parser.ExpressionTree$LeafNode.generateJDOFilterOverPartitions(ExpressionTree.java:357) at org.apache.hadoop.hive.metastore.parser.ExpressionTree$LeafNode.generateJDOFilter(ExpressionTree.java:279) at org.apache.hadoop.hive.metastore.parser.ExpressionTree.generateJDOFilterFragment(ExpressionTree.java:578) at org.apache.hadoop.hive.metastore.ObjectStore.makeQueryFilterString(ObjectStore.java:2615) at org.apache.hadoop.hive.metastore.ObjectStore.getPartitionsViaOrmFilter(ObjectStore.java:2199) at org.apache.hadoop.hive.metastore.ObjectStore.access$500(ObjectStore.java:160) at org.apache.hadoop.hive.metastore.ObjectStore$5.getJdoResult(ObjectStore.java:2530) at org.apache.hadoop.hive.metastore.ObjectStore$5.getJdoResult(ObjectStore.java:2515) at org.apache.hadoop.hive.metastore.ObjectStore$GetHelper.run(ObjectStore.java:2391) at org.apache.hadoop.hive.metastore.ObjectStore.getPartitionsByFilterInternal(ObjectStore.java:2515) at org.apache.hadoop.hive.metastore.ObjectStore.getPartitionsByFilter(ObjectStore.java:2335) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:114) at com.sun.proxy.$Proxy13.getPartitionsByFilter(Unknown Source) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_partitions_by_filter(HiveMetaStore.java:4442) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107) at com.sun.proxy.$Proxy15.get_partitions_by_filter(Unknown Source)
issueID:CARBONDATA-1939
type:Improvement
changed files:
texts:Added show segments validation test case
(1) Modified headers of show segments(2) Modified SDV test cases for validating headers and result
issueID:CARBONDATA-194
type:Bug
changed files:integration/spark-common/src/main/java/org/apache/carbondata/spark/load/CarbonLoaderUtil.java
processing/src/main/java/org/apache/carbondata/processing/model/CarbonLoadModel.java
texts:ArrayIndexOfBoundException thrown when number of columns in row more than the max number of columns in univocity parser settings
When the number of columns in csv data file while parsing a row are more than the number of columns in schema, the parser throws array index of bound exceptionCaused by: java.lang.ArrayIndexOutOfBoundsException: 18 at com.univocity.parsers.common.ParserOutput.valueParsed(ParserOutput.java:247) at com.univocity.parsers.csv.CsvParser.parseField(CsvParser.java:181) at com.univocity.parsers.csv.CsvParser.parseRecord(CsvParser.java:75)
issueID:CARBONDATA-1940
type:Bug
changed files:
texts:Select query on preaggregate table created with group by clause throws exception: Column does not exist
We create a carbon tablespark.sql(      s""" CREATE TABLE carbon_table( shortField SHORT, intField INT, bigintField LONG, doubleField DOUBLE, stringField STRING, timestampField TIMESTAMP, decimalField DECIMAL(18,2), dateField DATE, charField CHAR(5), floatField FLOAT, complexData ARRAY<STRING> ) STORED BY 'carbondata' TBLPROPERTIES('SORT_COLUMNS'='', 'DICTIONARY_INCLUDE'='dateField, charField')       """.stripMargin)Pre-aggregate create command:spark.sql("create datamap abcaggnew on table carbon_table using 'org.apache.carbondata.datamap.AggregateDataMapHandler' as select shortField, sum(shortField) from carbon_table group by shortfield,stringField").showwhen we perform query on preaggregate table using spark.sql("select shortField, sum(shortField) from carbon_table group by shortfield,stringField").showIt shows the following error:Exception in thread "main" org.apache.spark.sql.AnalysisException: Column does not exists in Pre Aggregate table;at org.apache.spark.sql.hive.CarbonPreAggregateQueryRules.getChildAttributeReference(CarbonPreAggregateRules.scala:491)at org.apache.spark.sql.hive.CarbonPreAggregateQueryRules$$anonfun$13$$anonfun$apply$4.applyOrElse(CarbonPreAggregateRules.scala:788)at org.apache.spark.sql.hive.CarbonPreAggregateQueryRules$$anonfun$13$$anonfun$apply$4.applyOrElse(CarbonPreAggregateRules.scala:786)at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:288)at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:288)at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:287)at org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:277)at org.apache.spark.sql.hive.CarbonPreAggregateQueryRules$$anonfun$13.apply(CarbonPreAggregateRules.scala:786)at org.apache.spark.sql.hive.CarbonPreAggregateQueryRules$$anonfun$13.apply(CarbonPreAggregateRules.scala:785)at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)at scala.collection.AbstractTraversable.map(Traversable.scala:104)at org.apache.spark.sql.hive.CarbonPreAggregateQueryRules.getUpdatedExpressions(CarbonPreAggregateRules.scala:785)at org.apache.spark.sql.hive.CarbonPreAggregateQueryRules$$anonfun$transformPreAggQueryPlan$1.applyOrElse(CarbonPreAggregateRules.scala:547)at org.apache.spark.sql.hive.CarbonPreAggregateQueryRules$$anonfun$transformPreAggQueryPlan$1.applyOrElse(CarbonPreAggregateRules.scala:540)
issueID:CARBONDATA-1941
type:Task
changed files:
texts:Document update for Lock Retry
Add documentation for lock retry Property Name: carbon.lock.retriesDefault Value: 3Details: Specifies the maximum number of retries to obtain the lock for any operations other than load.Poperty Name: carbon.lock.retry.timeout.sec Default Value: 5 Details: Specifies the interval between the retries to obtain the lock for any operation other than load.
issueID:CARBONDATA-1942
type:Task
changed files:
texts:Documentation for Concurrent Lock Retries
Add documenation for concurrent lock retriesProperty Name: carbon.concurrent.lock.retriesDefault Value:  100Deatils:  Specifies the maximum number of retries to obtain the lock for concurrent operations. This is used for concurrent loading.Property Name: carbon.concurrent.lock.retry.timeout.secDefault Value: 1Details:  Specifies the interval between the retries to obtain the lock for concurrent operations.
issueID:CARBONDATA-1943
type:Bug
changed files:
texts:Load static partition with LOAD COMMAND creates multiple partitions
When static partition data loaded with the following syntax creates multiple partitions instead of one.LOAD DATA local inpath '$resourcesPath/data.csv' INTO TABLE loadstaticpartitiononeissue PARTITION(empno='1')the above should show only one partition during show partition but shows many partitions
issueID:CARBONDATA-1944
type:Bug
changed files:hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonTableInputFormat.java
texts:Special character like comma (,) cannot be loaded on partition columns
The partition columns data which has special character like comma (,) cannot be loaded or queried.
issueID:CARBONDATA-1945
type:Improvement
changed files:
texts:documentation for bad records action needs to be updated
We should update bad record action's default value as while creating and loading data with some empty or bad record it is giving exception Error: java.lang.Exception: Data load failed due to bad record:Steps to reproduce:1) Create Table:CREATE TABLE uniqdata (CUST_ID int,CUST_NAME String,ACTIVE_EMUI_VERSION string, DOB timestamp, DOJ timestamp, BIGINT_COLUMN1 bigint,BIGINT_COLUMN2 bigint,DECIMAL_COLUMN1 decimal(30,10), DECIMAL_COLUMN2 decimal(36,10),Double_COLUMN1 double, Double_COLUMN2 double,INTEGER_COLUMN1 int) STORED BY 'org.apache.carbondata.format' TBLPROPERTIES ("TABLE_BLOCKSIZE"= "256 MB")2) Load data:LOAD DATA inpath 'hdfs://localhost:54310/Data/uniqdata/2000_UniqData.csv' INTO table uniqdata options('DELIMITER'=',', 'FILEHEADER'='CUST_ID, CUST_NAME, ACTIVE_EMUI_VERSION, DOB, DOJ, BIGINT_COLUMN1, BIGINT_COLUMN2, DECIMAL_COLUMN1, DECIMAL_COLUMN2, Double_COLUMN1, Double_COLUMN2, INTEGER_COLUMN1','TIMESTAMPFORMAT'='yyyy-MM-dd HH:mm:ss');3) Output: Expected output: data should be loaded successfully.Actual Output: Error: java.lang.Exception: Data load failed due to bad record: The value with column name dob and column data type TIMESTAMP is not a valid TIMESTAMP type. (state=,code=0)
issueID:CARBONDATA-1946
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/scan/result/vector/impl/CarbonColumnVectorImpl.java
core/src/main/java/org/apache/carbondata/core/scan/result/vector/CarbonColumnVector.java
core/src/main/java/org/apache/carbondata/core/util/DataTypeUtil.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/store/impl/safe/SafeVariableLengthDimensionDataChunkStore.java
core/src/main/java/org/apache/carbondata/core/scan/executor/util/RestructureUtil.java
core/src/main/java/org/apache/carbondata/core/scan/collector/impl/DictionaryBasedVectorResultCollector.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/store/impl/unsafe/UnsafeVariableLengthDimensionDataChunkStore.java
integration/spark-datasource/src/main/scala/org/apache/carbondata/spark/vectorreader/ColumnarVectorWrapper.java
integration/presto/src/main/java/org/apache/carbondata/presto/CarbonColumnVectorWrapper.java
texts:Exception thrown after alter data type change operation on dictionary exclude integer type column
Problem: After restructure change data type operation (INT to BIGINT) on dictionary exclude INT type column if select query is triggered then exception is thrown.Analysis: This is happening because while retrieving the data the vector is created for BIGINT type (size 8 bytes) which but the actual length of each data is 4 bytes and there is length check while reading the data which is failing.Steps to reproduce---------------------------1. create table table_sort(age int) stored by 'carbondata' TBLPROPERTIES('DICTIONARY_EXCLUDE'='age')2. insert into table_sort select 327943. alter table table_sort change age age bigint4. select * from table_sortException Trace--------------------------17/12/28 12:03:15 ERROR Executor: Exception in task 0.0 in stage 2.0 (TID 2)java.lang.IllegalArgumentException: Wrong length: 4, expected 8 at org.apache.carbondata.core.util.ByteUtil.explainWrongLengthOrOffset(ByteUtil.java:581) at org.apache.carbondata.core.util.ByteUtil.toLong(ByteUtil.java:553) at org.apache.carbondata.core.datastore.chunk.store.impl.safe.SafeVariableLengthDimensionDataChunkStore.fillRow(SafeVariableLengthDimensionDataChunkStore.java:156) at org.apache.carbondata.core.datastore.chunk.impl.VariableLengthDimensionDataChunk.fillConvertedChunkData(VariableLengthDimensionDataChunk.java:112) at org.apache.carbondata.core.scan.result.AbstractScannedResult.fillColumnarNoDictionaryBatch(AbstractScannedResult.java:246) at org.apache.carbondata.core.scan.collector.impl.DictionaryBasedVectorResultCollector.scanAndFillResult(DictionaryBasedVectorResultCollector.java:163) at org.apache.carbondata.core.scan.collector.impl.DictionaryBasedVectorResultCollector.collectVectorBatch(DictionaryBasedVectorResultCollector.java:155) at org.apache.carbondata.core.scan.processor.impl.DataBlockIteratorImpl.processNextBatch(DataBlockIteratorImpl.java:65) at org.apache.carbondata.core.scan.result.iterator.VectorDetailQueryResultIterator.processNextBatch(VectorDetailQueryResultIterator.java:46) at org.apache.carbondata.spark.vectorreader.VectorizedCarbonRecordReader.nextBatch(VectorizedCarbonRecordReader.java:267) at org.apache.carbondata.spark.vectorreader.VectorizedCarbonRecordReader.nextKeyValue(VectorizedCarbonRecordReader.java:155) at org.apache.carbondata.spark.rdd.CarbonScanRDD$$anon$1.hasNext(CarbonScanRDD.scala:370) at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.scan_nextBatch$(Unknown Source) at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source) at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43) at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:395) at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:234) at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:228) at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827) at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) at org.apache.spark.rdd.RDD.iterator(RDD.scala:287) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87) at org.apache.spark.scheduler.Task.run(Task.scala:108)
issueID:CARBONDATA-1947
type:Bug
changed files:processing/src/main/java/org/apache/carbondata/processing/merger/CarbonDataMergerUtil.java
texts:fix select * issue after compaction, delete and clean files operation
All data is deleted from compacted segment if a record is deleted and clean file command is run.1: create table tt2(id int,name string) stored by 'carbondata';2: insert into tt2 select 1,'abc';3: insert into tt2 select 2,'pqr';4: insert into tt2 select 3,'mno';5: insert into tt2 select 4,'ghi'6: Alter table tt2 compact 'minor';7: clean files for table tt2;8: delete from tt2 where id=3;9: clean files for table tt2;10: select * from tt2;select query gives empty result
issueID:CARBONDATA-1948
type:Sub-task
changed files:
texts:Update help document for the change made for CARBONDATA-1929
carbon.sort.file.write.buffer.size (minValue = 10 KB, maxValue=10MB, defaultValue =16 KB )carbon.sort.intermediate.files.limit (minValue = 2, maxValue=50, defaultValue =20 )
issueID:CARBONDATA-1949
type:Bug
changed files:
texts:DESC formatted command displays sort scope twice
I created a table using this command:CREATE TABLE uniqdata_29(CUST_ID int,CUST_NAME String,ACTIVE_EMUI_VERSION string, DOB timestamp, DOJ timestamp, BIGINT_COLUMN1 bigint,BIGINT_COLUMN2 bigint,DECIMAL_COLUMN1 decimal(30,10), DECIMAL_COLUMN2 decimal(36,10),Double_COLUMN1 double, Double_COLUMN2 double,INTEGER_COLUMN1 int) STORED BY 'org.apache.carbondata.format';when I executed  desc formatted uniqdata_29It shows the following output:0: jdbc:hive2://localhost:10000> desc formatted uniqdata_29;-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+               col_name                                                     data_type                                                                       comment                                  -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ cust_id                                int                                                                                MEASURE,null                                                               cust_name                              string                                                                             KEY COLUMN,null                                                            active_emui_version                    string                                                                             KEY COLUMN,null                                                            dob                                    timestamp                                                                          KEY COLUMN,null                                                            doj                                    timestamp                                                                          KEY COLUMN,null                                                            bigint_column1                         bigint                                                                             MEASURE,null                                                               bigint_column2                         bigint                                                                             MEASURE,null                                                               decimal_column1                        decimal(30,10)                                                                     MEASURE,null                                                               decimal_column2                        decimal(36,10)                                                                     MEASURE,null                                                               double_column1                         double                                                                             MEASURE,null                                                               double_column2                         double                                                                             MEASURE,null                                                               integer_column1                        int                                                                                MEASURE,null                                                                  ##Detailed Table Information             Database Name                          28dec                                                                               Table Name                             uniqdata_29                                                                         CARBON Store Path                      hdfs://localhost:54311/opt/carbonStore                                              Comment                                  Table Block Size                       1024 MB                                                                             Table Data Size                        101419                                                                              Table Index Size                       1902                                                                                Last Update Time                       1514535776276                                                                       SORT_SCOPE                             LOCAL_SORT                                                                         LOCAL_SORT                                                                 Streaming                              false                                                                               SORT_SCOPE                             LOCAL_SORT                                                                         LOCAL_SORT                                                                    ##Detailed Column property               ADAPTIVE                                 SORT_COLUMNS                           cust_name,active_emui_version,dob,doj                                              -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+29 rows selected (0.196 seconds)Here sort_scope is displayed twice
issueID:CARBONDATA-195
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/scan/filter/FilterExpressionProcessor.java
texts:Select query with AND filter failing for empty &#39;&#39; operand value of numeric column
Select query with AND filter failing for empty '' operand value of numeric columnQuery Syntax:select * from Table Int_col='' and str_col='';Throwing ClassCastException
issueID:CARBONDATA-1950
type:Bug
changed files:
texts:DESC FORMATTED command is displaying wrong comment for sort_scope as global_sort
I created a table using :CREATE TABLE uniqdata_dict_sort(CUST_ID int,CUST_NAME String,ACTIVE_EMUI_VERSION string, DOB timestamp, DOJ timestamp, BIGINT_COLUMN1 bigint,BIGINT_COLUMN2 bigint,DECIMAL_COLUMN1 decimal(30,10), DECIMAL_COLUMN2 decimal(36,10),Double_COLUMN1 double, Double_COLUMN2 double,INTEGER_COLUMN1 int) STORED BY 'org.apache.carbondata.format' TBLPROPERTIES('DICTIONARY_INCLUDE'='CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1,Double_COLUMN2,INTEGER_COLUMN1', 'SORT_SCOPE'='GLOBAL_SORT');When I executed the following query:desc formatted uniqdata_dict_sortIt shows the following output:0: jdbc:hive2://localhost:10000> desc formatted uniqdata_dict_sort;--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+               col_name                                                                                         data_type                                                                                                            comment                                  --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ cust_id                                int                                                                                                                                                         DICTIONARY, KEY COLUMN,null                                                cust_name                              string                                                                                                                                                      DICTIONARY, KEY COLUMN,null                                                active_emui_version                    string                                                                                                                                                      DICTIONARY, KEY COLUMN,null                                                dob                                    timestamp                                                                                                                                                   KEY COLUMN,null                                                            doj                                    timestamp                                                                                                                                                   KEY COLUMN,null                                                            bigint_column1                         bigint                                                                                                                                                      DICTIONARY, KEY COLUMN,null                                                bigint_column2                         bigint                                                                                                                                                      DICTIONARY, KEY COLUMN,null                                                decimal_column1                        decimal(30,10)                                                                                                                                              DICTIONARY, KEY COLUMN,null                                                decimal_column2                        decimal(36,10)                                                                                                                                              DICTIONARY, KEY COLUMN,null                                                double_column1                         double                                                                                                                                                      DICTIONARY, KEY COLUMN,null                                                double_column2                         double                                                                                                                                                      DICTIONARY, KEY COLUMN,null                                                integer_column1                        int                                                                                                                                                         DICTIONARY, KEY COLUMN,null                                                   ##Detailed Table Information             Database Name                          28dec                                                                                                                                                        Table Name                             uniqdata_dict_sort                                                                                                                                           CARBON Store Path                      hdfs://localhost:54311/opt/carbonStore                                                                                                                       Comment                                  Table Block Size                       1024 MB                                                                                                                                                      Table Data Size                        56526                                                                                                                                                        Table Index Size                       1961                                                                                                                                                         Last Update Time                       1514535819178                                                                                                                                                SORT_SCOPE                             global_sort                                                                                                                                                 LOCAL_SORT                                                                 Streaming                              false                                                                                                                                                        SORT_SCOPE                             global_sort                                                                                                                                                 LOCAL_SORT                                                                    ##Detailed Column property               ADAPTIVE                                 SORT_COLUMNS                           cust_id,cust_name,active_emui_version,dob,doj,bigint_column1,bigint_column2,decimal_column1,decimal_column2,double_column1,double_column2,integer_column1   --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+29 rows selected (0.369 seconds)Here the comment parameter for sort_scope is displaying wrong value
issueID:CARBONDATA-1952
type:Bug
changed files:
texts:Incorrect Result displays while applying delete query on table
Incorrect Result while applying delete query on the table.Steps to reproduce:1)Create table:CREATE TABLE uniqdata_delete (CUST_ID int,CUST_NAME String,ACTIVE_EMUI_VERSION string, DOB timestamp, DOJ timestamp, BIGINT_COLUMN1 bigint,BIGINT_COLUMN2 bigint,DECIMAL_COLUMN1 decimal(30,10), DECIMAL_COLUMN2 decimal(36,10),Double_COLUMN1 double, Double_COLUMN2 double, INTEGER_COLUMN1 int) STORED BY 'org.apache.carbondata.format' TBLPROPERTIES ("TABLE_BLOCKSIZE"= "256 MB")2)Load Data Into table:LOAD DATA INPATH 'HDFS_URL/BabuStore/Data/uniqdata/2000_UniqData.csv' into table uniqdata_delete OPTIONS('FILEHEADER'='CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1,Double_COLUMN2,INTEGER_COLUMN1')3)Execute Query:a) select count from uniqdata_delete;Output: -----------+ count(1)  -----------+ 2013      -----------+1 row selected (0.203 seconds)b)delete from uniqdata_delete where CUST_ID NOT IN(9996,9999);c) select count from uniqdata_delete;output: -----------+ count(1)  -----------+ 14        -----------+1 row selected (0.22 seconds)d) select * from uniqdata_delete;output:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- cust_id      cust_name         active_emui_version               dob                     doj            bigint_column1   bigint_column2       decimal_column1          decimal_column2         double_column1        double_column2      integer_column1  ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- NULL        NULL                    NULL                    NULL             NULL             NULL                     NULL                     NULL                  NULL                   NULL              NULL        NULL                    NULL                    1233720368578    NULL             NULL                     NULL                     NULL                  NULL                   NULL              NULL        NULL                    NULL                    NULL             -223372036854    NULL                     NULL                     NULL                  NULL                   NULL              NULL        NULL                    NULL                    NULL             NULL             12345678901.1234000000   NULL                     NULL                  NULL                   NULL              NULL        NULL                    NULL                    NULL             NULL             NULL                     22345678901.1234000000   NULL                  NULL                   NULL              NULL        NULL                    NULL                    NULL             NULL             NULL                     NULL                     1.12345674897976E10   NULL                   NULL              NULL        NULL                    NULL                    NULL             NULL             NULL                     NULL                     NULL                  -1.12345674897976E10   NULL              NULL        NULL                    NULL                    NULL             NULL             NULL                     NULL                     NULL                  NULL                   0                 NULL        NULL                    1970-01-01 12:00:03.0   NULL             NULL             NULL                     NULL                     NULL                  NULL                   NULL              NULL        1970-01-01 11:00:03.0   NULL                    NULL             NULL             NULL                     NULL                     NULL                  NULL                   NULL              NULL       ACTIVE_EMUI_VERSION_00000   NULL                    NULL                    NULL             NULL             NULL                     NULL                     NULL                  NULL                   NULL              NULL      CUST_NAME_00000    NULL                    NULL                    NULL             NULL             NULL                     NULL                     NULL                  NULL                   NULL              9996      CUST_NAME_00996   ACTIVE_EMUI_VERSION_00996   1972-09-23 01:00:03.0   1972-09-23 02:00:03.0   123372037850     -223372035858    12345679897.1234000000   22345679897.1234000000   1.12345674897976E10   -1.12345674897976E10   997               9999      CUST_NAME_00999   ACTIVE_EMUI_VERSION_00999   1972-09-26 01:00:03.0   1972-09-26 02:00:03.0   123372037853     -223372035855    12345679900.1234000000   22345679900.1234000000   1.12345674897976E10   -1.12345674897976E10   1000             -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------14 rows selected (0.377 seconds)4) Expected Result: It should delete all rows except rows whose cust_included in delete query.5) Actual Result: It did not delete all rows except that mentioned rows.
issueID:CARBONDATA-1953
type:Bug
changed files:
texts:Pre-aggregate Should inherit sort column,sort_scope,dictionary encoding from main table
Pre-aggregate Should inherit sort column,sort_scope,dictionary encoding from main table spark.sql("drop table if exists y ")    spark.sql("create table y(year int,month int,name string,salary int) stored by 'carbondata' tblproperties('NO_INVERTED_INDEX'='name','sort_scope'='Global_sort','table_blocksize'='23','Dictionary_include'='month','Dictionary_exclude'='year,name','sort_columns'='month,year,name')")    spark.sql("insert into y select 10,11,'babu',12")spark.sql("create datamap y1_sum1 on table y using 'preaggregate' as select year,month,name,sum(salary) from y group by year,month,name")    spark.sql("desc formatted y").show(100,false)    spark.sql("desc formatted y_y1_sum1").show(100,false)------col_name                            data_type                                                                       comment                                                                 ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------y_year                              int                                                                             KEY COLUMN,NOINVERTEDINDEX,null                                         y_month                             int                                                                             DICTIONARY, KEY COLUMN,NOINVERTEDINDEX,null                             y_name                              string                                                                          KEY COLUMN,null                                                         y_salary_sum                        bigint                                                                          MEASURE,null                                                               ##Detailed Table Information          Database Name                       default                                                                          Table Name                          y_y1_sum1                                                                        CARBON Store Path                   D:\code\carbondata\myfork\incubator-carbondata/examples/spark2/target/store      Comment                               Table Block Size                    1024 MB                                                                          Table Data Size                     1297                                                                             Table Index Size                    1076                                                                             Last Update Time                    1514546841061                                                                    SORT_SCOPE                          LOCAL_SORT                                                                      LOCAL_SORT                                                              Streaming                           false                                                                            SORT_SCOPE                          LOCAL_SORT
issueID:CARBONDATA-1954
type:Bug
changed files:
texts:CarbonHiveMetastore is not being updated while dropping the Pre-Aggregate table

issueID:CARBONDATA-1955
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/DefaultEncodingFactory.java
texts:Delta DataType calculation is incorrect for long type

issueID:CARBONDATA-1956
type:Bug
changed files:
texts:Select query with sum, count and avg throws exception for pre aggregate table
I create a datamap using the following command:create datamap uniqdata_agg_d on table uniqdata_29 using 'preaggregate' as select sum(decimal_column1), count(cust_id), avg(bigint_column1) from uniqdata_29 group by cust_id;The datamap creation was successfull, but when I tried the following query:select sum(decimal_column1), count(cust_id), avg(bigint_column1) from uniqdata_29 group by cust_id;It throws the following exception:Error: org.apache.spark.sql.AnalysisException: cannot resolve '(sum(uniqdata_29_uniqdata_agg_d.`uniqdata_29_bigint_column1_sum`) / sum(uniqdata_29_uniqdata_agg_d.`uniqdata_29_bigint_column1_count`))' due to data type mismatch: '(sum(uniqdata_29_uniqdata_agg_d.`uniqdata_29_bigint_column1_sum`) / sum(uniqdata_29_uniqdata_agg_d.`uniqdata_29_bigint_column1_count`))' requires (double or decimal) type, not bigint;;'Aggregate uniqdata_29_cust_id_count#244, sum(uniqdata_29_decimal_column1_sum#243) AS sum(decimal_column1)#274, sum(cast(uniqdata_29_cust_id_count#244 as bigint)) AS count(cust_id)#276L, (sum(uniqdata_29_bigint_column1_sum#245L) / sum(uniqdata_29_bigint_column1_count#246L)) AS avg(bigint_column1)#279+- Relationuniqdata_29_decimal_column1_sum#243,uniqdata_29_cust_id_count#244,uniqdata_29_bigint_column1_sum#245L,uniqdata_29_bigint_column1_count#246L CarbonDatasourceHadoopRelation [ Database name :28dec, Table name :uniqdata_29_uniqdata_agg_d, Schema :Some(StructType(StructField(uniqdata_29_decimal_column1_sum,DecimalType(30,10),true), StructField(uniqdata_29_cust_id_count,IntegerType,true), StructField(uniqdata_29_bigint_column1_sum,LongType,true), StructField(uniqdata_29_bigint_column1_count,LongType,true))) ] (state=,code=0)Steps for creation of maintable:CREATE TABLE uniqdata_29(CUST_ID int,CUST_NAME String,ACTIVE_EMUI_VERSION string, DOB timestamp, DOJ timestamp, BIGINT_COLUMN1 bigint,BIGINT_COLUMN2 bigint,DECIMAL_COLUMN1 decimal(30,10), DECIMAL_COLUMN2 decimal(36,10),Double_COLUMN1 double, Double_COLUMN2 double,INTEGER_COLUMN1 int) STORED BY 'org.apache.carbondata.format';Load command:LOAD DATA INPATH 'hdfs://localhost:54311/Files/2000_UniqData.csv' into table uniqdata_29 OPTIONS('DELIMITER'=',', 'QUOTECHAR'='"','BAD_RECORDS_ACTION'='FORCE','FILEHEADER'='CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1,Double_COLUMN2,INTEGER_COLUMN1');Datamap creation command:create datamap uniqdata_agg_d on table uniqdata_29 using 'preaggregate' as select sum(decimal_column1), count(cust_id), avg(bigint_column1) from uniqdata_29 group by cust_id;Note: select sum(decimal_column1), count(cust_id), avg(bigint_column1) from uniqdata_29 group by cust_id; executed successfully on maintable
issueID:CARBONDATA-1957
type:Bug
changed files:
texts:create datamap query fails on table having dictionary_include
I created a datamap using the following command:create datamap uniqdata_agg on table uniqdata using 'preaggregate' as select cust_id, cust_name,avg(decimal_column1) from uniqdata group by cust_id,cust_name;It throws the following error:Error: java.lang.Exception: DataLoad failure: (state=,code=0)Steps to reproduce:CREATE TABLE uniqdata(CUST_ID int,CUST_NAME String,ACTIVE_EMUI_VERSION string, DOB timestamp, DOJ timestamp, BIGINT_COLUMN1 bigint,BIGINT_COLUMN2 bigint,DECIMAL_COLUMN1 decimal(30,10), DECIMAL_COLUMN2 decimal(36,10),Double_COLUMN1 double, Double_COLUMN2 double,INTEGER_COLUMN1 int) STORED BY 'org.apache.carbondata.format' TBLPROPERTIES('DICTIONARY_INCLUDE'='CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1,Double_COLUMN2,INTEGER_COLUMN1')Load command:LOAD DATA INPATH 'HDFS_URL/BabuStore/Data/uniqdata/2000_UniqData.csv' into table uniqdata OPTIONS('DELIMITER'=',', 'QUOTECHAR'='"','BAD_RECORDS_ACTION'='FORCE','FILEHEADER'='CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1,Double_COLUMN2,INTEGER_COLUMN1')Create datamap commad:create datamap uniqdata_agg on table uniqdata using 'preaggregate' as select cust_id, cust_name,avg(decimal_column1) from uniqdata group by cust_id,cust_name;The above command throws the following exception:Error: java.lang.Exception: DataLoad failure: (state=,code=0)Here are the logs:18/01/02 11:46:58 ERROR ParallelReadMergeSorterImpl: SafeParallelSorterPool:uniqdata_uniqdata_agg java.lang.IllegalArgumentException: requirement failed: Decimal precision 2922 exceeds max precision 38 at scala.Predef$.require(Predef.scala:224) at org.apache.spark.sql.types.Decimal.set(Decimal.scala:113) at org.apache.spark.sql.types.Decimal$.apply(Decimal.scala:426) at org.apache.spark.sql.types.Decimal.apply(Decimal.scala) at org.apache.spark.sql.catalyst.expressions.UnsafeRow.getDecimal(UnsafeRow.java:409) at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.apply_0$(Unknown Source) at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.apply(Unknown Source) at scala.collection.Iterator$$anon$11.next(Iterator.scala:409) at scala.collection.Iterator$$anon$11.next(Iterator.scala:409) at org.apache.carbondata.spark.rdd.LazyRddIterator.next(NewCarbonDataLoadRDD.scala:514) at org.apache.carbondata.spark.rdd.LazyRddIterator.next(NewCarbonDataLoadRDD.scala:477) at org.apache.carbondata.processing.loading.steps.InputProcessorStepImpl$InputProcessorIterator.getBatch(InputProcessorStepImpl.java:239) at org.apache.carbondata.processing.loading.steps.InputProcessorStepImpl$InputProcessorIterator.next(InputProcessorStepImpl.java:200) at org.apache.carbondata.processing.loading.steps.InputProcessorStepImpl$InputProcessorIterator.next(InputProcessorStepImpl.java:129) at org.apache.carbondata.processing.loading.steps.DataConverterProcessorStepImpl$1.next(DataConverterProcessorStepImpl.java:97) at org.apache.carbondata.processing.loading.steps.DataConverterProcessorStepImpl$1.next(DataConverterProcessorStepImpl.java:83) at org.apache.carbondata.processing.loading.sort.impl.ParallelReadMergeSorterImpl$SortIteratorThread.run(ParallelReadMergeSorterImpl.java:218) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748)18/01/02 11:46:58 ERROR ForwardDictionaryCache: SafeParallelSorterPool:uniqdata_uniqdata_agg Error loading the dictionary: null18/01/02 11:46:58 ERROR ForwardDictionaryCache: SafeParallelSorterPool:uniqdata_uniqdata_agg Error loading the dictionary: null18/01/02 11:46:58 ERROR ForwardDictionaryCache: SafeParallelSorterPool:uniqdata_uniqdata_agg Error loading the dictionary: null18/01/02 11:46:58 ERROR ForwardDictionaryCache: SafeParallelSorterPool:uniqdata_uniqdata_agg Error loading the dictionary: null18/01/02 11:46:58 ERROR ParallelReadMergeSorterImpl: SafeParallelSorterPool:uniqdata_uniqdata_agg java.lang.IllegalArgumentException: requirement failed: Decimal precision 3128 exceeds max precision 38 at scala.Predef$.require(Predef.scala:224) at org.apache.spark.sql.types.Decimal.set(Decimal.scala:113) at org.apache.spark.sql.types.Decimal$.apply(Decimal.scala:426) at org.apache.spark.sql.types.Decimal.apply(Decimal.scala) at org.apache.spark.sql.catalyst.expressions.UnsafeRow.getDecimal(UnsafeRow.java:409) at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.apply_0$(Unknown Source) at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.apply(Unknown Source) at scala.collection.Iterator$$anon$11.next(Iterator.scala:409) at scala.collection.Iterator$$anon$11.next(Iterator.scala:409) at org.apache.carbondata.spark.rdd.LazyRddIterator.next(NewCarbonDataLoadRDD.scala:514) at org.apache.carbondata.spark.rdd.LazyRddIterator.next(NewCarbonDataLoadRDD.scala:477) at org.apache.carbondata.processing.loading.steps.InputProcessorStepImpl$InputProcessorIterator.getBatch(InputProcessorStepImpl.java:239) at org.apache.carbondata.processing.loading.steps.InputProcessorStepImpl$InputProcessorIterator.next(InputProcessorStepImpl.java:200) at org.apache.carbondata.processing.loading.steps.InputProcessorStepImpl$InputProcessorIterator.next(InputProcessorStepImpl.java:129) at org.apache.carbondata.processing.loading.steps.DataConverterProcessorStepImpl$1.next(DataConverterProcessorStepImpl.java:97) at org.apache.carbondata.processing.loading.steps.DataConverterProcessorStepImpl$1.next(DataConverterProcessorStepImpl.java:83) at org.apache.carbondata.processing.loading.sort.impl.ParallelReadMergeSorterImpl$SortIteratorThread.run(ParallelReadMergeSorterImpl.java:218) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748)18/01/02 11:46:58 ERROR ParallelReadMergeSorterImpl: SafeParallelSorterPool:uniqdata_uniqdata_agg java.lang.IllegalArgumentException: requirement failed: Decimal precision 2517 exceeds max precision 38 at scala.Predef$.require(Predef.scala:224) at org.apache.spark.sql.types.Decimal.set(Decimal.scala:113) at org.apache.spark.sql.types.Decimal$.apply(Decimal.scala:426) at org.apache.spark.sql.types.Decimal.apply(Decimal.scala) at org.apache.spark.sql.catalyst.expressions.UnsafeRow.getDecimal(UnsafeRow.java:409) at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.apply_0$(Unknown Source) at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.apply(Unknown Source) at scala.collection.Iterator$$anon$11.next(Iterator.scala:409) at scala.collection.Iterator$$anon$11.next(Iterator.scala:409) at org.apache.carbondata.spark.rdd.LazyRddIterator.next(NewCarbonDataLoadRDD.scala:514) at org.apache.carbondata.spark.rdd.LazyRddIterator.next(NewCarbonDataLoadRDD.scala:477) at org.apache.carbondata.processing.loading.steps.InputProcessorStepImpl$InputProcessorIterator.getBatch(InputProcessorStepImpl.java:239) at org.apache.carbondata.processing.loading.steps.InputProcessorStepImpl$InputProcessorIterator.next(InputProcessorStepImpl.java:200) at org.apache.carbondata.processing.loading.steps.InputProcessorStepImpl$InputProcessorIterator.next(InputProcessorStepImpl.java:129) at org.apache.carbondata.processing.loading.steps.DataConverterProcessorStepImpl$1.next(DataConverterProcessorStepImpl.java:97) at org.apache.carbondata.processing.loading.steps.DataConverterProcessorStepImpl$1.next(DataConverterProcessorStepImpl.java:83) at org.apache.carbondata.processing.loading.sort.impl.ParallelReadMergeSorterImpl$SortIteratorThread.run(ParallelReadMergeSorterImpl.java:218) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748)18/01/02 11:46:58 ERROR ParallelReadMergeSorterImpl: SafeParallelSorterPool:uniqdata_uniqdata_agg java.lang.IllegalArgumentException: requirement failed: Decimal precision 2439 exceeds max precision 38 at scala.Predef$.require(Predef.scala:224) at org.apache.spark.sql.types.Decimal.set(Decimal.scala:113) at org.apache.spark.sql.types.Decimal$.apply(Decimal.scala:426) at org.apache.spark.sql.types.Decimal.apply(Decimal.scala) at org.apache.spark.sql.catalyst.expressions.UnsafeRow.getDecimal(UnsafeRow.java:409) at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.apply_0$(Unknown Source) at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.apply(Unknown Source) at scala.collection.Iterator$$anon$11.next(Iterator.scala:409) at scala.collection.Iterator$$anon$11.next(Iterator.scala:409) at org.apache.carbondata.spark.rdd.LazyRddIterator.next(NewCarbonDataLoadRDD.scala:514) at org.apache.carbondata.spark.rdd.LazyRddIterator.next(NewCarbonDataLoadRDD.scala:477) at org.apache.carbondata.processing.loading.steps.InputProcessorStepImpl$InputProcessorIterator.getBatch(InputProcessorStepImpl.java:239) at org.apache.carbondata.processing.loading.steps.InputProcessorStepImpl$InputProcessorIterator.next(InputProcessorStepImpl.java:200) at org.apache.carbondata.processing.loading.steps.InputProcessorStepImpl$InputProcessorIterator.next(InputProcessorStepImpl.java:129) at org.apache.carbondata.processing.loading.steps.DataConverterProcessorStepImpl$1.next(DataConverterProcessorStepImpl.java:97) at org.apache.carbondata.processing.loading.steps.DataConverterProcessorStepImpl$1.next(DataConverterProcessorStepImpl.java:83) at org.apache.carbondata.processing.loading.sort.impl.ParallelReadMergeSorterImpl$SortIteratorThread.run(ParallelReadMergeSorterImpl.java:218) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748)18/01/02 11:46:58 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks18/01/02 11:46:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms18/01/02 11:46:58 ERROR ShuffleBlockFetcherIterator: Error occurred while fetching local blocksjava.lang.InterruptedException at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireInterruptibly(AbstractQueuedSynchronizer.java:1220) at java.util.concurrent.locks.ReentrantLock.lockInterruptibly(ReentrantLock.java:335) at java.util.concurrent.LinkedBlockingQueue.put(LinkedBlockingQueue.java:339) at org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchLocalBlocks(ShuffleBlockFetcherIterator.scala:262) at org.apache.spark.storage.ShuffleBlockFetcherIterator.initialize(ShuffleBlockFetcherIterator.scala:292) at org.apache.spark.storage.ShuffleBlockFetcherIterator.<init>(ShuffleBlockFetcherIterator.scala:120) at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:45) at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:169) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) at org.apache.spark.rdd.RDD.iterator(RDD.scala:287) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) at org.apache.spark.rdd.RDD.iterator(RDD.scala:287) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) at org.apache.spark.rdd.RDD.iterator(RDD.scala:287) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) at org.apache.spark.rdd.RDD.iterator(RDD.scala:287) at org.apache.carbondata.spark.rdd.LazyRddIterator.hasNext(NewCarbonDataLoadRDD.scala:504) at org.apache.carbondata.processing.loading.steps.InputProcessorStepImpl$InputProcessorIterator.internalHasNext(InputProcessorStepImpl.java:179) at org.apache.carbondata.processing.loading.steps.InputProcessorStepImpl$InputProcessorIterator.hasNext(InputProcessorStepImpl.java:171) at org.apache.carbondata.processing.loading.steps.DataConverterProcessorStepImpl$1.hasNext(DataConverterProcessorStepImpl.java:94) at org.apache.carbondata.processing.loading.sort.impl.ParallelReadMergeSorterImpl$SortIteratorThread.run(ParallelReadMergeSorterImpl.java:217) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748)18/01/02 11:46:58 ERROR ParallelReadMergeSorterImpl: SafeParallelSorterPool:uniqdata_uniqdata_agg org.apache.spark.shuffle.FetchFailedException at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:357) at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:332) at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:54) at scala.collection.Iterator$$anon$11.next(Iterator.scala:409) at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434) at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440) at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408) at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32) at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39) at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408) at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.agg_doAggregateWithKeys$(Unknown Source) at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source) at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43) at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377) at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408) at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408) at org.apache.carbondata.spark.rdd.LazyRddIterator.hasNext(NewCarbonDataLoadRDD.scala:509) at org.apache.carbondata.processing.loading.steps.InputProcessorStepImpl$InputProcessorIterator.internalHasNext(InputProcessorStepImpl.java:179) at org.apache.carbondata.processing.loading.steps.InputProcessorStepImpl$InputProcessorIterator.hasNext(InputProcessorStepImpl.java:171) at org.apache.carbondata.processing.loading.steps.DataConverterProcessorStepImpl$1.hasNext(DataConverterProcessorStepImpl.java:94) at org.apache.carbondata.processing.loading.sort.impl.ParallelReadMergeSorterImpl$SortIteratorThread.run(ParallelReadMergeSorterImpl.java:217) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748)Caused by: java.lang.InterruptedException at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireInterruptibly(AbstractQueuedSynchronizer.java:1220) at java.util.concurrent.locks.ReentrantLock.lockInterruptibly(ReentrantLock.java:335) at java.util.concurrent.LinkedBlockingQueue.put(LinkedBlockingQueue.java:339) at org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchLocalBlocks(ShuffleBlockFetcherIterator.scala:262) at org.apache.spark.storage.ShuffleBlockFetcherIterator.initialize(ShuffleBlockFetcherIterator.scala:292) at org.apache.spark.storage.ShuffleBlockFetcherIterator.<init>(ShuffleBlockFetcherIterator.scala:120) at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:45) at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:169) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) at org.apache.spark.rdd.RDD.iterator(RDD.scala:287) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) at org.apache.spark.rdd.RDD.iterator(RDD.scala:287) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) at org.apache.spark.rdd.RDD.iterator(RDD.scala:287) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) at org.apache.spark.rdd.RDD.iterator(RDD.scala:287) at org.apache.carbondata.spark.rdd.LazyRddIterator.hasNext(NewCarbonDataLoadRDD.scala:504) ... 7 more18/01/02 11:46:58 ERROR ParallelReadMergeSorterImpl: SafeParallelSorterPool:uniqdata_uniqdata_agg java.lang.IllegalArgumentException: requirement failed: Decimal precision 2415 exceeds max precision 38 at scala.Predef$.require(Predef.scala:224) at org.apache.spark.sql.types.Decimal.set(Decimal.scala:113) at org.apache.spark.sql.types.Decimal$.apply(Decimal.scala:426) at org.apache.spark.sql.types.Decimal.apply(Decimal.scala) at org.apache.spark.sql.catalyst.expressions.UnsafeRow.getDecimal(UnsafeRow.java:409) at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.apply_0$(Unknown Source) at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.apply(Unknown Source) at scala.collection.Iterator$$anon$11.next(Iterator.scala:409) at scala.collection.Iterator$$anon$11.next(Iterator.scala:409) at org.apache.carbondata.spark.rdd.LazyRddIterator.next(NewCarbonDataLoadRDD.scala:514) at org.apache.carbondata.spark.rdd.LazyRddIterator.next(NewCarbonDataLoadRDD.scala:477) at org.apache.carbondata.processing.loading.steps.InputProcessorStepImpl$InputProcessorIterator.getBatch(InputProcessorStepImpl.java:239) at org.apache.carbondata.processing.loading.steps.InputProcessorStepImpl$InputProcessorIterator.next(InputProcessorStepImpl.java:200) at org.apache.carbondata.processing.loading.steps.InputProcessorStepImpl$InputProcessorIterator.next(InputProcessorStepImpl.java:129) at org.apache.carbondata.processing.loading.steps.DataConverterProcessorStepImpl$1.next(DataConverterProcessorStepImpl.java:97) at org.apache.carbondata.processing.loading.steps.DataConverterProcessorStepImpl$1.next(DataConverterProcessorStepImpl.java:83) at org.apache.carbondata.processing.loading.sort.impl.ParallelReadMergeSorterImpl$SortIteratorThread.run(ParallelReadMergeSorterImpl.java:218) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748)18/01/02 11:46:58 INFO SortDataRows: &#91;Executor task launch worker-27&#93;&#91;partitionID:uniqdata;queryID:7124297644577&#93; File based sorting will be used18/01/02 11:46:58 INFO ParallelReadMergeSorterImpl: &#91;Executor task launch worker-27&#93;&#91;partitionID:uniqdata;queryID:7124297644577&#93; Record Processed For table: uniqdata_uniqdata_agg18/01/02 11:46:58 INFO NewDataFrameLoaderRDD: DataLoad failureorg.apache.carbondata.processing.loading.exception.CarbonDataLoadingException:  at org.apache.carbondata.processing.loading.sort.AbstractMergeSorter.checkError(AbstractMergeSorter.java:39) at org.apache.carbondata.processing.loading.sort.impl.ParallelReadMergeSorterImpl.sort(ParallelReadMergeSorterImpl.java:117) at org.apache.carbondata.processing.loading.steps.SortProcessorStepImpl.execute(SortProcessorStepImpl.java:62) at org.apache.carbondata.processing.loading.steps.DataWriterProcessorStepImpl.execute(DataWriterProcessorStepImpl.java:87) at org.apache.carbondata.processing.loading.DataLoadExecutor.execute(DataLoadExecutor.java:50) at org.apache.carbondata.spark.rdd.NewDataFrameLoaderRDD$$anon$2.<init>(NewCarbonDataLoadRDD.scala:392) at org.apache.carbondata.spark.rdd.NewDataFrameLoaderRDD.internalCompute(NewCarbonDataLoadRDD.scala:355) at org.apache.carbondata.spark.rdd.CarbonRDD.compute(CarbonRDD.scala:60) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) at org.apache.spark.rdd.RDD.iterator(RDD.scala:287) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87) at org.apache.spark.scheduler.Task.run(Task.scala:99) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748)Caused by: java.lang.IllegalArgumentException: requirement failed: Decimal precision 2922 exceeds max precision 38 at scala.Predef$.require(Predef.scala:224) at org.apache.spark.sql.types.Decimal.set(Decimal.scala:113) at org.apache.spark.sql.types.Decimal$.apply(Decimal.scala:426) at org.apache.spark.sql.types.Decimal.apply(Decimal.scala) at org.apache.spark.sql.catalyst.expressions.UnsafeRow.getDecimal(UnsafeRow.java:409) at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.apply_0$(Unknown Source) at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.apply(Unknown Source) at scala.collection.Iterator$$anon$11.next(Iterator.scala:409) at scala.collection.Iterator$$anon$11.next(Iterator.scala:409) at org.apache.carbondata.spark.rdd.LazyRddIterator.next(NewCarbonDataLoadRDD.scala:514) at org.apache.carbondata.spark.rdd.LazyRddIterator.next(NewCarbonDataLoadRDD.scala:477) at org.apache.carbondata.processing.loading.steps.InputProcessorStepImpl$InputProcessorIterator.getBatch(InputProcessorStepImpl.java:239) at org.apache.carbondata.processing.loading.steps.InputProcessorStepImpl$InputProcessorIterator.next(InputProcessorStepImpl.java:200) at org.apache.carbondata.processing.loading.steps.InputProcessorStepImpl$InputProcessorIterator.next(InputProcessorStepImpl.java:129) at org.apache.carbondata.processing.loading.steps.DataConverterProcessorStepImpl$1.next(DataConverterProcessorStepImpl.java:97) at org.apache.carbondata.processing.loading.steps.DataConverterProcessorStepImpl$1.next(DataConverterProcessorStepImpl.java:83) at org.apache.carbondata.processing.loading.sort.impl.ParallelReadMergeSorterImpl$SortIteratorThread.run(ParallelReadMergeSorterImpl.java:218) ... 3 more18/01/02 11:46:58 ERROR NewDataFrameLoaderRDD: &#91;Executor task launch worker-27&#93;&#91;partitionID:uniqdata;queryID:7124297644577&#93; org.apache.carbondata.processing.loading.exception.CarbonDataLoadingException:  at org.apache.carbondata.processing.loading.sort.AbstractMergeSorter.checkError(AbstractMergeSorter.java:39) at org.apache.carbondata.processing.loading.sort.impl.ParallelReadMergeSorterImpl.sort(ParallelReadMergeSorterImpl.java:117) at org.apache.carbondata.processing.loading.steps.SortProcessorStepImpl.execute(SortProcessorStepImpl.java:62) at org.apache.carbondata.processing.loading.steps.DataWriterProcessorStepImpl.execute(DataWriterProcessorStepImpl.java:87) at org.apache.carbondata.processing.loading.DataLoadExecutor.execute(DataLoadExecutor.java:50) at org.apache.carbondata.spark.rdd.NewDataFrameLoaderRDD$$anon$2.<init>(NewCarbonDataLoadRDD.scala:392) at org.apache.carbondata.spark.rdd.NewDataFrameLoaderRDD.internalCompute(NewCarbonDataLoadRDD.scala:355) at org.apache.carbondata.spark.rdd.CarbonRDD.compute(CarbonRDD.scala:60) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) at org.apache.spark.rdd.RDD.iterator(RDD.scala:287) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87) at org.apache.spark.scheduler.Task.run(Task.scala:99) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748)Caused by: java.lang.IllegalArgumentException: requirement failed: Decimal precision 2922 exceeds max precision 38 at scala.Predef$.require(Predef.scala:224) at org.apache.spark.sql.types.Decimal.set(Decimal.scala:113) at org.apache.spark.sql.types.Decimal$.apply(Decimal.scala:426) at org.apache.spark.sql.types.Decimal.apply(Decimal.scala) at org.apache.spark.sql.catalyst.expressions.UnsafeRow.getDecimal(UnsafeRow.java:409) at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.apply_0$(Unknown Source) at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.apply(Unknown Source) at scala.collection.Iterator$$anon$11.next(Iterator.scala:409) at scala.collection.Iterator$$anon$11.next(Iterator.scala:409) at org.apache.carbondata.spark.rdd.LazyRddIterator.next(NewCarbonDataLoadRDD.scala:514) at org.apache.carbondata.spark.rdd.LazyRddIterator.next(NewCarbonDataLoadRDD.scala:477) at org.apache.carbondata.processing.loading.steps.InputProcessorStepImpl$InputProcessorIterator.getBatch(InputProcessorStepImpl.java:239) at org.apache.carbondata.processing.loading.steps.InputProcessorStepImpl$InputProcessorIterator.next(InputProcessorStepImpl.java:200) at org.apache.carbondata.processing.loading.steps.InputProcessorStepImpl$InputProcessorIterator.next(InputProcessorStepImpl.java:129) at org.apache.carbondata.processing.loading.steps.DataConverterProcessorStepImpl$1.next(DataConverterProcessorStepImpl.java:97) at org.apache.carbondata.processing.loading.steps.DataConverterProcessorStepImpl$1.next(DataConverterProcessorStepImpl.java:83) at org.apache.carbondata.processing.loading.sort.impl.ParallelReadMergeSorterImpl$SortIteratorThread.run(ParallelReadMergeSorterImpl.java:218) ... 3 more18/01/02 11:46:58 INFO AbstractDataLoadProcessorStep: &#91;Executor task launch worker-27&#93;&#91;partitionID:uniqdata;queryID:7124297644577&#93; Total rows processed in step Data Writer: 018/01/02 11:46:58 INFO AbstractDataLoadProcessorStep: &#91;Executor task launch worker-27&#93;&#91;partitionID:uniqdata;queryID:7124297644577&#93; Total rows processed in step Sort Processor: 018/01/02 11:46:58 INFO AbstractDataLoadProcessorStep: &#91;Executor task launch worker-27&#93;&#91;partitionID:uniqdata;queryID:7124297644577&#93; Total rows processed in step Data Converter: 018/01/02 11:46:58 INFO AbstractDataLoadProcessorStep: &#91;Executor task launch worker-27&#93;&#91;partitionID:uniqdata;queryID:7124297644577&#93; Total rows processed in step Input Processor: 018/01/02 11:46:58 INFO UnsafeMemoryManager: &#91;Executor task launch worker-27&#93;&#91;partitionID:uniqdata;queryID:7124297644577&#93; Total memory used after task 7124349470619 is 1542 Current tasks running now are : &#91;5476760201267, 3629759666115, 5715198073676&#93;18/01/02 11:46:58 ERROR Executor: Exception in task 0.0 in stage 30.0 (TID 1034)org.apache.carbondata.processing.loading.exception.CarbonDataLoadingException:  at org.apache.carbondata.processing.loading.sort.AbstractMergeSorter.checkError(AbstractMergeSorter.java:39) at org.apache.carbondata.processing.loading.sort.impl.ParallelReadMergeSorterImpl.sort(ParallelReadMergeSorterImpl.java:117) at org.apache.carbondata.processing.loading.steps.SortProcessorStepImpl.execute(SortProcessorStepImpl.java:62) at org.apache.carbondata.processing.loading.steps.DataWriterProcessorStepImpl.execute(DataWriterProcessorStepImpl.java:87) at org.apache.carbondata.processing.loading.DataLoadExecutor.execute(DataLoadExecutor.java:50) at org.apache.carbondata.spark.rdd.NewDataFrameLoaderRDD$$anon$2.<init>(NewCarbonDataLoadRDD.scala:392) at org.apache.carbondata.spark.rdd.NewDataFrameLoaderRDD.internalCompute(NewCarbonDataLoadRDD.scala:355) at org.apache.carbondata.spark.rdd.CarbonRDD.compute(CarbonRDD.scala:60) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) at org.apache.spark.rdd.RDD.iterator(RDD.scala:287) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87) at org.apache.spark.scheduler.Task.run(Task.scala:99) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748)Caused by: java.lang.IllegalArgumentException: requirement failed: Decimal precision 2922 exceeds max precision 38 at scala.Predef$.require(Predef.scala:224) at org.apache.spark.sql.types.Decimal.set(Decimal.scala:113) at org.apache.spark.sql.types.Decimal$.apply(Decimal.scala:426) at org.apache.spark.sql.types.Decimal.apply(Decimal.scala) at org.apache.spark.sql.catalyst.expressions.UnsafeRow.getDecimal(UnsafeRow.java:409) at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.apply_0$(Unknown Source) at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.apply(Unknown Source) at scala.collection.Iterator$$anon$11.next(Iterator.scala:409) at scala.collection.Iterator$$anon$11.next(Iterator.scala:409) at org.apache.carbondata.spark.rdd.LazyRddIterator.next(NewCarbonDataLoadRDD.scala:514) at org.apache.carbondata.spark.rdd.LazyRddIterator.next(NewCarbonDataLoadRDD.scala:477) at org.apache.carbondata.processing.loading.steps.InputProcessorStepImpl$InputProcessorIterator.getBatch(InputProcessorStepImpl.java:239) at org.apache.carbondata.processing.loading.steps.InputProcessorStepImpl$InputProcessorIterator.next(InputProcessorStepImpl.java:200) at org.apache.carbondata.processing.loading.steps.InputProcessorStepImpl$InputProcessorIterator.next(InputProcessorStepImpl.java:129) at org.apache.carbondata.processing.loading.steps.DataConverterProcessorStepImpl$1.next(DataConverterProcessorStepImpl.java:97) at org.apache.carbondata.processing.loading.steps.DataConverterProcessorStepImpl$1.next(DataConverterProcessorStepImpl.java:83) at org.apache.carbondata.processing.loading.sort.impl.ParallelReadMergeSorterImpl$SortIteratorThread.run(ParallelReadMergeSorterImpl.java:218) ... 3 more18/01/02 11:46:58 INFO CarbonLoaderUtil: LocalFolderDeletionPool:uniqdata_uniqdata_agg Deleted the local store location: /tmp/7124352642546_0 : Time taken: 118/01/02 11:46:58 WARN TaskSetManager: Lost task 0.0 in stage 30.0 (TID 1034, localhost, executor driver): org.apache.carbondata.processing.loading.exception.CarbonDataLoadingException:  at org.apache.carbondata.processing.loading.sort.AbstractMergeSorter.checkError(AbstractMergeSorter.java:39) at org.apache.carbondata.processing.loading.sort.impl.ParallelReadMergeSorterImpl.sort(ParallelReadMergeSorterImpl.java:117) at org.apache.carbondata.processing.loading.steps.SortProcessorStepImpl.execute(SortProcessorStepImpl.java:62) at org.apache.carbondata.processing.loading.steps.DataWriterProcessorStepImpl.execute(DataWriterProcessorStepImpl.java:87) at org.apache.carbondata.processing.loading.DataLoadExecutor.execute(DataLoadExecutor.java:50) at org.apache.carbondata.spark.rdd.NewDataFrameLoaderRDD$$anon$2.<init>(NewCarbonDataLoadRDD.scala:392) at org.apache.carbondata.spark.rdd.NewDataFrameLoaderRDD.internalCompute(NewCarbonDataLoadRDD.scala:355) at org.apache.carbondata.spark.rdd.CarbonRDD.compute(CarbonRDD.scala:60) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) at org.apache.spark.rdd.RDD.iterator(RDD.scala:287) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87) at org.apache.spark.scheduler.Task.run(Task.scala:99) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748)Caused by: java.lang.IllegalArgumentException: requirement failed: Decimal precision 2922 exceeds max precision 38 at scala.Predef$.require(Predef.scala:224) at org.apache.spark.sql.types.Decimal.set(Decimal.scala:113) at org.apache.spark.sql.types.Decimal$.apply(Decimal.scala:426) at org.apache.spark.sql.types.Decimal.apply(Decimal.scala) at org.apache.spark.sql.catalyst.expressions.UnsafeRow.getDecimal(UnsafeRow.java:409) at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.apply_0$(Unknown Source) at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.apply(Unknown Source) at scala.collection.Iterator$$anon$11.next(Iterator.scala:409) at scala.collection.Iterator$$anon$11.next(Iterator.scala:409) at org.apache.carbondata.spark.rdd.LazyRddIterator.next(NewCarbonDataLoadRDD.scala:514) at org.apache.carbondata.spark.rdd.LazyRddIterator.next(NewCarbonDataLoadRDD.scala:477) at org.apache.carbondata.processing.loading.steps.InputProcessorStepImpl$InputProcessorIterator.getBatch(InputProcessorStepImpl.java:239) at org.apache.carbondata.processing.loading.steps.InputProcessorStepImpl$InputProcessorIterator.next(InputProcessorStepImpl.java:200) at org.apache.carbondata.processing.loading.steps.InputProcessorStepImpl$InputProcessorIterator.next(InputProcessorStepImpl.java:129) at org.apache.carbondata.processing.loading.steps.DataConverterProcessorStepImpl$1.next(DataConverterProcessorStepImpl.java:97) at org.apache.carbondata.processing.loading.steps.DataConverterProcessorStepImpl$1.next(DataConverterProcessorStepImpl.java:83) at org.apache.carbondata.processing.loading.sort.impl.ParallelReadMergeSorterImpl$SortIteratorThread.run(ParallelReadMergeSorterImpl.java:218) ... 3 more18/01/02 11:46:58 ERROR TaskSetManager: Task 0 in stage 30.0 failed 1 times; aborting job18/01/02 11:46:58 INFO TaskSchedulerImpl: Removed TaskSet 30.0, whose tasks have all completed, from pool 18/01/02 11:46:58 INFO TaskSchedulerImpl: Cancelling stage 3018/01/02 11:46:58 INFO DAGScheduler: ResultStage 30 (collect at CarbonDataRDDFactory.scala:987) failed in 0.057 s due to Job aborted due to stage failure: Task 0 in stage 30.0 failed 1 times, most recent failure: Lost task 0.0 in stage 30.0 (TID 1034, localhost, executor driver): org.apache.carbondata.processing.loading.exception.CarbonDataLoadingException:  at org.apache.carbondata.processing.loading.sort.AbstractMergeSorter.checkError(AbstractMergeSorter.java:39) at org.apache.carbondata.processing.loading.sort.impl.ParallelReadMergeSorterImpl.sort(ParallelReadMergeSorterImpl.java:117) at org.apache.carbondata.processing.loading.steps.SortProcessorStepImpl.execute(SortProcessorStepImpl.java:62) at org.apache.carbondata.processing.loading.steps.DataWriterProcessorStepImpl.execute(DataWriterProcessorStepImpl.java:87) at org.apache.carbondata.processing.loading.DataLoadExecutor.execute(DataLoadExecutor.java:50) at org.apache.carbondata.spark.rdd.NewDataFrameLoaderRDD$$anon$2.<init>(NewCarbonDataLoadRDD.scala:392) at org.apache.carbondata.spark.rdd.NewDataFrameLoaderRDD.internalCompute(NewCarbonDataLoadRDD.scala:355) at org.apache.carbondata.spark.rdd.CarbonRDD.compute(CarbonRDD.scala:60) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) at org.apache.spark.rdd.RDD.iterator(RDD.scala:287) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87) at org.apache.spark.scheduler.Task.run(Task.scala:99) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748)Caused by: java.lang.IllegalArgumentException: requirement failed: Decimal precision 2922 exceeds max precision 38 at scala.Predef$.require(Predef.scala:224) at org.apache.spark.sql.types.Decimal.set(Decimal.scala:113) at org.apache.spark.sql.types.Decimal$.apply(Decimal.scala:426) at org.apache.spark.sql.types.Decimal.apply(Decimal.scala) at org.apache.spark.sql.catalyst.expressions.UnsafeRow.getDecimal(UnsafeRow.java:409) at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.apply_0$(Unknown Source) at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.apply(Unknown Source) at scala.collection.Iterator$$anon$11.next(Iterator.scala:409) at scala.collection.Iterator$$anon$11.next(Iterator.scala:409) at org.apache.carbondata.spark.rdd.LazyRddIterator.next(NewCarbonDataLoadRDD.scala:514) at org.apache.carbondata.spark.rdd.LazyRddIterator.next(NewCarbonDataLoadRDD.scala:477) at org.apache.carbondata.processing.loading.steps.InputProcessorStepImpl$InputProcessorIterator.getBatch(InputProcessorStepImpl.java:239) at org.apache.carbondata.processing.loading.steps.InputProcessorStepImpl$InputProcessorIterator.next(InputProcessorStepImpl.java:200) at org.apache.carbondata.processing.loading.steps.InputProcessorStepImpl$InputProcessorIterator.next(InputProcessorStepImpl.java:129) at org.apache.carbondata.processing.loading.steps.DataConverterProcessorStepImpl$1.next(DataConverterProcessorStepImpl.java:97) at org.apache.carbondata.processing.loading.steps.DataConverterProcessorStepImpl$1.next(DataConverterProcessorStepImpl.java:83) at org.apache.carbondata.processing.loading.sort.impl.ParallelReadMergeSorterImpl$SortIteratorThread.run(ParallelReadMergeSorterImpl.java:218) ... 3 moreDriver stacktrace:18/01/02 11:46:58 INFO DAGScheduler: Job 17 failed: collect at CarbonDataRDDFactory.scala:987, took 0.155142 s18/01/02 11:46:58 ERROR CarbonDataRDDFactory$: pool-23-thread-27 load data frame failedorg.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 30.0 failed 1 times, most recent failure: Lost task 0.0 in stage 30.0 (TID 1034, localhost, executor driver): org.apache.carbondata.processing.loading.exception.CarbonDataLoadingException:  at org.apache.carbondata.processing.loading.sort.AbstractMergeSorter.checkError(AbstractMergeSorter.java:39) at org.apache.carbondata.processing.loading.sort.impl.ParallelReadMergeSorterImpl.sort(ParallelReadMergeSorterImpl.java:117) at org.apache.carbondata.processing.loading.steps.SortProcessorStepImpl.execute(SortProcessorStepImpl.java:62) at org.apache.carbondata.processing.loading.steps.DataWriterProcessorStepImpl.execute(DataWriterProcessorStepImpl.java:87) at org.apache.carbondata.processing.loading.DataLoadExecutor.execute(DataLoadExecutor.java:50) at org.apache.carbondata.spark.rdd.NewDataFrameLoaderRDD$$anon$2.<init>(NewCarbonDataLoadRDD.scala:392) at org.apache.carbondata.spark.rdd.NewDataFrameLoaderRDD.internalCompute(NewCarbonDataLoadRDD.scala:355) at org.apache.carbondata.spark.rdd.CarbonRDD.compute(CarbonRDD.scala:60) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) at org.apache.spark.rdd.RDD.iterator(RDD.scala:287) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87) at org.apache.spark.scheduler.Task.run(Task.scala:99) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748)Caused by: java.lang.IllegalArgumentException: requirement failed: Decimal precision 2922 exceeds max precision 38 at scala.Predef$.require(Predef.scala:224) at org.apache.spark.sql.types.Decimal.set(Decimal.scala:113) at org.apache.spark.sql.types.Decimal$.apply(Decimal.scala:426) at org.apache.spark.sql.types.Decimal.apply(Decimal.scala) at org.apache.spark.sql.catalyst.expressions.UnsafeRow.getDecimal(UnsafeRow.java:409) at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.apply_0$(Unknown Source) at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.apply(Unknown Source) at scala.collection.Iterator$$anon$11.next(Iterator.scala:409) at scala.collection.Iterator$$anon$11.next(Iterator.scala:409) at org.apache.carbondata.spark.rdd.LazyRddIterator.next(NewCarbonDataLoadRDD.scala:514) at org.apache.carbondata.spark.rdd.LazyRddIterator.next(NewCarbonDataLoadRDD.scala:477) at org.apache.carbondata.processing.loading.steps.InputProcessorStepImpl$InputProcessorIterator.getBatch(InputProcessorStepImpl.java:239) at org.apache.carbondata.processing.loading.steps.InputProcessorStepImpl$InputProcessorIterator.next(InputProcessorStepImpl.java:200) at org.apache.carbondata.processing.loading.steps.InputProcessorStepImpl$InputProcessorIterator.next(InputProcessorStepImpl.java:129) at org.apache.carbondata.processing.loading.steps.DataConverterProcessorStepImpl$1.next(DataConverterProcessorStepImpl.java:97) at org.apache.carbondata.processing.loading.steps.DataConverterProcessorStepImpl$1.next(DataConverterProcessorStepImpl.java:83) at org.apache.carbondata.processing.loading.sort.impl.ParallelReadMergeSorterImpl$SortIteratorThread.run(ParallelReadMergeSorterImpl.java:218) ... 3 moreDriver stacktrace: at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435) at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423) at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422) at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59) at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48) at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422) at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802) at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802) at scala.Option.foreach(Option.scala:257) at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802) at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650) at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605) at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594) at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48) at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628) at org.apache.spark.SparkContext.runJob(SparkContext.scala:1918) at org.apache.spark.SparkContext.runJob(SparkContext.scala:1931) at org.apache.spark.SparkContext.runJob(SparkContext.scala:1944) at org.apache.spark.SparkContext.runJob(SparkContext.scala:1958) at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:935) at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151) at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112) at org.apache.spark.rdd.RDD.withScope(RDD.scala:362) at org.apache.spark.rdd.RDD.collect(RDD.scala:934) at org.apache.carbondata.spark.rdd.CarbonDataRDDFactory$.loadDataFrame(CarbonDataRDDFactory.scala:987) at org.apache.carbondata.spark.rdd.CarbonDataRDDFactory$.loadCarbonData(CarbonDataRDDFactory.scala:351) at org.apache.spark.sql.execution.command.management.CarbonLoadDataCommand.loadData(CarbonLoadDataCommand.scala:433) at org.apache.spark.sql.execution.command.management.CarbonLoadDataCommand.processData(CarbonLoadDataCommand.scala:223) at org.apache.spark.sql.execution.command.DataCommand.run(package.scala:71) at org.apache.spark.sql.execution.command.preaaggregate.PreAggregateUtil$.startDataLoadForDataMap(PreAggregateUtil.scala:529) at org.apache.spark.sql.execution.command.preaaggregate.CreatePreAggregateTableCommand.processData(CreatePreAggregateTableCommand.scala:140) at org.apache.spark.sql.execution.command.datamap.CarbonCreateDataMapCommand.processData(CarbonCreateDataMapCommand.scala:116) at org.apache.spark.sql.execution.command.AtomicRunnableCommand.run(package.scala:86) at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58) at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56) at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74) at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114) at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114) at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:135) at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151) at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:132) at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:113) at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:87) at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:87) at org.apache.spark.sql.Dataset.<init>(Dataset.scala:185) at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:64) at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:592) at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:699) at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:220) at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1$$anon$2.run(SparkExecuteStatementOperation.scala:163) at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1$$anon$2.run(SparkExecuteStatementOperation.scala:160) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1698) at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1.run(SparkExecuteStatementOperation.scala:173) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748)Caused by: org.apache.carbondata.processing.loading.exception.CarbonDataLoadingException:  at org.apache.carbondata.processing.loading.sort.AbstractMergeSorter.checkError(AbstractMergeSorter.java:39) at org.apache.carbondata.processing.loading.sort.impl.ParallelReadMergeSorterImpl.sort(ParallelReadMergeSorterImpl.java:117) at org.apache.carbondata.processing.loading.steps.SortProcessorStepImpl.execute(SortProcessorStepImpl.java:62) at org.apache.carbondata.processing.loading.steps.DataWriterProcessorStepImpl.execute(DataWriterProcessorStepImpl.java:87) at org.apache.carbondata.processing.loading.DataLoadExecutor.execute(DataLoadExecutor.java:50) at org.apache.carbondata.spark.rdd.NewDataFrameLoaderRDD$$anon$2.<init>(NewCarbonDataLoadRDD.scala:392) at org.apache.carbondata.spark.rdd.NewDataFrameLoaderRDD.internalCompute(NewCarbonDataLoadRDD.scala:355) at org.apache.carbondata.spark.rdd.CarbonRDD.compute(CarbonRDD.scala:60) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) at org.apache.spark.rdd.RDD.iterator(RDD.scala:287) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87) at org.apache.spark.scheduler.Task.run(Task.scala:99) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282) ... 3 moreCaused by: java.lang.IllegalArgumentException: requirement failed: Decimal precision 2922 exceeds max precision 38 at scala.Predef$.require(Predef.scala:224) at org.apache.spark.sql.types.Decimal.set(Decimal.scala:113) at org.apache.spark.sql.types.Decimal$.apply(Decimal.scala:426) at org.apache.spark.sql.types.Decimal.apply(Decimal.scala) at org.apache.spark.sql.catalyst.expressions.UnsafeRow.getDecimal(UnsafeRow.java:409) at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.apply_0$(Unknown Source) at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.apply(Unknown Source) at scala.collection.Iterator$$anon$11.next(Iterator.scala:409) at scala.collection.Iterator$$anon$11.next(Iterator.scala:409) at org.apache.carbondata.spark.rdd.LazyRddIterator.next(NewCarbonDataLoadRDD.scala:514) at org.apache.carbondata.spark.rdd.LazyRddIterator.next(NewCarbonDataLoadRDD.scala:477) at org.apache.carbondata.processing.loading.steps.InputProcessorStepImpl$InputProcessorIterator.getBatch(InputProcessorStepImpl.java:239) at org.apache.carbondata.processing.loading.steps.InputProcessorStepImpl$InputProcessorIterator.next(InputProcessorStepImpl.java:200) at org.apache.carbondata.processing.loading.steps.InputProcessorStepImpl$InputProcessorIterator.next(InputProcessorStepImpl.java:129) at org.apache.carbondata.processing.loading.steps.DataConverterProcessorStepImpl$1.next(DataConverterProcessorStepImpl.java:97) at org.apache.carbondata.processing.loading.steps.DataConverterProcessorStepImpl$1.next(DataConverterProcessorStepImpl.java:83) at org.apache.carbondata.processing.loading.sort.impl.ParallelReadMergeSorterImpl$SortIteratorThread.run(ParallelReadMergeSorterImpl.java:218) ... 3 more18/01/02 11:46:58 INFO CarbonDataRDDFactory$: pool-23-thread-27 DataLoad failure: 18/01/02 11:46:58 ERROR CarbonDataRDDFactory$: pool-23-thread-27 org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 30.0 failed 1 times, most recent failure: Lost task 0.0 in stage 30.0 (TID 1034, localhost, executor driver): org.apache.carbondata.processing.loading.exception.CarbonDataLoadingException:  at org.apache.carbondata.processing.loading.sort.AbstractMergeSorter.checkError(AbstractMergeSorter.java:39) at org.apache.carbondata.processing.loading.sort.impl.ParallelReadMergeSorterImpl.sort(ParallelReadMergeSorterImpl.java:117) at org.apache.carbondata.processing.loading.steps.SortProcessorStepImpl.execute(SortProcessorStepImpl.java:62) at org.apache.carbondata.processing.loading.steps.DataWriterProcessorStepImpl.execute(DataWriterProcessorStepImpl.java:87) at org.apache.carbondata.processing.loading.DataLoadExecutor.execute(DataLoadExecutor.java:50) at org.apache.carbondata.spark.rdd.NewDataFrameLoaderRDD$$anon$2.<init>(NewCarbonDataLoadRDD.scala:392) at org.apache.carbondata.spark.rdd.NewDataFrameLoaderRDD.internalCompute(NewCarbonDataLoadRDD.scala:355) at org.apache.carbondata.spark.rdd.CarbonRDD.compute(CarbonRDD.scala:60) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) at org.apache.spark.rdd.RDD.iterator(RDD.scala:287) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87) at org.apache.spark.scheduler.Task.run(Task.scala:99) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748)Caused by: java.lang.IllegalArgumentException: requirement failed: Decimal precision 2922 exceeds max precision 38 at scala.Predef$.require(Predef.scala:224) at org.apache.spark.sql.types.Decimal.set(Decimal.scala:113) at org.apache.spark.sql.types.Decimal$.apply(Decimal.scala:426) at org.apache.spark.sql.types.Decimal.apply(Decimal.scala) at org.apache.spark.sql.catalyst.expressions.UnsafeRow.getDecimal(UnsafeRow.java:409) at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.apply_0$(Unknown Source) at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.apply(Unknown Source) at scala.collection.Iterator$$anon$11.next(Iterator.scala:409) at scala.collection.Iterator$$anon$11.next(Iterator.scala:409) at org.apache.carbondata.spark.rdd.LazyRddIterator.next(NewCarbonDataLoadRDD.scala:514) at org.apache.carbondata.spark.rdd.LazyRddIterator.next(NewCarbonDataLoadRDD.scala:477) at org.apache.carbondata.processing.loading.steps.InputProcessorStepImpl$InputProcessorIterator.getBatch(InputProcessorStepImpl.java:239) at org.apache.carbondata.processing.loading.steps.InputProcessorStepImpl$InputProcessorIterator.next(InputProcessorStepImpl.java:200) at org.apache.carbondata.processing.loading.steps.InputProcessorStepImpl$InputProcessorIterator.next(InputProcessorStepImpl.java:129) at org.apache.carbondata.processing.loading.steps.DataConverterProcessorStepImpl$1.next(DataConverterProcessorStepImpl.java:97) at org.apache.carbondata.processing.loading.steps.DataConverterProcessorStepImpl$1.next(DataConverterProcessorStepImpl.java:83) at org.apache.carbondata.processing.loading.sort.impl.ParallelReadMergeSorterImpl$SortIteratorThread.run(ParallelReadMergeSorterImpl.java:218) ... 3 moreDriver stacktrace: at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435) at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423) at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422) at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59) at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48) at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422) at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802) at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802) at scala.Option.foreach(Option.scala:257) at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802) at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650) at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605) at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594) at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48) at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628) at org.apache.spark.SparkContext.runJob(SparkContext.scala:1918) at org.apache.spark.SparkContext.runJob(SparkContext.scala:1931) at org.apache.spark.SparkContext.runJob(SparkContext.scala:1944) at org.apache.spark.SparkContext.runJob(SparkContext.scala:1958) at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:935) at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151) at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112) at org.apache.spark.rdd.RDD.withScope(RDD.scala:362) at org.apache.spark.rdd.RDD.collect(RDD.scala:934) at org.apache.carbondata.spark.rdd.CarbonDataRDDFactory$.loadDataFrame(CarbonDataRDDFactory.scala:987) at org.apache.carbondata.spark.rdd.CarbonDataRDDFactory$.loadCarbonData(CarbonDataRDDFactory.scala:351) at org.apache.spark.sql.execution.command.management.CarbonLoadDataCommand.loadData(CarbonLoadDataCommand.scala:433) at org.apache.spark.sql.execution.command.management.CarbonLoadDataCommand.processData(CarbonLoadDataCommand.scala:223) at org.apache.spark.sql.execution.command.DataCommand.run(package.scala:71) at org.apache.spark.sql.execution.command.preaaggregate.PreAggregateUtil$.startDataLoadForDataMap(PreAggregateUtil.scala:529) at org.apache.spark.sql.execution.command.preaaggregate.CreatePreAggregateTableCommand.processData(CreatePreAggregateTableCommand.scala:140) at org.apache.spark.sql.execution.command.datamap.CarbonCreateDataMapCommand.processData(CarbonCreateDataMapCommand.scala:116) at org.apache.spark.sql.execution.command.AtomicRunnableCommand.run(package.scala:86) at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58) at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56) at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74) at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114) at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114) at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:135) at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151) at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:132) at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:113) at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:87) at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:87) at org.apache.spark.sql.Dataset.<init>(Dataset.scala:185) at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:64) at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:592) at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:699) at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:220) at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1$$anon$2.run(SparkExecuteStatementOperation.scala:163) at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1$$anon$2.run(SparkExecuteStatementOperation.scala:160) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1698) at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1.run(SparkExecuteStatementOperation.scala:173) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748)Caused by: org.apache.carbondata.processing.loading.exception.CarbonDataLoadingException:  at org.apache.carbondata.processing.loading.sort.AbstractMergeSorter.checkError(AbstractMergeSorter.java:39) at org.apache.carbondata.processing.loading.sort.impl.ParallelReadMergeSorterImpl.sort(ParallelReadMergeSorterImpl.java:117) at org.apache.carbondata.processing.loading.steps.SortProcessorStepImpl.execute(SortProcessorStepImpl.java:62) at org.apache.carbondata.processing.loading.steps.DataWriterProcessorStepImpl.execute(DataWriterProcessorStepImpl.java:87) at org.apache.carbondata.processing.loading.DataLoadExecutor.execute(DataLoadExecutor.java:50) at org.apache.carbondata.spark.rdd.NewDataFrameLoaderRDD$$anon$2.<init>(NewCarbonDataLoadRDD.scala:392) at org.apache.carbondata.spark.rdd.NewDataFrameLoaderRDD.internalCompute(NewCarbonDataLoadRDD.scala:355) at org.apache.carbondata.spark.rdd.CarbonRDD.compute(CarbonRDD.scala:60) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) at org.apache.spark.rdd.RDD.iterator(RDD.scala:287) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87) at org.apache.spark.scheduler.Task.run(Task.scala:99) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282) ... 3 moreCaused by: java.lang.IllegalArgumentException: requirement failed: Decimal precision 2922 exceeds max precision 38 at scala.Predef$.require(Predef.scala:224) at org.apache.spark.sql.types.Decimal.set(Decimal.scala:113) at org.apache.spark.sql.types.Decimal$.apply(Decimal.scala:426) at org.apache.spark.sql.types.Decimal.apply(Decimal.scala) at org.apache.spark.sql.catalyst.expressions.UnsafeRow.getDecimal(UnsafeRow.java:409) at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.apply_0$(Unknown Source) at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.apply(Unknown Source) at scala.collection.Iterator$$anon$11.next(Iterator.scala:409) at scala.collection.Iterator$$anon$11.next(Iterator.scala:409) at org.apache.carbondata.spark.rdd.LazyRddIterator.next(NewCarbonDataLoadRDD.scala:514) at org.apache.carbondata.spark.rdd.LazyRddIterator.next(NewCarbonDataLoadRDD.scala:477) at org.apache.carbondata.processing.loading.steps.InputProcessorStepImpl$InputProcessorIterator.getBatch(InputProcessorStepImpl.java:239) at org.apache.carbondata.processing.loading.steps.InputProcessorStepImpl$InputProcessorIterator.next(InputProcessorStepImpl.java:200) at org.apache.carbondata.processing.loading.steps.InputProcessorStepImpl$InputProcessorIterator.next(InputProcessorStepImpl.java:129) at org.apache.carbondata.processing.loading.steps.DataConverterProcessorStepImpl$1.next(DataConverterProcessorStepImpl.java:97) at org.apache.carbondata.processing.loading.steps.DataConverterProcessorStepImpl$1.next(DataConverterProcessorStepImpl.java:83) at org.apache.carbondata.processing.loading.sort.impl.ParallelReadMergeSorterImpl$SortIteratorThread.run(ParallelReadMergeSorterImpl.java:218) ... 3 more18/01/02 11:46:58 INFO HdfsFileLock: pool-23-thread-27 HDFS lock path:hdfs://localhost:54311/opt/carbonStore/28dec/uniqdata_uniqdata_agg/tablestatus.lock18/01/02 11:46:58 INFO CarbonLoaderUtil: pool-23-thread-27 Acquired lock for table28dec.uniqdata_uniqdata_agg for table status updation18/01/02 11:46:58 INFO HdfsFileLock: pool-23-thread-27 Deleted the lock file hdfs://localhost:54311/opt/carbonStore/28dec/uniqdata_uniqdata_agg/tablestatus.lock18/01/02 11:46:58 INFO CarbonLoaderUtil: pool-23-thread-27 Table unlocked successfully after table status updation28dec.uniqdata_uniqdata_agg18/01/02 11:46:58 INFO CarbonDataRDDFactory$: pool-23-thread-27 *******starting clean up*********18/01/02 11:46:58 INFO CarbonDataRDDFactory$: pool-23-thread-27 *******clean up done*********18/01/02 11:46:58 AUDIT CarbonDataRDDFactory$: &#91;geetika-Vostro-15-3568&#93;&#91;geetika&#93;&#91;Thread-1655&#93;Data load is failed for 28dec.uniqdata_uniqdata_agg18/01/02 11:46:58 WARN CarbonDataRDDFactory$: pool-23-thread-27 Cannot write load metadata file as data load failed18/01/02 11:46:58 ERROR CarbonLoadDataCommand: pool-23-thread-27 java.lang.Exception: DataLoad failure:  at org.apache.carbondata.spark.rdd.CarbonDataRDDFactory$.loadCarbonData(CarbonDataRDDFactory.scala:491) at org.apache.spark.sql.execution.command.management.CarbonLoadDataCommand.loadData(CarbonLoadDataCommand.scala:433) at org.apache.spark.sql.execution.command.management.CarbonLoadDataCommand.processData(CarbonLoadDataCommand.scala:223) at org.apache.spark.sql.execution.command.DataCommand.run(package.scala:71) at org.apache.spark.sql.execution.command.preaaggregate.PreAggregateUtil$.startDataLoadForDataMap(PreAggregateUtil.scala:529) at org.apache.spark.sql.execution.command.preaaggregate.CreatePreAggregateTableCommand.processData(CreatePreAggregateTableCommand.scala:140) at org.apache.spark.sql.execution.command.datamap.CarbonCreateDataMapCommand.processData(CarbonCreateDataMapCommand.scala:116) at org.apache.spark.sql.execution.command.AtomicRunnableCommand.run(package.scala:86) at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58) at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56) at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74) at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114) at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114) at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:135) at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151) at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:132) at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:113) at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:87) at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:87) at org.apache.spark.sql.Dataset.<init>(Dataset.scala:185) at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:64) at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:592) at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:699) at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:220) at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1$$anon$2.run(SparkExecuteStatementOperation.scala:163) at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1$$anon$2.run(SparkExecuteStatementOperation.scala:160) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1698) at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1.run(SparkExecuteStatementOperation.scala:173) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748)
issueID:CARBONDATA-1958
type:Bug
changed files:
texts:CarbonSqlCliDriver not show result in case of select query
1.start the carbonsparksqlcli drivergo to bin folder of carbon project and execute command./carbon-spark-sql2.create carbon data tablespark-sql> create table sparktable(id int)stored by 'carbondata';3.load data in carbontablespark-sql> insert into sparktable values(1);4.query the data18/01/02 12:06:45 INFO TableInfo: main Table block size not specified for default_sparktable. Therefore considering the default value 1024 MB18/01/02 12:06:45 INFO BlockletDataMap: main Time taken to load blocklet datamap from file : /home/anubhav/Documents/carbondata/carbondata/bin/carbonsqlclistore/default/sparktable/Fact/Part0/Segment_0/0_batchno0-0-1514874975013.carbonindexis 1918/01/02 12:06:45 INFO CarbonScanRDD:  Identified no.of.blocks: 0, no.of.tasks: 0, no.of.nodes: 0, parallelism: 418/01/02 12:06:45 INFO SparkContext: Starting job: processCmd at CliDriver.java:37618/01/02 12:06:45 INFO DAGScheduler: Job 4 finished: processCmd at CliDriver.java:376, took 0.000073 sTime taken: 0.378 seconds18/01/02 12:06:45 INFO CliDriver: Time taken: 0.378 secondshere no records get displayed
issueID:CARBONDATA-1959
type:Task
changed files:
texts:Support compaction on S3 table

issueID:CARBONDATA-1960
type:Task
changed files:
texts:Add example for creating a local table and load CSV data which is stored in S3.

issueID:CARBONDATA-1961
type:Task
changed files:
texts:Support data update/delete on S3 table

issueID:CARBONDATA-1962
type:Task
changed files:
texts:Support alter table add columns/drop columns on S3 table

issueID:CARBONDATA-1963
type:Task
changed files:
texts:Support S3 table with dictionary

issueID:CARBONDATA-1964
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/util/SessionParams.java
texts:SET command does not set the parameters correctly
I created the following table: CREATE TABLE uniqdata(CUST_ID int,CUST_NAME String,ACTIVE_EMUI_VERSION string, DOB timestamp, DOJ timestamp, BIGINT_COLUMN1 bigint,BIGINT_COLUMN2 bigint,DECIMAL_COLUMN1 decimal(30,10), DECIMAL_COLUMN2 decimal(36,10),Double_COLUMN1 double, Double_COLUMN2 double,INTEGER_COLUMN1 int) STORED BY 'org.apache.carbondata.format' TBLPROPERTIES('DICTIONARY_INCLUDE'='CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1,Double_COLUMN2,INTEGER_COLUMN1');then I set the parameter carbon.options.bad.records.action using: *set carbon.options.bad.records.action=fail; * Load command: LOAD DATA INPATH 'hdfs://localhost:54311/Files/2000_UniqData.csv' into table uniqdata OPTIONS('DELIMITER'=',', 'QUOTECHAR'='"','FILEHEADER'='CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1,Double_COLUMN2,INTEGER_COLUMN1','timestampformat'='dd/mm/yyyy'); ---------+Result---------+ ---------+ No rows selected (1.43 seconds)The load executed successfully. However, no data is loaded into the table due to mismatch of timestamp format.Then I again set the parameter carbon.options.bad.records.action using: *set carbon.options.bad.records.action=FAIL; * This time the same load command gave me the following exception:Error: java.lang.Exception: Data load failed due to bad record: The value with column name dob and column data type TIMESTAMP is not a valid TIMESTAMP type.Please enable bad record logger to know the detail reason. (state=,code=0)The first case should behave in the same as manner as the second case. So the SET command does not set the parameter values correctly and it does not even throw an exception when the value is not set correctly.
issueID:CARBONDATA-1965
type:Bug
changed files:
texts:SET command is not setting the parameter carbon.options.sort.scope
I use set command as:set carbon.options.sort.scope=GLOBAL_SORT;then I create table as create table xyz_sort1(id int) stored by 'carbondata';use desc command:desc formatted xyz_sort1;-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+               col_name                                                     data_type                                                                       comment                                  -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ id                                     int                                                                                MEASURE,null                                                                  ##Detailed Table Information             Database Name                          28dec                                                                               Table Name                             xyz_sort1                                                                           CARBON Store Path                      hdfs://localhost:54311/opt/carbonStore                                              Comment                                  Table Block Size                       1024 MB                                                                             Table Data Size                        0                                                                                   Table Index Size                       0                                                                                   Last Update Time                       0                                                                                   SORT_SCOPE                             LOCAL_SORT                                                                         LOCAL_SORT                                                                 Streaming                              false                                                                               SORT_SCOPE                             LOCAL_SORT                                                                         LOCAL_SORT                                                                    ##Detailed Column property               ADAPTIVE                                 SORT_COLUMNS                            -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+18 rows selected (0.169 seconds)The sort_scope for table should be global_sort
issueID:CARBONDATA-1966
type:Bug
changed files:
texts:SET command for carbon.properties.filepath is not setting the property
I executed the following command:set carbon.properties.filepath=/homeIt is executed successfully, but it's value is not updated on Spark UI in spark properties.It shows the following output:jdbc:hive2://localhost:10000> set carbon.properties.filepath;-----------------------------------             key              value  ----------------------------------- carbon.properties.filepath   /home  -----------------------------------1 row selected (0.029 seconds)
issueID:CARBONDATA-1967
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/util/DeleteLoadFolders.java
core/src/main/java/org/apache/carbondata/core/util/path/CarbonTablePath.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonOutputCommitter.java
texts:Auto compaction is not working for partition table. And carbon indexfiles are merging even after configured as false
Auto compaction is not working for partition table. Auto compaction is not working for partition table. And carbon indexfiles are merging even after configured as false
issueID:CARBONDATA-1968
type:New Feature
changed files:core/src/main/java/org/apache/carbondata/core/metadata/schema/table/CarbonTable.java
texts:Support external table
User should be able to create external table with existing carbondata files
issueID:CARBONDATA-1970
type:Improvement
changed files:
texts:(Carbon1.3.0 - Spark 2.2) Use Spark 2.2.1 as default version for Profile Spark-2.2
Spark 2.2.1 had released for a while, and there were more than 200+ issues fixed, so it's better to replace spark 2.2.0 to 2.2.1 for profile spark-2.2.Reminder: would not support both 2.2.0 and 2.2.1.
issueID:CARBONDATA-1972
type:Bug
changed files:hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonOutputCommitter.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonTableOutputFormat.java
texts:Compaction after update of whole data fails in partition table.
After updation of whole data and then if we do compaction then it fails.
issueID:CARBONDATA-1973
type:Bug
changed files:
texts:User Should not Be able to give the duplicate column name in partition even if its case sensitive
1.carbon.sql("CREATE TABLE uniqdata_char2(name char,id int) partitioned by (NAME char)stored by 'carbondata' ") name &#91;uniqdata_char2&#93;18/01/03 12:44:44 WARN HiveExternalCatalog: Couldn't find corresponding Hive SerDe for data source provider org.apache.spark.sql.CarbonSource. Persisting data source table `default`.`uniqdata_char2` into Hive metastore in Spark SQL specific format, which is NOT compatible with Hive.18/01/03 12:44:44 AUDIT CarbonCreateTableCommand: &#91;anubhav-Vostro-3559&#93;&#91;anubhav&#93;&#91;Thread-1&#93;Table created with Database name &#91;default&#93; and Table name &#91;uniqdata_char2&#93;res30: org.apache.spark.sql.DataFrame = []as we can see table get created successfully2.try same thing on hivecarbon.sql("CREATE TABLE uniqdata_char2_hive(name char,id int) partitioned by (NAME char) ")it gives exceptionorg.apache.spark.sql.AnalysisException: Found duplicate column(s) in table definition of `uniqdata_char2_hive`: name;  at org.apache.spark.sql.execution.datasources.AnalyzeCreateTable.org$apache$spark$sql$execution$datasources$AnalyzeCreateTable$$failAnalysis(rules.scala:198)behaviour of carbondata should be similiar to hive
issueID:CARBONDATA-1974
type:Bug
changed files:
texts:Exception when to load data using static partition for uniqdata table
1.CREATE TABLE uniqdata_string(CUST_ID int,CUST_NAME String,DOB timestamp,DOJ timestamp, BIGINT_COLUMN1 bigint,BIGINT_COLUMN2 bigint,DECIMAL_COLUMN1 decimal(30,10),DECIMAL_COLUMN2 decimal(36,10),Double_COLUMN1 double, Double_COLUMN2 double,INTEGER_COLUMN1 int) PARTITIONED BY(ACTIVE_EMUI_VERSION string) STORED BY 'org.apache.carbondata.format' TBLPROPERTIES ('TABLE_BLOCKSIZE'= '256 MB');2,jdbc:hive2://localhost:10000/default> LOAD DATA INPATH 'hdfs://localhost:54311/2000_UniqData.csv' into table uniqdata_string partition(ACTIVE_EMUI_VERSION='abc') OPTIONS('FILEHEADER'='CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ, BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1, Double_COLUMN2,INTEGER_COLUMN1','BAD_RECORDS_ACTION'='FORCE');Error: org.apache.spark.sql.AnalysisException: Cannot insert into table `default`.`uniqdata_string` because the number of columns are different: need 11 columns, but query has 12 columns.; (state=,code=0)
issueID:CARBONDATA-1975
type:Bug
changed files:
texts:Wrong input metrics displayed for carbon
Input metrics is updated twice. Record count is updated twice and it is wrongly displayed in Spark UI
issueID:CARBONDATA-1976
type:Bug
changed files:hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonOutputCommitter.java
texts:Support combination of dynamic and static partitions. And fix concurrent partition load issue.
Support combination of dynamic and static partitions. Like user can give as followssql(s"""LOAD DATA local inpath '$resourcesPath/data.csv' INTO TABLE loadstaticpartitiondynamic PARTITION(empno='1', empname) OPTIONS('DELIMITER'= ',', 'QUOTECHAR'= '"')""") And fix the concurrent partition load issue as sometimes it gives file not found exception while deleting temporary folders.
issueID:CARBONDATA-1977
type:Bug
changed files:core/src/main/java/org/apache/carbondata/events/OperationContext.java
processing/src/main/java/org/apache/carbondata/processing/loading/events/LoadEvents.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonOutputCommitter.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonTableOutputFormat.java
texts:Aggregate table loading is working in partition table
During loading of partition table aggregate tables are loading.
issueID:CARBONDATA-1978
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/metadata/schema/table/CarbonTable.java
core/src/main/java/org/apache/carbondata/core/metadata/converter/ThriftWrapperSchemaConverterImpl.java
core/src/main/java/org/apache/carbondata/core/util/DataTypeUtil.java
texts:Preaggregate table loading failed when using HiveMetastore
spark.carbon.hive.schema.store1. set spark.carbon.hive.schema.store=true in carbon properties2. create main table 3. load data into maintable4. create pre-aggregate tableException:Caused by: java.lang.IllegalArgumentException: unsupported data type:INT at org.apache.carbondata.processing.sort.sortdata.SortDataRows.writeData(SortDataRows.java:314) at org.apache.carbondata.processing.sort.sortdata.SortDataRows.writeDataTofile(SortDataRows.java:235) at org.apache.carbondata.processing.sort.sortdata.SortDataRows.startSorting(SortDataRows.java:215) at org.apache.carbondata.processing.loading.sort.impl.ParallelReadMergeSorterImpl.processRowToNextStep(ParallelReadMergeSorterImpl.java:174) at org.apache.carbondata.processing.loading.sort.impl.ParallelReadMergeSorterImpl.sort(ParallelReadMergeSorterImpl.java:112)
issueID:CARBONDATA-1979
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/scan/filter/ColumnFilterInfo.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockletDataMap.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/ImplicitIncludeFilterExecutorImpl.java
texts:implicit column filtering logic to directly validate the blocklet ID instead of Block

issueID:CARBONDATA-198
type:Bug
changed files:integration/spark-common/src/main/java/org/apache/carbondata/spark/merger/CarbonCompactionUtil.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
integration/spark-common/src/main/java/org/apache/carbondata/spark/merger/CarbonDataMergerUtil.java
texts:Implementing system level lock for compaction.
1. making DDL of compaction as blocking call.2. implemented System level compaction locking. only one compaction allowed , other requests will create compaction request file. 3. Added configuration "carbon.concurrent.compaction" default to false4. When compaction is in progress and another request for compaction comes, request is queued and taken after current compaction complete.
issueID:CARBONDATA-1980
type:Bug
changed files:
texts:Partition information is added while restore or refresh the table. And also query is not working if there is nay upper case letter in partition column.
Partition information is added while restore or refresh the table. And also query is not working if there is nay upper case letter in partition column.
issueID:CARBONDATA-1981
type:Bug
changed files:
texts:Error occurs while building project in windows environment
Encounter error while building project in Windows environment, the error messages are shown as bellow:```&#91;INFO&#93; Apache CarbonData :: Parent ........................ SUCCESS [ 3.915 s]&#91;INFO&#93; Apache CarbonData :: Common ........................ SUCCESS [ 12.007 s]&#91;INFO&#93; Apache CarbonData :: Format ........................ SUCCESS [ 30.059 s]&#91;INFO&#93; Apache CarbonData :: Core .......................... FAILURE [ 3.604 s]&#91;INFO&#93; ------------------------------------------------------------------------&#91;INFO&#93; BUILD FAILURE&#91;INFO&#93; ------------------------------------------------------------------------&#91;INFO&#93; Total time: 50.099 s&#91;INFO&#93; Finished at: 2018-01-04T11:19:28+08:00&#91;INFO&#93; Final Memory: 62M/612M&#91;INFO&#93; ------------------------------------------------------------------------&#91;ERROR&#93; Failed to execute goal org.apache.maven.plugins:maven-antrun-plugin:1.8:run (default) on project carbondata-core: An Ant BuildException has occured: Execute failed: java.io.IOException: Cannot run program "bash" (in directory "D:\01_workspace\carbondata\core"): CreateProcess error=2, 系统找不到指定的文件。(cannot find the specified file)&#91;ERROR&#93; around Ant part ...... @ 4:27 in D:\01_workspace\carbondata\core\target\antrun\build-main.xml&#91;ERROR&#93; -> &#91;Help 1&#93;```It is because in the pom, it calls bash script, which is not incompatible in windows.
issueID:CARBONDATA-1982
type:Bug
changed files:
texts:Loading data into partition table with invalid partition column should throw proper exception
I created a partitioned table using: CREATE TABLE uniqdata_int_dec(CUST_NAME String,ACTIVE_EMUI_VERSION string, DOB timestamp, DOJ timestamp, BIGINT_COLUMN1 bigint,BIGINT_COLUMN2 bigint, DECIMAL_COLUMN2 decimal(36,10),Double_COLUMN1 double, Double_COLUMN2 double, INTEGER_COLUMN1 int) Partitioned by (cust_id int, decimal_column1 decimal(30,10)) STORED BY 'org.apache.carbondata.format' TBLPROPERTIES ("TABLE_BLOCKSIZE"= "256 MB")Load data command: LOAD DATA INPATH 'hdfs://localhost:54311/2000_UniqData.csv' into table uniqdata_int_dec partition(cust_id123='1', abc='12345678901.1234') OPTIONS ('FILEHEADER'='CUST_ID,CUST_NAME ,ACTIVE_EMUI_VERSION,DOB,DOJ, BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1, Double_COLUMN2,INTEGER_COLUMN1','BAD_RECORDS_ACTION'='FORCE');OUTPUT: 0: jdbc:hive2://localhost:10000> LOAD DATA INPATH 'hdfs://localhost:54311/2000_UniqData.csv' into table uniqdata_int_dec partition(cust_id123='1', decimal_column1='12345678901.1234') OPTIONS ('FILEHEADER'='CUST_ID,CUST_NAME ,ACTIVE_EMUI_VERSION,DOB,DOJ, BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1, Double_COLUMN2,INTEGER_COLUMN1','BAD_RECORDS_ACTION'='FORCE'); Error: java.lang.IndexOutOfBoundsException: Index: 1, Size: 1 (state=,code=0)The above command throws java.lang.IndexOutOfBoundsException: Index: 1, Size: 1 whereas it should throw a proper exception like invalid column expression for partition load command.
issueID:CARBONDATA-1983
type:Improvement
changed files:processing/src/main/java/org/apache/carbondata/processing/store/writer/v3/CarbonFactDataWriterImplV3.java
processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactDataHandlerColumnar.java
processing/src/main/java/org/apache/carbondata/processing/store/CarbonDataWriterFactory.java
processing/src/main/java/org/apache/carbondata/processing/store/writer/AbstractFactDataWriter.java
texts:Remove unnecessary WriterVo creation

issueID:CARBONDATA-1984
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/adaptive/AdaptiveFloatingCodec.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/adaptive/AdaptiveDeltaFloatingCodec.java
texts:Double datatype Compression Bug
Double datatype compression is going wrong as multiplying the value with a factor is giving undefined result. As both the variables are double, multiplying doubles doesnt guarantee exact same expected result every time. So while decoding we are getting some other output which is not expected.
issueID:CARBONDATA-1985
type:Bug
changed files:
texts:Insert into failed for multi partitioned table for static partition
I created a table using:CREATE TABLE uniqdata_int_string(ACTIVE_EMUI_VERSION string, DOB timestamp,DOJ timestamp, BIGINT_COLUMN1 bigint,BIGINT_COLUMN2 bigint,DECIMAL_COLUMN1 decimal(30,10),DECIMAL_COLUMN2 decimal(36,10),Double_COLUMN1 double, Double_COLUMN2 double,INTEGER_COLUMN1 int) Partitioned by (cust_id int, cust_name string) STORED BY 'org.apache.carbondata.format' TBLPROPERTIES ("TABLE_BLOCKSIZE"= "256 MB")Hive create and load table command:CREATE TABLE uniqdata_hive (CUST_ID int,CUST_NAME String,ACTIVE_EMUI_VERSION string, DOB timestamp,DOJ timestamp, BIGINT_COLUMN1 bigint,BIGINT_COLUMN2 bigint,DECIMAL_COLUMN1 decimal(30,10),DECIMAL_COLUMN2 decimal(36,10),Double_COLUMN1 double, Double_COLUMN2 double,INTEGER_COLUMN1 int)ROW FORMAT DELIMITEDFIELDS TERMINATED BY ',';LOAD DATA LOCAL INPATH 'file:///home/geetika/Downloads/2000_UniqData.csv' into table UNIQDATA_HIVE;  Insert into table command:insert into uniqdata_int_string partition(cust_id='1',cust_name='CUST_NAME_00002') select * from uniqdata_hive limit 10;Output:Error: java.lang.IndexOutOfBoundsException: Index: 4, Size: 4 (state=,code=0)Here are the logs:18/01/04 16:24:45 ERROR CarbonLoadDataCommand: pool-23-thread-6 org.apache.spark.sql.AnalysisException: Cannot insert into table `28dec`.`uniqdata_int_string` because the number of columns are different: need 10 columns, but query has 12 columns.; at org.apache.spark.sql.execution.datasources.PreprocessTableInsertion.org$apache$spark$sql$execution$datasources$PreprocessTableInsertion$$preprocess(rules.scala:222) at org.apache.spark.sql.execution.datasources.PreprocessTableInsertion$$anonfun$apply$3.applyOrElse(rules.scala:280) at org.apache.spark.sql.execution.datasources.PreprocessTableInsertion$$anonfun$apply$3.applyOrElse(rules.scala:272) at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:288) at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:288) at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70) at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:287) at org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:277) at org.apache.spark.sql.execution.datasources.PreprocessTableInsertion.apply(rules.scala:272) at org.apache.spark.sql.execution.datasources.PreprocessTableInsertion.apply(rules.scala:207) at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:85) at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:82) at scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:124) at scala.collection.immutable.List.foldLeft(List.scala:84) at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:82) at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:74) at scala.collection.immutable.List.foreach(List.scala:381) at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:74) at org.apache.spark.sql.hive.CarbonAnalyzer.execute(CarbonSessionState.scala:242) at org.apache.spark.sql.hive.CarbonAnalyzer.execute(CarbonSessionState.scala:237) at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:64) at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:62) at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:50) at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:63) at org.apache.spark.sql.execution.command.management.CarbonLoadDataCommand.loadDataWithPartition(CarbonLoadDataCommand.scala:641) at org.apache.spark.sql.execution.command.management.CarbonLoadDataCommand.loadData(CarbonLoadDataCommand.scala:431) at org.apache.spark.sql.execution.command.management.CarbonLoadDataCommand.processData(CarbonLoadDataCommand.scala:223) at org.apache.spark.sql.execution.command.DataCommand.run(package.scala:71) at org.apache.spark.sql.execution.command.management.CarbonInsertIntoCommand.processData(CarbonInsertIntoCommand.scala:48) at org.apache.spark.sql.execution.command.DataCommand.run(package.scala:71) at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58) at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56) at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74) at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114) at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114) at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:135) at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151) at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:132) at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:113) at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:87) at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:87) at org.apache.spark.sql.Dataset.<init>(Dataset.scala:185) at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:64) at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:592) at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:699) at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:220) at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1$$anon$2.run(SparkExecuteStatementOperation.scala:163) at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1$$anon$2.run(SparkExecuteStatementOperation.scala:160) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1698) at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1.run(SparkExecuteStatementOperation.scala:173) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748)P.S: Load command using static partition was successfulLOAD DATA INPATH 'hdfs://localhost:54311/Files/2000_UniqData.csv' into table uniqdata_int_string partition(cust_id='1', cust_name='CUST_NAME_00002') OPTIONS ('FILEHEADER'='CUST_ID,CUST_NAME ,ACTIVE_EMUI_VERSION,DOB,DOJ, BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1, Double_COLUMN2,INTEGER_COLUMN1','BAD_RECORDS_ACTION'='FORCE');
issueID:CARBONDATA-1986
type:Bug
changed files:
texts:Insert over write into partitioned table with dynamic partition throws error
Create table command:CREATE TABLE uniqdata_string(CUST_ID int,CUST_NAME String,DOB timestamp,DOJ timestamp, BIGINT_COLUMN1 bigint,BIGINT_COLUMN2 bigint,DECIMAL_COLUMN1 decimal(30,10),DECIMAL_COLUMN2 decimal(36,10),Double_COLUMN1 double, Double_COLUMN2 double,INTEGER_COLUMN1 int) PARTITIONED BY(ACTIVE_EMUI_VERSION string) STORED BY 'org.apache.carbondata.format' TBLPROPERTIES ('TABLE_BLOCKSIZE'= '256 MB');Hive command:CREATE TABLE uniqdata_hive (CUST_ID int,CUST_NAME String,ACTIVE_EMUI_VERSION string, DOB timestamp,DOJ timestamp, BIGINT_COLUMN1 bigint,BIGINT_COLUMN2 bigint,DECIMAL_COLUMN1 decimal(30,10),DECIMAL_COLUMN2 decimal(36,10),Double_COLUMN1 double, Double_COLUMN2 double,INTEGER_COLUMN1 int)ROW FORMAT DELIMITEDFIELDS TERMINATED BY ',';LOAD DATA LOCAL INPATH 'file:///home/anubhav/Downloads/csv/2000_UniqData.csv' into table UNIQDATA_HIVE;  Insert overwrite command:0: jdbc:hive2://localhost:10000> insert overwrite table uniqdata_string partition(active_emui_version) select CUST_ID, CUST_NAME,DOB,doj, bigint_column1, bigint_column2, decimal_column1, decimal_column2,double_column1, double_column2,integer_column1 from uniqdata_hive limit 10;Error: java.lang.IndexOutOfBoundsException: Index: 7, Size: 7 (state=,code=0)Logs:18/01/04 18:12:27 ERROR CarbonLoadDataCommand: pool-23-thread-4 java.util.NoSuchElementException: None.get at scala.None$.get(Option.scala:347) at scala.None$.get(Option.scala:345) at org.apache.spark.sql.execution.command.management.CarbonLoadDataCommand$$anonfun$overwritePartition$1.apply(CarbonLoadDataCommand.scala:707) at org.apache.spark.sql.execution.command.management.CarbonLoadDataCommand$$anonfun$overwritePartition$1.apply(CarbonLoadDataCommand.scala:707) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234) at scala.collection.immutable.Map$Map1.foreach(Map.scala:116) at scala.collection.TraversableLike$class.map(TraversableLike.scala:234) at scala.collection.AbstractTraversable.map(Traversable.scala:104) at org.apache.spark.sql.execution.command.management.CarbonLoadDataCommand.overwritePartition(CarbonLoadDataCommand.scala:707) at org.apache.spark.sql.execution.command.management.CarbonLoadDataCommand.loadDataWithPartition(CarbonLoadDataCommand.scala:639) at org.apache.spark.sql.execution.command.management.CarbonLoadDataCommand.loadData(CarbonLoadDataCommand.scala:431) at org.apache.spark.sql.execution.command.management.CarbonLoadDataCommand.processData(CarbonLoadDataCommand.scala:223) at org.apache.spark.sql.execution.command.DataCommand.run(package.scala:71) at org.apache.spark.sql.execution.command.management.CarbonInsertIntoCommand.processData(CarbonInsertIntoCommand.scala:48) at org.apache.spark.sql.execution.command.DataCommand.run(package.scala:71) at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58) at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56) at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74) at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114) at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114) at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:135) at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151) at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:132) at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:113) at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:87) at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:87) at org.apache.spark.sql.Dataset.<init>(Dataset.scala:185) at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:64) at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:592) at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:699) at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:220) at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1$$anon$2.run(SparkExecuteStatementOperation.scala:163) at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1$$anon$2.run(SparkExecuteStatementOperation.scala:160) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1698) at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1.run(SparkExecuteStatementOperation.scala:173) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748)18/01/04 18:12:27 INFO HdfsFileLock: pool-23-thread-4 HDFS lock path:hdfs://localhost:54311/opt/carbonStore/28dec/uniqdata_string/tablestatus.lock18/01/04 18:12:27 INFO CarbonLoaderUtil: pool-23-thread-4 Acquired lock for table28dec.uniqdata_string for table status updation18/01/04 18:12:27 INFO HdfsFileLock: pool-23-thread-4 Deleted the lock file hdfs://localhost:54311/opt/carbonStore/28dec/uniqdata_string/tablestatus.lock18/01/04 18:12:27 INFO CarbonLoaderUtil: pool-23-thread-4 Table unlocked successfully after table status updation28dec.uniqdata_string18/01/04 18:12:27 ERROR SparkExecuteStatementOperation: Error executing query, currentState RUNNING, java.lang.IndexOutOfBoundsException: Index: 7, Size: 7 at java.util.ArrayList.rangeCheck(ArrayList.java:657) at java.util.ArrayList.get(ArrayList.java:433) at org.apache.carbondata.processing.util.CarbonLoaderUtil.recordNewLoadMetadata(CarbonLoaderUtil.java:226) at org.apache.carbondata.processing.util.CarbonLoaderUtil.updateTableStatusForFailure(CarbonLoaderUtil.java:410) at org.apache.spark.sql.execution.command.management.CarbonLoadDataCommand.processData(CarbonLoadDataCommand.scala:248) at org.apache.spark.sql.execution.command.DataCommand.run(package.scala:71) at org.apache.spark.sql.execution.command.management.CarbonInsertIntoCommand.processData(CarbonInsertIntoCommand.scala:48) at org.apache.spark.sql.execution.command.DataCommand.run(package.scala:71) at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58) at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56) at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74) at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114) at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114) at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:135) at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151) at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:132) at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:113) at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:87) at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:87) at org.apache.spark.sql.Dataset.<init>(Dataset.scala:185) at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:64) at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:592) at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:699) at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:220) at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1$$anon$2.run(SparkExecuteStatementOperation.scala:163) at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1$$anon$2.run(SparkExecuteStatementOperation.scala:160) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1698) at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1.run(SparkExecuteStatementOperation.scala:173) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748)
issueID:CARBONDATA-1987
type:Bug
changed files:
texts:Make package name and directory paths consistent;remove duplicate file CarbonColumnValidator
synchronize file path and package name;Delete duplicate class CarbonColumnValidator present in spark2 moduleAdd the missing coveralls repoToken
issueID:CARBONDATA-1988
type:Bug
changed files:
texts:Drop partition is not removing the partition folder from hdfs
I created table using: CREATE TABLE uniqdata_string(CUST_ID int,CUST_NAME String,DOB timestamp,DOJ timestamp, BIGINT_COLUMN1 bigint,BIGINT_COLUMN2 bigint,DECIMAL_COLUMN1 decimal(30,10),DECIMAL_COLUMN2 decimal(36,10),Double_COLUMN1 double, Double_COLUMN2 double,INTEGER_COLUMN1 int) PARTITIONED BY(ACTIVE_EMUI_VERSION string) STORED BY 'org.apache.carbondata.format' TBLPROPERTIES ('TABLE_BLOCKSIZE'= '256 MB');Load command: LOAD DATA INPATH 'hdfs://localhost:54311/2000_UniqData.csv'  into table uniqdata_string partition(active_emui_version='abc') OPTIONS('FILEHEADER'='CUST_ID,CUST_NAME ,ACTIVE_EMUI_VERSION,DOB,DOJ, BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1, Double_COLUMN2,INTEGER_COLUMN1','BAD_RECORDS_ACTION'='FORCE');LOAD DATA INPATH 'hdfs://localhost:54311/2000_UniqData.csv'  into table uniqdata_string partition(active_emui_version='xyz') OPTIONS('FILEHEADER'='CUST_ID,CUST_NAME ,ACTIVE_EMUI_VERSION,DOB,DOJ, BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1, Double_COLUMN2,INTEGER_COLUMN1','BAD_RECORDS_ACTION'='FORCE');show partitions command: show partitions uniqdata_string;Output: 0: jdbc:hive2://localhost:10000> show partitions uniqdata_string; ----------------------------+partition----------------------------+active_emui_version=abcactive_emui_version=xyz----------------------------+drop partition: alter table uniqdata_string drop partition(active_emui_version='xyz');show partitions: 0: jdbc:hive2://localhost:10000> show partitions uniqdata_string; ----------------------------+partition----------------------------+active_emui_version=abc----------------------------+The partition is successfully dropped but it's folder is not deleted from hdfs.
issueID:CARBONDATA-1989
type:Bug
changed files:
texts:Drop partition is dropping table data
I created table using: CREATE TABLE uniqdata_string_new(CUST_ID int,CUST_NAME String,DOB timestamp,DOJ timestamp, BIGINT_COLUMN1 bigint,BIGINT_COLUMN2 bigint,DECIMAL_COLUMN1 decimal(30,10),DECIMAL_COLUMN2 decimal(36,10),Double_COLUMN1 double, Double_COLUMN2 double,INTEGER_COLUMN1 int) PARTITIONED BY(ACTIVE_EMUI_VERSION string) STORED BY 'org.apache.carbondata.format' TBLPROPERTIES ('TABLE_BLOCKSIZE'= '256 MB');Load command: LOAD DATA INPATH 'hdfs://localhost:54311/2000_UniqData.csv'  into table uniqdata_string_new partition(active_emui_version='abc') OPTIONS('FILEHEADER'='CUST_ID,CUST_NAME ,ACTIVE_EMUI_VERSION,DOB,DOJ, BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1, Double_COLUMN2,INTEGER_COLUMN1','BAD_RECORDS_ACTION'='FORCE');LOAD DATA INPATH 'hdfs://localhost:54311/2000_UniqData.csv'  into table uniqdata_string_new partition(active_emui_version='xyz') OPTIONS('FILEHEADER'='CUST_ID,CUST_NAME ,ACTIVE_EMUI_VERSION,DOB,DOJ, BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1, Double_COLUMN2,INTEGER_COLUMN1','BAD_RECORDS_ACTION'='FORCE');show partitions command: show partitions uniqdata_string_new;Output: 0: jdbc:hive2://localhost:10000> show partitions uniqdata_string_new; --------------------------+ partition --------------------------+ active_emui_version=abc active_emui_version=xyz --------------------------+select query: 0: jdbc:hive2://localhost:10000> select count from uniqdata_string_new; -----------+count(1)-----------+4026-----------+Drop partition query: alter table uniqdata_string_new drop partition(active_emui_version='xyz');show partitions uniqdata_string_new; --------------------------+partition--------------------------+active_emui_version=abc--------------------------+Select query: 0: jdbc:hive2://localhost:10000> select count from uniqdata_string_new; -----------+count(1)-----------+2013-----------+Previously there were 4026 records and after dropping the partition, the records got reduced to 2013.
issueID:CARBONDATA-199
type:Bug
changed files:
texts:when subquery with sort and filter the result is empty
When the query has subquery with sort and filter, it can not return resullt
issueID:CARBONDATA-1990
type:Bug
changed files:
texts:Null values shown when the basic word count example is tried on carbon streaming table
import org.apache.spark.sql.SparkSessionimport org.apache.spark.sql.CarbonSession._import org.apache.spark.sql.types._import org.apache.spark.sql.{AnalysisException, DataFrame, DataFrameWriter, Row, SaveMode}val carbon = SparkSession.builder().config(sc.getConf) .getOrCreateCarbonSession("hdfs://localhost:54311/newCarbonStore","/tmp") carbon.sql("drop table if exists stream_table_csvSource") carbon.sql("create table stream_table_csvSource(word string,count string) stored by 'carbondata' TBLPROPERTIES('streaming'='true')")import carbon.sqlContext.implicits._val userSchema = StructType(Array(StructField("name", StringType)))val lines = carbon.readStream.format("csv").option("path", "file:///home/anubhav/Documents/csv").option("sep",",").schema(userSchema).load()lines.printSchemaval words = lines.as&#91;String&#93;.flatMap(_.split("\n"))val wordCounts = words.groupBy("value").count()val qry = wordCounts.writeStream              .outputMode("complete")             .format("carbondata")             .option("checkpointLocation","file/home/anubhav/Downloads/checkpointlist155")            .option("dbName", "default")            .option("tableName", "stream_table_csvSource")            .start()          qry.awaitTermination()then i close my shell and select data from table there are null values shown  scala> carbon.sql("select * from stream_table_csvSource").show18/01/05 14:14:34 ERROR CarbonProperties: Executor task launch worker-0 Configured value for property carbon.number.of.cores.while.loading is wrong. Falling back to the default value 2----------+   wordcount----------+anubhav nullgeetika null prince null----------+
issueID:CARBONDATA-1991
type:Bug
changed files:streaming/src/main/java/org/apache/carbondata/streaming/CarbonStreamRecordReader.java
texts:Select query from a streaming table throws ClassCastException
Start spark shell using:./bin/spark-shell --jars carbonlib/carbondata_2.11-1.3.0-SNAPSHOT-shade-hadoop2.7.2.jarExecute the following code: import org.apache.spark.sql.SparkSessionimport org.apache.spark.sql.CarbonSession._val carbon = SparkSession.builder().config(sc.getConf) .getOrCreateCarbonSession("hdfs://localhost:54311/newCarbonStore","/tmp")import org.apache.carbondata.core.constants.CarbonCommonConstantsimport org.apache.carbondata.core.util.CarbonPropertiesCarbonProperties.getInstance().addProperty(CarbonCommonConstants.CARBON_BAD_RECORDS_ACTION, "FORCE") carbon.sql("drop table if exists uniqdata_stream")carbon.sql("create table uniqdata_stream(CUST_ID int,CUST_NAME String,DOB timestamp,DOJ timestamp, BIGINT_COLUMN1 bigint,BIGINT_COLUMN2 bigint,DECIMAL_COLUMN1 decimal(30,10),DECIMAL_COLUMN2 decimal(36,10),Double_COLUMN1 double, Double_COLUMN2 double,INTEGER_COLUMN1 int) STORED BY 'org.apache.carbondata.format' TBLPROPERTIES ('TABLE_BLOCKSIZE'= '256 MB', 'streaming'='true')");import carbon.sqlContext.implicits._val lines = carbon.readStream  .format("socket")  .option("host", "localhost")  .option("port", 9999)  .load()Now start a server socket using:nc -lk 9999 and write the 2000_uniqdata.csv data to that terminalNow execute the following on spark shellval qry = lines.writeStream             .format("carbondata")             .option("checkpointLocation","/newtmp1")            .option("dbName", "default")            .option("tableName", "uniqdata_stream")            .start()          qry.awaitTermination()Stop your socket and spark-shell when you are done with data writing,Again start the spark-shell and perform the following query:carbon.sql("select * from uniqdata_stream").showIt throws the following exception:18/01/05 18:25:04 ERROR Executor: Exception in task 0.0 in stage 0.0 (TID 0)java.lang.ClassCastException: java.math.BigDecimal cannot be cast to org.apache.spark.sql.types.Decimal at org.apache.carbondata.hadoop.streaming.CarbonStreamRecordReader.putRowToColumnBatch(CarbonStreamRecordReader.java:702) at org.apache.carbondata.hadoop.streaming.CarbonStreamRecordReader.scanBlockletAndFillVector(CarbonStreamRecordReader.java:424) at org.apache.carbondata.hadoop.streaming.CarbonStreamRecordReader.nextColumnarBatch(CarbonStreamRecordReader.java:324) at org.apache.carbondata.hadoop.streaming.CarbonStreamRecordReader.nextKeyValue(CarbonStreamRecordReader.java:305) at org.apache.carbondata.spark.rdd.CarbonScanRDD$$anon$1.hasNext(CarbonScanRDD.scala:370) at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.scan_nextBatch$(Unknown Source) at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source) at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43) at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377) at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:231) at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:225) at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:826) at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:826) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) at org.apache.spark.rdd.RDD.iterator(RDD.scala:287) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87) at org.apache.spark.scheduler.Task.run(Task.scala:99) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748)
issueID:CARBONDATA-1998
type:Sub-task
changed files:integration/hive/src/main/java/org/apache/carbondata/hive/MapredCarbonInputFormat.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/IndexWrapper.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/SchemaReader.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/TableSchemaBuilder.java
core/src/main/java/org/apache/carbondata/core/scan/model/QueryModelBuilder.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonTableInputFormat.java
hadoop/src/main/java/org/apache/carbondata/hadoop/util/CarbonInputFormatUtil.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/CarbonTable.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonInputFormat.java
hadoop/src/main/java/org/apache/carbondata/hadoop/CarbonProjection.java
core/src/main/java/org/apache/carbondata/core/scan/executor/impl/AbstractQueryExecutor.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonFileInputFormat.java
store/sdk/src/main/java/org/apache/carbondata/sdk/file/CarbonReaderBuilder.java
core/src/main/java/org/apache/carbondata/core/indexstore/BlockletDetailInfo.java
store/sdk/src/main/java/org/apache/carbondata/sdk/file/CarbonReader.java
texts:Support FileReader Java API for file level carbondata

issueID:CARBONDATA-1999
type:Sub-task
changed files:core/src/main/java/org/apache/carbondata/core/statusmanager/SegmentStatusManager.java
texts:Block drop table and delete streaming segment while streaming is in progress
Block drop table and delete streaming segment while streaming is in progress
issueID:CARBONDATA-2
type:New Feature
changed files:processing/src/main/java/org/apache/carbondata/processing/loading/converter/impl/FieldEncoderFactory.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/SortDataRows.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/SortParameters.java
processing/src/main/java/org/apache/carbondata/processing/loading/converter/impl/AbstractDictionaryFieldConverterImpl.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/NewRowComparator.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/IntermediateFileMerger.java
processing/src/main/java/org/apache/carbondata/processing/loading/steps/DataWriterProcessorStepImpl.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/SingleThreadFinalSortFilesMerger.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/NewRowComparatorForNormalDims.java
processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactDataHandlerColumnar.java
processing/src/main/java/org/apache/carbondata/processing/csvreaderstep/BlockDetails.java
processing/src/main/java/org/apache/carbondata/processing/loading/DataLoadExecutor.java
processing/src/main/java/org/apache/carbondata/processing/datatypes/StructDataType.java
processing/src/main/java/org/apache/carbondata/processing/datatypes/GenericDataType.java
processing/src/main/java/org/apache/carbondata/processing/util/NonDictionaryUtil.java
processing/src/main/java/org/apache/carbondata/processing/loading/parser/CarbonParserFactory.java
processing/src/main/java/org/apache/carbondata/processing/loading/constants/DataLoadProcessorConstants.java
core/src/main/java/org/apache/carbondata/core/metadata/CarbonTableIdentifier.java
processing/src/main/java/org/apache/carbondata/processing/loading/dictionary/DirectDictionary.java
processing/src/main/java/org/apache/carbondata/processing/loading/parser/impl/ArrayParserImpl.java
processing/src/main/java/org/apache/carbondata/processing/loading/BadRecordsLogger.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/impl/ParallelReadMergeSorterImpl.java
integration/spark-common/src/main/java/org/apache/carbondata/spark/merger/CarbonDataMergerUtil.java
processing/src/main/java/org/apache/carbondata/processing/loading/DataLoadProcessBuilder.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/SortTempFileChunkHolder.java
integration/spark-common/src/main/java/org/apache/carbondata/spark/load/CarbonLoaderUtil.java
integration/spark-common/src/main/java/org/apache/carbondata/spark/load/DeleteLoadFolders.java
processing/src/main/java/org/apache/carbondata/processing/loading/model/CarbonLoadModel.java
processing/src/main/java/org/apache/carbondata/processing/loading/converter/impl/NonDictionaryFieldConverterImpl.java
processing/src/main/java/org/apache/carbondata/processing/loading/parser/impl/StructParserImpl.java
processing/src/main/java/org/apache/carbondata/processing/datatypes/PrimitiveDataType.java
processing/src/main/java/org/apache/carbondata/processing/loading/parser/impl/PrimitiveParserImpl.java
processing/src/main/java/org/apache/carbondata/processing/csvload/CSVRecordReaderIterator.java
processing/src/main/java/org/apache/carbondata/processing/loading/converter/RowConverter.java
core/src/main/java/org/apache/carbondata/core/datastore/row/CarbonRow.java
processing/src/main/java/org/apache/carbondata/processing/loading/parser/impl/RowParserImpl.java
integration/spark-common/src/main/java/org/apache/carbondata/spark/merger/RowResultMerger.java
processing/src/main/java/org/apache/carbondata/processing/loading/converter/impl/RowConverterImpl.java
processing/src/main/java/org/apache/carbondata/processing/loading/steps/InputProcessorStepImpl.java
processing/src/main/java/org/apache/carbondata/processing/loading/exception/BadRecordFoundException.java
processing/src/main/java/org/apache/carbondata/processing/loading/parser/GenericParser.java
processing/src/main/java/org/apache/carbondata/processing/loading/steps/SortProcessorStepImpl.java
processing/src/main/java/org/apache/carbondata/processing/loading/converter/impl/DirectDictionaryFieldConverterImpl.java
processing/src/main/java/org/apache/carbondata/processing/loading/steps/DataConverterProcessorStepImpl.java
processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactDataHandlerModel.java
processing/src/main/java/org/apache/carbondata/processing/csvload/CSVInputFormat.java
processing/src/main/java/org/apache/carbondata/processing/util/CarbonDataProcessorUtil.java
core/src/main/java/org/apache/carbondata/core/keygenerator/directdictionary/timestamp/TimeStampDirectDictionaryGenerator.java
processing/src/main/java/org/apache/carbondata/processing/datatypes/ArrayDataType.java
processing/src/main/java/org/apache/carbondata/processing/loading/converter/FieldConverter.java
processing/src/main/java/org/apache/carbondata/processing/loading/converter/impl/MeasureFieldConverterImpl.java
processing/src/main/java/org/apache/carbondata/processing/loading/DataField.java
processing/src/main/java/org/apache/carbondata/processing/loading/converter/impl/ComplexFieldConverterImpl.java
processing/src/main/java/org/apache/carbondata/processing/loading/converter/BadRecordLogHolder.java
texts:Remove kettle for loading data
Remove kettle for loading data module
issueID:CARBONDATA-20
type:Improvement
changed files:
texts:Column Group Filter (Rowlevel and Exclude)
Below query scenario is not working with column group columnsselect * from comb01 where num not in (145);select * from comb01 where num like '14%';select * from comb01 where num not like '14%'; select * from comb01 where num not between 146 and 150;select * from comb01 where num regexp 2;select * from comb01 where num not regexp 4;
issueID:CARBONDATA-200
type:New Feature
changed files:core/src/main/java/org/apache/carbondata/core/stats/QueryStatisticsConstants.java
core/src/main/java/org/apache/carbondata/core/stats/QueryStatistic.java
core/src/main/java/org/apache/carbondata/core/stats/QueryStatisticsRecorder.java
core/src/main/java/org/apache/carbondata/core/scan/result/iterator/DetailQueryResultIterator.java
core/src/main/java/org/apache/carbondata/core/carbon/querystatistics/DriverQueryStatisticsRecorderImpl.java
core/src/main/java/org/apache/carbondata/core/util/CarbonTimeStatisticsFactory.java
core/src/main/java/org/apache/carbondata/core/scan/executor/impl/AbstractQueryExecutor.java
core/src/main/java/org/apache/carbondata/core/scan/result/iterator/AbstractDetailQueryResultIterator.java
core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
texts:Add performance statistics logs to record the query time taken by carbon
Add performance statistics logs to record the query time taken by carbon,show the query statistics like a table, so users can easily analyzing the query performance.
issueID:CARBONDATA-2001
type:Bug
changed files:
texts:Unable to save a dataframe result as carbondata streaming table
1.create carbonsession import org.apache.spark.sql.SparkSessionimport org.apache.spark.sql.CarbonSession._val carbon = SparkSession.builder().config(sc.getConf) .getOrCreateCarbonSession("hdfs://localhost:54311/newCarbonStore","/tmp"2.create a dataframe with carbonsessionimport carbon.sqlContext.implicits._carbon.sql("drop table if exists streamingtable"); val df =carbon.sparkContext.parallelize(1 to 5).toDF("colId")3.register dataframe as carbon streaming table df.write.format("carbondata").option("tableName","streamingTable").option("streaming","true").mode(SaveMode.Overwrite).save4,desc formatted the tablecarbon.sql("describe formatted streamingTable").show(100)--------------------------------------------------------            col_name           data_type             comment--------------------------------------------------------colid            ...int              ...MEASURE,null     ...                 ...                 ...                 ...##Detailed Table ...                 ...                 ...Database Name    ...default          ...                 ...Table Name       ...streamingtable   ...                 ...CARBON Store Path...hdfs://localhost:...                 ...Comment          ...                 ...                 ...Table Block Size ...1024 MB          ...                 ...Table Data Size  ...316              ...                 ...Table Index Size ...283              ...                 ...Last Update Time ...1515393447642    ...                 ...SORT_SCOPE       ...LOCAL_SORT       ...LOCAL_SORT       ...Streaming        ...false            ...                 ...SORT_SCOPE       ...LOCAL_SORT       ...LOCAL_SORT       ...                 ...                 ...                 ...##Detailed Column...                 ...                 ...ADAPTIVE         ...                 ...                 ...SORT_COLUMNS     ...                 ...                 ...--------------------------------------------------------here property  streaming is false it should be true
issueID:CARBONDATA-2002
type:Bug
changed files:
texts:Streaming segment status is not getting updated to finished or success
I created a streaming table and loaded data into it using the following commands on spark shell:import org.apache.spark.sql.SparkSessionimport org.apache.spark.sql.CarbonSession._import org.apache.carbondata.core.util.CarbonPropertiesimport org.apache.spark.sql.streaming.{ProcessingTime, StreamingQuery}val carbon = SparkSession.builder().config(sc.getConf) .getOrCreateCarbonSession("hdfs://localhost:54311/newCarbonStore","/tmp")import org.apache.carbondata.core.constants.CarbonCommonConstantsimport org.apache.carbondata.core.util.CarbonPropertiesCarbonProperties.getInstance().addProperty(CarbonCommonConstants.CARBON_BAD_RECORDS_ACTION, "FORCE")carbon.sql("drop table if exists uniqdata_stream")carbon.sql("create table uniqdata_stream(CUST_ID int,CUST_NAME String,DOB timestamp,DOJ timestamp, BIGINT_COLUMN1 bigint,BIGINT_COLUMN2 bigint,DECIMAL_COLUMN1 decimal(30,10),DECIMAL_COLUMN2 decimal(36,10),Double_COLUMN1 double, Double_COLUMN2 double,INTEGER_COLUMN1 int) STORED BY 'org.apache.carbondata.format' TBLPROPERTIES ('TABLE_BLOCKSIZE'= '256 MB', 'streaming'='true')");import carbon.sqlContext.implicits._import org.apache.spark.sql.types._val uniqdataSch = StructType(Array(StructField("CUST_ID", IntegerType),StructField("CUST_NAME", StringType),StructField("DOB", TimestampType), StructField("DOJ", TimestampType), StructField("BIGINT_COLUMN1", LongType), StructField("BIGINT_COLUMN2", LongType), StructField("DECIMAL_COLUMN1", org.apache.spark.sql.types.DecimalType(30, 10)), StructField("DECIMAL_COLUMN2", org.apache.spark.sql.types.DecimalType(36,10)), StructField("Double_COLUMN1", DoubleType), StructField("Double_COLUMN2", DoubleType), StructField("INTEGER_COLUMN1", IntegerType)))val streamDf = carbon.readStream.schema(uniqdataSch).option("sep", ",").csv("file:///home/geetika/Downloads/uniqdata")val qry = streamDf.writeStream.format("carbondata").trigger(ProcessingTime("5 seconds"))             .option("checkpointLocation","/stream/uniq")            .option("dbName", "default")            .option("tableName", "uniqdata_stream")            .start()          qry.awaitTermination()//Press ctrl+c to terminatestart the spark shell again import org.apache.spark.sql.SparkSessionimport org.apache.spark.sql.CarbonSession._val carbon = SparkSession.builder().config(sc.getConf) .getOrCreateCarbonSession("hdfs://localhost:54311/newCarbonStore","/tmp")carbon.sql("show segments for table uniqdata_stream").showIt shows the following output:---------------------------------------------------------------------+SegmentSequenceId   Status     Load Start TimeLoad End TimeMerged ToFile Format---------------------------------------------------------------------+                0Streaming2018-01-05 18:23:...         null       NA     ROW_V1---------------------------------------------------------------------+Status for the segment is not updated
issueID:CARBONDATA-2004
type:Bug
changed files:
texts:Incorrect result displays while loading data into a partitioned table.
Incorrect result displays while loading data into a partitioned table.Steps to reproduce:1)create a partitioned table:CREATE TABLE uniqdata_timestamp (CUST_ID int,CUST_NAME String,ACTIVE_EMUI_VERSION string, DOJ timestamp, BIGINT_COLUMN1 bigint,BIGINT_COLUMN2 bigint,DECIMAL_COLUMN1 decimal(30,10), DECIMAL_COLUMN2 decimal(36,10),Double_COLUMN1 double, Double_COLUMN2 double, INTEGER_COLUMN1 int) partitioned by(dob timestamp) STORED BY 'org.apache.carbondata.format' TBLPROPERTIES ("TABLE_BLOCKSIZE"= "256 MB")2) Load data into table:LOAD DATA INPATH 'hdfs://localhost:54310/Data/uniqdata/2000_UniqData.csv' into table uniqdata_timestamp partition(dob='1') OPTIONS ('FILEHEADER'='CUST_ID,CUST_NAME ,ACTIVE_EMUI_VERSION,DOB,DOJ, BIGINT_COLUMN1, BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1, Double_COLUMN2,INTEGER_COLUMN1','BAD_RECORDS_ACTION'='FORCE');3) Expected result: It should throw an error as invalid partition value.4) Actual Result: it displays notification of successful load, but there is no data into the table5) execute query: select count from uniqdata_timestamp;output:-----------+ count(1)  -----------+ 0         -----------+
issueID:CARBONDATA-2005
type:Bug
changed files:
texts:Location attribute with table properties in create table command throws parser exception
I created a table using:import org.apache.spark.sql.SparkSessionimport org.apache.spark.sql.CarbonSession._import org.apache.carbondata.core.util.CarbonPropertiesimport org.apache.spark.sql.streaming.{ProcessingTime, StreamingQuery}val carbon = SparkSession.builder().config(sc.getConf) .getOrCreateCarbonSession("hdfs://localhost:54311/newCarbonStore","/tmp")carbon.sql("create table jan8(id int, name string) stored by 'carbondata' tblproperties('streaming'='true') location '/home/geetika'").showIt show the following exception:scala> carbon.sql("create table jan8(id int, name string) stored by 'carbondata' tblproperties('streaming'='true') location '/home/geetika'").showorg.apache.spark.sql.AnalysisException: == Parse1 ==mismatched input 'location' expecting <EOF>(line 1, pos 96)== SQL ==create table jan8(id int, name string) stored by 'carbondata' tblproperties('streaming'='true') location '/home/geetika'------------------------------------------------------------------------------------------------^^^== Parse2 ==&#91;1.8&#93; failure: identifier matching regex (?i)DATAMAP expectedcreate table jan8(id int, name string) stored by 'carbondata' tblproperties('streaming'='true') location '/home/geetika'       ^;;  at org.apache.spark.sql.util.CarbonException$.analysisException(CarbonException.scala:23)  at org.apache.spark.sql.parser.CarbonSparkSqlParser.parsePlan(CarbonSparkSqlParser.scala:64)  at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:592)  ... 50 elided
issueID:CARBONDATA-2007
type:Bug
changed files:
texts:Unexpected behavior while inserting data into a partitioned table
Unexpected behavior while inserting data into a partitioned tableSteps to reproduce1) Create hive table:a) CREATE TABLE uniqdata_hive (CUST_ID int,CUST_NAME String,ACTIVE_EMUI_VERSION string, DOB timestamp, DOJ timestamp, BIGINT_COLUMN1 bigint,BIGINT_COLUMN2 bigint,DECIMAL_COLUMN1 decimal(30,10), DECIMAL_COLUMN2 decimal(36,10),Double_COLUMN1 double, Double_COLUMN2 double, INTEGER_COLUMN1 int)ROW FORMAT DELIMITED FIELDS TERMINATED BY ',';b) Load data:LOAD DATA LOCAL INPATH '/home/knoldus/Desktop/csv/TestData/Data/uniqdata/2000_UniqData.csv' into table UNIQDATA_HIVE;2) create Partoitioned table:a) CREATE TABLE uniqdata_bigint (CUST_ID int,CUST_NAME String,ACTIVE_EMUI_VERSION string, DOB timestamp, DOJ timestamp,BIGINT_COLUMN2 bigint,DECIMAL_COLUMN1 decimal(30,10), DECIMAL_COLUMN2 decimal(36,10),Double_COLUMN1 double, Double_COLUMN2 double, INTEGER_COLUMN1 int) partitioned by (BIGINT_COLUMN1 bigint) STORED BY 'org.apache.carbondata.format' TBLPROPERTIES ("TABLE_BLOCKSIZE"= "256 MB");b) CREATE TABLE uniqdata_double (CUST_ID int,CUST_NAME String,ACTIVE_EMUI_VERSION string, DOB timestamp, DOJ timestamp, BIGINT_COLUMN1 bigint,BIGINT_COLUMN2 bigint, DECIMAL_COLUMN2 decimal(36,10), Double_COLUMN2 double, INTEGER_COLUMN1 int) partitioned by (Double_COLUMN1 double) STORED BY 'org.apache.carbondata.format' TBLPROPERTIES ("TABLE_BLOCKSIZE"= "256 MB");3) Load data into the partitioned table from hive table:a) insert into uniqdata_bigint partition(BIGINT_COLUMN1)select * from uniqdata_hive limit 5;output:---------+ Result  ---------+---------+b) select * from uniqdata_bigint order by cust_id;output:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- cust_id      cust_name         active_emui_version               dob             doj   bigint_column2   decimal_column1   decimal_column2   double_column1   double_column2   integer_column1   bigint_column1  ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- NULL        NULL                    NULL   NULL             NULL              NULL              NULL             NULL             NULL              NULL             NULL        1970-01-01 11:00:03.0   NULL   NULL             NULL              NULL              NULL             NULL             NULL              NULL             NULL       ACTIVE_EMUI_VERSION_00000   NULL                    NULL   NULL             NULL              NULL              NULL             NULL             NULL              NULL             NULL      CUST_NAME_00000    NULL                    NULL   NULL             NULL              NULL              NULL             NULL             NULL              NULL             8999        NULL                    NULL   NULL             NULL              NULL              NULL             NULL             NULL              NULL            -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------5 rows selected (0.769 seconds)c)insert into uniqdata_double partition(Double_COLUMN1)select * from uniqdata_hive limit 5;Expected Output: data should be inserted successfully into the table.Actual Output:Error: org.apache.spark.sql.AnalysisException: Cannot insert into table `newcarbon`.`uniqdata_double` because the number of columns are different: need 11 columns, but query has 12 columns.; (state=,code=0)
issueID:CARBONDATA-2009
type:Sub-task
changed files:
texts:REFRESH TABLE Limitation When HiveMetaStore is used
Refresh table command will not register the carbon table if the old table is stored in the CarbonHiveMetastore
issueID:CARBONDATA-201
type:Bug
changed files:integration/spark-common/src/main/java/org/apache/carbondata/spark/load/CarbonLoaderUtil.java
processing/src/main/java/org/apache/carbondata/processing/model/CarbonLoadModel.java
texts:Add comment Option
add csv comment option to csv paser, and fix one bug when passing qutochar.
issueID:CARBONDATA-2010
type:Sub-task
changed files:core/src/main/java/org/apache/carbondata/core/metadata/schema/table/CarbonTable.java
texts:block aggregation main table to set streaming property

issueID:CARBONDATA-2011
type:Bug
changed files:
texts:CarbonStreamingQueryListener throwing ClassCastException
Java.lang.ClassCastException: org.apache.spark.sql.execution.streaming.StreamingQueryWrapper cannot be cast to org.apache.spark.sql.execution.streaming.StreamExecution
issueID:CARBONDATA-2012
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/datastore/filesystem/LocalCarbonFile.java
core/src/main/java/org/apache/carbondata/core/util/path/CarbonTablePath.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonOutputCommitter.java
core/src/main/java/org/apache/carbondata/core/statusmanager/SegmentStatusManager.java
processing/src/main/java/org/apache/carbondata/processing/loading/events/LoadEvents.java
processing/src/main/java/org/apache/carbondata/processing/util/CarbonLoaderUtil.java
texts:Add Transaction support for pre-aggregation table load
Currently the load process is like this:1. load main table 2. load preagg1 and write table status3. load preagg2 and write table status4. write table status for maintableImproved process:1. load main table2. load preagg13. load preagg24. write table status for preagg25. write table status for preagg16. write table status for maintable
issueID:CARBONDATA-2013
type:Bug
changed files:
texts:executing alter query on non-carbon table gives error, "table can not found in database"
create table test(id int,time string) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' STORED AS TEXTFILE;alter table test rename to new;  =>  gives error  test not found in database default
issueID:CARBONDATA-2014
type:Bug
changed files:processing/src/main/java/org/apache/carbondata/processing/util/CarbonLoaderUtil.java
texts:update table status for load failure only after first entry
update table status for load failure only after first entry and before calling to update the table status for failure, check whether it is hive partition table in the same way as it is checked while updating in progress status to table status
issueID:CARBONDATA-2015
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
processing/src/main/java/org/apache/carbondata/processing/loading/csvinput/CSVInputFormat.java
processing/src/main/java/org/apache/carbondata/processing/loading/converter/impl/NonDictionaryFieldConverterImpl.java
texts:Restricted maximum length of bytes per column
Validation for number of bytes for a column is added.We have limited the number of characters per column to 32000.For example, a single unicode character takes 3 bytes. So in this case, if my column has 30,000 unicode characters, then 32000 * 3 exceeds the short range. So, load will fail.
issueID:CARBONDATA-2016
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/scan/executor/util/RestructureUtil.java
texts:Exception displays while executing compaction with alter query
Exception displays while implementing compaction with alter query.Steps to reproduce:1) Create a table :CREATE TABLE CUSTOMER1 ( C_CUSTKEY INT , C_NAME STRING , C_ADDRESS STRING , C_NATIONKEY INT , C_PHONE STRING , C_ACCTBAL DECIMAL(15,2) , C_MKTSEGMENT STRING , C_COMMENT STRING) stored by 'carbondata';2) Insert data into the table:a) insert into customer1 values(1,'vandana','noida',1,'123456789',45987.78,'hello','comment')b) insert into customer1 values(2,'vandana','noida',2,'123456789',487.78,'hello','comment')c) insert into customer1 values(3,'geetika','delhi',3,'123456789',487897.78,'hello','comment')d) insert into customer1 values(4,'sangeeta','delhi',3,'123456789',48789.78,'hello','comment')3) Perform alter table query: alter table customer1 add columns (intfield int) TBLPROPERTIES ('DEFAULT.VALUE.intfield'='10');4) show segments for displaying segments before compactionshow segments for table customer1;output:-------------------------------------------------------------------------------------------------- SegmentSequenceId    Status       Load Start Time            Load End Time        Merged To   File Format  -------------------------------------------------------------------------------------------------- 3                   Success   2018-01-10 16:16:53.611   2018-01-10 16:16:54.99    NA          COLUMNAR_V3   2                   Success   2018-01-10 16:16:46.878   2018-01-10 16:16:47.75    NA          COLUMNAR_V3   1                   Success   2018-01-10 16:16:38.096   2018-01-10 16:16:38.972   NA          COLUMNAR_V3   0                   Success   2018-01-10 16:16:31.979   2018-01-10 16:16:33.293   NA          COLUMNAR_V3  --------------------------------------------------------------------------------------------------4 rows selected (0.029 seconds)5) alter table query for compaction:alter table customer1 compact 'minor';Expected Result: Table should be compacted successfully.Actual Result: Error: org.apache.spark.sql.AnalysisException: Compaction failed. Please check logs for more info. Exception in compaction Compaction Failure in Merger Rdd.; (state=,code=0)thriftserver logs:18/01/10 16:17:12 ERROR CompactionResultSortProcessor: &#91;Executor task launch worker-36&#93;&#91;partitionID:customer1;queryID:15798380253871&#93; Compaction failed: java.lang.Long cannot be cast to java.lang.Integerjava.lang.ClassCastException: java.lang.Long cannot be cast to java.lang.Integer at org.apache.carbondata.processing.sort.sortdata.SortDataRows.writeDataToFile(SortDataRows.java:273) at org.apache.carbondata.processing.sort.sortdata.SortDataRows.startSorting(SortDataRows.java:214) at org.apache.carbondata.processing.merger.CompactionResultSortProcessor.processResult(CompactionResultSortProcessor.java:226) at org.apache.carbondata.processing.merger.CompactionResultSortProcessor.execute(CompactionResultSortProcessor.java:159) at org.apache.carbondata.spark.rdd.CarbonMergerRDD$$anon$1.<init>(CarbonMergerRDD.scala:234) at org.apache.carbondata.spark.rdd.CarbonMergerRDD.internalCompute(CarbonMergerRDD.scala:81) at org.apache.carbondata.spark.rdd.CarbonRDD.compute(CarbonRDD.scala:60) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) at org.apache.spark.rdd.RDD.iterator(RDD.scala:287) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87) at org.apache.spark.scheduler.Task.run(Task.scala:99) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745)18/01/10 16:17:12 INFO UnsafeMemoryManager: &#91;Executor task launch worker-36&#93;&#91;partitionID:customer1;queryID:15798380253871&#93; Total memory used after task 15798371335347 is 5313 Current tasks running now are : &#91;6856382704941, 14621295743743, 14461639534151, 4378916027096, 15798216567589&#93;18/01/10 16:17:12 INFO UnsafeMemoryManager: &#91;Executor task launch worker-36&#93;&#91;partitionID:customer1;queryID:15798380253871&#93; Total memory used after task 15798371335347 is 5313 Current tasks running now are : &#91;6856382704941, 14621295743743, 14461639534151, 4378916027096, 15798216567589&#93;18/01/10 16:17:12 INFO UnsafeMemoryManager: &#91;Executor task launch worker-36&#93;&#91;partitionID:customer1;queryID:15798380253871&#93; Total memory used after task 15798371335347 is 5313 Current tasks running now are : &#91;6856382704941, 14621295743743, 14461639534151, 4378916027096, 15798216567589&#93;18/01/10 16:17:12 INFO UnsafeMemoryManager: &#91;Executor task launch worker-36&#93;&#91;partitionID:customer1;queryID:15798380253871&#93; Total memory used after task 15798371335347 is 5313 Current tasks running now are : &#91;6856382704941, 14621295743743, 14461639534151, 4378916027096, 15798216567589&#93;18/01/10 16:17:12 INFO CarbonLoaderUtil: LocalFolderDeletionPool:customer1 Deleted the local store location: /tmp/15798371407704_0 : Time taken: 218/01/10 16:17:12 INFO Executor: Finished task 0.0 in stage 75.0 (TID 490). 1037 bytes result sent to driver18/01/10 16:17:12 INFO TaskSetManager: Finished task 0.0 in stage 75.0 (TID 490) in 39 ms on localhost (executor driver) (1/1)18/01/10 16:17:12 INFO TaskSchedulerImpl: Removed TaskSet 75.0, whose tasks have all completed, from pool 18/01/10 16:17:12 INFO DAGScheduler: ResultStage 75 (collect at CarbonTableCompactor.scala:211) finished in 0.039 s18/01/10 16:17:12 INFO DAGScheduler: Job 76 finished: collect at CarbonTableCompactor.scala:211, took 0.063051 s18/01/10 16:17:12 AUDIT CarbonTableCompactor: &#91;knoldus&#93;&#91;hduser&#93;&#91;Thread-125&#93;Compaction request failed for table newcarbon.customer118/01/10 16:17:12 ERROR CarbonTableCompactor: pool-23-thread-7 Compaction request failed for table newcarbon.customer118/01/10 16:17:12 ERROR CarbonTableCompactor: pool-23-thread-7 Exception in compaction thread Compaction Failure in Merger Rdd.java.lang.Exception: Compaction Failure in Merger Rdd. at org.apache.carbondata.spark.rdd.CarbonTableCompactor.triggerCompaction(CarbonTableCompactor.scala:269) at org.apache.carbondata.spark.rdd.CarbonTableCompactor.scanSegmentsAndSubmitJob(CarbonTableCompactor.scala:120) at org.apache.carbondata.spark.rdd.CarbonTableCompactor.executeCompaction(CarbonTableCompactor.scala:71) at org.apache.carbondata.spark.rdd.CarbonDataRDDFactory$$anon$2.run(CarbonDataRDDFactory.scala:182) at org.apache.carbondata.spark.rdd.CarbonDataRDDFactory$.startCompactionThreads(CarbonDataRDDFactory.scala:269) at org.apache.spark.sql.execution.command.management.CarbonAlterTableCompactionCommand.alterTableForCompaction(CarbonAlterTableCompactionCommand.scala:255) at org.apache.spark.sql.execution.command.management.CarbonAlterTableCompactionCommand.processData(CarbonAlterTableCompactionCommand.scala:111) at org.apache.spark.sql.execution.command.DataCommand.run(package.scala:71) at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58) at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56) at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74) at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114) at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114) at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:135) at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151) at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:132) at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:113) at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:87) at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:87) at org.apache.spark.sql.Dataset.<init>(Dataset.scala:185) at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:64) at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:592) at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:699) at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:220) at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1$$anon$2.run(SparkExecuteStatementOperation.scala:163) at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1$$anon$2.run(SparkExecuteStatementOperation.scala:160) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1698) at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1.run(SparkExecuteStatementOperation.scala:173) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745)18/01/10 16:17:12 ERROR CarbonDataRDDFactory$: pool-23-thread-7 Exception in compaction thread Compaction Failure in Merger Rdd.18/01/10 16:17:12 INFO HdfsFileLock: pool-23-thread-7 Deleted the lock file hdfs://localhost:54310/opt/prestocarbonStore/newcarbon/customer1/compaction.lock18/01/10 16:17:12 ERROR CarbonAlterTableCompactionCommand: pool-23-thread-7 Exception in start compaction thread. Exception in compaction Compaction Failure in Merger Rdd.18/01/10 16:17:12 ERROR HdfsFileLock: pool-23-thread-7 Not able to delete the lock file because it is not existed in location hdfs://localhost:54310/opt/prestocarbonStore/newcarbon/customer1/compaction.lock18/01/10 16:17:12 ERROR SparkExecuteStatementOperation: Error executing query, currentState RUNNING, org.apache.spark.sql.AnalysisException: Compaction failed. Please check logs for more info. Exception in compaction Compaction Failure in Merger Rdd.; at org.apache.spark.sql.util.CarbonException$.analysisException(CarbonException.scala:23) at org.apache.spark.sql.execution.command.management.CarbonAlterTableCompactionCommand.processData(CarbonAlterTableCompactionCommand.scala:120) at org.apache.spark.sql.execution.command.DataCommand.run(package.scala:71) at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58) at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56) at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74) at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114) at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114) at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:135) at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151) at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:132) at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:113) at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:87) at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:87) at org.apache.spark.sql.Dataset.<init>(Dataset.scala:185) at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:64) at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:592) at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:699) at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:220) at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1$$anon$2.run(SparkExecuteStatementOperation.scala:163) at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1$$anon$2.run(SparkExecuteStatementOperation.scala:160) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1698) at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1.run(SparkExecuteStatementOperation.scala:173) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745)18/01/10 16:17:12 ERROR SparkExecuteStatementOperation: Error running hive query: org.apache.hive.service.cli.HiveSQLException: org.apache.spark.sql.AnalysisException: Compaction failed. Please check logs for more info. Exception in compaction Compaction Failure in Merger Rdd.; at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:258) at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1$$anon$2.run(SparkExecuteStatementOperation.scala:163) at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1$$anon$2.run(SparkExecuteStatementOperation.scala:160) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1698) at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1.run(SparkExecuteStatementOperation.scala:173) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745)
issueID:CARBONDATA-2017
type:Bug
changed files:
texts:Error occurs when loading multiple files
Problem:Carbon supports loading from multiple file paths at once, but we find that Carbon will throw an exception like "The input file does not exist" when loading multiple files on HDFS.For example:ex1: LOAD DATA INPATH '/data/source.csv,/data/source2.csv' INTO TABLE test_tableex2: LOAD DATA INPATH 'hdfs://ha/data/source.csv,hdfs://ha/data/source2.csv' INTO TABLE test_tableex1 will throw an exception saying that source2.csv does not exist.ex2 will execute normally.Solution:We found that carbon takes the PATH as a whole and checks its prefix before spliting it into multiplt paths. So the problem will be solved when we do the prefix checking job for each path after spliting PATH into multiplt paths.
issueID:CARBONDATA-2018
type:Improvement
changed files:processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/SortDataRows.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/holder/UnsafeInmemoryHolder.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/NewRowComparator.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/IntermediateSortTempRowComparator.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/merger/UnsafeIntermediateFileMerger.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/IntermediateFileMerger.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/comparator/UnsafeRowComparator.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/SingleThreadFinalSortFilesMerger.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/UnsafeCarbonRowPage.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/holder/SortTempChunkHolder.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/NewRowComparatorForNormalDims.java
processing/src/main/java/org/apache/carbondata/processing/util/CarbonLoaderUtil.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/comparator/UnsafeRowComparatorForNormalDims.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/SortStepRowHandler.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/TableFieldStat.java
processing/src/main/java/org/apache/carbondata/processing/loading/row/IntermediateSortTempRow.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/FalseFilterExecutor.java
core/src/main/java/org/apache/carbondata/core/util/NonDictionaryUtil.java
core/src/main/java/org/apache/carbondata/core/util/path/CarbonTablePath.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/holder/UnsafeSortTempFileChunkHolder.java
processing/src/main/java/org/apache/carbondata/processing/merger/CompactionResultSortProcessor.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/SortTempFileChunkHolder.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/UnsafeSortDataRows.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/holder/UnsafeFinalMergePageHolder.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/merger/UnsafeSingleThreadFinalSortFilesMerger.java
texts:Optimization in reading/writing for sort temp row during data loading
SCENARIOCurrently in carbondata data loading, during sort process step, records will be sorted partially and spilled to the disk. And then carbondata will read these records and do merge sort.Since sort step is CPU-tense, during writing/reading these records, we can optimize the serialization/deserialization for these rows and reduce CPU consumption in parsing the rows.This should enhance the data loading performance. RESOLVEWe can pick up the un-sorted fields in the row and pack them as bytes array and skip paring them. RESULTI've tested it in my cluster and seen about 8% performance gained (74MB/s/Node -> 81MB/s/Node).
issueID:CARBONDATA-2019
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/util/AbstractDataFileFooterConverter.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/SegmentIndexFileStore.java
core/src/main/java/org/apache/carbondata/core/util/CarbonMetadataUtil.java
core/src/main/java/org/apache/carbondata/core/writer/CarbonIndexFileMergeWriter.java
texts:Enhancement of merge index compaction feature to support creation of merge index file on old store where index file does not contain the blocklet info
Enhancement of merge index compaction feature to support creation of merge index file on old store where index file does not contain the blocklet info.Old store created with carbondata 1.1 version does not contain the blocklet info in the index file. On that store if merge index file is created then blocklet information will not be present in the merge index file and for first time query again carbondata file footer will be read for blocklet information retrieval.Benefits:1. Support merge index file creation on old store2. Improve first time query performance.Note: First time query performance will be improved only if merge index file is created before running the first query
issueID:CARBONDATA-202
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
texts:Exception thrown in Beeline for data loading when dictionary file content is not in correct format
Exception thrown in Beeline for data loading when dictionary file content is not in correct format
issueID:CARBONDATA-2020
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/scan/filter/executer/ImplicitIncludeFilterExecutorImpl.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/IndexWrapper.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockletDataMap.java
core/src/main/java/org/apache/carbondata/core/indexstore/BlockletDataMapIndexStore.java
core/src/main/java/org/apache/carbondata/core/scan/scanner/impl/BlockletFilterScanner.java
core/src/main/java/org/apache/carbondata/core/scan/executor/impl/AbstractQueryExecutor.java
core/src/main/java/org/apache/carbondata/core/datastore/block/TableBlockInfo.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockletDataRefNode.java
core/src/main/java/org/apache/carbondata/core/scan/filter/ColumnFilterInfo.java
core/src/main/java/org/apache/carbondata/core/indexstore/BlockletDetailInfo.java
integration/presto/src/main/java/org/apache/carbondata/presto/impl/CarbonLocalInputSplit.java
texts:Fisrt time query performance after upgrade from old version 1.1 to latest 1.3 version is degraded
The query performance got degraded after upgrading old version 1.1 to latest 1.3
issueID:CARBONDATA-2021
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
core/src/main/java/org/apache/carbondata/core/mutate/CarbonUpdateUtil.java
processing/src/main/java/org/apache/carbondata/processing/util/CarbonLoaderUtil.java
texts:when delete is success and update is failed while writing status file  then a stale carbon data file is created.
when delete is success and update is failed while writing status file then a stale carbon data file is created. so removing that file on clean up . and also not considering that one during query.when the update operation is running and the user stops it abruptly, then the carbon data file will be remained in the store .so extra data is coming.during the next update the clean up of the files need to be handled. and in query also new data file should be excluded. 
issueID:CARBONDATA-2022
type:Bug
changed files:
texts:Query With table alias is not hitting pre aggregate table
*Problem:*Query with table alias is Not hitting pre Aggregate table.  *Solution:* Problem is table alias query is plan is coming as SubQueryAlias(alias, SubqueryAlias) ans this case is not present in tranform query plan for pre aggregate
issueID:CARBONDATA-2023
type:Improvement
changed files:processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/SortDataRows.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/NewRowComparator.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/IntermediateSortTempRowComparator.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/merger/UnsafeIntermediateFileMerger.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockletDataMapFactory.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/comparator/UnsafeRowComparator.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/SingleThreadFinalSortFilesMerger.java
datamap/examples/src/minmaxdatamap/main/java/org/apache/carbondata/datamap/examples/MinMaxIndexDataMapFactory.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/holder/SortTempChunkHolder.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/NewRowComparatorForNormalDims.java
core/src/main/java/org/apache/carbondata/core/datastore/block/TableBlockInfo.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/comparator/UnsafeRowComparatorForNormalDims.java
core/src/main/java/org/apache/carbondata/core/util/CarbonProperties.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/SortStepRowHandler.java
core/src/main/java/org/apache/carbondata/core/util/NonDictionaryUtil.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/holder/UnsafeSortTempFileChunkHolder.java
processing/src/main/java/org/apache/carbondata/processing/merger/NodeMultiBlockRelation.java
processing/src/main/java/org/apache/carbondata/processing/merger/CompactionResultSortProcessor.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/SortTempFileChunkHolder.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/UnsafeSortDataRows.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/holder/UnsafeFinalMergePageHolder.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/holder/UnsafeInmemoryHolder.java
processing/src/main/java/org/apache/carbondata/processing/merger/RowResultMergerProcessor.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonLoadOptionConstants.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/UnsafeCarbonRowPage.java
processing/src/main/java/org/apache/carbondata/processing/util/CarbonLoaderUtil.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/TableFieldStat.java
processing/src/main/java/org/apache/carbondata/processing/loading/row/IntermediateSortTempRow.java
processing/src/main/java/org/apache/carbondata/processing/datamap/DataMapWriterListener.java
processing/src/main/java/org/apache/carbondata/processing/merger/CarbonDataMergerUtil.java
processing/src/main/java/org/apache/carbondata/processing/util/CarbonDataProcessorUtil.java
core/src/main/java/org/apache/carbondata/core/datamap/dev/DataMapFactory.java
datamap/examples/src/minmaxdatamap/main/java/org/apache/carbondata/datamap/examples/MinMaxDataWriter.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/IntermediateFileMerger.java
core/src/main/java/org/apache/carbondata/core/datamap/dev/DataMapWriter.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/merger/UnsafeSingleThreadFinalSortFilesMerger.java
texts:Optimization in data loading for skewed data
In one of my cases, carbondata has to load skewed data files. The size of data file ranges from 1KB to about 5GB.In current implementation, carbondata will distribute the file blocks(splits) among the nodes to maximum the data locality and data evenly distributed, we call it `block-node-assignment` for short.However, the current implementation has some problems.The assignment is block number based. The goal is to make sure that all the nodes deal the same amount number of blocks. In the skewed data scenario described above, the block of a small file and the block of a big file are very different from its size (1KB v.s. 64MB). As a result, the difference of total data size assigned for each data node is very large.In order to solve this problem, the size of block should be considered during the block-node-assignment. One node can deal more blocks than another as long as the total size of blocks are almost the same.
issueID:CARBONDATA-2024
type:Bug
changed files:
texts:After update empty folder is being created for compacted segments
Please follow the following steps to reproduce : spark.sql("drop table if exists t")    spark.sql("create table t (c1 string, c2 string, c3 int, c4 string) stored by 'carbondata'")    spark.sql("insert into t select 'asd','sdf',1,'dfg'")    spark.sql("insert into t select 'asdf','sadf',2,'dafg'")    spark.sql("insert into t select 'asdq','sqdf',3,'dqfg'")    spark.sql("insert into t select 'aswd','sdfw',4,'dfgw'")    spark.sql("insert into t select 'aesd','sdef',5,'dfge'")    spark.sql("alter table t compact 'minor'")    spark.sql("clean files for table t")    spark.sql("update t set(c4) = ('yyy') where c3 = 3").show()
issueID:CARBONDATA-2027
type:Improvement
changed files:
texts:Fix the Randomly failing Concurrent Test Cases  For Ci

issueID:CARBONDATA-2028
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
texts:Select Query failed with preagg having timeseries and normal agg table together
sql("drop table if exists maintabletime") sql("create table maintabletime(year int,month int,name string,salary int,dob timestamp) stored by 'carbondata' tblproperties('sort_scope'='Global_sort','table_blocksize'='23','sort_columns'='month,year,name')") sql("insert into maintabletime select 10,11,'babu',12,'2014-01-01 00:00:00'") sql("create datamap agg0 on table maintabletime using 'preaggregate' as select dob,name from maintabletime group by dob,name") sql("create datamap agg1 on table maintabletime using 'preaggregate' DMPROPERTIES ('timeseries.eventTime'='dob', 'timeseries.hierarchy'='hour=1,day=1,month=1,year=1') as select dob,name from maintabletime group by dob,name") val df = sql("select timeseries(dob,'year') from maintabletime group by timeseries(dob,'year')")  Exception Exception in thread "main" org.apache.spark.sql.AnalysisException: Column does not exists in Pre Aggregate table; at org.apache.spark.sql.hive.CarbonPreAggregateQueryRules.getChildAttributeReference(CarbonPreAggregateRules.scala:719) at org.apache.spark.sql.hive.CarbonPreAggregateQueryRules$$anonfun$19$$anonfun$4.applyOrElse(CarbonPreAggregateRules.scala:855)
issueID:CARBONDATA-2029
type:Bug
changed files:
texts:Query with expression is giving wrong result
Create Maintable:CREATE TABLE mainTable(id int, name string, city string, age string) STORED BY 'org.apache.carbondata.format Create datamapcreate datamap agg1 on table mainTable using 'preaggregate' as select name,sum(id) from mainTable group by nameLoad data Run query :select sum(id)+count(id) from maintable is giving wrong resultProblem: When query has expression it is not checking which aggregate function is applied on table and based on table it is selecting aggregate tableSolution: While extracting the aggregate expression from query plan in case if any expression is present extract which aggregate function applied on column to select the aggregate table  
issueID:CARBONDATA-203
type:Bug
changed files:
texts:Use static string to set Hadoop configuration
Should use static string in FileInputFormat class as the configuration key to set the corresponding configuration value in Hadoop ConfigurationSome places currently is not doing like this, such as CarbonDataRDDFactory, SplitUtilssearch for "hadoopConfiguration.set"
issueID:CARBONDATA-2030
type:Bug
changed files:
texts:avg with Aggregate table for double data type is failed.
spark.sql("drop table if exists y ")spark.sql("create table y(year int,month int,name string,salary double) stored by 'carbondata' tblproperties('sort_scope'='Global_sort','table_blocksize'='23','sort_columns'='month,year,name')")spark.sql("insert into y select 10,11,'babu',12.89")spark.sql("insert into y select 10,11,'babu',12.89")spark.sql("create datamap y1_sum1 on table y using 'preaggregate' as select name,avg(salary) from y group by name")spark.sql("select name,avg(salary) from y group by name").show(false)  Exception in thread "main" org.apache.spark.sql.AnalysisException: cannot resolve '(sum(y_y1_sum1.`y_salary_sum`) / sum(y_y1_sum1.`y_salary_count`))' due to data type mismatch: differing types in '(sum(y_y1_sum1.`y_salary_sum`) / sum(y_y1_sum1.`y_salary_count`))' (double and bigint).;;'Aggregate y_name#25, y_name#25 AS name#41, (sum(y_salary_sum#26) / sum(y_salary_count#27L)) AS avg(salary)#46+- Relationy_name#25,y_salary_sum#26,y_salary_count#27L CarbonDatasourceHadoopRelation [ Database name :default, Table name :y_y1_sum1, Schema :Some(StructType(StructField(y_name,StringType,true), StructField(y_salary_sum,DoubleType,true), StructField(y_salary_count,LongType,true))) ] 
issueID:CARBONDATA-2031
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/scan/filter/executer/ExcludeFilterExecuterImpl.java
texts:Select column with is null for no_inverted_index column throws java.lang.ArrayIndexOutOfBoundsException
steps:1) create table zerorows_part (c1 string,c2 int,c3 string,c5 string) STORED BY 'carbondata' TBLPROPERTIES('DICTIONARY_INCLUDE'='C2','NO_INVERTED_INDEX'='C2')2)LOAD DATA LOCAL INPATH '$filepath/dest.csv' INTO table zerorows_part OPTIONS('delimiter'=',','fileheader'='c1,c2,c3,c5')3)select c2 from zerorows_part where c2 is null Previous exception in task: java.util.concurrent.ExecutionException: java.lang.ArrayIndexOutOfBoundsException: 0    org.apache.carbondata.core.scan.processor.AbstractDataBlockIterator.updateScanner(AbstractDataBlockIterator.java:136)    org.apache.carbondata.core.scan.processor.impl.DataBlockIteratorImpl.processNextBatch(DataBlockIteratorImpl.java:64)    org.apache.carbondata.core.scan.result.iterator.VectorDetailQueryResultIterator.processNextBatch(VectorDetailQueryResultIterator.java:46)    org.apache.carbondata.spark.vectorreader.VectorizedCarbonRecordReader.nextBatch(VectorizedCarbonRecordReader.java:283)    org.apache.carbondata.spark.vectorreader.VectorizedCarbonRecordReader.nextKeyValue(VectorizedCarbonRecordReader.java:171)    org.apache.carbondata.spark.rdd.CarbonScanRDD$$anon$1.hasNext(CarbonScanRDD.scala:370)    org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.scan_nextBatch$(Unknown Source)    org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)    org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)    org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:395)    org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:234)    org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:228)    org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827)    org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827)    org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)    org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)    org.apache.spark.rdd.RDD.iterator(RDD.scala:287)    org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)    org.apache.spark.scheduler.Task.run(Task.scala:108)    org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)    java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)    java.lang.Thread.run(Thread.java:748)    at org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:138)    at org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:116)    at org.apache.spark.scheduler.Task.run(Task.scala:118)    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)    at java.lang.Thread.run(Thread.java:748)  dest.csv    **
issueID:CARBONDATA-2032
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/datastore/impl/FileFactory.java
processing/src/main/java/org/apache/carbondata/processing/store/writer/v3/CarbonFactDataWriterImplV3.java
core/src/main/java/org/apache/carbondata/core/datastore/filesystem/LocalCarbonFile.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonLoadOptionConstants.java
processing/src/main/java/org/apache/carbondata/processing/store/writer/AbstractFactDataWriter.java
processing/src/main/java/org/apache/carbondata/processing/util/CarbonLoaderUtil.java
core/src/main/java/org/apache/carbondata/core/datastore/filesystem/AbstractDFSCarbonFile.java
core/src/main/java/org/apache/carbondata/core/datastore/filesystem/CarbonFile.java
core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
texts:Skip writing final data files to local disk to save disk IO in data loading
Currently in data loading, carbondata write the final data files to local disk and then copy it to HDFS.For saving disk IO, carbondata can skip this procedure and directly write these files to HDFS.
issueID:CARBONDATA-2034
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/scan/executor/util/QueryUtil.java
texts:Improve query performance
Problem: Dictionary loading is taking more time in executor side when number of nodes is high.Solution: During query no need to load non complex dimension dictionary. Dictionary decoder will take care of loading and decoding the dictionary column
issueID:CARBONDATA-2035
type:Bug
changed files:
texts:Incorrect assert in code leads to tests failed
Today I came across tests failure when I ran tests in Intellij Idea. The code was on the master branch, and it had not been modified after I fetched the latest code. The failed test lies in `TestLoadDataGeneral.test("test load / insert / update with data more than 32000 bytes - dictionary_exclude")`.  It complains that `"DataLoad failure: there is an unexpected error: null" did not contain "Dataload failure: DataLoad failed, String size cannot exceed 32000 bytes"`. After I study the code, I find out that in `NonDictionaryFieldConverterImpl#line74 and #line77`, carbondata use an assert to raise the exception. But the exception is not thrown when I debug this tests. After some research, I found that the tests will success only if I enable `-ea` option. (run -> Edit configurations. Configuration -> VM parameters)By default, the "VM parameters" is empty and `-ea` is disabled, so the assert sentence in code will not come into effect, thus causing this problem.In conclusion, carbondata should not detect&throw exception in this way, we need to correct it.
issueID:CARBONDATA-2036
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
core/src/main/java/org/apache/carbondata/core/indexstore/UnsafeMemoryDMStore.java
core/src/main/java/org/apache/carbondata/core/util/CarbonProperties.java
texts:Insert overwrite on static partition cannot work properly
When trying to insert overwrite on the static partition with 0 at first on int column has an issue.Example : create table test(d1 string) partition by (c1 int, c2 int, c3 int)And use insert overwrite table partition(01, 02, 03) select "s1" The above case has a problem as 01 is not converting to actual integer to partition map file.  
issueID:CARBONDATA-2037
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/datastore/filesystem/ViewFSCarbonFile.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockletDataMapModel.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/SegmentIndexFileStore.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockletDataMap.java
core/src/main/java/org/apache/carbondata/core/indexstore/UnsafeMemoryDMStore.java
core/src/main/java/org/apache/carbondata/core/indexstore/BlockletDataMapIndexStore.java
core/src/main/java/org/apache/carbondata/core/datastore/filesystem/AbstractDFSCarbonFile.java
core/src/main/java/org/apache/carbondata/core/datastore/filesystem/CarbonFile.java
core/src/main/java/org/apache/carbondata/core/datastore/filesystem/LocalCarbonFile.java
core/src/main/java/org/apache/carbondata/core/indexstore/row/DataMapRowImpl.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonTableInputFormat.java
core/src/main/java/org/apache/carbondata/core/indexstore/row/UnsafeDataMapRow.java
core/src/main/java/org/apache/carbondata/core/datastore/filesystem/AlluxioCarbonFile.java
core/src/main/java/org/apache/carbondata/core/datastore/filesystem/HDFSCarbonFile.java
core/src/main/java/org/apache/carbondata/core/indexstore/ExtendedBlocklet.java
texts:Store carbondata locations in datamap to make the datamap retrieval faster
Store carbondata locations in datamap to make the datamap retrieval faster
issueID:CARBONDATA-2038
type:Bug
changed files:
texts:Java tests should use JUnit assertion instead of the Java native one
When I was working on CARBONDATA-2035, I noticed that some tests in our project use Java native assert as assertion. This may cause problems if developers run tests from their IDE and don't enable the `-ea` parameter for the JVM – The assertion will be skipped and always be true. We should rely on JUnit assertion instead of the Java native assertion as its behavior is not affected by the JVM parameter. Please notice that in scala tests, we use scalatests framework. The scalatests also provides an `assert` that hides the one in scala native. So there will be no problem if we use assert in scala tests. Please refer to [this| http://www.scalatest.org/user_guide/using_assertions] to get more information.
issueID:CARBONDATA-2039
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockletDataMap.java
texts:Add relative blocklet id during initialization in the blocklet data map
Add relative blocklet id during initialization in the blocklet data map. ProblemCurrently while loading the blocklets in data map all the blocklets are stored in the unsafe manager on array indexes for all the blocklets in one segment. So lets say if 1 segment has 7 task and each task has 10 part files with 3 blocklets each, total number of blocklets in the segment would be 210. Unsafe store will store all these blocklets in one array with start index as 0 and end index as 210.Due to this while filling the blocklet information the blocklet id taken is the array index which can be any number from 0 to 210. This is leading to loss in actual mapping of blocklet with respect to carbondata file. SolutionAdd the relative blocklet id during loading of blocklets in the unsafe store (relative blocklet id is the id of blocklet in the carbondata file)
issueID:CARBONDATA-204
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/stats/QueryStatisticsConstants.java
core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
core/src/main/java/org/apache/carbondata/core/carbon/querystatistics/DriverQueryStatisticsRecorderImpl.java
texts:Query statistics issue
Query statistics issue:1. some query statistics that never be printed will be keeped into querystatisticsMap, so it will cause "out of memory" for long time running2. in some sceniaro, the driver can't record "sql_parse_time" , the driver statistics logs will not be output
issueID:CARBONDATA-2040
type:Task
changed files:
texts:Add standardpartition example and optimize partition test cases
Add StandardPartitionExample Optimize partition test cases.
issueID:CARBONDATA-2041
type:Bug
changed files:
texts:Not able to load data into a partitioned table using insert overwrite
Not able to load data into a partitioned table using insert overwriteSteps to reproduce:1) Create Hive table and load data in it:a) CREATE TABLE uniqdata_hive (CUST_ID int,CUST_NAME String,ACTIVE_EMUI_VERSION string, DOB timestamp, DOJ timestamp, BIGINT_COLUMN1 bigint,BIGINT_COLUMN2 bigint,DECIMAL_COLUMN1 decimal(30,10), DECIMAL_COLUMN2 decimal(36,10),Double_COLUMN1 double, Double_COLUMN2 double, INTEGER_COLUMN1 int)ROW FORMAT DELIMITED FIELDS TERMINATED BY ',';b) LOAD DATA LOCAL INPATH '/home/knoldus/Desktop/csv/TestData/Data/uniqdata/2000_UniqData.csv' into table UNIQDATA_HIVE;2) Create a partitioned table(hive and carbon both):a) CREATE TABLE uniqdata_string(CUST_ID int,CUST_NAME String,DOB timestamp,DOJ timestamp, BIGINT_COLUMN1 bigint,BIGINT_COLUMN2 bigint,DECIMAL_COLUMN1 decimal(30,10),DECIMAL_COLUMN2 decimal(36,10),Double_COLUMN1 double, Double_COLUMN2 double,INTEGER_COLUMN1 int) PARTITIONED BY(ACTIVE_EMUI_VERSION string) STORED BY 'org.apache.carbondata.format' TBLPROPERTIES ('TABLE_BLOCKSIZE'= '256 MB');b) CREATE TABLE uniqdata_hive_partition (CUST_ID int,CUST_NAME String, DOB timestamp, DOJ timestamp, BIGINT_COLUMN1 bigint,BIGINT_COLUMN2 bigint,DECIMAL_COLUMN1 decimal(30,10), DECIMAL_COLUMN2 decimal(36,10),Double_COLUMN1 double, Double_COLUMN2 double, INTEGER_COLUMN1 int) partitioned by (active_emui_version string)ROW FORMAT DELIMITED FIELDS TERMINATED BY ','; 3) Load data into the partitioned table using insert overwrite:a) load into the hive_partitioned table:insert overwrite table uniqdata_hive_partition partition (active_emui_version) select CUST_ID,CUST_NAME, DOB, DOJ, BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1, DECIMAL_COLUMN2,Double_COLUMN1, Double_COLUMN2, INTEGER_COLUMN1,active_emui_version from uniqdata_hive limit 10; output: Data successfully loaded into the table:validation Query:select count from uniqdata_hive_partitionoutput:count(1) |-----------+ 10 -----------–+ b) load into carbon partitioned table:insert overwrite table uniqdata_string partition(active_emui_version) select CUST_ID, CUST_NAME,DOB,doj, bigint_column1, bigint_column2, decimal_column1, decimal_column2,double_column1, double_column2,integer_column1,active_emui_version from uniqdata_hive limit 10; Expected Result: Data should be loaded successfullyActual Result:Error: org.apache.spark.SparkException: Job aborted. (state=,code=0) logs:org.apache.spark.sql.AnalysisException: org.apache.hadoop.hive.ql.metadata.HiveException: partition spec is invalid; field active_emui_version does not exist or is empty;at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:98)at org.apache.spark.sql.hive.HiveExternalCatalog.createPartitions(HiveExternalCatalog.scala:842)at org.apache.spark.sql.catalyst.catalog.SessionCatalog.createPartitions(SessionCatalog.scala:679)at org.apache.spark.sql.hive.CarbonSessionCatalog.createPartitions(CarbonSessionState.scala:155)at org.apache.spark.sql.execution.command.AlterTableAddPartitionCommand.run(ddl.scala:361)at org.apache.spark.sql.execution.datasources.DataSourceAnalysis$$anonfun$apply$1.org$apache$spark$sql$execution$datasources$DataSourceAnalysis$$anonfun$$refreshPartitionsCallback$1(DataSourceStrategy.scala:221)at org.apache.spark.sql.execution.datasources.DataSourceAnalysis$$anonfun$apply$1$$anonfun$8.apply(DataSourceStrategy.scala:243)at org.apache.spark.sql.execution.datasources.DataSourceAnalysis$$anonfun$apply$1$$anonfun$8.apply(DataSourceStrategy.scala:243)at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply$mcV$sp(FileFormatWriter.scala:143)at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:121)at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:121)at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57)at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:121)at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:101)at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114)at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114)at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:135)at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:132)at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:113)at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:87)at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:87)at org.apache.spark.sql.Dataset.<init>(Dataset.scala:185)at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:64)at org.apache.spark.sql.execution.command.management.CarbonLoadDataCommand.overwritePartition(CarbonLoadDataCommand.scala:830)at org.apache.spark.sql.execution.command.management.CarbonLoadDataCommand.loadDataWithPartition(CarbonLoadDataCommand.scala:635)at org.apache.spark.sql.execution.command.management.CarbonLoadDataCommand.loadData(CarbonLoadDataCommand.scala:447)at org.apache.spark.sql.execution.command.management.CarbonLoadDataCommand.processData(CarbonLoadDataCommand.scala:230)at org.apache.spark.sql.execution.command.DataCommand.run(package.scala:71)at org.apache.spark.sql.execution.command.management.CarbonInsertIntoCommand.processData(CarbonInsertIntoCommand.scala:48)at org.apache.spark.sql.execution.command.DataCommand.run(package.scala:71)at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114)at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114)at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:135)at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:132)at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:113)at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:87)at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:87)at org.apache.spark.sql.Dataset.<init>(Dataset.scala:185)at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:64)at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:592)at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:699)at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:220)at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1$$anon$2.run(SparkExecuteStatementOperation.scala:163)at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1$$anon$2.run(SparkExecuteStatementOperation.scala:160)at java.security.AccessController.doPrivileged(Native Method)at javax.security.auth.Subject.doAs(Subject.java:422)at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1698)at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1.run(SparkExecuteStatementOperation.scala:173)at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)at java.util.concurrent.FutureTask.run(FutureTask.java:266)at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)at java.lang.Thread.run(Thread.java:745) 
issueID:CARBONDATA-2042
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/preagg/TimeSeriesUDF.java
texts:Data Mismatch issue in case of Timeseries Year, Month and Day level table
sql(s"LOAD DATA LOCAL INPATH '$resourcesPath/timeseriestest.csv' into table mainTable")sql("CREATE TABLE table_03 (imei string,age int,mac string,productdate timestamp,updatedate timestamp,gamePointId double,contractid double ) STORED BY 'org.apache.carbondata.format'")sql(s"LOAD DATA inpath '$resourcesPath/data_sort.csv' INTO table table_03 options ('DELIMITER'=',', 'QUOTECHAR'='','FILEHEADER'='imei,age,mac,productdate,updatedate,gamePointId,contractid')")sql("create datamap ag1 on table table_03 using 'preaggregate' DMPROPERTIES ( 'timeseries.eventtime'='productdate','timeseries.hierarchy'='second=1,minute=1,hour=1,day=1,month=1,year=1')as select productdate,mac,sum(age) from table_03 group by productdate,mac")
issueID:CARBONDATA-2043
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
core/src/main/java/org/apache/carbondata/core/util/CarbonProperties.java
texts:Configurable wait time for requesting executors and minimum registered executors ratio to continue the block distribution
Configurable wait time for requesting executors and minimum registered executors ratio to continue the block distribution carbon.dynamicallocation.schedulertimeout : to configure wait time. defalt 5sec, Min 5 sec and max 15 sec. carbon.scheduler.minregisteredresourcesratio : min 0.1, max 1.0 and default to 0.8 to configure minimum registered executors ratio.
issueID:CARBONDATA-2044
type:Sub-task
changed files:
texts:IDG update for CARBONDATA-2043  Configurable wait time for requesting executors and minimum registered executors ratio to continue the block distribution
Added below parameters:carbon.dynamicallocation.schedulertimeout :                                                               Description: To configure wait time for executors to bring up.                                             default 5 sec, Min 5 sec and max 15 sec.carbon.scheduler.minregisteredresourcesratio :                                                    Description: To configure minimum registered/started executors ratio to start block distribution.  default to 0.8min 0.1, max 1.0Removed Parameters:carbon.max.executor.startup.timeout – Remove it from IDG
issueID:CARBONDATA-2045
type:Bug
changed files:
texts:Query from segment set is not effective when pre-aggregate table is present
1. Create a tablecreate table if not exists lineitem1(L_SHIPDATE string,L_SHIPMODE string,L_SHIPINSTRUCT string,L_RETURNFLAG string,L_RECEIPTDATE string,L_ORDERKEY string,L_PARTKEY string,L_SUPPKEY string,L_LINENUMBER int,L_QUANTITY double,L_EXTENDEDPRICE double,L_DISCOUNT double,L_TAX double,L_LINESTATUS string,L_COMMITDATE string,L_COMMENT string) STORED BY 'org.apache.carbondata.format' TBLPROPERTIES ('table_blocksize'='128','NO_INVERTED_INDEX'='L_SHIPDATE,L_SHIPMODE,L_SHIPINSTRUCT,L_RETURNFLAG,L_RECEIPTDATE,L_ORDERKEY,L_PARTKEY,L_SUPPKEY','sort_columns'='');2. Run load :load data inpath "hdfs://hacluster/user/test/lineitem.tbl.1" into table lineitem1 options('DELIMITER'='|','FILEHEADER'='L_ORDERKEY,L_PARTKEY,L_SUPPKEY,L_LINENUMBER,L_QUANTITY,L_EXTENDEDPRICE,L_DISCOUNT,L_TAX,L_RETURNFLAG,L_LINESTATUS,L_SHIPDATE,L_COMMITDATE,L_RECEIPTDATE,L_SHIPINSTRUCT,L_SHIPMODE,L_COMMENT');3. create pre-agg table create datamap agr_lineitem3 ON TABLE lineitem3 USING "org.apache.carbondata.datamap.AggregateDataMapHandler" as select L_RETURNFLAG,L_LINESTATUS,sum(L_QUANTITY),sum(L_EXTENDEDPRICE) from lineitem3 group by L_RETURNFLAG, L_LINESTATUS;3. Check table content using aggregate query:select l_returnflag,l_linestatus,sum(l_quantity),sum(l_extendedprice) from lineitem3 group by l_returnflag, l_linestatus;---------------------------+----------------------------------------+-l_returnflagl_linestatussum(l_quantity)sum(l_extendedprice)---------------------------+----------------------------------------+-NF4913382.07.369901176949993E9AF1.88818373E82.8310705145736383E11NO3.82400594E85.734650756707479E11RF1.88960009E82.833523780876951E11---------------------------+----------------------------------------+-4 rows selected (1.568 seconds)4. Load one more time:load data inpath "hdfs://hacluster/user/test/lineitem.tbl.1" into table lineitem1 options('DELIMITER'='|','FILEHEADER'='L_ORDERKEY,L_PARTKEY,L_SUPPKEY,L_LINENUMBER,L_QUANTITY,L_EXTENDEDPRICE,L_DISCOUNT,L_TAX,L_RETURNFLAG,L_LINESTATUS,L_SHIPDATE,L_COMMITDATE,L_RECEIPTDATE,L_SHIPINSTRUCT,L_SHIPMODE,L_COMMENT');5. Check table content using aggregate query:select l_returnflag,l_linestatus,sum(l_quantity),sum(l_extendedprice) from lineitem3 group by l_returnflag, l_linestatus;---------------------------+----------------------------------------+-l_returnflagl_linestatussum(l_quantity)sum(l_extendedprice)---------------------------+----------------------------------------+-NF9826764.01.4739802353899986E10AF3.77636746E85.662141029147278E11NO7.64801188E81.1469301513414958E12RF3.77920018E85.667047561753901E11---------------------------+----------------------------------------+-6. Set query from segment 1:0: jdbc:hive2://10.18.98.48:23040> set carbon.input.segments.test_db1.lilneitem1=1;-------------------------------------------------++-keyvalue-------------------------------------------------++-carbon.input.segments.test_db1.lilneitem11-------------------------------------------------++-7. Check table content using aggregate query:select l_returnflag,l_linestatus,sum(l_quantity),sum(l_extendedprice) from lineitem3 group by l_returnflag, l_linestatus;Expected: It should return the values from segment 1 alone.Actual : : It returns values from both segments---------------------------+----------------------------------------+-l_returnflagl_linestatussum(l_quantity)sum(l_extendedprice)---------------------------+----------------------------------------+-NF9826764.01.4739802353899986E10AF3.77636746E85.662141029147278E11NO7.64801188E81.1469301513414958E12RF3.77920018E85.667047561753901E11---------------------------+----------------------------------------+-
issueID:CARBONDATA-2046
type:Bug
changed files:
texts:agg Query failed when non supported aggregate is present in Query
Run below Query where var_samp is not supported aggregate for aggregate tablespark.sql( s"""create datamap preagg_sum on table tbl_1 using 'preaggregate' as select mac,avg(age) from tbl_1 group by mac""" .stripMargin)spark.sql("select var_samp(mac) from tbl_1 where mac='Mikaa1' ").explain()Exception :-Exception in thread "main" org.apache.spark.sql.AnalysisException: resolved attribute(s) mac#2 missing from tbl_1_mac#56,tbl_1_age_sum#57L,tbl_1_age_count#58L in operator !Aggregate var_samp(cast(mac#2 as double)) AS var_samp(CAST(mac AS DOUBLE))#59;;!Aggregate var_samp(cast(mac#2 as double)) AS var_samp(CAST(mac AS DOUBLE))#59+- Filter (tbl_1_mac#56 = Mikaa1)   +- Relationtbl_1_mac#56,tbl_1_age_sum#57L,tbl_1_age_count#58L CarbonDatasourceHadoopRelation [ Database name :default, Table name :tbl_1_preagg_sum, Schema :Some(StructType(StructField(tbl_1_mac,StringType,true), StructField(tbl_1_age_sum,LongType,true), StructField(tbl_1_age_count,LongType,true))) ]
issueID:CARBONDATA-2047
type:Improvement
changed files:hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonTableOutputFormat.java
processing/src/main/java/org/apache/carbondata/processing/store/writer/AbstractFactDataWriter.java
texts:Clean up temp folder after task completion in case of partitioning
Clean up temp folder after task completion in case of partitioning
issueID:CARBONDATA-2048
type:Bug
changed files:
texts:Data delete should be rejected when insert overwrite is in progress

issueID:CARBONDATA-2049
type:Bug
changed files:
texts:CarbonCleanFilesCommand table path problem
In CarbonCleanFilesCommand datbaseLocation is being passed instead of the tablePath in case of forceclean.And in case of cleanGarbageData, storeLocation is being passed instead of the tablePath.
issueID:CARBONDATA-205
type:Bug
changed files:
texts:Can&#39;t pass compile, the case of DataCompactionLockTest is failed.
Can't pass compile, the case of DataCompactionLockTest is failed.[32mDataCompactionLockTest: [0m [31m- check if compaction is failed or not. *** FAILED *** [0m [31m  status was false (DataCompactionLockTest.scala:118) [0mfrom/incubator-carbondata/integration/spark/target/surefire-reports/CarbonTestSuite.txt
issueID:CARBONDATA-2050
type:Task
changed files:
texts:Add example of query data with specified segments
Add example of query data with specified segments and also consider compaction sceanrios 
issueID:CARBONDATA-2051
type:Bug
changed files:
texts:Added like query ends with and contains with filter push down suport to carbondata
ProblemCurrent like filter with start with expression is only pushed down to carbondata. In case of ends with and contains like filter all the data is given back to spark and then spark applies the filter on it.This behavior is fine for the queries which has lesser number of queried columns. But as the number of columns and data increases there is performance impact because the data being sent to spark becomes more thereby increasing the IO. If like filter is push down then first filter column is read and blocks are pruned. In this cases the data returned to the spark is after applying the filter and only blocklets matching the data are fully read. This reduces IO and increases the query performance.SolutionModify code to push down like query with ends and contains with filter
issueID:CARBONDATA-2053
type:Bug
changed files:
texts:Add events for streaming

issueID:CARBONDATA-2054
type:Task
changed files:streaming/src/main/java/org/apache/carbondata/streaming/segment/StreamSegment.java
texts:Add an example: how to use CarbonData batch load to integrate with Spark Streaming.
This example introduces how to use CarbonData batch load to integrate with Spark Streaming:Use CarbonSession.createDataFrame to convert rdd to DataFrame in DStream.foreachRDD, and then write batch data into CarbonData table which support auto compaction.
issueID:CARBONDATA-2055
type:New Feature
changed files:streaming/src/main/java/org/apache/carbondata/streaming/parser/CarbonStreamParser.java
texts:Support integrating Streaming table with Spark Streaming
Currently CarbonData just support integrating with Spark Structured Streaming which requires Kafka's version must be >= 0.10. But there are still many users  integrating Spark Streaming with kafka 0.8, the cost of upgrading kafka is too much. So CarbonData need to integrate with Spark Streaming too.Please see the discussion in mailing list:http://apache-carbondata-dev-mailing-list-archive.1130556.n5.nabble.com/Should-CarbonData-need-to-integrate-with-Spark-Streaming-too-td35341.html 
issueID:CARBONDATA-2056
type:Bug
changed files:
texts:Hadoop Configuration with access key and secret key should be passed while creating InputStream of distributed carbon file.

issueID:CARBONDATA-2057
type:Bug
changed files:
texts:Support specify path when creating pre-aggregate table
When creating datamap of pre-aggreagate table, user should be able to specify the location of it. By using syntax:CREATE DATAMAP agg ON TABLE main USING 'preaggregate'DMPROPERTIES ('path'='datamap_storage_path')AS SELECT ... 
issueID:CARBONDATA-2058
type:Bug
changed files:streaming/src/main/java/org/apache/carbondata/streaming/segment/StreamSegment.java
streaming/src/main/java/org/apache/carbondata/streaming/CarbonStreamRecordWriter.java
texts:Streaming throw NullPointerException after batch loading
Driver stacktrace:at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1478)at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1466)at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1465)at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1465)at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:813)at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:813)at scala.Option.foreach(Option.scala:257)at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:813)at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1693)at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1648)at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1637)at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:639)at org.apache.spark.SparkContext.runJob(SparkContext.scala:1949)at org.apache.spark.SparkContext.runJob(SparkContext.scala:1962)at org.apache.spark.SparkContext.runJob(SparkContext.scala:1982)at org.apache.spark.sql.execution.streaming.CarbonAppendableStreamSink$$anonfun$writeDataFileJob$1.apply$mcV$sp(CarbonAppendableStreamSink.scala:197)... 20 moreCaused by: org.apache.carbondata.streaming.CarbonStreamException: Task failed while writing rowsat org.apache.spark.sql.execution.streaming.CarbonAppendableStreamSink$.writeDataFileTask(CarbonAppendableStreamSink.scala:295)at org.apache.spark.sql.execution.streaming.CarbonAppendableStreamSink$$anonfun$writeDataFileJob$1$$anonfun$apply$mcV$sp$1.apply(CarbonAppendableStreamSink.scala:199)at org.apache.spark.sql.execution.streaming.CarbonAppendableStreamSink$$anonfun$writeDataFileJob$1$$anonfun$apply$mcV$sp$1.apply(CarbonAppendableStreamSink.scala:198)at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)at org.apache.spark.scheduler.Task.run(Task.scala:99)at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)at java.lang.Thread.run(Thread.java:748)Caused by: java.lang.NullPointerExceptionat org.apache.carbondata.hadoop.streaming.CarbonStreamRecordWriter.appendBlockletToDataFile(CarbonStreamRecordWriter.java:287)at org.apache.carbondata.hadoop.streaming.CarbonStreamRecordWriter.close(CarbonStreamRecordWriter.java:300)at org.apache.carbondata.streaming.segment.StreamSegment.appendBatchData(StreamSegment.java:276)at org.apache.spark.sql.execution.streaming.CarbonAppendableStreamSink$$anonfun$writeDataFileTask$1.apply$mcV$sp(CarbonAppendableStreamSink.scala:286)at org.apache.spark.sql.execution.streaming.CarbonAppendableStreamSink$$anonfun$writeDataFileTask$1.apply(CarbonAppendableStreamSink.scala:276)at org.apache.spark.sql.execution.streaming.CarbonAppendableStreamSink$$anonfun$writeDataFileTask$1.apply(CarbonAppendableStreamSink.scala:276)at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1388)at org.apache.spark.sql.execution.streaming.CarbonAppendableStreamSink$.writeDataFileTask(CarbonAppendableStreamSink.scala:288)... 8 more
issueID:CARBONDATA-206
type:Improvement
changed files:
texts:Two same name class, need to optimize.
there are two same name classes, need to optimize :integration/spark/src/test/scala/org/apache/carbondata/spark/load/CarbonLoaderUtilTest.javaintegration/spark/src/test/java/org/carbondata/integration/spark/load/CarbonLoaderUtilTest.javaPropose to rename or delete one class to avoid confusion.
issueID:CARBONDATA-2060
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonOutputCommitter.java
texts:Fix InsertOverwrite on partition table
when partition table overwrite with empty table , it is not overwriting the partition table , and when insert overwrite is done on dynamic partition table , overwrite was not happening. sql("create table partitionLoadTable(name string, age int) PARTITIONED BY(address string) stored by 'carbondata'")sql("insert into partitionLoadTable select 'abc',4,'def'")sql("insert into partitionLoadTable select 'abd',5,'xyz'")sql("create table noLoadTable (name string, age int, address string) stored by 'carbondata'")sql("insert overwrite table partitionLoadTable select * from noLoadTable")when we do select * after insert overwrite operation, ideally it should give empty data, but it is giving all data. sql("CREATE TABLE uniqdata_hive_static (CUST_ID int,CUST_NAME String,ACTIVE_EMUI_VERSION string, DOB timestamp, DOJ timestamp, BIGINT_COLUMN1 bigint,BIGINT_COLUMN2 bigint,DECIMAL_COLUMN1 decimal(30,10), DECIMAL_COLUMN2 decimal(36,10),Double_COLUMN1 double, Double_COLUMN2 double, INTEGER_COLUMN1 int)ROW FORMAT DELIMITED FIELDS TERMINATED BY ','")sql("CREATE TABLE uniqdata_string_static(CUST_ID int,CUST_NAME String,DOB timestamp,DOJ timestamp, BIGINT_COLUMN1 bigint,BIGINT_COLUMN2 bigint,DECIMAL_COLUMN1 decimal(30,10),DECIMAL_COLUMN2 decimal(36,10),Double_COLUMN1 double, Double_COLUMN2 double,INTEGER_COLUMN1 int) PARTITIONED BY(ACTIVE_EMUI_VERSION string) STORED BY 'org.apache.carbondata.format' TBLPROPERTIES ('TABLE_BLOCKSIZE'= '256 MB')")sql(s"LOAD DATA INPATH '$resourcesPath/partData.csv' into table uniqdata_string_static OPTIONS('FILEHEADER'='CUST_ID,CUST_NAME ,ACTIVE_EMUI_VERSION,DOB,DOJ, BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1, Double_COLUMN2,INTEGER_COLUMN1','BAD_RECORDS_ACTION'='FORCE')")sql(s"LOAD DATA INPATH '$resourcesPath/partData.csv' into table uniqdata_string_static OPTIONS('FILEHEADER'='CUST_ID,CUST_NAME ,ACTIVE_EMUI_VERSION,DOB,DOJ, BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1, Double_COLUMN2,INTEGER_COLUMN1','BAD_RECORDS_ACTION'='FORCE')")sql("insert overwrite table uniqdata_string_static select CUST_ID, CUST_NAME,DOB,doj, bigint_column1, bigint_column2, decimal_column1, decimal_column2,double_column1, double_column2,integer_column1,active_emui_version from uniqdata_hive_static limit 10") after this, select * was giving result, ideally it should give empty result. 
issueID:CARBONDATA-2061
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/statusmanager/SegmentStatusManager.java
processing/src/main/java/org/apache/carbondata/processing/util/CarbonLoaderUtil.java
texts:Check for only valid IN_PROGRESS segments
While checking for IN_PROGRESS segments of a table during other operations, we should check only for valid IN_PROGRESS segments. Some segments may be invalid like cancelled and may still in IN_PROGRESS state,those segments should be considered as stale segments.
issueID:CARBONDATA-2063
type:Bug
changed files:
texts:Tests should not depend on each other
Tests should not depend on each other, otherwise tests will fail in developer's IDE if they don't execute all the tests at once.
issueID:CARBONDATA-2064
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
processing/src/main/java/org/apache/carbondata/processing/loading/events/LoadEvents.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonOutputCommitter.java
processing/src/main/java/org/apache/carbondata/processing/merger/CompactionType.java
texts:Add compaction listener

issueID:CARBONDATA-2066
type:Bug
changed files:
texts:uniqwithoutheader.csv file gets deleted when "duplicate values" test case is run
when "duplicate values" UT is run, uniqwithoutheader.csv file used to load into hive gets deleted.
issueID:CARBONDATA-2067
type:Bug
changed files:
texts:Streaming hand off operation throw NullPointerException
18/01/23 16:01:10 ERROR CompactionResultSortProcessor: Executor task launch worker for task 0 Compaction failed: null java.lang.NullPointerException at org.apache.carbondata.processing.util.CarbonDataProcessorUtil.getLocalDataFolderLocation(CarbonDataProcessorUtil.java:152) at org.apache.carbondata.processing.merger.CompactionResultSortProcessor.initTempStoreLocation(CompactionResultSortProcessor.java:424) at org.apache.carbondata.processing.merger.CompactionResultSortProcessor.execute(CompactionResultSortProcessor.java:156) at org.apache.carbondata.streaming.StreamHandoffRDD.internalCompute(StreamHandoffRDD.scala:113) at org.apache.carbondata.spark.rdd.CarbonRDD.compute(CarbonRDD.scala:60) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) at org.apache.spark.rdd.RDD.iterator(RDD.scala:287) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87) at org.apache.spark.scheduler.Task.run(Task.scala:108) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745)
issueID:CARBONDATA-2068
type:Bug
changed files:
texts:Drop datamap should  work for timeseries
Drop datamap is not  work after creating timeseries datamap for preaggregate table,but it should  work.refer：https://issues.apache.org/jira/browse/CARBONDATA-1516
issueID:CARBONDATA-2069
type:Bug
changed files:
texts:Data is not loaded into preaggregate table when table is created when data load is in progress for main table
Problem: Load data into maintable create datamap parallellypreaggregate table will not have any data while data load is successful for main table. This will make the pre-aggregate table inconsistent
issueID:CARBONDATA-207
type:Improvement
changed files:
texts:The document of "DDL operations on CarbonData", "MINOR/MAJOR" in compaction section need to provide more detail explanation
The document of "DDL operations on CarbonData", "MINOR/MAJOR" in compaction section need to provide more detail explanation.
issueID:CARBONDATA-2070
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/metadata/schema/table/CarbonTable.java
texts:when hive metastore is enabled, create preaggregate table on decimal column of main table is failing
steps:Enable hive metastore and run the following queries1)CREATE TABLE uniqdata(CUST_ID int,CUST_NAME String,ACTIVE_EMUI_VERSION string,DOB timestamp,DOJ timestamp, BIGINT_COLUMN1 bigint,BIGINT_COLUMN2 bigint,DECIMAL_COLUMN1 decimal(30,10),DECIMAL_COLUMN2 decimal(36,10),Double_COLUMN1 double, Double_COLUMN2 double,INTEGER_COLUMN1 int) STORED BY 'org.apache.carbondata.format' 2)insert into uniqdata select 9000,'CUST_NAME_00000','ACTIVE_EMUI_VERSION_00000','1970-01-01 01:00:03','1970-01-01 02:00:03',123372036854,-223372036854,12345678901.1234000000,22345678901.1234000000,11234567489.7976000000,-11234567489.7976000000,1 3)create datamap uniqdata_agg on table uniqdata using 'preaggregate' as select min(DECIMAL_COLUMN1) from uniqdata group by DECIMAL_COLUMN1 java.lang.ClassCastException: org.apache.carbondata.core.metadata.datatype.DataType cannot be cast to org.apache.carbondata.core.metadata.datatype.DecimalType    at org.apache.carbondata.core.metadata.schema.table.column.ColumnSchema.write(ColumnSchema.java:478)    at org.apache.carbondata.core.metadata.schema.table.TableSchema.write(TableSchema.java:215)    at org.apache.carbondata.core.metadata.schema.table.DataMapSchema.write(DataMapSchema.java:99)    at org.apache.carbondata.core.metadata.schema.table.TableInfo.write(TableInfo.java:245)    at org.apache.carbondata.core.metadata.schema.table.TableInfo.serialize(TableInfo.java:304)    at org.apache.spark.sql.CarbonDatasourceHadoopRelation.buildScan(CarbonDatasourceHadoopRelation.scala:83)    at org.apache.spark.sql.execution.strategy.CarbonLateDecodeStrategy$$anonfun$1.apply(CarbonLateDecodeStrategy.scala:63)    at org.apache.spark.sql.execution.strategy.CarbonLateDecodeStrategy$$anonfun$1.apply(CarbonLateDecodeStrategy.scala:63)    at org.apache.spark.sql.execution.strategy.CarbonLateDecodeStrategy$$anonfun$pruneFilterProject$1.apply(CarbonLateDecodeStrategy.scala:178)    at org.apache.spark.sql.execution.strategy.CarbonLateDecodeStrategy$$anonfun$pruneFilterProject$1.apply(CarbonLateDecodeStrategy.scala:177)    at org.apache.spark.sql.execution.strategy.CarbonLateDecodeStrategy.getDataSourceScan(CarbonLateDecodeStrategy.scala:366)    at org.apache.spark.sql.execution.strategy.CarbonLateDecodeStrategy.pruneFilterProjectRaw(CarbonLateDecodeStrategy.scala:299)    at org.apache.spark.sql.execution.strategy.CarbonLateDecodeStrategy.pruneFilterProject(CarbonLateDecodeStrategy.scala:172)    at org.apache.spark.sql.execution.strategy.CarbonLateDecodeStrategy.apply(CarbonLateDecodeStrategy.scala:59)    at org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$1.apply(QueryPlanner.scala:62)    at org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$1.apply(QueryPlanner.scala:62)    at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)    at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)    at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)    at org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:92)    at org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$2$$anonfun$apply$2.apply(QueryPlanner.scala:77)    at org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$2$$anonfun$apply$2.apply(QueryPlanner.scala:74)    at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)    at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)    at scala.collection.Iterator$class.foreach(Iterator.scala:893)    at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)    at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157)
issueID:CARBONDATA-2071
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockletDataMapModel.java
hadoop/src/main/java/org/apache/carbondata/hadoop/CarbonMultiBlockSplit.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockletDataMap.java
core/src/main/java/org/apache/carbondata/core/indexstore/BlockletDataMapIndexStore.java
core/src/main/java/org/apache/carbondata/core/indexstore/BlockMetaInfo.java
core/src/main/java/org/apache/carbondata/core/indexstore/BlockletDetailInfo.java
texts:Add block size to BblockletDataMap while initialising
Add block size to blocklet datamap so that features like small file merge can use them
issueID:CARBONDATA-2073
type:Test
changed files:
texts:Add test cases for pre aggregate table
Add test cases for pre aggregate table, include timeseries and so on.
issueID:CARBONDATA-2075
type:Bug
changed files:common/src/main/java/org/apache/carbondata/common/exceptions/sql/NoSuchDataMapException.java
texts:When dropping datamap without IF EXIST, if the datamap does not exist, we should throw MalformedCarbonCommandException
When dropping datamap without IF EXIST, if the datamap does not exist, we should throw MalformedCarbonCommandException, currently it does not.
issueID:CARBONDATA-2076
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/util/SessionParams.java
processing/src/main/java/org/apache/carbondata/processing/loading/events/LoadEvents.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonOutputCommitter.java
core/src/main/java/org/apache/carbondata/core/indexstore/BlockletDataMapIndexStore.java
texts:Refactored code segregated process meta and process data in load command
Refactored code segregated process meta and process data in load and Compaction command class
issueID:CARBONDATA-2077
type:Bug
changed files:
texts:Drop datamap should throw exception if table doesn&#39;t exist, even though there is IF EXISTS
Drop datamap should throw exception if table doesn't exist, even though there is IF EXISTS
issueID:CARBONDATA-2078
type:Improvement
changed files:
texts:Add “IF NOT EXISTS” feature for create datamap
Add “IF NOT EXISTS” feature for create datamap
issueID:CARBONDATA-2079
type:Bug
changed files:
texts:Error displays while executing minor compaction on the cluster
Error displays while executing minor compaction on the cluster:Steps to Reproduce:1) Create Table:CREATE TABLE uniqdata_batchsort_compact (CUST_ID int,CUST_NAME String,ACTIVE_EMUI_VERSION string, DOB timestamp, DOJ timestamp, BIGINT_COLUMN1 bigint,BIGINT_COLUMN2 bigint,DECIMAL_COLUMN1 decimal(30,10), DECIMAL_COLUMN2 decimal(36,10),Double_COLUMN1 double, Double_COLUMN2 double,INTEGER_COLUMN1 int) STORED BY 'carbondata' TBLPROPERTIES('SORT_SCOPE'='BATCH_SORT')2) Load Data :LOAD DATA INPATH 'HDFS_URL/BabuStore/Data/uniqdata/7000_UniqData.csv' into table uniqdata_batchsort_compact OPTIONS('DELIMITER'=',' , 'QUOTECHAR'='"','BAD_RECORDS_ACTION'='FORCE','FILEHEADER'='CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1,Double_COLUMN2,INTEGER_COLUMN1','batch_sort_size_inmb'='1')LOAD DATA INPATH 'HDFS_URL/BabuStore/Data/uniqdata/7000_UniqData.csv' into table uniqdata_batchsort_compact OPTIONS('DELIMITER'=',' , 'QUOTECHAR'='"','BAD_RECORDS_ACTION'='FORCE','FILEHEADER'='CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1,Double_COLUMN2,INTEGER_COLUMN1','batch_sort_size_inmb'='1')LOAD DATA INPATH 'HDFS_URL/BabuStore/Data/uniqdata/7000_UniqData.csv' into table uniqdata_batchsort_compact OPTIONS('DELIMITER'=',' , 'QUOTECHAR'='"','BAD_RECORDS_ACTION'='FORCE','FILEHEADER'='CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1,Double_COLUMN2,INTEGER_COLUMN1','batch_sort_size_inmb'='1')LOAD DATA INPATH 'HDFS_URL/BabuStore/Data/uniqdata/7000_UniqData.csv' into table uniqdata_batchsort_compact OPTIONS('DELIMITER'=',' , 'QUOTECHAR'='"','BAD_RECORDS_ACTION'='FORCE','FILEHEADER'='CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1,Double_COLUMN2,INTEGER_COLUMN1','batch_sort_size_inmb'='1')3)Execute Queries:alter table uniqdata_batchsort_compact compact 'minor'output:Compaction failed. Please check logs for more info. Exception in compaction / by zerologs:&#91;exec&#93; 18/01/24 16:47:33 INFO SelectQuery: Executing Query: alter table uniqdata_batchsort_compact compact 'minor' &#91;exec&#93; 18/01/24 16:47:33 INFO CarbonSparkSqlParser: Parsing command: alter table uniqdata_batchsort_compact compact 'minor' &#91;exec&#93; 18/01/24 16:47:33 INFO CarbonLateDecodeRule: main skip CarbonOptimizer &#91;exec&#93; 18/01/24 16:47:33 INFO CarbonLateDecodeRule: main Skip CarbonOptimizer &#91;exec&#93; 18/01/24 16:47:33 INFO HiveMetaStore: 0: get_table : db=default tbl=uniqdata_batchsort_compact &#91;exec&#93; 18/01/24 16:47:33 INFO audit: ugi=root ip=unknown-ip-addr cmd=get_table : db=default tbl=uniqdata_batchsort_compact  &#91;exec&#93; 18/01/24 16:47:33 INFO CatalystSqlParser: Parsing command: array<string> &#91;exec&#93; 18/01/24 16:47:33 INFO HiveMetaStore: 0: get_table : db=default tbl=uniqdata_batchsort_compact &#91;exec&#93; 18/01/24 16:47:33 INFO audit: ugi=root ip=unknown-ip-addr cmd=get_table : db=default tbl=uniqdata_batchsort_compact  &#91;exec&#93; 18/01/24 16:47:33 INFO CatalystSqlParser: Parsing command: array<string> &#91;exec&#93; 18/01/24 16:47:33 AUDIT CarbonAlterTableCompactionCommand: &#91;hadoop-master&#93;&#91;root&#93;&#91;Thread-1&#93;Compaction request received for table default.uniqdata_batchsort_compact &#91;exec&#93; 18/01/24 16:47:33 INFO HdfsFileLock: main HDFS lock path:hdfs://hadoop-master:54311//opt/CarbonStore/default/uniqdata_batchsort_compact/compaction.lock &#91;exec&#93; 18/01/24 16:47:34 INFO CarbonAlterTableCompactionCommand: main Acquired the compaction lock for table default.uniqdata_batchsort_compact &#91;exec&#93; 18/01/24 16:47:34 INFO CarbonTableCompactor: main loads identified for merge is 0 &#91;exec&#93; 18/01/24 16:47:34 INFO CarbonTableCompactor: main loads identified for merge is 1 &#91;exec&#93; 18/01/24 16:47:34 INFO CarbonTableCompactor: main loads identified for merge is 2 &#91;exec&#93; 18/01/24 16:47:34 INFO CarbonTableCompactor: main loads identified for merge is 3 &#91;exec&#93; 18/01/24 16:47:34 INFO CarbonTableCompactor: main spark.executor.instances property is set to = 3 &#91;exec&#93; 18/01/24 16:47:34 INFO TableInfo: main Table block size not specified for default_uniqdata_batchsort_compact. Therefore considering the default value 1024 MB &#91;exec&#93; 18/01/24 16:47:34 INFO BlockletDataMap: main Time taken to load blocklet datamap from file : hdfs://hadoop-master:54311//opt/CarbonStore/default/uniqdata_batchsort_compact/Fact/Part0/Segment_0/0_batchno0-0-1516792651041.carbonindexis 0 &#91;exec&#93; 18/01/24 16:47:34 INFO BlockletDataMap: main Time taken to load blocklet datamap from file : hdfs://hadoop-master:54311//opt/CarbonStore/default/uniqdata_batchsort_compact/Fact/Part0/Segment_1/0_batchno0-0-1516792651780.carbonindexis 1 &#91;exec&#93; 18/01/24 16:47:34 INFO BlockletDataMap: main Time taken to load blocklet datamap from file : hdfs://hadoop-master:54311//opt/CarbonStore/default/uniqdata_batchsort_compact/Fact/Part0/Segment_2/0_batchno0-0-1516792652481.carbonindexis 0 &#91;exec&#93; 18/01/24 16:47:34 INFO BlockletDataMap: main Time taken to load blocklet datamap from file : hdfs://hadoop-master:54311//opt/CarbonStore/default/uniqdata_batchsort_compact/Fact/Part0/Segment_3/0_batchno0-0-1516792653232.carbonindexis 0 &#91;exec&#93; 18/01/24 16:47:34 ERROR CarbonTableCompactor: main Exception in compaction thread / by zero &#91;exec&#93; java.lang.ArithmeticException: / by zero &#91;exec&#93; at org.apache.carbondata.processing.util.CarbonLoaderUtil.nodeBlockMapping(CarbonLoaderUtil.java:524) &#91;exec&#93; at org.apache.carbondata.processing.util.CarbonLoaderUtil.nodeBlockMapping(CarbonLoaderUtil.java:453) &#91;exec&#93; at org.apache.carbondata.spark.rdd.CarbonMergerRDD.getPartitions(CarbonMergerRDD.scala:400) &#91;exec&#93; at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:252) &#91;exec&#93; at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:250) &#91;exec&#93; at scala.Option.getOrElse(Option.scala:121) &#91;exec&#93; at org.apache.spark.rdd.RDD.partitions(RDD.scala:250) &#91;exec&#93; at org.apache.spark.SparkContext.runJob(SparkContext.scala:1958) &#91;exec&#93; at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:935) &#91;exec&#93; at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151) &#91;exec&#93; at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112) &#91;exec&#93; at org.apache.spark.rdd.RDD.withScope(RDD.scala:362) &#91;exec&#93; at org.apache.spark.rdd.RDD.collect(RDD.scala:934) &#91;exec&#93; at org.apache.carbondata.spark.rdd.CarbonTableCompactor.triggerCompaction(CarbonTableCompactor.scala:211) &#91;exec&#93; at org.apache.carbondata.spark.rdd.CarbonTableCompactor.scanSegmentsAndSubmitJob(CarbonTableCompactor.scala:120) &#91;exec&#93; at org.apache.carbondata.spark.rdd.CarbonTableCompactor.executeCompaction(CarbonTableCompactor.scala:71) &#91;exec&#93; at org.apache.carbondata.spark.rdd.CarbonDataRDDFactory$$anon$2.run(CarbonDataRDDFactory.scala:182) &#91;exec&#93; at org.apache.carbondata.spark.rdd.CarbonDataRDDFactory$.startCompactionThreads(CarbonDataRDDFactory.scala:269) &#91;exec&#93; at org.apache.spark.sql.execution.command.management.CarbonAlterTableCompactionCommand.alterTableForCompaction(CarbonAlterTableCompactionCommand.scala:258) &#91;exec&#93; at org.apache.spark.sql.execution.command.management.CarbonAlterTableCompactionCommand.processData(CarbonAlterTableCompactionCommand.scala:111) &#91;exec&#93; at org.apache.spark.sql.execution.command.DataCommand.run(package.scala:71) &#91;exec&#93; at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58) &#91;exec&#93; at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56) &#91;exec&#93; at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74) &#91;exec&#93; at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114) &#91;exec&#93; at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114) &#91;exec&#93; at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:135) &#91;exec&#93; at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151) &#91;exec&#93; at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:132) &#91;exec&#93; at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:113) &#91;exec&#93; at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:87) &#91;exec&#93; at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:87) &#91;exec&#93; at org.apache.spark.sql.Dataset.<init>(Dataset.scala:185) &#91;exec&#93; at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:64) &#91;exec&#93; at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:592) &#91;exec&#93; at com.huawei.spark.SessionManager.sql(SessionManager.java:42) &#91;exec&#93; at com.huawei.querymanagement.QueryManagement.sql(QueryManagement.java:62) &#91;exec&#93; at com.huawei.querymanagement.SelectQuery.testQuery(SelectQuery.java:70) &#91;exec&#93; at sun.reflect.GeneratedMethodAccessor66.invoke(Unknown Source) &#91;exec&#93; at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) &#91;exec&#93; at java.lang.reflect.Method.invoke(Method.java:498) &#91;exec&#93; at org.junit.internal.runners.TestMethod.invoke(TestMethod.java:59) &#91;exec&#93; at org.junit.internal.runners.MethodRoadie.runTestMethod(MethodRoadie.java:98) &#91;exec&#93; at org.junit.internal.runners.MethodRoadie$2.run(MethodRoadie.java:79) &#91;exec&#93; at org.junit.internal.runners.MethodRoadie.runBeforesThenTestThenAfters(MethodRoadie.java:87) &#91;exec&#93; at org.junit.internal.runners.MethodRoadie.runTest(MethodRoadie.java:77) &#91;exec&#93; at org.junit.internal.runners.MethodRoadie.run(MethodRoadie.java:42) &#91;exec&#93; at org.junit.internal.runners.JUnit4ClassRunner.invokeTestMethod(JUnit4ClassRunner.java:88) &#91;exec&#93; at org.junit.internal.runners.JUnit4ClassRunner.runMethods(JUnit4ClassRunner.java:51) &#91;exec&#93; at org.junit.runners.Parameterized$TestClassRunnerForParameters.run(Parameterized.java:98) &#91;exec&#93; at org.junit.internal.runners.CompositeRunner.runChildren(CompositeRunner.java:33) &#91;exec&#93; at org.junit.runners.Parameterized.access$000(Parameterized.java:55) &#91;exec&#93; at org.junit.runners.Parameterized$1.run(Parameterized.java:131) &#91;exec&#93; at org.junit.internal.runners.ClassRoadie.runUnprotected(ClassRoadie.java:27) &#91;exec&#93; at org.junit.internal.runners.ClassRoadie.runProtected(ClassRoadie.java:37) &#91;exec&#93; at org.junit.runners.Parameterized.run(Parameterized.java:129) &#91;exec&#93; at org.junit.internal.runners.CompositeRunner.runChildren(CompositeRunner.java:33) &#91;exec&#93; at org.junit.internal.runners.CompositeRunner.run(CompositeRunner.java:28) &#91;exec&#93; at org.junit.runner.JUnitCore.run(JUnitCore.java:130) &#91;exec&#93; at org.junit.runner.JUnitCore.run(JUnitCore.java:109) &#91;exec&#93; at org.junit.runner.JUnitCore.run(JUnitCore.java:100) &#91;exec&#93; at org.junit.runner.JUnitCore.runClasses(JUnitCore.java:60) &#91;exec&#93; at com.huawei.querymanagement.SelectQuerySuite.main(SelectQuerySuite.java:18) &#91;exec&#93; at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) &#91;exec&#93; at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) &#91;exec&#93; at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) &#91;exec&#93; at java.lang.reflect.Method.invoke(Method.java:498) &#91;exec&#93; at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:738) &#91;exec&#93; at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187) &#91;exec&#93; at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212) &#91;exec&#93; at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126) &#91;exec&#93; at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala) &#91;exec&#93; 18/01/24 16:47:34 ERROR CarbonDataRDDFactory$: main Exception in compaction thread / by zero &#91;exec&#93; 18/01/24 16:47:34 INFO HdfsFileLock: main Deleted the lock file hdfs://hadoop-master:54311//opt/CarbonStore/default/uniqdata_batchsort_compact/compaction.lock &#91;exec&#93; 18/01/24 16:47:34 ERROR CarbonAlterTableCompactionCommand: main Exception in start compaction thread. Exception in compaction / by zero &#91;exec&#93; 18/01/24 16:47:34 ERROR HdfsFileLock: main Not able to delete the lock file because it is not existed in location hdfs://hadoop-master:54311//opt/CarbonStore/default/uniqdata_batchsort_compact/compaction.lock &#91;exec&#93; 18/01/24 16:47:34 ERROR SelectQuery: An exception has occurred:  &#91;exec&#93; org.apache.spark.sql.AnalysisException: Compaction failed. Please check logs for more info. Exception in compaction / by zero; &#91;exec&#93; at org.apache.spark.sql.util.CarbonException$.analysisException(CarbonException.scala:23) &#91;exec&#93; at org.apache.spark.sql.execution.command.management.CarbonAlterTableCompactionCommand.processData(CarbonAlterTableCompactionCommand.scala:120) &#91;exec&#93; at org.apache.spark.sql.execution.command.DataCommand.run(package.scala:71) &#91;exec&#93; at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58) &#91;exec&#93; at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56) &#91;exec&#93; at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74) &#91;exec&#93; at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114) &#91;exec&#93; at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114) &#91;exec&#93; at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:135) &#91;exec&#93; at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151) &#91;exec&#93; at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:132) &#91;exec&#93; at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:113) &#91;exec&#93; at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:87) &#91;exec&#93; at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:87) &#91;exec&#93; at org.apache.spark.sql.Dataset.<init>(Dataset.scala:185) &#91;exec&#93; at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:64) &#91;exec&#93; at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:592) &#91;exec&#93; at com.huawei.spark.SessionManager.sql(SessionManager.java:42) &#91;exec&#93; at com.huawei.querymanagement.QueryManagement.sql(QueryManagement.java:62) &#91;exec&#93; at com.huawei.querymanagement.SelectQuery.testQuery(SelectQuery.java:70) &#91;exec&#93; at sun.reflect.GeneratedMethodAccessor66.invoke(Unknown Source) &#91;exec&#93; at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) &#91;exec&#93; at java.lang.reflect.Method.invoke(Method.java:498) &#91;exec&#93; at org.junit.internal.runners.TestMethod.invoke(TestMethod.java:59) &#91;exec&#93; at org.junit.internal.runners.MethodRoadie.runTestMethod(MethodRoadie.java:98) &#91;exec&#93; at org.junit.internal.runners.MethodRoadie$2.run(MethodRoadie.java:79) &#91;exec&#93; at org.junit.internal.runners.MethodRoadie.runBeforesThenTestThenAfters(MethodRoadie.java:87) &#91;exec&#93; at org.junit.internal.runners.MethodRoadie.runTest(MethodRoadie.java:77) &#91;exec&#93; at org.junit.internal.runners.MethodRoadie.run(MethodRoadie.java:42) &#91;exec&#93; at org.junit.internal.runners.JUnit4ClassRunner.invokeTestMethod(JUnit4ClassRunner.java:88) &#91;exec&#93; at org.junit.internal.runners.JUnit4ClassRunner.runMethods(JUnit4ClassRunner.java:51) &#91;exec&#93; at org.junit.runners.Parameterized$TestClassRunnerForParameters.run(Parameterized.java:98) &#91;exec&#93; at org.junit.internal.runners.CompositeRunner.runChildren(CompositeRunner.java:33) &#91;exec&#93; at org.junit.runners.Parameterized.access$000(Parameterized.java:55) &#91;exec&#93; at org.junit.runners.Parameterized$1.run(Parameterized.java:131) &#91;exec&#93; at org.junit.internal.runners.ClassRoadie.runUnprotected(ClassRoadie.java:27) &#91;exec&#93; at org.junit.internal.runners.ClassRoadie.runProtected(ClassRoadie.java:37) &#91;exec&#93; at org.junit.runners.Parameterized.run(Parameterized.java:129) &#91;exec&#93; at org.junit.internal.runners.CompositeRunner.runChildren(CompositeRunner.java:33) &#91;exec&#93; at org.junit.internal.runners.CompositeRunner.run(CompositeRunner.java:28) &#91;exec&#93; at org.junit.runner.JUnitCore.run(JUnitCore.java:130) &#91;exec&#93; at org.junit.runner.JUnitCore.run(JUnitCore.java:109) &#91;exec&#93; at org.junit.runner.JUnitCore.run(JUnitCore.java:100) &#91;exec&#93; at org.junit.runner.JUnitCore.runClasses(JUnitCore.java:60) &#91;exec&#93; at com.huawei.querymanagement.SelectQuerySuite.main(SelectQuerySuite.java:18) &#91;exec&#93; at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) &#91;exec&#93; at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) &#91;exec&#93; at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) &#91;exec&#93; at java.lang.reflect.Method.invoke(Method.java:498) &#91;exec&#93; at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:738) &#91;exec&#93; at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187) &#91;exec&#93; at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212) &#91;exec&#93; at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126) &#91;exec&#93; at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
issueID:CARBONDATA-208
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/stats/QueryStatisticsRecorder.java
core/src/main/java/org/apache/carbondata/core/stats/DriverQueryStatisticsRecorderImpl.java
core/src/main/java/org/apache/carbondata/core/stats/QueryStatisticsRecorderImpl.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
core/src/main/java/org/apache/carbondata/core/util/CarbonTimeStatisticsFactory.java
core/src/main/java/org/apache/carbondata/core/scan/executor/impl/AbstractQueryExecutor.java
core/src/main/java/org/apache/carbondata/core/stats/DriverQueryStatisticsRecorderDummy.java
core/src/main/java/org/apache/carbondata/core/stats/QueryStatisticsRecorderDummy.java
texts:User should be able to turn on and off the STATISTIC log
Currently there are many STATISTIC log for performance tuning purpose, but it should be configurable by the user.
issueID:CARBONDATA-2080
type:Bug
changed files:
texts:Hadoop Conf not propagated from driver to executor in S3
On loading data in distributed environment using S3 as location. The load fails because of not getting hadoop conf on executors.Logs Info : 18/01/24 07:38:20 WARN TaskSetManager: Lost task 0.0 in stage 5.0 (TID 7, hadoop-slave-1, executor 1): com.amazonaws.AmazonClientException: Unable to load AWS credentials from any provider in the chainat com.amazonaws.auth.AWSCredentialsProviderChain.getCredentials(AWSCredentialsProviderChain.java:117)at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:3521)at com.amazonaws.services.s3.AmazonS3Client.headBucket(AmazonS3Client.java:1031)at com.amazonaws.services.s3.AmazonS3Client.doesBucketExist(AmazonS3Client.java:994)at org.apache.hadoop.fs.s3a.S3AFileSystem.initialize(S3AFileSystem.java:297)at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2669)at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:94)at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2703)at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2685)at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:373)at org.apache.hadoop.fs.Path.getFileSystem(Path.java:295)at org.apache.carbondata.core.datastore.filesystem.AbstractDFSCarbonFile.<init>(AbstractDFSCarbonFile.java:67)at org.apache.carbondata.core.datastore.filesystem.AbstractDFSCarbonFile.<init>(AbstractDFSCarbonFile.java:59)at org.apache.carbondata.core.datastore.filesystem.HDFSCarbonFile.<init>(HDFSCarbonFile.java:42)at org.apache.carbondata.core.datastore.impl.DefaultFileTypeProvider.getCarbonFile(DefaultFileTypeProvider.java:47)at org.apache.carbondata.core.datastore.impl.FileFactory.getCarbonFile(FileFactory.java:86)at org.apache.carbondata.core.indexstore.blockletindex.SegmentIndexFileStore.getCarbonIndexFiles(SegmentIndexFileStore.java:204)at org.apache.carbondata.core.writer.CarbonIndexFileMergeWriter.mergeCarbonIndexFilesOfSegment(CarbonIndexFileMergeWriter.java:52)at org.apache.carbondata.core.writer.CarbonIndexFileMergeWriter.mergeCarbonIndexFilesOfSegment(CarbonIndexFileMergeWriter.java:119)at org.apache.carbondata.spark.rdd.CarbonMergeFilesRDD$$anon$1.<init>(CarbonMergeFilesRDD.scala:58)at org.apache.carbondata.spark.rdd.CarbonMergeFilesRDD.internalCompute(CarbonMergeFilesRDD.scala:53)at org.apache.carbondata.spark.rdd.CarbonRDD.compute(CarbonRDD.scala:60)at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)at org.apache.spark.scheduler.Task.run(Task.scala:99)
issueID:CARBONDATA-2081
type:Bug
changed files:
texts:some time Spark-sql and beeline operation is not being reflected to each other.
To realize this issue follow these steps :Scenario: 11. open spark-sql and beeline.2. create main table in spark-sql3. create preaggreagate table in beeline.4. drop main table in spark-sql.5. perform 'show tables' operation . PreAggregate table is still not deleted. Scenario: 2perform following operation in same session:create table t5 (c1 string, c2 int) stored by 'carbondata'insert into t5 select 'asd',1alter table t5 rename to t6create table t5 (c1 string, c2 int,c3 string) stored by 'carbondata'insert into t5 select 'asd',1,'sdf' (query is failing)  
issueID:CARBONDATA-2082
type:Bug
changed files:
texts:Timeseries pre-aggregate table should support the blank space
timeseries pre-aggregate table should support the blank space1.scenario 1   test("test timeseries create table 35: support event_time and granularity key with space") {     sql("DROP DATAMAP IF EXISTS agg1_month ON TABLE maintable")     sql(       s"""CREATE DATAMAP agg1_month ON TABLE mainTable          |USING '$timeSeries'          |DMPROPERTIES (          |   'event_time '=' dataTime',          |   'MONTH_GRANULARITY '='1')          |AS SELECT dataTime, SUM(age) FROM mainTable          |GROUP BY dataTime         """.stripMargin)     checkExistence(sql("SHOW TABLES"), true, "maintable_agg1_month")   }problem: NPE java.lang.NullPointerException was thrown. java.lang.NullPointerException  at org.apache.spark.sql.execution.command.timeseries.TimeSeriesUtil$.validateTimeSeriesEventTime(TimeSeriesUtil.scala:50)  at org.apache.spark.sql.execution.command.preaaggregate.CreatePreAggregateTableCommand.processMetadata(CreatePreAggregateTableCommand.scala:104)  at org.apache.spark.sql.execution.command.datamap.CarbonCreateDataMapCommand.processMetadata(CarbonCreateDataMapCommand.scala:75)  at org.apache.spark.sql.execution.command.AtomicRunnableCommand.run(package.scala:84)  at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)2.scenario 2    test("test timeseries create table 36: support event_time and granularity key with space") {      sql("DROP DATAMAP IF EXISTS agg1_month ON TABLE maintable")      sql(        s"""CREATE DATAMAP agg1_month ON TABLE mainTable           |USING '$timeSeries'           |DMPROPERTIES (           |   'event_time '='dataTime',           |   'MONTH_GRANULARITY '=' 1')           |AS SELECT dataTime, SUM(age) FROM mainTable           |GROUP BY dataTime          """.stripMargin)      checkExistence(sql("SHOW TABLES"), true, "maintable_agg1_month")    } problem: Granularity only support 1 org.apache.carbondata.spark.exception.MalformedDataMapCommandException: Granularity only support 1  at org.apache.spark.sql.execution.command.timeseries.TimeSeriesUtil$.getTimeSeriesGranularityDetails(TimeSeriesUtil.scala:118)  at org.apache.spark.sql.execution.command.datamap.CarbonCreateDataMapCommand.processMetadata(CarbonCreateDataMapCommand.scala:58)  at org.apache.spark.sql.execution.command.AtomicRunnableCommand.run(package.scala:84)  at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)  at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)  at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:67)  at org.apache.spark.sql.Dataset.<init>(Dataset.scala:183)  at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:68)  at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:632)
issueID:CARBONDATA-2083
type:Bug
changed files:
texts:Timeseries pre-aggregate table should support hour != 1 , others are the same
test case find the error:  test("test timeseries create table 13: don't support hour=2") {    try {      sql(        """create datamap agg1 on table mainTable using 'preaggregate'          |DMPROPERTIES (          |   'timeseries.eventTime'='dataTime',          |   'timeseries.hierarchy'='hour=2')          |as select dataTime, sum(age) from mainTable          |group by dataTime        """.stripMargin)      assert(false)    } catch {      case e: MalformedCarbonCommandException =>        assert(e.getMessage.contains("Unsupported Value for hierarchy:hour=2"))    } finally {      checkExistence(sql("show tables"), false, "maintable_agg1_hour")    }  }
issueID:CARBONDATA-2084
type:Bug
changed files:
texts:Timeseries pre-aggregate table should support min and max when the create datamap don&#39;t as select max and min
override def beforeAll: Unit = {    SparkUtil4Test.createTaskMockUp(sqlContext)    CarbonProperties.getInstance()      .addProperty(CarbonCommonConstants.CARBON_TIMESTAMP_FORMAT,        CarbonCommonConstants.CARBON_TIMESTAMP_DEFAULT_FORMAT)    sql("drop table if exists mainTable")    sql("CREATE TABLE mainTable(mytime timestamp, name string, age int) STORED BY 'org.apache.carbondata.format'")    sql("create datamap agg0 on table mainTable using 'preaggregate' DMPROPERTIES ('timeseries.eventTime'='mytime', 'timeseries.hierarchy'='second=1,minute=1,hour=1,day=1,month=1,year=1') as select mytime, sum(age) from mainTable group by mytime")    sql(s"LOAD DATA LOCAL INPATH '$resourcesPath/timeseriestest.csv' into table mainTable")  }  // TODO: support max  ignore("test PreAggregate table selection 38: filter < and >=, max") {    val df = sql(      """        | select        |   timeseries(mytime,'second') as secondLevel,        |   max(age) as maxValue        | from mainTable        | where timeseries(mytime,'second')<'2016-02-23 01:02:00' and timeseries(mytime,'second')>='2016-02-23 01:01:00'        | group by        |   timeseries(mytime,'second')        | order by        |   timeseries(mytime,'second')      """.stripMargin)    df.show()    checkAnswer(df, Seq(Row(Timestamp.valueOf("2016-02-23 01:01:50"), 30)))  }  ignore("test PreAggregate table selection 39: filter < and >=, min") {    val df = sql(      """        | select        |   timeseries(mytime,'second') as secondLevel,        |   min(age) as minValue        | from mainTable        | where timeseries(mytime,'second')<'2016-02-23 01:02:00' and timeseries(mytime,'second')>='2016-02-23 01:01:00'        | group by        |   timeseries(mytime,'second')        | order by        |   timeseries(mytime,'second')      """.stripMargin)    df.show()    checkAnswer(df, Seq(Row(Timestamp.valueOf("2016-02-23 01:01:30"), 10)))  }detail please see my branch: https://github.com/xubo245/carbondata/tree/usingTimeseries
issueID:CARBONDATA-2085
type:Bug
changed files:
texts:It&#39;s different between load twice and create datamap with load again after load data and create datamap
It's different between two test casetest case 1: load twice and create datamap , and then querytest case 2:load once , create datamap and load again, and then query+  test("load data into mainTable after create timeseries datamap on table 1") { +    sql("drop table if exists mainTable") +    sql( +      """ +        | CREATE TABLE mainTable( +        |   mytime timestamp, +        |   name string, +        |   age int) +        | STORED BY 'org.apache.carbondata.format' +      """.stripMargin) + +    sql(s"LOAD DATA LOCAL INPATH '$resourcesPath/timeseriestest.csv' into table mainTable") + +    sql( +      """ +        | create datamap agg0 on table mainTable +        | using 'preaggregate' +        | DMPROPERTIES ( +        |   'timeseries.eventTime'='mytime', +        |   'timeseries.hierarchy'='second=1,minute=1,hour=1,day=1,month=1,year=1') +        | as select mytime, sum(age) +        | from mainTable +        | group by mytime""".stripMargin) + +    sql(s"LOAD DATA LOCAL INPATH '$resourcesPath/timeseriestest.csv' into table mainTable") +    val df = sql( +      """ +        | select +        |   timeseries(mytime,'minute') as minuteLevel, +        |   sum(age) as sum +        | from mainTable +        | where timeseries(mytime,'minute')>='2016-02-23 01:01:00' +        | group by +        |   timeseries(mytime,'minute') +        | order by +        |   timeseries(mytime,'minute') +      """.stripMargin) + +    // only for test, it need remove before merge +    df.show() +    sql("select * from maintable_agg0_minute").show(100) + +    checkAnswer(df, +      Seq(Row(Timestamp.valueOf("2016-02-23 01:01:00"), 120), +        Row(Timestamp.valueOf("2016-02-23 01:02:00"), 280))) + +  } + +  test("load data into mainTable after create timeseries datamap on table 2") { +    sql("drop table if exists mainTable") +    sql( +      """ +        | CREATE TABLE mainTable( +        |   mytime timestamp, +        |   name string, +        |   age int) +        | STORED BY 'org.apache.carbondata.format' +      """.stripMargin) + +    sql(s"LOAD DATA LOCAL INPATH '$resourcesPath/timeseriestest.csv' into table mainTable") +    sql(s"LOAD DATA LOCAL INPATH '$resourcesPath/timeseriestest.csv' into table mainTable") +    sql( +      """ +        | create datamap agg0 on table mainTable +        | using 'preaggregate' +        | DMPROPERTIES ( +        |   'timeseries.eventTime'='mytime', +        |   'timeseries.hierarchy'='second=1,minute=1,hour=1,day=1,month=1,year=1') +        | as select mytime, sum(age) +        | from mainTable +        | group by mytime""".stripMargin) + + +    val df = sql( +      """ +        | select +        |   timeseries(mytime,'minute') as minuteLevel, +        |   sum(age) as sum +        | from mainTable +        | where timeseries(mytime,'minute')>='2016-02-23 01:01:00' +        | group by +        |   timeseries(mytime,'minute') +        | order by +        |   timeseries(mytime,'minute') +      """.stripMargin) + +    // only for test, it need remove before merge +    df.show() +    sql("select * from maintable_agg0_minute").show(100) + + +    checkAnswer(df, +      Seq(Row(Timestamp.valueOf("2016-02-23 01:01:00"), 120), +        Row(Timestamp.valueOf("2016-02-23 01:02:00"), 280))) +  } +test case 1 and 2 should success , but test case 1 fail
issueID:CARBONDATA-2086
type:Bug
changed files:
texts:Create datamap should throw exception if using improper string
Create datamap should throw exception if using improper string  // TODO: to be fixed, it should throw exception  ignore("test timeseries create table 34: using") {    val e: Exception = intercept[Exception] {      sql(        """create datamap agg1 on table mainTable          | using 'abc'          | DMPROPERTIES (          |   'timeseries.eventTime'='dataTime',          |   'timeseries.hierarchy'='second=1,year=1')          | as select dataTime, sum(age) from mainTable          | group by dataTime        """.stripMargin)    }    assert(e.getMessage.contains("Don't support abc"))  }
issueID:CARBONDATA-2087
type:Bug
changed files:
texts:Order rule should keep consistent for create datamap
It success:  test("test timeseries create table 18: support day=1,year=1,month=1") {    try {      sql(        """create datamap agg1 on table mainTable          | using 'preaggregate'          | DMPROPERTIES (          |    'timeseries.eventTime'='dataTime',          |    'timeseries.hierarchy'='day=1,year=1,month=1')          | as select dataTime, sum(age) from mainTable          | group by dataTime        """.stripMargin)      checkExistence(sql("show tables"), false, "maintable_agg1_hour")      checkExistence(sql("show tables"), true, "maintable_agg1_day", "maintable_agg1_month", "maintable_agg1_year")    } catch {      case _: Exception =>        assert(false)    }  }but it fail:  test("test timeseries create table 21: don't support year=1,month=1") {    try {      sql(        """create datamap agg1 on table mainTable          |using 'preaggregate'          |DMPROPERTIES (          |   'timeseries.eventTime'='dataTime',          |   'timeseries.hierarchy'='year=1,month=1')          |as select dataTime, sum(age) from mainTable          |group by dataTime""".stripMargin)      assert(false)    } catch {      case e: MalformedCarbonCommandException =>        assert(e.getMessage.contains("year=1,month=1 is in wrong order"))    } finally {      checkExistence(sql("show tables"), false, "maintable_agg1_year")    }  }
issueID:CARBONDATA-2088
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/datamap/Granularity.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/datamap/DataMapClassProvider.java
common/src/main/java/org/apache/carbondata/common/exceptions/sql/MalformedDataMapCommandException.java
texts:Optimize syntax for creating timeseries pre-aggregate table
change using 'timeseries' instead of using preaggregate for creating timeseries pre-aggregate tablechange timeseries.eventTime and hour_granularity and so ongranularity only support oneIt should throw UnsupportDataMapException if don't use timeseries or preaggregate to create datamap
issueID:CARBONDATA-2089
type:Bug
changed files:
texts:Test cases is incorrect because it always run success no matter whether the SQL thrown exception
Test cases is incorrect because it always run success no matter whether the SQL thrown exception,if the sql run success, then assert(false), and then catch exception and assert(true), so it always true.Test cases is incorrect,like:  test("test timeseries create table seven") {    try {      sql(        "create datamap agg0 on table mainTable using 'preaggregate' DMPROPERTIES ('timeseries.eventTime'='dataTime', 'timeseries.hierarchy'='hour=1,day=1,year=1,month=1') as select dataTime, sum(age) from mainTable group by dataTime")      assert(false)    } catch {      case _:Exception =>        assert(true)    }  }It will always run success.SDV have many similar test case, we need to fix it!
issueID:CARBONDATA-209
type:Improvement
changed files:
texts:DROP TABLE in all testcase
Some test cases do not drop table, which will cause test case to fail when running second time.
issueID:CARBONDATA-2090
type:Improvement
changed files:
texts:Should fix the error message of alter streaming property
if the table is already a streaming table, should prompt "Streaming property can not be changed once it is 'true'". if the table is a normal table, should prompt "Streaming property value is incorrect" when the value is not 'true'.
issueID:CARBONDATA-2091
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/metadata/schema/SortColumnRangeInfo.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/SortDataRows.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/SortParameters.java
processing/src/main/java/org/apache/carbondata/processing/loading/partition/impl/RawRowComparator.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/merger/UnsafeIntermediateMerger.java
processing/src/main/java/org/apache/carbondata/processing/loading/steps/DataWriterProcessorStepImpl.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/SingleThreadFinalSortFilesMerger.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonLoadOptionConstants.java
processing/src/main/java/org/apache/carbondata/processing/loading/converter/RowConverter.java
processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactDataHandlerColumnar.java
processing/src/main/java/org/apache/carbondata/processing/loading/partition/impl/HashPartitionerImpl.java
core/src/main/java/org/apache/carbondata/core/datastore/row/CarbonRow.java
processing/src/main/java/org/apache/carbondata/processing/loading/converter/impl/RowConverterImpl.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/ColumnRangeInfo.java
processing/src/main/java/org/apache/carbondata/processing/loading/model/CarbonLoadModelBuilder.java
processing/src/main/java/org/apache/carbondata/processing/loading/model/LoadOption.java
core/src/main/java/org/apache/carbondata/core/metadata/converter/ThriftWrapperSchemaConverterImpl.java
processing/src/main/java/org/apache/carbondata/processing/loading/steps/DataConverterProcessorStepImpl.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/impl/ParallelReadMergeSorterWithColumnRangeImpl.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/BucketingInfo.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/SortIntermediateFileMerger.java
processing/src/main/java/org/apache/carbondata/processing/loading/DataLoadProcessBuilder.java
processing/src/main/java/org/apache/carbondata/processing/loading/partition/impl/RangePartitionerImpl.java
processing/src/main/java/org/apache/carbondata/processing/loading/CarbonDataLoadConfiguration.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/SorterFactory.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/UnsafeSortDataRows.java
processing/src/main/java/org/apache/carbondata/processing/loading/model/CarbonLoadModel.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/impl/UnsafeParallelReadMergeSorterWithColumnRangeImpl.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/merger/UnsafeSingleThreadFinalSortFilesMerger.java
texts:Enhance data loading performance by specifying range bounds for sort columns
Currently in carbondata, data loading using node_sort (also known as local_sort) has the following procedures: convert the input data in batch. (Convert) sort the batch and write to the sort temp files. (TempSort) combine the sort temp files and do merge sort to get a bigger ordered sort temp file. (MergeSort) combine all the sort temp files and do a final sort, its results will feed the next procedure. (FinalSort) get rows in order and convert rows to carbondata columnar format pages. (produce) Write bundles of pages to files and write the corresponding index file. (consume)The Step1~Step3 are done concurrently using multi-thread. The Step4 is done using only one thread. The Step5 is done using multi-thread. So the Step4 is the bottleneck among all the procedures. When observing the data loading performance, we can see that the CPU usage after Step3 is low. We can enhance the data loading performance by parallelizing Step4. User can specify range bounds for the sort columns and carbondata internally distributes the records to different ranges and process the data concurrently in different ranges.
issueID:CARBONDATA-2092
type:Bug
changed files:processing/src/main/java/org/apache/carbondata/processing/merger/CarbonCompactionUtil.java
core/src/main/java/org/apache/carbondata/core/util/CarbonMetadataUtil.java
processing/src/main/java/org/apache/carbondata/processing/store/writer/AbstractFactDataWriter.java
core/src/main/java/org/apache/carbondata/hadoop/CarbonInputSplit.java
core/src/main/java/org/apache/carbondata/core/util/AbstractDataFileFooterConverter.java
core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
texts:Fix compaction bug to prevent the compaction flow from going through the restructure compaction flow
Problem and analysis:----------------------------------------During data load current schema timestamp is written to the carbondata fileHeader. This is used during compaction to decide whether the block is a restructured block or the block is according to the latest schema.As the blocklet information is now stored in the index file, while laoding it in memory the carbondata file header is not read and due to this the schema timestamp is not getting set to the blocklet information. Due to this during compaction flow there is a mismatch on comparing the current schema time stamp with the timestamp stored in the block and the flow goes through the restructure compaction flow instead of normal compaction flow.Impact:-------------Compaction performance degradation as restructure compaction flow involves sorting of data again.
issueID:CARBONDATA-2094
type:Bug
changed files:
texts:Filter DataMap Tables in "Show Table Command"
Currently Show Table command shows datamap tables (agg tablels) but show table command should not show aggregate tables.
issueID:CARBONDATA-2095
type:Bug
changed files:
texts:Incorrect data is displayed after stream segment is converted to batch segment .
After handoff stream segment(ROW format) to batch segment (column format), data is incorrect, there are repetitive data.Solution:Copy data of GenericInternalRow for each iteration when converting stream segment to batch segment.
issueID:CARBONDATA-2096
type:Improvement
changed files:
texts:Should add test case for &#39;merge_small_files&#39; distribution

issueID:CARBONDATA-2097
type:New Feature
changed files:
texts:Restriction added to partition table on alter command

issueID:CARBONDATA-2098
type:Bug
changed files:
texts:Add documentation for pre-aggregate tables

issueID:CARBONDATA-2099
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RestructureExcludeFilterExecutorImpl.java
core/src/main/java/org/apache/carbondata/core/datastore/impl/DefaultFileTypeProvider.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/TableInfo.java
core/src/main/java/org/apache/carbondata/core/scan/result/iterator/ChunkRowIterator.java
core/src/main/java/org/apache/carbondata/core/util/DataFileFooterConverter.java
core/src/main/java/org/apache/carbondata/core/scan/collector/impl/RawBasedResultCollector.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/TrueFilterExecutor.java
core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/impl/DimensionRawColumnChunk.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelRangeGrtrThanEquaToFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/scan/model/ProjectionDimension.java
core/src/main/java/org/apache/carbondata/core/datastore/block/SegmentProperties.java
core/src/main/java/org/apache/carbondata/core/metadata/blocklet/SegmentInfo.java
integration/spark-datasource/src/main/scala/org/apache/carbondata/spark/vectorreader/VectorizedCarbonRecordReader.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelRangeLessThanFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RangeValueFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/SegmentIndexFileStore.java
core/src/main/java/org/apache/carbondata/core/mutate/DeleteDeltaBlockletDetails.java
core/src/main/java/org/apache/carbondata/core/scan/executor/impl/AbstractQueryExecutor.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/reader/dimension/AbstractDimensionChunkReader.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/ExcludeFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/reader/DimensionColumnChunkReader.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/FilterExecuter.java
core/src/main/java/org/apache/carbondata/core/scan/processor/BlockletIterator.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonTableInputFormat.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/OrFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RestructureIncludeFilterExecutorImpl.java
core/src/main/java/org/apache/carbondata/core/scan/processor/RawBlockletColumnChunks.java
core/src/main/java/org/apache/carbondata/core/scan/complextypes/StructQueryType.java
core/src/main/java/org/apache/carbondata/core/scan/result/vector/MeasureDataVectorProcessor.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/ImplicitIncludeFilterExecutorImpl.java
core/src/main/java/org/apache/carbondata/core/scan/result/RowBatch.java
core/src/main/java/org/apache/carbondata/core/scan/result/impl/NonFilterQueryScannedResult.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/AndFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/scan/model/ProjectionColumn.java
core/src/main/java/org/apache/carbondata/core/scan/result/iterator/PartitionSpliterRawResultIterator.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/store/impl/safe/SafeVariableLengthDimensionDataChunkStore.java
core/src/main/java/org/apache/carbondata/core/scan/executor/util/RestructureUtil.java
core/src/main/java/org/apache/carbondata/core/scan/result/vector/ColumnVectorInfo.java
core/src/main/java/org/apache/carbondata/core/scan/model/QueryProjection.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/reader/measure/v3/MeasureChunkPageReaderV3.java
core/src/main/java/org/apache/carbondata/core/scan/scanner/BlockletScanner.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/DimensionColumnPage.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/store/impl/unsafe/UnsafeVariableLengthDimensionDataChunkStore.java
core/src/main/java/org/apache/carbondata/core/datastore/impl/FileReaderImpl.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/reader/MeasureColumnChunkReader.java
core/src/main/java/org/apache/carbondata/core/scan/result/iterator/DetailQueryResultIterator.java
core/src/main/java/org/apache/carbondata/core/memory/HeapMemoryAllocator.java
core/src/main/java/org/apache/carbondata/core/scan/filter/FilterExpressionProcessor.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/reader/measure/v3/MeasureChunkReaderV3.java
core/src/main/java/org/apache/carbondata/core/scan/filter/GenericQueryType.java
integration/spark-datasource/src/main/scala/org/apache/carbondata/spark/vectorreader/ColumnarVectorWrapper.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/RowLevelRangeFilterResolverImpl.java
core/src/main/java/org/apache/carbondata/core/scan/result/vector/CarbonColumnarBatch.java
integration/hive/src/main/java/org/apache/carbondata/hive/MapredCarbonInputFormat.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/IndexWrapper.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/impl/VariableLengthDimensionColumnPage.java
core/src/main/java/org/apache/carbondata/core/mutate/DeleteDeltaBlockDetails.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/impl/FixedLengthDimensionColumnPage.java
core/src/main/java/org/apache/carbondata/core/scan/collector/impl/AbstractScannedResultCollector.java
core/src/main/java/org/apache/carbondata/core/scan/collector/impl/DictionaryBasedResultCollector.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/resolverinfo/TrueConditionalResolverImpl.java
core/src/main/java/org/apache/carbondata/core/scan/collector/impl/RestructureBasedRawResultCollector.java
core/src/main/java/org/apache/carbondata/core/datastore/impl/FileFactory.java
core/src/main/java/org/apache/carbondata/core/scan/collector/impl/RestructureBasedVectorResultCollector.java
hadoop/src/main/java/org/apache/carbondata/hadoop/util/CarbonInputFormatUtil.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/column/CarbonDimension.java
core/src/main/java/org/apache/carbondata/core/scan/collector/impl/DictionaryBasedVectorResultCollector.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelRangeGrtThanFiterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/scan/executor/infos/BlockExecutionInfo.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/RelationIdentifier.java
core/src/main/java/org/apache/carbondata/core/scan/scanner/impl/BlockletFilterScanner.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/RowLevelFilterResolverImpl.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/store/impl/safe/SafeFixedLengthDimensionDataChunkStore.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelFilterExecuterImpl.java
integration/presto/src/main/java/org/apache/carbondata/presto/CarbondataPageSource.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/store/impl/unsafe/UnsafeFixedLengthDimensionDataChunkStore.java
core/src/main/java/org/apache/carbondata/core/scan/processor/DataBlockIterator.java
core/src/main/java/org/apache/carbondata/core/datastore/impl/DFSFileReaderImpl.java
core/src/main/java/org/apache/carbondata/core/scan/result/iterator/RawResultIterator.java
core/src/main/java/org/apache/carbondata/core/scan/model/QueryModel.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/store/DimensionDataChunkStore.java
core/src/main/java/org/apache/carbondata/core/util/AbstractDataFileFooterConverter.java
core/src/main/java/org/apache/carbondata/core/scan/collector/ScannedResultCollector.java
core/src/main/java/org/apache/carbondata/core/stats/QueryStatisticsModel.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/impl/MeasureRawColumnChunk.java
core/src/main/java/org/apache/carbondata/core/scan/wrappers/ByteArrayWrapper.java
processing/src/main/java/org/apache/carbondata/processing/merger/CarbonCompactionExecutor.java
core/src/main/java/org/apache/carbondata/core/scan/scanner/impl/BlockletFullScanner.java
core/src/main/java/org/apache/carbondata/core/mutate/CarbonUpdateUtil.java
core/src/main/java/org/apache/carbondata/core/datastore/impl/FileTypeInterface.java
core/src/main/java/org/apache/carbondata/core/indexstore/BlockletDetailInfo.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockletDataRefNode.java
core/src/main/java/org/apache/carbondata/core/scan/collector/impl/RestructureBasedDictionaryResultCollector.java
core/src/main/java/org/apache/carbondata/core/scan/complextypes/ComplexQueryType.java
core/src/main/java/org/apache/carbondata/core/scan/result/iterator/VectorDetailQueryResultIterator.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/store/ColumnPageWrapper.java
core/src/main/java/org/apache/carbondata/core/datastore/FileReader.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/EncodingFactory.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/IncludeFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/scan/expression/FilterModificationNode.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/reader/dimension/v3/DimensionChunkPageReaderV3.java
core/src/main/java/org/apache/carbondata/core/scan/complextypes/PrimitiveQueryType.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/impl/AbstractDimensionColumnPage.java
core/src/main/java/org/apache/carbondata/core/scan/expression/RangeExpressionEvaluator.java
streaming/src/main/java/org/apache/carbondata/streaming/CarbonStreamRecordReader.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/reader/dimension/v3/DimensionChunkReaderV3.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/resolverinfo/MeasureColumnResolvedFilterInfo.java
core/src/main/java/org/apache/carbondata/core/scan/result/iterator/AbstractDetailQueryResultIterator.java
core/src/main/java/org/apache/carbondata/core/scan/executor/util/QueryUtil.java
core/src/main/java/org/apache/carbondata/core/datastore/DataRefNode.java
core/src/main/java/org/apache/carbondata/core/scan/filter/optimizer/RangeFilterOptmizer.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/resolverinfo/visitor/RangeDictionaryColumnVisitor.java
core/src/main/java/org/apache/carbondata/core/scan/result/BlockletScannedResult.java
core/src/main/java/org/apache/carbondata/core/statusmanager/SegmentStatusManager.java
core/src/main/java/org/apache/carbondata/core/scan/complextypes/ArrayQueryType.java
core/src/main/java/org/apache/carbondata/core/scan/filter/FilterUtil.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/ConditionalFilterResolverImpl.java
core/src/main/java/org/apache/carbondata/core/statusmanager/LoadMetadataDetails.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/column/CarbonColumn.java
core/src/main/java/org/apache/carbondata/core/scan/expression/ColumnExpression.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/CarbonTable.java
processing/src/main/java/org/apache/carbondata/processing/util/CarbonQueryUtil.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelRangeLessThanEqualFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/scan/filter/FilterProcessor.java
core/src/main/java/org/apache/carbondata/core/scan/result/impl/FilterQueryScannedResult.java
integration/presto/src/main/java/org/apache/carbondata/presto/PrestoCarbonVectorizedRecordReader.java
core/src/main/java/org/apache/carbondata/core/scan/model/ProjectionMeasure.java
core/src/main/java/org/apache/carbondata/core/scan/executor/impl/DetailQueryExecutor.java
texts:Refactor on query scan process to improve readability

issueID:CARBONDATA-21
type:Bug
changed files:
texts:Filter query issue for >, <, <= than filter
1. select count from a12 where dob > '2014-07-01 12:07:28'   throwing runtime exception2. select count from a12 where dob < '2014-07-01 12:07:28'   is including the null values also.3. select count from a12 where dob <=  '2014-07-01 12:07:28'   is including the null value Null should not considered in less than filter.data:empid,ename,sal,deptno,mgr,gender,dob,comm,desc1,abc,1233,10,2,,2014-07-01 12:07:28,1234.191,string_null2,bcd,1322,,3,f,2014-07-01 12:07:28,19.99,int_null3,cde,4322,,4,m,,16.996,date_null4,    ,43243,,5,m,,999.117,string_space5,,43242,20,6,m,2017-07-01 12:07:28,99.999,string_null6,ijk,,20,6,m,2017-07-01 12:07:28,50089,double_null7,pqr,2422,20,6,m,2017-07-01 12:07:28,32.339,decimal_null8
issueID:CARBONDATA-210
type:New Feature
changed files:core/src/main/java/org/apache/carbondata/core/datastore/impl/FileFactory.java
texts:Support loading BZIP2 compressed CSV file
Support BZIP2 compressed CSV file, file extension name is bz2
issueID:CARBONDATA-2100
type:Improvement
changed files:
texts:Should add test case to  the result of hand off

issueID:CARBONDATA-2101
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/util/SessionParams.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
texts:Restrict Direct query on aggregation and timeseries data map
Restrict direct query on timeseries and pre-aggregate data map 
issueID:CARBONDATA-2102
type:Bug
changed files:
texts:Fix measure min/max value problem while reading from old store
Problem:For old store the measure min and max values are written opposite (i.e min in place of max and max in place of min). Due to this computing of measure filter with current code is impacted.This problem specifically comes when measure data has negative values.ImpactFilter query on measure
issueID:CARBONDATA-2103
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
texts:Avoid 2 time lookup in ShowTables command
Currently in show table command 2 time lookup is happening . Improve it with 1 lookup. CarbonShowTablesCommand.scala # filterDataMaps
issueID:CARBONDATA-2104
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/statusmanager/SegmentStatusManager.java
core/src/main/java/org/apache/carbondata/core/exception/ConcurrentOperationException.java
processing/src/main/java/org/apache/carbondata/processing/util/CarbonLoaderUtil.java
integration/spark-common/src/main/java/org/apache/carbondata/spark/exception/ProcessMetaDataException.java
texts:Add concurrent command testcase for insert overwrite and insert

issueID:CARBONDATA-2105
type:Bug
changed files:
texts:Incorrect result displays after creating data map
Incorrect result displays after creating data mapSteps to Reproduce:1. Create a TAble:CREATE TABLE uniqdata(CUST_ID int,CUST_NAME String,ACTIVE_EMUI_VERSION string, DOB timestamp, DOJ timestamp, BIGINT_COLUMN1 bigint,BIGINT_COLUMN2 bigint,DECIMAL_COLUMN1 decimal(30,10), DECIMAL_COLUMN2 decimal(36,10),Double_COLUMN1 double, Double_COLUMN2 double,INTEGER_COLUMN1 int) STORED BY 'org.apache.carbondata.format' TBLPROPERTIES('DICTIONARY_INCLUDE'='CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1,Double_COLUMN2,INTEGER_COLUMN1')2. Load Dataa) LOAD DATA INPATH 'HDFS_URL/BabuStore/Data/uniqdata/2000_UniqData.csv' into table uniqdata OPTIONS('DELIMITER'=',', 'QUOTECHAR'='"','BAD_RECORDS_ACTION'='FORCE','FILEHEADER'='CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1,Double_COLUMN2,INTEGER_COLUMN1')b) LOAD DATA INPATH 'HDFS_URL/BabuStore/Data/uniqdata/2000_UniqData.csv' into table uniqdata OPTIONS('DELIMITER'=',', 'QUOTECHAR'='"','BAD_RECORDS_ACTION'='FORCE','FILEHEADER'='CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1,Double_COLUMN2,INTEGER_COLUMN1')c) LOAD DATA INPATH 'HDFS_URL/BabuStore/Data/uniqdata/2000_UniqData.csv' into table uniqdata OPTIONS('DELIMITER'=',', 'QUOTECHAR'='"','BAD_RECORDS_ACTION'='FORCE','FILEHEADER'='CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1,Double_COLUMN2,INTEGER_COLUMN1')Execute Query:a) select avg(cust_id) from uniqdata group by cust_idoutput:  9460.0  9671.0  10403.0  10725.0  10867.0 ---------------+ avg(cust_id) ---------------+ 9067.0  9901.0 ---------------+2,002 rows selected (1.718 seconds)b) create data map:create datamap uniqdata_agg on table uniqdata using 'preaggregate' as select avg(cust_id) from uniqdata group by cust_id;c) select avg(cust_id) from uniqdata group by cust_id;output: NULL  NULL  NULL ---------------+ avg(cust_id) ---------------+ NULL  NULL ---------------+2,002 rows selected (0.895 seconds)Expected result: it should display similar result as before creating datamap.
issueID:CARBONDATA-2106
type:Task
changed files:
texts:Update product document with page level reader property

issueID:CARBONDATA-2107
type:Bug
changed files:
texts:Average query is failing when data map has both sum(column) and avg(column) of big int, int type
Average query is failing when data map has both sum(column) and avg(column) of big int, int type
issueID:CARBONDATA-2108
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
core/src/main/java/org/apache/carbondata/core/memory/UnsafeSortMemoryManager.java
core/src/main/java/org/apache/carbondata/core/memory/UnsafeMemoryManager.java
core/src/main/java/org/apache/carbondata/core/util/CarbonProperties.java
texts:RefactorUnsafe sort property
Deprecated old property: sort.inmemory.size.inmb,Add new property: carbon.sort.storage.inmemory.size.inmb, If user has configured old property(sort.inmemory.size.inmb) then internally it will be converted to new property for ex: If user has configured sort.inmemory.size.inmb then 20% memory will be used as working memory and rest for storage memory
issueID:CARBONDATA-2109
type:Improvement
changed files:
texts:config of dataframe load with tempCSV is invalid ,such as QUOTECHAR
use datafame to load data with tempCSV true(dataset.writer..format("carbondata").save), carbondata will generate sql to load datathe sql's configs only have SINGLE_PASS, other configs is invalid,such as QUOTECHAR.the code is in CarbonDataFrameWriter:  private def makeLoadString(csvFolder: String, options: CarbonOption): String = {    val dbName = CarbonEnv.getDatabaseName(options.dbName)(sqlContext.sparkSession)    s""" LOAD DATA INPATH '$csvFolder' INTO TABLE $dbName.${options.tableName} OPTIONS ('FILEHEADER' = '${dataFrame.columns.mkString(",")}', 'SINGLE_PASS' = '${options.singlePass}')     """.stripMargin  }
issueID:CARBONDATA-211
type:New Feature
changed files:
texts:Support compress CarbonData file create table options
Currently CarbonData uses Snappy as codec to compress its columnar file, this Issue is targeted to support:1. support compresssion codec other than Snappy, including LZO, LZ4, ZLIB2. add table property in CREATE TABLE syntax. Users can specify the codec in CREATE TABLE statement.
issueID:CARBONDATA-2110
type:Bug
changed files:
texts:option of TempCsv should be removed since the default delimiter may conflicts with field value
Currently in carbondata, an option named ‘tempCSV’ is available during loading dataframe. After enabling this option, Carbondata will write the dataframe to a standard csv file at first and then load the data files. The delimiters of the standard csv file, such as field delimiter / escape char/ quote char/ multi-line/ line separator and so on may conflict with the actual field value. For example, if a field contains ',', then it will cause problem in further data loading if we save the tempCSV using ',' as field separator. Since we are not sure about the content of dataframe, I think it's better to deprecate this option. To make forward compatible, user can still use this option but will get warning about it.
issueID:CARBONDATA-2111
type:Improvement
changed files:
texts:TPCH query which has multiple joins inside does not return any rows.
The below TPCH query which has multiple joins does not return any roes.sql(    *"select s_acctbal, s_name, n_name, p_partkey, p_mfgr, s_address, s_phone, s_comment from "* +    *"part, supplier, partsupp, nation, region where p_partkey = ps_partkey and s_suppkey = "* +    *"ps_suppkey and p_size = 15 and p_type like '%BRASS' and s_nationkey = n_nationkey and "* +    *"n_regionkey = r_regionkey and r_name = 'EUROPE' and ps_supplycost = ( select min"* +    *"(ps_supplycost) from partsupp, supplier,nation, region where p_partkey = ps_partkey and "* +    *"s_suppkey = ps_suppkey and s_nationkey = n_nationkey and n_regionkey = r_regionkey and "* +    *"r_name = 'EUROPE' ) order by s_acctbal desc, n_name, s_name, p_partkey limit 100"*) 
issueID:CARBONDATA-2112
type:Bug
changed files:
texts:Data getting garbled after datamap creation when table is created with GLOBAL SORT
Data is getting garbled after datamap creation when table is created with BATCH_SORT/GLOBAL_SORT. Steps to reproduce :spark.sql("drop table if exists uniqdata_batchsort_compact3")spark.sql("CREATE TABLE uniqdata_batchsort_compact3 (CUST_ID int,CUST_NAME String,ACTIVE_EMUI_VERSION string, DOB timestamp, DOJ timestamp, BIGINT_COLUMN1 bigint,BIGINT_COLUMN2 bigint,DECIMAL_COLUMN1 decimal(30,10), DECIMAL_COLUMN2 decimal(36,10),Double_COLUMN1 double, Double_COLUMN2 double,INTEGER_COLUMN1 int) STORED BY 'carbondata' TBLPROPERTIES('SORT_SCOPE'='GLOBAL_SORT')").show()spark.sql("LOAD DATA INPATH '/home/sangeeta/Desktop/2000_UniqData.csv' into table " + "uniqdata_batchsort_compact3 OPTIONS('DELIMITER'=',' , 'QUOTECHAR'='\"'," + "'BAD_RECORDS_ACTION'='FORCE','FILEHEADER'='CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION," + "DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2," + "Double_COLUMN1,Double_COLUMN2,INTEGER_COLUMN1','batch_sort_size_inmb'='1')")spark.sql("LOAD DATA INPATH '/home/sangeeta/Desktop/2000_UniqData.csv' into table " + "uniqdata_batchsort_compact3 OPTIONS('DELIMITER'=',' , 'QUOTECHAR'='\"'," + "'BAD_RECORDS_ACTION'='FORCE','FILEHEADER'='CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION," + "DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2," + "Double_COLUMN1,Double_COLUMN2,INTEGER_COLUMN1','batch_sort_size_inmb'='1')")spark.sql("LOAD DATA INPATH '/home/sangeeta/Desktop/2000_UniqData.csv' into table " + "uniqdata_batchsort_compact3 OPTIONS('DELIMITER'=',' , 'QUOTECHAR'='\"'," + "'BAD_RECORDS_ACTION'='FORCE','FILEHEADER'='CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION," + "DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2," + "Double_COLUMN1,Double_COLUMN2,INTEGER_COLUMN1','batch_sort_size_inmb'='1')")spark.sql("select cust_id, avg(cust_id) from uniqdata_batchsort_compact3 group by cust_id ").show(50)-----------------+cust_idavg(cust_id)-----------------+ 9376 9376.0 9427 9427.0 9465 9465.0 9852 9852.0 9900 9900.0 10206 10206.0 10362 10362.0 10623 10623.0 10817 10817.0 9182 9182.0 9564 9564.0 9879 9879.0 10081 10081.0 10121 10121.0 10230 10230.0 10462 10462.0 10703 10703.0 10914 10914.0 9162 9162.0 9383 9383.0 9454 9454.0 9517 9517.0 9558 9558.0 10708 10708.0 10798 10798.0 10862 10862.0 9071 9071.0 9169 9169.0 9946 9946.0 10468 10468.0 10745 10745.0 10768 10768.0 9153 9153.0 9206 9206.0 9403 9403.0 9597 9597.0 9647 9647.0 9775 9775.0 10032 10032.0 10395 10395.0 10527 10527.0 10567 10567.0 10632 10632.0 10788 10788.0 10815 10815.0 10840 10840.0 9181 9181.0 9344 9344.0 9575 9575.0 9675 9675.0-----------------+only showing top 50 rowsNote: Here the cust_id is coming correct .spark.sql("create datamap uniqdata_agg on table uniqdata_batchsort_compact3 using " + "'preaggregate' as select avg(cust_id) from uniqdata_batchsort_compact3 group by cust_id")spark.sql("select cust_id, avg(cust_id) from uniqdata_batchsort_compact3 group by cust_id ").show(50)-----------------+cust_idavg(cust_id)-----------------+ 27651 9217.0 31944 10648.0 32667 10889.0 28242 9414.0 29841 9947.0 28728 9576.0 27255 9085.0 32571 10857.0 30276 10092.0 27276 9092.0 31503 10501.0 27687 9229.0 27183 9061.0 29334 9778.0 29913 9971.0 28683 9561.0 31545 10515.0 30405 10135.0 27693 9231.0 29649 9883.0 30537 10179.0 32709 10903.0 29586 9862.0 32895 10965.0 32415 10805.0 31644 10548.0 30030 10010.0 31713 10571.0 28083 9361.0 27813 9271.0 27171 9057.0 27189 9063.0 30444 10148.0 28623 9541.0 28566 9522.0 32655 10885.0 31164 10388.0 30321 10107.0 31452 10484.0 29829 9943.0 27468 9156.0 31212 10404.0 32154 10718.0 27531 9177.0 27654 9218.0 27105 9035.0 31113 10371.0 28479 9493.0 29094 9698.0 31551 10517.0-----------------+only showing top 50 rowsNote: But after datamap creation, cust_id is coming incorrect. It is coming as thrice(equivalent to number of loads) of its original value and avg(cust_id) is correct.
issueID:CARBONDATA-2113
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/constants/CarbonVersionConstants.java
core/src/main/java/org/apache/carbondata/core/util/DataFileFooterConverter2.java
core/src/main/java/org/apache/carbondata/core/mutate/CarbonUpdateUtil.java
core/src/main/java/org/apache/carbondata/core/util/AbstractDataFileFooterConverter.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockletDataRefNode.java
texts:Count(*) and select * are not working on old store with V2 format
Count and select * are not working on old store of V2 format in 1.3.0  1) count * is giving zero, for old store after refresh0: jdbc:hive2://X.X.X.X:22550/default> refresh table brinjal5; -----------+Result-----------+ -----------+ No rows selected (3.419 seconds) 0: jdbc:hive2://X.X.X.X:22550/default> select count from brinjal5; -------------+count(1)-------------+0-------------+ 
issueID:CARBONDATA-2114
type:Bug
changed files:
texts:carbondata issues on sparksql
Hi all:    We are testing carbondata for our project. There are some problems on hdfs 2.6, spark 2.1 carbondata 1.3.    1.no multiple levels partitions, we need three levels partions.    2.spark need import carbondata jar, differ from parquet.    3.low stability, insert failure frequently. 
issueID:CARBONDATA-2116
type:Sub-task
changed files:
texts:Documentation for CTAS
Add documentation for CTAS
issueID:CARBONDATA-2117
type:Bug
changed files:
texts:Fixed Synchronization issue while creating multiple carbon session
Problem: When creating multiple session (100) session initialisation is failing with below errorjava.lang.IllegalArgumentException: requirement failed: Config entry enable.unsafe.sort already registered!Solution: Currently in CarbonEnv we are updating global configuration(shared) and location configuration in class level synchronized block. In case of multiple session class level lock will not work , need to add global level lock so only one thread will update the global configuration 
issueID:CARBONDATA-2119
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/metadata/schema/table/CarbonTable.java
processing/src/main/java/org/apache/carbondata/processing/loading/model/CarbonDataLoadSchema.java
texts:CarbonDataWriterException thrown when loading using global_sort
CREATE TABLE uniqdata_globalsort1 (CUST_ID int,CUST_NAME String,ACTIVE_EMUI_VERSION string, DOB timestamp, DOJ timestamp, BIGINT_COLUMN1 bigint,BIGINT_COLUMN2 bigint,DECIMAL_COLUMN1 decimal(30,10), DECIMAL_COLUMN2 decimal(36,10),Double_COLUMN1 double, Double_COLUMN2 double,INTEGER_COLUMN1 int) STORED BY 'carbondata' TBLPROPERTIES('SORT_SCOPE'='GLOBAL_SORT')LOAD DATA INPATH 'hdfs://hacluster/chetan/2000_UniqData.csv' into table uniqdata_globalsort1 OPTIONS('DELIMITER'=',' , 'QUOTECHAR'='"','BAD_RECORDS_ACTION'='FORCE','FILEHEADER'='CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1,Double_COLUMN2,INTEGER_COLUMN1'); EXCEPTIONThere is an unexpected error: unable to generate the mdkey     org.apache.carbondata.spark.load.DataLoadProcessorStepOnSpark$.writeFunc(DataLoadProcessorStepOnSpark.scala:222)org.apache.carbondata.spark.load.DataLoadProcessBuilderOnSpark$$anonfun$loadDataUsingGlobalSort$1.apply(DataLoadProcessBuilderOnSpark.scala:136)     org.apache.carbondata.spark.load.DataLoadProcessBuilderOnSpark$$anonfun$loadDataUsingGlobalSort$1.apply(DataLoadProcessBuilderOnSpark.scala:135)     org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)     org.apache.spark.scheduler.Task.run(Task.scala:99)     org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)   java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)java.lang.Thread.run(Thread.java:748)at org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:138)at org.apache.spark.TaskContextImpl.markTaskFailed(TaskContextImpl.scala:106)at org.apache.spark.scheduler.Task.run(Task.scala:104)... 4 moreCaused by: org.apache.carbondata.processing.loading.exception.CarbonDataLoadingException: unable to generate the mdkeyat org.apache.carbondata.processing.loading.steps.DataWriterProcessorStepImpl.processRow(DataWriterProcessorStepImpl.java:189)at org.apache.carbondata.spark.load.DataLoadProcessorStepOnSpark$.writeFunc(DataLoadProcessorStepOnSpark.scala:207)     
issueID:CARBONDATA-212
type:New Feature
changed files:core/src/main/java/org/apache/carbondata/core/metadata/schema/SchemaReader.java
core/src/main/java/org/apache/carbondata/core/util/path/CarbonTablePath.java
core/src/main/java/org/apache/carbondata/core/metadata/AbsoluteTableIdentifier.java
texts:Use SQLContext to read CarbonData file
User should be enable to use Spark's SQLContext to read CarbonData files. SQLContext will use datasource API to read corresponding CarbonData files.
issueID:CARBONDATA-2120
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/util/path/CarbonTablePath.java
core/src/main/java/org/apache/carbondata/core/datastore/page/statistics/LVStringStatsCollector.java
texts:Fixed data mismatch for No dictionary numeric data type
Problem: Is null filter is failing for numeric data type(No dictionary column).Root cause: Min max calculation is wrong when no dictionary column is not the first column. As it is not the first column null value can come in between and min max for null value is getting updated only when first row is nullSolution: Update the min max in all the case when value is null or not null for all type  
issueID:CARBONDATA-2121
type:Bug
changed files:
texts:Remove tempCSV option for Carbon Dataframe Writer

issueID:CARBONDATA-2122
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
texts:Redirect Bad Record Path Should Throw Exception on Empty Location
Data Load having bad record redirect with empty location should throw exception of Invalid Path.
issueID:CARBONDATA-2123
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/metadata/converter/ThriftWrapperSchemaConverterImpl.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/datamap/DataMapClassProvider.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/DataMapSchemaFactory.java
texts:Refactor datamap schema thrift and datamap provider to use short name and classname
Update schema thrift file for datamap schema to correct the typo errors and updated the names. Added class name to schema file and updated short name for each enum.
issueID:CARBONDATA-2124
type:Bug
changed files:
texts:data are null in streaming ingest from file source
When I dived into the tests provided in `TestStreamingTableOperation`, I found that streaming ingest from file source are null.
issueID:CARBONDATA-2125
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/TrueFilterExecutor.java
texts:like% filter is giving ArrayIndexOutOfBoundException in case of table having more pages
java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.ArrayIndexOutOfBoundsException: 1 at org.apache.carbondata.core.scan.processor.AbstractDataBlockIterator.close(AbstractDataBlockIterator.java:247) at org.apache.carbondata.core.scan.result.iterator.AbstractDetailQueryResultIterator.close(AbstractDetailQueryResultIterator.java:307) at org.apache.carbondata.core.scan.executor.impl.AbstractQueryExecutor.finish(AbstractQueryExecutor.java:590) at org.apache.carbondata.spark.vectorreader.VectorizedCarbonRecordReader.close(VectorizedCarbonRecordReader.java:162) at org.apache.carbondata.spark.rdd.CarbonScanRDD$$anon$1$$anonfun$17.apply(CarbonScanRDD.scala:385) at org.apache.carbondata.spark.rdd.CarbonScanRDD$$anon$1$$anonfun$17.apply(CarbonScanRDD.scala:384) at org.apache.spark.TaskContext$$anon$1.onTaskCompletion(TaskContext.scala:128) at org.apache.spark.TaskContextImpl$$anonfun$markTaskCompleted$1.apply(TaskContextImpl.scala:117) at org.apache.spark.TaskContextImpl$$anonfun$markTaskCompleted$1.apply(TaskContextImpl.scala:117) at org.apache.spark.TaskContextImpl$$anonfun$invokeListeners$1.apply(TaskContextImpl.scala:130) at org.apache.spark.TaskContextImpl$$anonfun$invokeListeners$1.apply(TaskContextImpl.scala:128) at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59) at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48) at org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:128) at org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:116) at org.apache.spark.scheduler.Task.run(Task.scala:109) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:325) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745)Caused by: java.util.concurrent.ExecutionException: java.lang.ArrayIndexOutOfBoundsException: 1 at java.util.concurrent.FutureTask.report(FutureTask.java:122) at java.util.concurrent.FutureTask.get(FutureTask.java:192) at org.apache.carbondata.core.scan.processor.AbstractDataBlockIterator.close(AbstractDataBlockIterator.java:242) ... 19 moreCaused by: java.lang.ArrayIndexOutOfBoundsException: 1 at org.apache.carbondata.core.scan.filter.executer.RowLevelFilterExecuterImpl.applyFilter(RowLevelFilterExecuterImpl.java:225) at org.apache.carbondata.core.scan.scanner.impl.FilterScanner.fillScannedResult(FilterScanner.java:168) at org.apache.carbondata.core.scan.scanner.impl.FilterScanner.scanBlocklet(FilterScanner.java:100) at org.apache.carbondata.core.scan.processor.AbstractDataBlockIterator$1.call(AbstractDataBlockIterator.java:201) at org.apache.carbondata.core.scan.processor.AbstractDataBlockIterator$1.call(AbstractDataBlockIterator.java:188) at java.util.concurrent.FutureTask.run(FutureTask.java:266) ... 3 more
issueID:CARBONDATA-2126
type:Sub-task
changed files:
texts:Documentation for Create Database
Create documentation for create database and also for custom location.
issueID:CARBONDATA-2127
type:Sub-task
changed files:
texts:Documentation for Hive Standard Partition
Documentation for hive standard partition 
issueID:CARBONDATA-2128
type:Sub-task
changed files:
texts:Documentation update for Table Path
Add documentation for table path while creating the table
issueID:CARBONDATA-2129
type:Improvement
changed files:
texts:Carbon  should give a remind when user use old syntax to create timeseries pre-aggregate table
After optimizing syntax for creating timeseries pre-aggregate table in https://issues.apache.org/jira/browse/CARBONDATA-2088, there are still old syntax  were used, like: https://issues.apache.org/jira/browse/CARBONDATA-2094Carbon  should give a remind when user use old syntax to create timeseries pre-aggregate table.
issueID:CARBONDATA-213
type:New Feature
changed files:
texts:Remove thrift complier dependency

issueID:CARBONDATA-2130
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/keygenerator/factory/KeyGeneratorFactory.java
core/src/main/java/org/apache/carbondata/core/util/LoadStatistics.java
core/src/main/java/org/apache/carbondata/core/datastore/block/SegmentProperties.java
core/src/main/java/org/apache/carbondata/core/scan/executor/impl/QueryExecutorProperties.java
processing/src/main/java/org/apache/carbondata/processing/loading/converter/impl/RowConverterImpl.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/reader/measure/v3/MeasureChunkPageReaderV3.java
core/src/main/java/org/apache/carbondata/core/scan/executor/impl/AbstractQueryExecutor.java
core/src/main/java/org/apache/carbondata/core/util/CarbonLoadStatisticsImpl.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/store/DimensionChunkStoreFactory.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/reader/dimension/v3/DimensionChunkPageReaderV3.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/reader/measure/v3/MeasureChunkReaderV3.java
core/src/main/java/org/apache/carbondata/core/util/CarbonLoadStatisticsDummy.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/store/impl/unsafe/UnsafeVariableLengthDimensionDataChunkStore.java
core/src/main/java/org/apache/carbondata/core/metadata/converter/ThriftWrapperSchemaConverterImpl.java
texts:Find some Spelling error in CarbonData
Find some Spelling error in CarbonData:like:realtioncloumn
issueID:CARBONDATA-2131
type:Bug
changed files:
texts:Alter table adding long datatype is failing but Create table with long type is successful, in Spark 2.1
create table test4(a1 int) stored by 'carbondata'; ---------+ | Result  | ---------+ ---------+ No rows selected (1.757 seconds)** alter table test4 add columns (a6 long); Error: java.lang.RuntimeException: BaseSqlParser>>>> == Parse1 == Operation not allowed: alter table add columns(line 1, pos 0) == SQL == alter table test4 add columns (a6 long) ^^^ == Parse2 == &#91;1.35&#93; failure: identifier matching regex (?i)VARCHAR expected alter table test4 add columns (a6 long)                                   ^; CarbonSqlParser>>>> &#91;1.35&#93; failure: identifier matching regex (?i)VARCHAR expected alter table test4 add columns (a6 long)                                   ^ (state=,code=0)
issueID:CARBONDATA-2133
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/scan/executor/util/RestructureUtil.java
core/src/main/java/org/apache/carbondata/core/scan/collector/impl/RestructureBasedVectorResultCollector.java
texts:Exception displays after performing select query on newly added Boolean data type
Exception displays after performing select query on newly added Boolean data typeSteps to Reproduce:1) Create Table:CREATE TABLE uniqdata_carbon1 (CUST_ID int,CUST_NAME String,ACTIVE_EMUI_VERSION string, DOB timestamp, DOJ timestamp, BIGINT_COLUMN1 bigint,BIGINT_COLUMN2 bigint,DECIMAL_COLUMN1 decimal(30,10), DECIMAL_COLUMN2 decimal(36,10),Double_COLUMN1 double, Double_COLUMN2 double, INTEGER_COLUMN1 int)stored by 'carbondata' tblproperties('no_inverted_index'='cust_id ')2)Load Data into the table:LOAD DATA INPATH 'hdfs://localhost:54310/Data/uniqdata/2000_UniqData.csv' into table uniqdata_carbon1 OPTIONS('FILEHEADER'='CUST_ID,CUST_NAME ,ACTIVE_EMUI_VERSION,DOB,DOJ, BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1, Double_COLUMN2,INTEGER_COLUMN1','BAD_RECORDS_ACTION'='FORCE') 3) Execute Alter Query:alter table uniqdata_carbon1 add columns (booleanfield boolean) tblproperties('default.value.booleanfield'='true'); 4) execute desc table:desc uniqdata_carbon1Output:---------------------------------------------+ col_name  data_type  comment ---------------------------------------------+ cust_id  int  NULL  cust_name  string  NULL  active_emui_version  string  NULL  dob  timestamp  NULL  doj  timestamp  NULL  bigint_column1  bigint  NULL  bigint_column2  bigint  NULL  decimal_column1  decimal(30,10)  NULL  decimal_column2  decimal(36,10)  NULL  double_column1  double  NULL  double_column2  double  NULL  integer_column1  int  NULL  booleanfield  boolean  NULL ---------------------------------------------+5) Execute Select Query on added Column:select booleanfield from uniqdata_carbon1Actual Output:Error: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 71.0 failed 1 times, most recent failure: Lost task 0.0 in stage 71.0 (TID 322, localhost, executor driver): java.lang.NumberFormatException: For input string: "true" at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043) at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110) at java.lang.Double.parseDouble(Double.java:538) at java.lang.Double.valueOf(Double.java:502) at org.apache.carbondata.core.scan.executor.util.RestructureUtil.getMeasureDefaultValue(RestructureUtil.java:306) at org.apache.carbondata.core.scan.executor.util.RestructureUtil.createMeasureInfoAndGetCurrentBlockQueryMeasures(RestructureUtil.java:395) at org.apache.carbondata.core.scan.executor.impl.AbstractQueryExecutor.getCurrentBlockQueryMeasures(AbstractQueryExecutor.java:504) at org.apache.carbondata.core.scan.executor.impl.AbstractQueryExecutor.getBlockExecutionInfoForBlock(AbstractQueryExecutor.java:308) at org.apache.carbondata.core.scan.executor.impl.AbstractQueryExecutor.getBlockExecutionInfos(AbstractQueryExecutor.java:258) at org.apache.carbondata.core.scan.executor.impl.VectorDetailQueryExecutor.execute(VectorDetailQueryExecutor.java:36) at org.apache.carbondata.spark.vectorreader.VectorizedCarbonRecordReader.initialize(VectorizedCarbonRecordReader.java:123) at org.apache.carbondata.spark.rdd.CarbonScanRDD.internalCompute(CarbonScanRDD.scala:365) at org.apache.carbondata.spark.rdd.CarbonRDD.compute(CarbonRDD.scala:60) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) at org.apache.spark.rdd.RDD.iterator(RDD.scala:287) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) at org.apache.spark.rdd.RDD.iterator(RDD.scala:287) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) at org.apache.spark.rdd.RDD.iterator(RDD.scala:287) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87) at org.apache.spark.scheduler.Task.run(Task.scala:99) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745)Driver stacktrace: (state=,code=0) Expected Output: It should display correct output. 
issueID:CARBONDATA-2134
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/scan/filter/FilterUtil.java
integration/spark-common/src/main/java/org/apache/carbondata/spark/util/Util.java
texts:Prevent implicit column filter list from getting serialized while submitting task to executor
*Problem*In the current store blocklet pruning in driver and no further pruning takes place in the executor side. But still the implicit column filter list being sent to executor. As the size of list grows the cost of serializing and deserializing the list is increasing which can impact the query performance.*Solution*Remove the list from the filter expression before submitting the task to executor.
issueID:CARBONDATA-2135
type:Task
changed files:
texts:Documentation for Table Comment and Column Comment
Add documentation for Table Comment and Column Comment
issueID:CARBONDATA-2137
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/locks/HdfsFileLock.java
core/src/main/java/org/apache/carbondata/core/locks/LocalFileLock.java
core/src/main/java/org/apache/carbondata/core/mutate/DeleteDeltaBlockDetails.java
texts:Delete query is taking more time while processing the carbondata.
Expected Output : Delete query should take less timeActual Output : Delete Query is taking 20minFollowing the steps to reproduce :  create table and load 500 million records create hive table with 10% of data delete the data in main-table using hive table check the performanceFollowing is the configuration used : SPARK_EXECUTOR_MEMORY : 200G SPARK_DRIVER_MEMORY : 20G SPARK_EXECUTOR_CORES : 32 SPARK_EXECUTOR_INSTANCEs : 3
issueID:CARBONDATA-2138
type:Task
changed files:
texts:Documentation for HEADER option
Add documentation for HEADER option as per the discussion in the below mailing list.http://apache-carbondata-dev-mailing-list-archive.1130556.n5.nabble.com/Discussion-Add-HEADER-option-to-load-data-sql-td17080.html#a17138
issueID:CARBONDATA-2139
type:Improvement
changed files:
texts:Optimize CTAS documentation and test case
Optimize CTAS: optimize documentation add test case drop table  after finishing run test acse, remove the file of table from disk
issueID:CARBONDATA-214
type:Bug
changed files:
texts:The binary file need be removed under core/src/test/resources/part-xxx
The binary file need be removed under core/src/test/resources/part-xxx, need to auto-generate the binary file while running test case
issueID:CARBONDATA-2140
type:Improvement
changed files:
texts:Presto Integration - Code Refactoring
Presto Integration - Code Refactoring to remove unnecessary class and improve the performance.
issueID:CARBONDATA-2142
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/locks/CarbonLockUtil.java
core/src/main/java/org/apache/carbondata/core/metadata/AbsoluteTableIdentifier.java
texts:Fixed aggregate data map creation issue in case of hive metastore
Fixed aggregate data map creation issue in case of hive metastore
issueID:CARBONDATA-2143
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/scan/executor/impl/AbstractQueryExecutor.java
hadoop/src/main/java/org/apache/carbondata/hadoop/AbstractRecordReader.java
texts:Fixed query memory leak issue for task failure during initialization of record reader
Problem: Whenever a query is executed, in the internalCompute method of CarbonScanRdd class record reader is initialized. A task completion listener is attached to each task after initialization of the record reader. During record reader initialization, queryResultIterator is initialized and one blocklet is processed. The blocklet processed will use available unsafe memory. Lets say there are 100 columns and 80 columns get the space but there is no space left for the remaining columns to be stored in the unsafe memory. This will result is memory exception and record reader initialization will fail leading to failure in query. In the above case the unsafe memory allocated for 80 columns will not be freed and will always remain occupied till the JVM process persists.Impact It is memory leak in the system and can lead to query failures for queries executed after one one query fails due to the above reason.Exception Tracejava.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.lang.RuntimeException: org.apache.carbondata.core.memory.MemoryException: Not enough memory               at org.apache.carbondata.core.scan.processor.AbstractDataBlockIterator.updateScanner(AbstractDataBlockIterator.java:136)               at org.apache.carbondata.core.scan.processor.impl.DataBlockIteratorImpl.next(DataBlockIteratorImpl.java:50)               at org.apache.carbondata.core.scan.processor.impl.DataBlockIteratorImpl.next(DataBlockIteratorImpl.java:32)               at org.apache.carbondata.core.scan.result.iterator.DetailQueryResultIterator.getBatchResult(DetailQueryResultIterator.java:49)               at org.apache.carbondata.core.scan.result.iterator.DetailQueryResultIterator.next(DetailQueryResultIterator.java:41)               at org.apache.carbondata.core.scan.result.iterator.DetailQueryResultIterator.next(DetailQueryResultIterator.java:31)               at org.apache.carbondata.core.scan.result.iterator.ChunkRowIterator.<init>(ChunkRowIterator.java:41)               at org.apache.carbondata.hadoop.CarbonRecordReader.initialize(CarbonRecordReader.java:84)               at org.apache.carbondata.spark.rdd.CarbonScanRDD.internalCompute(CarbonScanRDD.scala:378)               at org.apache.carbondata.spark.rdd.CarbonRDD.compute(CarbonRDD.scala:60)               at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)               at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)               at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)               at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)               at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)               at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)               at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)               at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)               at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)               at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)               at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)               at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)               at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)               at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)               at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)               at org.apache.spark.scheduler.Task.run(Task.scala:99)               at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)               at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)               at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
issueID:CARBONDATA-2144
type:Improvement
changed files:
texts:There are some improper place in pre-aggregate documentation
Optimize pre-aggregate documentation: add blank space upper caselike:Carbondata supports pre aggregating of data so that OLAP kind of queries can fetch data much faster.Aggregate tables are created as datamaps so that the handling is as efficient as other indexing support.Users can create as many aggregate tables they require as datamaps to improve their query performance,provided the storage requirements and loading speeds are acceptable.For main table called sales which is defined asCREATE TABLE sales (order_time timestamp,user_id string,sex string,country string,quantity int,price bigint)STORED BY 'carbondata')need to
issueID:CARBONDATA-2145
type:Improvement
changed files:
texts:Refactor PreAggregate functionality for dictionary include.
If in maintable, the column is dictionary type then only add the count to measure column.
issueID:CARBONDATA-2146
type:Bug
changed files:
texts:Preaggregate table is not dropped from metastore if creation fails

issueID:CARBONDATA-2147
type:Bug
changed files:streaming/src/main/java/org/apache/carbondata/streaming/parser/CSVStreamParserImp.java
streaming/src/main/java/org/apache/carbondata/streaming/parser/CarbonStreamParser.java
texts:Exception displays while loading data with streaming
Exception displays while loading data with streamingSteps to reproduce:1) start spark-shell:./spark-shell --jars /opt/spark/spark-2.2.1/carbonlib/carbondata_2.11-1.3.0-SNAPSHOT-shade-hadoop2.7.2.jar2) Execute following script:import org.apache.spark.sql.SparkSessionimport org.apache.spark.sql.CarbonSession._import org.apache.carbondata.core.util.CarbonPropertiesimport org.apache.spark.sql.streaming.{ProcessingTime, StreamingQuery}val carbon = SparkSession.builder().config(sc.getConf) .getOrCreateCarbonSession("hdfs://localhost:54310/newCarbonStore","/tmp")import org.apache.carbondata.core.constants.CarbonCommonConstantsimport org.apache.carbondata.core.util.CarbonPropertiesCarbonProperties.getInstance().addProperty(CarbonCommonConstants.CARBON_BAD_RECORDS_ACTION, "FORCE")carbon.sql("drop table if exists uniqdata_stream")carbon.sql("create table uniqdata_stream(CUST_ID int,CUST_NAME String,DOB timestamp,DOJ timestamp, BIGINT_COLUMN1 bigint,BIGINT_COLUMN2 bigint,DECIMAL_COLUMN1 decimal(30,10),DECIMAL_COLUMN2 decimal(36,10),Double_COLUMN1 double, Double_COLUMN2 double,INTEGER_COLUMN1 int) STORED BY 'org.apache.carbondata.format' TBLPROPERTIES ('TABLE_BLOCKSIZE'= '256 MB', 'streaming'='true')");import carbon.sqlContext.implicits._import org.apache.spark.sql.types._val uniqdataSch = StructType(Array(StructField("CUST_ID", IntegerType),StructField("CUST_NAME", StringType),StructField("DOB", TimestampType), StructField("DOJ", TimestampType), StructField("BIGINT_COLUMN1", LongType), StructField("BIGINT_COLUMN2", LongType), StructField("DECIMAL_COLUMN1", org.apache.spark.sql.types.DecimalType(30, 10)), StructField("DECIMAL_COLUMN2", org.apache.spark.sql.types.DecimalType(36,10)), StructField("Double_COLUMN1", DoubleType), StructField("Double_COLUMN2", DoubleType), StructField("INTEGER_COLUMN1", IntegerType)))val streamDf = carbon.readStream.schema(uniqdataSch).option("sep", ",").csv("file:///home/knoldus/Documents/uniqdata")val qry = streamDf.writeStream.format("carbondata").trigger(ProcessingTime("5 seconds")) .option("checkpointLocation","/stream/uniq") .option("dbName", "default") .option("tableName", "uniqdata_stream") .start() 3) Error logs:warning: there was one deprecation warning; re-run with -deprecation for detailsuniqdataSch: org.apache.spark.sql.types.StructType = StructType(StructField(CUST_ID,IntegerType,true), StructField(CUST_NAME,StringType,true), StructField(DOB,TimestampType,true), StructField(DOJ,TimestampType,true), StructField(BIGINT_COLUMN1,LongType,true), StructField(BIGINT_COLUMN2,LongType,true), StructField(DECIMAL_COLUMN1,DecimalType(30,10),true), StructField(DECIMAL_COLUMN2,DecimalType(36,10),true), StructField(Double_COLUMN1,DoubleType,true), StructField(Double_COLUMN2,DoubleType,true), StructField(INTEGER_COLUMN1,IntegerType,true))streamDf: org.apache.spark.sql.DataFrame = &#91;CUST_ID: int, CUST_NAME: string ... 9 more fields&#93;qry: org.apache.spark.sql.streaming.StreamingQuery = org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@d0e155cscala> 18/02/08 16:38:53 ERROR StreamSegment: Executor task launch worker for task 5 Failed to append batch data to stream segment: hdfs://localhost:54310/newCarbonStore/default/uniqdata_stream1/Fact/Part0/Segment_0java.lang.NullPointerException at org.apache.spark.sql.catalyst.InternalRow.getString(InternalRow.scala:32) at org.apache.carbondata.streaming.parser.CSVStreamParserImp.parserRow(CSVStreamParserImp.java:40) at org.apache.spark.sql.execution.streaming.CarbonAppendableStreamSink$InputIterator.next(CarbonAppendableStreamSink.scala:337) at org.apache.spark.sql.execution.streaming.CarbonAppendableStreamSink$InputIterator.next(CarbonAppendableStreamSink.scala:331) at org.apache.carbondata.streaming.segment.StreamSegment.appendBatchData(StreamSegment.java:244) at org.apache.spark.sql.execution.streaming.CarbonAppendableStreamSink$$anonfun$writeDataFileTask$1.apply$mcV$sp(CarbonAppendableStreamSink.scala:315) at org.apache.spark.sql.execution.streaming.CarbonAppendableStreamSink$$anonfun$writeDataFileTask$1.apply(CarbonAppendableStreamSink.scala:305) at org.apache.spark.sql.execution.streaming.CarbonAppendableStreamSink$$anonfun$writeDataFileTask$1.apply(CarbonAppendableStreamSink.scala:305) at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1371) at org.apache.spark.sql.execution.streaming.CarbonAppendableStreamSink$.writeDataFileTask(CarbonAppendableStreamSink.scala:317) at org.apache.spark.sql.execution.streaming.CarbonAppendableStreamSink$$anonfun$writeDataFileJob$1$$anonfun$apply$mcV$sp$1.apply(CarbonAppendableStreamSink.scala:228) at org.apache.spark.sql.execution.streaming.CarbonAppendableStreamSink$$anonfun$writeDataFileJob$1$$anonfun$apply$mcV$sp$1.apply(CarbonAppendableStreamSink.scala:227) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87) at org.apache.spark.scheduler.Task.run(Task.scala:108) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745)18/02/08 16:38:53 ERROR Utils: Aborting taskjava.lang.NullPointerException at org.apache.spark.sql.catalyst.InternalRow.getString(InternalRow.scala:32) at org.apache.carbondata.streaming.parser.CSVStreamParserImp.parserRow(CSVStreamParserImp.java:40) at org.apache.spark.sql.execution.streaming.CarbonAppendableStreamSink$InputIterator.next(CarbonAppendableStreamSink.scala:337) at org.apache.spark.sql.execution.streaming.CarbonAppendableStreamSink$InputIterator.next(CarbonAppendableStreamSink.scala:331) at org.apache.carbondata.streaming.segment.StreamSegment.appendBatchData(StreamSegment.java:244) at org.apache.spark.sql.execution.streaming.CarbonAppendableStreamSink$$anonfun$writeDataFileTask$1.apply$mcV$sp(CarbonAppendableStreamSink.scala:315) at org.apache.spark.sql.execution.streaming.CarbonAppendableStreamSink$$anonfun$writeDataFileTask$1.apply(CarbonAppendableStreamSink.scala:305) at org.apache.spark.sql.execution.streaming.CarbonAppendableStreamSink$$anonfun$writeDataFileTask$1.apply(CarbonAppendableStreamSink.scala:305) at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1371) at org.apache.spark.sql.execution.streaming.CarbonAppendableStreamSink$.writeDataFileTask(CarbonAppendableStreamSink.scala:317) at org.apache.spark.sql.execution.streaming.CarbonAppendableStreamSink$$anonfun$writeDataFileJob$1$$anonfun$apply$mcV$sp$1.apply(CarbonAppendableStreamSink.scala:228) at org.apache.spark.sql.execution.streaming.CarbonAppendableStreamSink$$anonfun$writeDataFileJob$1$$anonfun$apply$mcV$sp$1.apply(CarbonAppendableStreamSink.scala:227) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87) at org.apache.spark.scheduler.Task.run(Task.scala:108) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745)18/02/08 16:38:53 ERROR CarbonAppendableStreamSink$: Executor task launch worker for task 5 Job job_20180208163853_0005 aborted.18/02/08 16:38:53 ERROR Executor: Exception in task 0.0 in stage 5.0 (TID 5)org.apache.carbondata.streaming.CarbonStreamException: Task failed while writing rows at org.apache.spark.sql.execution.streaming.CarbonAppendableStreamSink$.writeDataFileTask(CarbonAppendableStreamSink.scala:324) at org.apache.spark.sql.execution.streaming.CarbonAppendableStreamSink$$anonfun$writeDataFileJob$1$$anonfun$apply$mcV$sp$1.apply(CarbonAppendableStreamSink.scala:228) at org.apache.spark.sql.execution.streaming.CarbonAppendableStreamSink$$anonfun$writeDataFileJob$1$$anonfun$apply$mcV$sp$1.apply(CarbonAppendableStreamSink.scala:227) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87) at org.apache.spark.scheduler.Task.run(Task.scala:108) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745)Caused by: java.lang.NullPointerException at org.apache.spark.sql.catalyst.InternalRow.getString(InternalRow.scala:32) at org.apache.carbondata.streaming.parser.CSVStreamParserImp.parserRow(CSVStreamParserImp.java:40) at org.apache.spark.sql.execution.streaming.CarbonAppendableStreamSink$InputIterator.next(CarbonAppendableStreamSink.scala:337) at org.apache.spark.sql.execution.streaming.CarbonAppendableStreamSink$InputIterator.next(CarbonAppendableStreamSink.scala:331) at org.apache.carbondata.streaming.segment.StreamSegment.appendBatchData(StreamSegment.java:244) at org.apache.spark.sql.execution.streaming.CarbonAppendableStreamSink$$anonfun$writeDataFileTask$1.apply$mcV$sp(CarbonAppendableStreamSink.scala:315) at org.apache.spark.sql.execution.streaming.CarbonAppendableStreamSink$$anonfun$writeDataFileTask$1.apply(CarbonAppendableStreamSink.scala:305) at org.apache.spark.sql.execution.streaming.CarbonAppendableStreamSink$$anonfun$writeDataFileTask$1.apply(CarbonAppendableStreamSink.scala:305) at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1371) at org.apache.spark.sql.execution.streaming.CarbonAppendableStreamSink$.writeDataFileTask(CarbonAppendableStreamSink.scala:317) ... 8 more
issueID:CARBONDATA-2148
type:Improvement
changed files:streaming/src/main/java/org/apache/carbondata/streaming/parser/CSVStreamParserImp.java
streaming/src/main/java/org/apache/carbondata/streaming/parser/CarbonStreamParser.java
texts:Use Row parser to replace current default parser:CSVStreamParserImp
Currently the default value of 'carbon.stream.parser' is CSVStreamParserImp, it transforms InternalRow(0) to Array&#91;Object&#93;, InternalRow(0) represents the value of one line which is received from Socket. When it receives data from Kafka, the schema of InternalRow is changed, either it need to assemble the fields of kafka data Row into a String and stored it as InternalRow(0), or define a new parser to convert kafka data Row to Array&#91;Object&#93;. It needs the same operation for every table.Solution:Use a new parser called RowStreamParserImpl as the default parser instead of CSVStreamParserImpl, this new parser will automatically convert InternalRow to Array&#91;Object&#93; according to the schema. In general, we will transform source data to a structed Row object, using this way, we do not need to define a parser for every table. 
issueID:CARBONDATA-2149
type:Bug
changed files:hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonTableOutputFormat.java
texts:Displayed complex type data is error when use DataFrame to write complex type data.
The default value of 'complex_delimiter_level_1' and 'complex_delimiter_level_2' is wrong, it must be '$' and ':', not be '\ \ $' and '\ \ :'. Escape characters '\ \ ' need to be added only when using delimiters in ArrayParserImpl or StructParserImpl. 
issueID:CARBONDATA-215
type:Bug
changed files:
texts:Correct the file headers of classes
Extra file header is present in the following classes. These files should not have any other file headers apart from Apache's../integration/spark/src/main/java/org/apache/carbondata/spark/partition/api/DataPartitioner.java 2. ./integration/spark/src/main/java/org/apache/carbondata/spark/partition/api/impl/CSVFilePartitioner.java 3. ./integration/spark/src/main/java/org/apache/carbondata/spark/partition/api/impl/DefaultLoadBalancer.java 4. ./integration/spark/src/main/java/org/apache/carbondata/spark/partition/api/impl/PartitionImpl.java 5. ./integration/spark/src/main/java/org/apache/carbondata/spark/partition/api/impl/QueryPartitionHelper.java 6. ./integration/spark/src/main/java/org/apache/carbondata/spark/partition/api/Partition.java
issueID:CARBONDATA-2150
type:Bug
changed files:
texts:Unwanted updatetable status files are being generated for the delete operation where no records are deleted
Unwanted updatetable status files are being generated for the delete operation where no records are deleted
issueID:CARBONDATA-2151
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelRangeGrtrThanEquaToFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RangeValueFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/FilterExecuter.java
streaming/src/main/java/org/apache/carbondata/streaming/CarbonStreamRecordReader.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelRangeGrtThanFiterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelRangeLessThanEqualFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/scan/filter/FilterUtil.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelRangeLessThanFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/ExcludeFilterExecuterImpl.java
texts:Filter query on Timestamp/Date column of streaming table throwing exception
at org.apache.carbondata.hadoop.streaming.CarbonStreamRecordReader.scanBlockletAndFillVector(CarbonStreamRecordReader.java:435) at org.apache.carbondata.hadoop.streaming.CarbonStreamRecordReader.nextColumnarBatch(CarbonStreamRecordReader.java:324) at org.apache.carbondata.hadoop.streaming.CarbonStreamRecordReader.nextKeyValue(CarbonStreamRecordReader.java:305) at org.apache.carbondata.spark.rdd.CarbonScanRDD$$anon$1.hasNext(CarbonScanRDD.scala:382) at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.scan_nextBatch$(Unknown Source) at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source) at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43) at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377) at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:231) at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:225) at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:826) at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:826) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) at org.apache.spark.rdd.RDD.iterator(RDD.scala:287) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87) at org.apache.spark.scheduler.Task.run(Task.scala:99) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745)Caused by: org.apache.carbondata.core.scan.expression.exception.FilterUnsupportedException:  at org.apache.carbondata.core.scan.filter.executer.RowLevelFilterExecuterImpl.applyFilter(RowLevelFilterExecuterImpl.java:280) at org.apache.carbondata.core.scan.filter.executer.AndFilterExecuterImpl.applyFilter(AndFilterExecuterImpl.java:56) at org.apache.carbondata.hadoop.streaming.CarbonStreamRecordReader.scanBlockletAndFillVector(CarbonStreamRecordReader.java:430) ... 20 moreCaused by: org.apache.carbondata.core.scan.expression.exception.FilterIllegalMemberException: Cannot convertTIMESTAMP to Time type value at org.apache.carbondata.core.scan.expression.ExpressionResult.getTime(ExpressionResult.java:387) at org.apache.carbondata.core.scan.expression.conditional.GreaterThanEqualToExpression.evaluate(GreaterThanEqualToExpression.java:64) at org.apache.carbondata.core.scan.filter.executer.RowLevelFilterExecuterImpl.applyFilter(RowLevelFilterExecuterImpl.java:278) ... 22 more
issueID:CARBONDATA-2152
type:Bug
changed files:
texts:Min function working incorrectly for string type with dictionary include in presto.
Steps to reproduce:1) Create and Load in carbondata.create table if not exists lineitem_carbon1(L_SHIPDATE date,L_SHIPMODE string,L_SHIPINSTRUCT string,L_RETURNFLAG string,L_RECEIPTDATE date,L_ORDERKEY string,L_PARTKEY string,L_SUPPKEY   string,L_LINENUMBER int,L_QUANTITY double,L_EXTENDEDPRICE double,L_DISCOUNT double,L_TAX double,L_LINESTATUS string,L_COMMITDATE date,L_COMMENT  string) STORED BY 'carbondata'TBLPROPERTIES ('DICTIONARY_INCLUDE'='L_SHIPMODE,L_SHIPINSTRUCT,L_RETURNFLAG,L_LINESTATUS', 'table_blocksize'='300', 'no_inverted_index'='L_ORDERKEY, L_PARTKEY, L_SUPPKEY, L_COMMENT');load data inpath "hdfs://localhost:54310/user/hduser/input-files/lineitem.csv" into table lineitem_carbon1 options('DATEFORMAT' = 'yyyy-MM-dd','DELIMITER'='|','FILEHEADER'='L_ORDERKEY,L_PARTKEY,L_SUPPKEY,L_LINENUMBER,L_QUANTITY,L_EXTENDEDPRICE,L_DISCOUNT,L_TAX,L_RETURNFLAG,L_LINESTATUS,L_SHIPDATE,L_COMMITDATE,L_RECEIPTDATE,L_SHIPINSTRUCT,L_SHIPMODE,L_COMMENT','BAD_RECORDS_LOGGER_ENABLE'='true', 'BAD_RECORDS_ACTION'='FORCE');  0: jdbc:hive2://localhost:10000> select min(l_shipmode) from lineitem_carbon1;------------------+ min(l_shipmode)  ------------------+ AIR              ------------------+2) Connect to carbondata store from presto and perform the below query from presto-cli:presto:performance> select min(l_shipmode) from lineitem_carbon1;_col0 ----------@NU#LL$! (1 row) Expected: On presto also, it should give the correct output as shown on carbondata. 
issueID:CARBONDATA-2155
type:Bug
changed files:
texts:IS NULL not working correctly on string datatype with dictionary_include in presto integration
Steps to reproduce:1) Create table on carbondata and load data to it.create table if not exists lineitem_carbon1(L_SHIPDATE date,L_SHIPMODE string,L_SHIPINSTRUCT string,L_RETURNFLAG string,L_RECEIPTDATE date,L_ORDERKEY string,L_PARTKEY string,L_SUPPKEY   string,L_LINENUMBER int,L_QUANTITY double,L_EXTENDEDPRICE double,L_DISCOUNT double,L_TAX double,L_LINESTATUS string,L_COMMITDATE date,L_COMMENT  string) STORED BY 'carbondata'TBLPROPERTIES ('DICTIONARY_INCLUDE'='L_SHIPMODE,L_SHIPINSTRUCT,L_RETURNFLAG,L_LINESTATUS', 'table_blocksize'='300', 'no_inverted_index'='L_ORDERKEY, L_PARTKEY, L_SUPPKEY, L_COMMENT');load data inpath "hdfs://localhost:54310/user/hduser/input-files/lineitem.csv" into table lineitem_carbon1 options('DATEFORMAT' = 'yyyy-MM-dd','DELIMITER'='|','FILEHEADER'='L_ORDERKEY,L_PARTKEY,L_SUPPKEY,L_LINENUMBER,L_QUANTITY,L_EXTENDEDPRICE,L_DISCOUNT,L_TAX,L_RETURNFLAG,L_LINESTATUS,L_SHIPDATE,L_COMMITDATE,L_RECEIPTDATE,L_SHIPINSTRUCT,L_SHIPMODE,L_COMMENT','BAD_RECORDS_LOGGER_ENABLE'='true', 'BAD_RECORDS_ACTION'='FORCE');1: jdbc:hive2://localhost:10000> select l_shipmode from lineitem_carbon1 where l_shipmode is NULL;-------------+ l_shipmode -------------+ NULL -------------–+2) Access the same table from presto-cli and try to run select query form there:presto:performance> select l_shipmode from lineitem_carbon1 where l_shipmode is NULL; l_shipmode ------------(0 rows) Expected Result: It should be same as result from carbon.
issueID:CARBONDATA-2159
type:Improvement
changed files:common/src/main/java/org/apache/carbondata/common/exceptions/sql/MalformedDataMapCommandException.java
common/src/main/java/org/apache/carbondata/common/exceptions/sql/MalformedCarbonCommandException.java
common/src/main/java/org/apache/carbondata/common/exceptions/TableStatusLockException.java
core/src/main/java/org/apache/carbondata/core/exception/ConcurrentOperationException.java
processing/src/main/java/org/apache/carbondata/processing/util/CarbonLoaderUtil.java
processing/src/main/java/org/apache/carbondata/processing/loading/model/CarbonLoadModelBuilder.java
processing/src/main/java/org/apache/carbondata/processing/loading/model/LoadOption.java
core/src/main/java/org/apache/carbondata/core/util/DeleteLoadFolders.java
common/src/main/java/org/apache/carbondata/common/Strings.java
core/src/main/java/org/apache/carbondata/core/statusmanager/SegmentStatusManager.java
processing/src/main/java/org/apache/carbondata/processing/loading/model/CarbonLoadModel.java
common/src/main/java/org/apache/carbondata/common/exceptions/sql/NoSuchDataMapException.java
core/src/main/java/org/apache/carbondata/core/datamap/TableDataMap.java
common/src/main/java/org/apache/carbondata/common/Maps.java
store/sdk/src/main/java/org/apache/carbondata/sdk/file/CarbonWriterBuilder.java
common/src/main/java/org/apache/carbondata/common/exceptions/sql/InvalidLoadOptionException.java
texts:Remove carbon-spark dependency for sdk module
store-sdk module should not depend on carbon-spark module
issueID:CARBONDATA-216
type:Bug
changed files:
texts:Files should be deleted as this feature not supported now.
Following files and its references need to deleted from carbon as this feature is not used now.1. ./core/src/test/java/org/apache/carbondata/scan/QueryExecutor_UT.java 2./integration/spark/src/main/java/org/apache/carbondata/spark/partition/reader/CSVParser.java 3. ./integration/spark/src/main/java/org/apache/carbondata/spark/partition/reader/CSVReader.java 4. ./integration/spark/src/main/java/org/apache/carbondata/spark/partition/reader/CSVWriter.java 5. ./integration/spark/src/main/java/org/apache/carbondata/spark/partition/reader/ResultSetHelper.java 6. ./integration/spark/src/main/java/org/apache/carbondata/spark/partition/reader/ResultSetHelperService.java
issueID:CARBONDATA-2160
type:Bug
changed files:
texts:Compacted streaming segment&#39;s Merged To is empty
SegmentSequenceIdStatusLoad Start TimeLoad End TimeMerged ToFile Format7Success2018-02-11 02:02:37.1052018-02-11 02:02:37.293NACOLUMNAR_V36Success2018-02-11 02:02:15.0682018-02-11 02:02:15.194NACOLUMNAR_V35Compacted2018-02-11 02:02:15.0622018-02-11 02:02:37.102NAROW_V14Compacted2018-02-11 02:02:06.9812018-02-11 02:02:15.062NAROW_V13Success2018-02-11 02:02:04.3482018-02-11 02:02:04.462NACOLUMNAR_V32Success2018-02-11 02:02:04.0722018-02-11 02:02:04.346NACOLUMNAR_V31Compacted2018-02-11 02:01:50.1882018-02-11 02:02:04.066NAROW_V10Compacted2018-02-11 02:01:44.062018-02-11 02:01:50.188NAROW_V1
issueID:CARBONDATA-2161
type:Bug
changed files:
texts:Compacted Segment of Streaming Table should update "mergeTo" column
When Handoff is trigger , ROW file format will be converted into COLUMNAR  and that segment status will be updated as "Compacted". But "Merged TO" column is not updated.----------------------------------------------------------------------------------+SegmentSequenceIdStatus   Load Start Time        Load End Time          Merged ToFile Format----------------------------------------------------------------------------------+2                Success  2018-02-11 18:17:24.1572018-02-11 18:17:25.899NA       COLUMNAR_V31                Streaming2018-02-11 18:17:24.137null                   NA       ROW_V1     0                Compacted2018-02-11 18:15:54.2622018-02-11 18:17:24.137NA        ROW_V1     ----------------------------------------------------------------------------------+Expected ----------------------------------------------------------------------------------+SegmentSequenceIdStatus   Load Start Time        Load End Time          Merged ToFile Format----------------------------------------------------------------------------------+2                Success  2018-02-11 18:17:24.1572018-02-11 18:17:25.899NA       COLUMNAR_V31                Streaming2018-02-11 18:17:24.137null                   NA       ROW_V1     0                Compacted2018-02-11 18:15:54.2622018-02-11 18:17:24.1372        ROW_V1     ----------------------------------------------------------------------------------+
issueID:CARBONDATA-2165
type:Sub-task
changed files:streaming/src/main/java/org/apache/carbondata/streaming/StreamBlockletWriter.java
integration/spark2/src/main/scala/org/apache/carbondata/stream/CarbonStreamRecordReader.java
hadoop/src/main/java/org/apache/carbondata/hadoop/stream/StreamBlockletReader.java
streaming/src/main/java/org/apache/carbondata/streaming/CarbonStreamOutputFormat.java
core/src/main/java/org/apache/carbondata/core/util/DataTypeConverter.java
hadoop/src/main/java/org/apache/carbondata/hadoop/stream/CarbonStreamInputFormat.java
streaming/src/main/java/org/apache/carbondata/streaming/CarbonStreamRecordWriter.java
streaming/src/main/java/org/apache/carbondata/streaming/segment/StreamSegment.java
integration/spark-datasource/src/main/scala/org/apache/carbondata/converter/SparkDataTypeConverterImpl.java
core/src/main/java/org/apache/carbondata/core/util/DataTypeConverterImpl.java
texts:Remove spark dependency in carbon-hadoop module

issueID:CARBONDATA-2166
type:Bug
changed files:
texts:Default value of cutoff timestamp is wrong
In the configuration-parameters.md, it says that the default value of `carbon.cutoffTimestamp` is `1970-01-01 05:30:00`. But actually in `TimeStampDirectDictionaryGenerator` it use empty as default value. As  a result, some tests in module `SDVTests` ran failed in my local machine.For example,  testcase of `BadRecord_Dataload_006` ran failed in maven but it ran successfully in IDE. Besides, the TimeZone should also be set accordingly to make the tests right.
issueID:CARBONDATA-2168
type:Improvement
changed files:processing/src/main/java/org/apache/carbondata/processing/loading/iterator/CarbonOutputIteratorWrapper.java
core/src/main/java/org/apache/carbondata/core/util/DataTypeUtil.java
processing/src/main/java/org/apache/carbondata/processing/loading/steps/InputProcessorStepWithNoConverterImpl.java
processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactDataHandlerColumnar.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonTableOutputFormat.java
core/src/main/java/org/apache/carbondata/core/datastore/page/SafeDecimalColumnPage.java
core/src/main/java/org/apache/carbondata/core/datastore/page/SafeVarLengthColumnPage.java
processing/src/main/java/org/apache/carbondata/processing/util/CarbonDataProcessorUtil.java
core/src/main/java/org/apache/carbondata/core/datastore/page/SafeFixLengthColumnPage.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonOutputCommitter.java
processing/src/main/java/org/apache/carbondata/processing/loading/DataLoadProcessBuilder.java
processing/src/main/java/org/apache/carbondata/processing/loading/converter/impl/MeasureFieldConverterImpl.java
processing/src/main/java/org/apache/carbondata/processing/loading/DataField.java
processing/src/main/java/org/apache/carbondata/processing/loading/model/CarbonLoadModel.java
processing/src/main/java/org/apache/carbondata/processing/loading/converter/impl/NonDictionaryFieldConverterImpl.java
core/src/main/java/org/apache/carbondata/hadoop/internal/ObjectArrayWritable.java
texts:Support global sort on partition tables
Currently, user cannot use global sort on standard hive partitioned tables. Better support global sort on partitioned tables to get better resource utilization while loading and concurrent performance while querying tables
issueID:CARBONDATA-217
type:Bug
changed files:integration/spark-common/src/main/java/org/apache/carbondata/spark/merger/RowResultMerger.java
texts:Data mismatch issue in After compaction
Step1: Run Data load and restart serverStep2: Start compaxctionStep3: Run QueryQuery is giving different result after compactionProblem:In compaction flow we are not setting the useInvertedIndex boolean for key column so it is false and it is not writing column data in sorted order and binary search is failing Solution: Need to set the useInvertedIndex array from carbon table
issueID:CARBONDATA-2172
type:Sub-task
changed files:integration/spark2/src/main/java/org/apache/carbondata/datamap/IndexDataMapProvider.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/TableInfo.java
core/src/main/java/org/apache/carbondata/core/datamap/DataMapStoreManager.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockletDataMapFactory.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/datamap/DataMapClassProvider.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/CarbonTable.java
integration/spark2/src/main/java/org/apache/carbondata/datamap/DataMapManager.java
core/src/main/java/org/apache/carbondata/core/datamap/dev/DataMapFactory.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/DataMapSchema.java
datamap/lucene/src/main/java/org/apache/carbondata/datamap/lucene/LuceneDataMapFactoryBase.java
processing/src/main/java/org/apache/carbondata/processing/loading/model/CarbonLoadModelBuilder.java
processing/src/main/java/org/apache/carbondata/processing/loading/model/LoadOption.java
core/src/main/java/org/apache/carbondata/core/metadata/converter/ThriftWrapperSchemaConverterImpl.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/TableSchema.java
texts:Create Lucene DataMap with &#39;text_columns&#39; property
create Lucene DataMap with 'text_columns' property and build Lucene DataMap for all exists segmentscreate datamap <datamapName> on <tableName>  using 'lucene'  dmproperties('text_columns'='col1,col2')
issueID:CARBONDATA-2173
type:Sub-task
changed files:
texts:Build Lucene DataMap for the segment
load data should build Lucene DataMap for the segment 
issueID:CARBONDATA-2174
type:Bug
changed files:
texts:Query with Lucene DataMap while filters contain match UDF
Query with Lucene DataMap while filters contain match UDF 
issueID:CARBONDATA-2175
type:Sub-task
changed files:
texts:Query with Lucene DataMap while filters contain match UDF
query with Lucene DataMap while filters contain match UDF  Limitation:1. only support one match in the where condition at first 
issueID:CARBONDATA-218
type:Improvement
changed files:
texts:Remove Dependency: spark-csv and Unify CSV Reader for dataloading

issueID:CARBONDATA-2182
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/util/CarbonSessionInfo.java
core/src/main/java/org/apache/carbondata/core/util/SessionParams.java
texts:add one more param called ExtraParmas in SessionParams for session Level operations
add one more param called ExtraParmas in SessionParams for session Level operations
issueID:CARBONDATA-2183
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/datastore/impl/FileFactory.java
core/src/main/java/org/apache/carbondata/core/datastore/filesystem/LocalCarbonFile.java
core/src/main/java/org/apache/carbondata/core/datastore/filesystem/AbstractDFSCarbonFile.java
core/src/main/java/org/apache/carbondata/core/datastore/filesystem/CarbonFile.java
texts:fix compaction when segment is delete during compaction and remove unnecessary parameters in functions
when compaction is started and job is running, and parallelly the segment involved in the compaction is deleted using DeleteSegmentByID, then compaction should be aborted and failed.
issueID:CARBONDATA-2184
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/memory/HeapMemoryAllocator.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
core/src/main/java/org/apache/carbondata/core/util/CarbonProperties.java
texts:Improve memory reuse for heap memory in `HeapMemoryAllocator`
The description in SPARK-21860:In `HeapMemoryAllocator`, when allocating memory from pool, and the key of pool is memory size.Actually some size of memory ,such as 1025bytes,1026bytes,......1032bytes, we can think they are the same，because we allocate memory in multiples of 8 bytes.In this case, we can improve memory reuse.
issueID:CARBONDATA-2185
type:Bug
changed files:streaming/src/main/java/org/apache/carbondata/streaming/CarbonStreamRecordReader.java
texts:add InputMetrics for Streaming Reader
Run Select Query in Streaming Table . Result::- Record count in Inputmetrics is always 0. 
issueID:CARBONDATA-2187
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockletDataMapFactory.java
core/src/main/java/org/apache/carbondata/core/indexstore/PartitionSpec.java
core/src/main/java/org/apache/carbondata/core/util/DataTypeUtil.java
core/src/main/java/org/apache/carbondata/core/indexstore/BlockletDataMapIndexStore.java
core/src/main/java/org/apache/carbondata/core/indexstore/TableBlockIndexUniqueIdentifier.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockletDataMapDistributable.java
core/src/main/java/org/apache/carbondata/core/datastore/filesystem/AbstractDFSCarbonFile.java
core/src/main/java/org/apache/carbondata/core/datamap/DataMapDistributable.java
core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
core/src/main/java/org/apache/carbondata/core/util/DataTypeConverterImpl.java
core/src/main/java/org/apache/carbondata/core/util/path/CarbonTablePath.java
core/src/main/java/org/apache/carbondata/core/metadata/SegmentFileStore.java
core/src/main/java/org/apache/carbondata/core/indexstore/BlockletDetailsFetcher.java
core/src/main/java/org/apache/carbondata/core/statusmanager/SegmentStatusManager.java
processing/src/main/java/org/apache/carbondata/processing/loading/DataLoadProcessBuilder.java
processing/src/main/java/org/apache/carbondata/processing/loading/CarbonDataLoadConfiguration.java
processing/src/main/java/org/apache/carbondata/processing/merger/CompactionResultSortProcessor.java
integration/spark-datasource/src/main/scala/org/apache/carbondata/converter/SparkDataTypeConverterImpl.java
core/src/main/java/org/apache/carbondata/core/statusmanager/LoadMetadataDetails.java
processing/src/main/java/org/apache/carbondata/processing/loading/model/CarbonLoadModel.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockletDataMapModel.java
core/src/main/java/org/apache/carbondata/core/locks/HdfsFileLock.java
processing/src/main/java/org/apache/carbondata/processing/merger/RowResultMergerProcessor.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/SegmentIndexFileStore.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockletDataMap.java
core/src/main/java/org/apache/carbondata/core/mutate/CarbonUpdateUtil.java
core/src/main/java/org/apache/carbondata/core/scan/executor/impl/AbstractQueryExecutor.java
processing/src/main/java/org/apache/carbondata/processing/util/CarbonLoaderUtil.java
core/src/main/java/org/apache/carbondata/core/datamap/Segment.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonTableOutputFormat.java
core/src/main/java/org/apache/carbondata/core/datamap/DistributableDataMapFormat.java
processing/src/main/java/org/apache/carbondata/processing/datamap/DataMapWriterListener.java
core/src/main/java/org/apache/carbondata/core/util/DeleteLoadFolders.java
processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactDataHandlerModel.java
processing/src/main/java/org/apache/carbondata/processing/merger/CarbonDataMergerUtil.java
processing/src/main/java/org/apache/carbondata/processing/util/CarbonDataProcessorUtil.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonTableInputFormat.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonOutputCommitter.java
core/src/main/java/org/apache/carbondata/core/datamap/dev/DataMap.java
core/src/main/java/org/apache/carbondata/hadoop/CarbonInputSplit.java
core/src/main/java/org/apache/carbondata/core/datamap/dev/DataMapFactory.java
core/src/main/java/org/apache/carbondata/core/datamap/TableDataMap.java
processing/src/main/java/org/apache/carbondata/processing/loading/converter/impl/MeasureFieldConverterImpl.java
core/src/main/java/org/apache/carbondata/core/locks/LocalFileLock.java
texts:Restructure the partition folders as per the standard hive folders

issueID:CARBONDATA-2189
type:Sub-task
changed files:common/src/main/java/org/apache/carbondata/common/exceptions/sql/MalformedDataMapCommandException.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockletDataMapModel.java
core/src/main/java/org/apache/carbondata/core/datamap/dev/fgdatamap/FineGrainDataMapFactory.java
core/src/main/java/org/apache/carbondata/core/datamap/DataMapStoreManager.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockletDataMapFactory.java
integration/spark2/src/main/java/org/apache/carbondata/datamap/DataMapManager.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockletDataMap.java
datamap/examples/src/minmaxdatamap/main/java/org/apache/carbondata/datamap/examples/MinMaxIndexDataMapFactory.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/CarbonTable.java
core/src/main/java/org/apache/carbondata/core/indexstore/BlockletDataMapIndexStore.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/DataMapSchemaFactory.java
core/src/main/java/org/apache/carbondata/core/datamap/dev/fgdatamap/FineGrainDataMap.java
core/src/main/java/org/apache/carbondata/core/datamap/dev/expr/DataMapExprWrapperImpl.java
core/src/main/java/org/apache/carbondata/core/datamap/DataMapProvider.java
integration/spark2/src/main/java/org/apache/carbondata/datamap/IndexDataMapProvider.java
processing/src/main/java/org/apache/carbondata/processing/datamap/DataMapWriterListener.java
core/src/main/java/org/apache/carbondata/core/datamap/dev/cgdatamap/CoarseGrainDataMap.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonTableInputFormat.java
core/src/main/java/org/apache/carbondata/core/datamap/dev/cgdatamap/CoarseGrainDataMapFactory.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/datamap/DataMapProperty.java
core/src/main/java/org/apache/carbondata/core/datamap/dev/DataMap.java
core/src/main/java/org/apache/carbondata/core/datamap/dev/DataMapFactory.java
core/src/main/java/org/apache/carbondata/core/datamap/TableDataMap.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/DataMapSchema.java
datamap/examples/src/minmaxdatamap/main/java/org/apache/carbondata/datamap/examples/MinMaxIndexDataMap.java
core/src/main/java/org/apache/carbondata/core/datamap/DataMapChooser.java
core/src/main/java/org/apache/carbondata/core/datamap/DataMapRegistry.java
core/src/main/java/org/apache/carbondata/core/datamap/dev/DataMapWriter.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/TableSchema.java
texts:Support add and drop interface

issueID:CARBONDATA-219
type:Bug
changed files:
texts:compaction with out data load is failing.

issueID:CARBONDATA-2194
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/util/SessionParams.java
texts:Exception message is improper when use incorrect bad record action type
Exception message is improper when use incorrect bad record action type
issueID:CARBONDATA-2196
type:Improvement
changed files:processing/src/main/java/org/apache/carbondata/processing/util/CarbonDataProcessorUtil.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonTableInputFormat.java
core/src/main/java/org/apache/carbondata/core/datamap/TableDataMap.java
processing/src/main/java/org/apache/carbondata/processing/merger/CompactionResultSortProcessor.java
texts:during stream sometime carbontable is null in executor side

issueID:CARBONDATA-2198
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/datastore/row/CarbonRow.java
streaming/src/main/java/org/apache/carbondata/streaming/StreamBlockletWriter.java
streaming/src/main/java/org/apache/carbondata/streaming/CarbonStreamRecordWriter.java
texts:Streaming data to a table with bad_records_action as IGNORE throws ClassCastException
Steps to reproduce:/* Licensed to the Apache Software Foundation (ASF) under one or more contributor license agreements. See the NOTICE file distributed with this work for additional information regarding copyright ownership. The ASF licenses this file to You under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with the License. You may obtain a copy of the License at * http://www.apache.org/licenses/LICENSE-2.0 * Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. */package org.apache.carbondata.examplesimport java.io.{File, PrintWriter} import java.net.ServerSocketimport org.apache.spark.sql.{CarbonEnv, SparkSession} import org.apache.spark.sql.streaming.{ProcessingTime, StreamingQuery}import org.apache.carbondata.core.constants.CarbonCommonConstants import org.apache.carbondata.core.util.CarbonProperties import org.apache.carbondata.core.util.path.{CarbonStorePath, CarbonTablePath}// scalastyle:off println object CarbonStructuredStreamingExample { def main(args: Array&#91;String&#93;) {// setup paths val rootPath = new File(this.getClass.getResource("/").getPath + "../../../..").getCanonicalPath val storeLocation = s"$rootPath/examples/spark2/target/store" val warehouse = s"$rootPath/examples/spark2/target/warehouse" val metastoredb = s"$rootPath/examples/spark2/target" val streamTableName = s"stream_table"CarbonProperties.getInstance() .addProperty(CarbonCommonConstants.CARBON_TIMESTAMP_FORMAT, "yyyy/MM/dd")import org.apache.spark.sql.CarbonSession._ val spark = SparkSession .builder() .master("local") .appName("CarbonStructuredStreamingExample") .config("spark.sql.warehouse.dir", warehouse) .getOrCreateCarbonSession(storeLocation, metastoredb)spark.sparkContext.setLogLevel("ERROR")val requireCreateTable = true val useComplexDataType = falseif (requireCreateTable) { // drop table if exists previously spark.sql(s"DROP TABLE IF EXISTS ${ streamTableName }") // Create target carbon table and populate with initial data if (useComplexDataType) { spark.sql( s"""CREATE TABLE ${ streamTableName }(id INT,name STRING,city STRING,salary FLOAT,file struct<school:array<string>, age:int>)STORED BY 'carbondata'TBLPROPERTIES('streaming'='true', 'sort_columns'='name', 'dictionary_include'='city')""".stripMargin) } else { spark.sql( s"""CREATE TABLE ${ streamTableName }(id INT,name STRING,city STRING,salary FLOAT)STORED BY 'carbondata'TBLPROPERTIES('streaming'='true', 'sort_columns'='name')""".stripMargin) }val carbonTable = CarbonEnv.getCarbonTable(Some("default"), streamTableName)(spark) val tablePath = CarbonStorePath.getCarbonTablePath(carbonTable.getAbsoluteTableIdentifier)// streaming ingest val serverSocket = new ServerSocket(7071) val thread1 = startStreaming(spark, tablePath) val thread2 = writeSocket(serverSocket)System.out.println("type enter to interrupt streaming") System.in.read() thread1.interrupt() thread2.interrupt() serverSocket.close() }spark.sql(s"select * from $streamTableName").show spark.stop() System.out.println("streaming finished") }def showTableCount(spark: SparkSession, tableName: String): Thread = { val thread = new Thread() { override def run(): Unit = { for (_ <- 0 to 1000){ spark.sql(s"select count(*) from $tableName").show(truncate = false) Thread.sleep(1000 * 3) }} } thread.start() thread }def startStreaming(spark: SparkSession, tablePath: CarbonTablePath): Thread = { val thread = new Thread() { override def run(): Unit = { var qry: StreamingQuery = null try{ val readSocketDF = spark.readStream .format("socket") .option("host", "localhost") .option("port", 7071) .load() // Write data from socket stream to carbondata file qry = readSocketDF.writeStream .format("carbondata") .trigger(ProcessingTime("5 seconds")) .option("checkpointLocation", tablePath.getStreamingCheckpointDir) .option("bad_records_action", "ignore") .option("dbName", "default") .option("tableName", "stream_table") .start() qry.awaitTermination() }catch{ case ex: Exception => ex.printStackTrace() println("Done reading and writing streaming data") }finally{ qry.stop() }} } thread.start() thread }def writeSocket(serverSocket: ServerSocket): Thread = { val thread = new Thread() { override def run(): Unit = { // wait for client to connection request and accept val clientSocket = serverSocket.accept() val socketWriter = new PrintWriter(clientSocket.getOutputStream()) var index = 0 for (_ <- 1 to 1000) { // write 5 records per iteration for (_ <- 0 to 1000){ index = index + 1 socketWriter.println("null" + ",name_" + index + ",city_" + index + "," + (index * 10000.00).toString + ",school_" + index + ":school_" + index + index + "$" + index) }socketWriter.flush() Thread.sleep(1000) } socketWriter.close() System.out.println("Socket closed") } } thread.start() thread } } // scalastyle:on printlnIn the above example we are streaming data to table with bad_records_action as IGNORE, it throws ClassCastException. Here are the logs:18/02/23 16:09:50 ERROR StreamSegment: Executor task launch worker-0 Failed to append batch data to stream segment: /home/geetika/Workspace/incubator-carbondata/examples/spark2/target/store/default/stream_table/Fact/Part0/Segment_0 java.lang.ClassCastException: java.lang.String cannot be cast to java.lang.Double at org.apache.carbondata.hadoop.streaming.CarbonStreamRecordWriter.write(CarbonStreamRecordWriter.java:241) at org.apache.carbondata.streaming.segment.StreamSegment.appendBatchData(StreamSegment.java:244) at org.apache.spark.sql.execution.streaming.CarbonAppendableStreamSink$$anonfun$writeDataFileTask$1.apply$mcV$sp(CarbonAppendableStreamSink.scala:315) at org.apache.spark.sql.execution.streaming.CarbonAppendableStreamSink$$anonfun$writeDataFileTask$1.apply(CarbonAppendableStreamSink.scala:305) at org.apache.spark.sql.execution.streaming.CarbonAppendableStreamSink$$anonfun$writeDataFileTask$1.apply(CarbonAppendableStreamSink.scala:305) at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1341) at org.apache.spark.sql.execution.streaming.CarbonAppendableStreamSink$.writeDataFileTask(CarbonAppendableStreamSink.scala:317) at org.apache.spark.sql.execution.streaming.CarbonAppendableStreamSink$$anonfun$writeDataFileJob$1$$anonfun$apply$mcV$sp$1.apply(CarbonAppendableStreamSink.scala:228) at org.apache.spark.sql.execution.streaming.CarbonAppendableStreamSink$$anonfun$writeDataFileJob$1$$anonfun$apply$mcV$sp$1.apply(CarbonAppendableStreamSink.scala:227) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87) at org.apache.spark.scheduler.Task.run(Task.scala:99) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748) 18/02/23 16:09:50 ERROR Utils: Aborting task java.lang.ClassCastException: java.lang.String cannot be cast to java.lang.Double at org.apache.carbondata.hadoop.streaming.CarbonStreamRecordWriter.write(CarbonStreamRecordWriter.java:241) at org.apache.carbondata.streaming.segment.StreamSegment.appendBatchData(StreamSegment.java:244) at org.apache.spark.sql.execution.streaming.CarbonAppendableStreamSink$$anonfun$writeDataFileTask$1.apply$mcV$sp(CarbonAppendableStreamSink.scala:315) at org.apache.spark.sql.execution.streaming.CarbonAppendableStreamSink$$anonfun$writeDataFileTask$1.apply(CarbonAppendableStreamSink.scala:305) at org.apache.spark.sql.execution.streaming.CarbonAppendableStreamSink$$anonfun$writeDataFileTask$1.apply(CarbonAppendableStreamSink.scala:305) at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1341) at org.apache.spark.sql.execution.streaming.CarbonAppendableStreamSink$.writeDataFileTask(CarbonAppendableStreamSink.scala:317) at org.apache.spark.sql.execution.streaming.CarbonAppendableStreamSink$$anonfun$writeDataFileJob$1$$anonfun$apply$mcV$sp$1.apply(CarbonAppendableStreamSink.scala:228) at org.apache.spark.sql.execution.streaming.CarbonAppendableStreamSink$$anonfun$writeDataFileJob$1$$anonfun$apply$mcV$sp$1.apply(CarbonAppendableStreamSink.scala:227) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87) at org.apache.spark.scheduler.Task.run(Task.scala:99) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748) 18/02/23 16:09:50 ERROR CarbonAppendableStreamSink$: Executor task launch worker-0 Job job_20180223160950_0000 aborted. 18/02/23 16:09:50 ERROR Executor: Exception in task 0.0 in stage 0.0 (TID 0) org.apache.carbondata.streaming.CarbonStreamException: Task failed while writing rows at org.apache.spark.sql.execution.streaming.CarbonAppendableStreamSink$.writeDataFileTask(CarbonAppendableStreamSink.scala:324) at org.apache.spark.sql.execution.streaming.CarbonAppendableStreamSink$$anonfun$writeDataFileJob$1$$anonfun$apply$mcV$sp$1.apply(CarbonAppendableStreamSink.scala:228) at org.apache.spark.sql.execution.streaming.CarbonAppendableStreamSink$$anonfun$writeDataFileJob$1$$anonfun$apply$mcV$sp$1.apply(CarbonAppendableStreamSink.scala:227) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87) at org.apache.spark.scheduler.Task.run(Task.scala:99) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748) Caused by: java.lang.ClassCastException: java.lang.String cannot be cast to java.lang.Double at org.apache.carbondata.hadoop.streaming.CarbonStreamRecordWriter.write(CarbonStreamRecordWriter.java:241) at org.apache.carbondata.streaming.segment.StreamSegment.appendBatchData(StreamSegment.java:244) at org.apache.spark.sql.execution.streaming.CarbonAppendableStreamSink$$anonfun$writeDataFileTask$1.apply$mcV$sp(CarbonAppendableStreamSink.scala:315) at org.apache.spark.sql.execution.streaming.CarbonAppendableStreamSink$$anonfun$writeDataFileTask$1.apply(CarbonAppendableStreamSink.scala:305) at org.apache.spark.sql.execution.streaming.CarbonAppendableStreamSink$$anonfun$writeDataFileTask$1.apply(CarbonAppendableStreamSink.scala:305) at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1341) at org.apache.spark.sql.execution.streaming.CarbonAppendableStreamSink$.writeDataFileTask(CarbonAppendableStreamSink.scala:317) ... 8 more
issueID:CARBONDATA-2199
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/scan/executor/util/RestructureUtil.java
core/src/main/java/org/apache/carbondata/core/scan/collector/impl/DictionaryBasedVectorResultCollector.java
core/src/main/java/org/apache/carbondata/core/scan/executor/impl/AbstractQueryExecutor.java
core/src/main/java/org/apache/carbondata/core/scan/executor/infos/DimensionInfo.java
texts:Exception occurs when change the datatype of measure having sort_column
Use a measure columns in sort_column and change the datatype of that columnsSteps to replicateCREATE TABLE non_partitiontable7(id Int,vin String,phonenumber Long,area String,salary Int, country String,logdate date)STORED BY 'org.apache.carbondata.format'TBLPROPERTIES('SORT_COLUMNS'='id,vin','sort_scope'='global_sort');insert into non_partitiontable7 select 1,'A42151477823',125371344,'OutSpace',10000,'China','2017-02-12';insert into non_partitiontable7 select 1,'Y42151477823',125371344,'midasia',10000,'China','2017-02-13';insert into non_partitiontable7 select 1,'B42151477823',125371346,'OutSpace',10000,'US','2018-02-12';insert into non_partitiontable7 select 1,'C42151477823',125371348,'InnerSpace',10001,'UK','2019-02-12';select * from non_partitiontable7;alter table non_partitiontable7 add columns (c1 int);select * from non_partitiontable7;alter table non_partitiontable7 change id id bigint;select * from non_partitiontable7;Exception StackTraceError: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 16.0 failed 4 times, most recent failure: Lost task 1.3 in stage 16.0 (TID 80, BLR1000023654, executor 3): java.lang.IllegalArgumentException: Wrong length: 4, expected 8 at org.apache.carbondata.core.util.ByteUtil.explainWrongLengthOrOffset(ByteUtil.java:581) at org.apache.carbondata.core.util.ByteUtil.toLong(ByteUtil.java:553) at org.apache.carbondata.core.util.DataTypeUtil.getDataBasedOnRestructuredDataType(DataTypeUtil.java:847) at org.apache.carbondata.core.datastore.chunk.store.impl.unsafe.UnsafeVariableLengthDimesionDataChunkStore.fillRow(UnsafeVariableLengthDimesionDataChunkStore.java:181) at org.apache.carbondata.core.datastore.chunk.impl.VariableLengthDimensionDataChunk.fillConvertedChunkData(VariableLengthDimensionDataChunk.java:112) at org.apache.carbondata.core.scan.result.AbstractScannedResult.fillColumnarNoDictionaryBatch(AbstractScannedResult.java:256) at org.apache.carbondata.core.scan.collector.impl.DictionaryBasedVectorResultCollector.scanAndFillResult(DictionaryBasedVectorResultCollector.java:163) at org.apache.carbondata.core.scan.collector.impl.RestructureBasedVectorResultCollector.collectVectorBatch(RestructureBasedVectorResultCollector.java:128) at org.apache.carbondata.core.scan.processor.impl.DataBlockIteratorImpl.processNextBatch(DataBlockIteratorImpl.java:65) at org.apache.carbondata.core.scan.result.iterator.VectorDetailQueryResultIterator.processNextBatch(VectorDetailQueryResultIterator.java:46) at org.apache.carbondata.spark.vectorreader.VectorizedCarbonRecordReader.nextBatch(VectorizedCarbonRecordReader.java:283) at org.apache.carbondata.spark.vectorreader.VectorizedCarbonRecordReader.nextKeyValue(VectorizedCarbonRecordReader.java:171) at org.apache.carbondata.spark.rdd.CarbonScanRDD$$anon$1.hasNext(CarbonScanRDD.scala:402) at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.scan_nextBatch$(Unknown Source) at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source) at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43) at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:395) at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:234) at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:228) at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827) at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) at org.apache.spark.rdd.RDD.iterator(RDD.scala:287) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87) at org.apache.spark.scheduler.Task.run(Task.scala:108) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745) 
issueID:CARBONDATA-220
type:Bug
changed files:
texts:TimeStampDirectDictionaryGenerator_UT.java is not running in the build
TimeStampDirectDictionaryGenerator_UT.java is not running in the build.And generateDirectSurrogateKey test is failing.
issueID:CARBONDATA-2200
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelFilterExecuterImpl.java
texts:Like operation on streaming table throwing Exception
In TestStreamingTableOperation.scala file,for test case "query on stream table with dictionary, sort_columns"the following SQL is not working:select * from stream_table_filter where name like '%me%'error log:java.io.IOException: Failed to filter row in vector reader at org.apache.carbondata.hadoop.streaming.CarbonStreamRecordReader.scanBlockletAndFillVector(CarbonStreamRecordReader.java:449) at org.apache.carbondata.hadoop.streaming.CarbonStreamRecordReader.nextColumnarBatch(CarbonStreamRecordReader.java:328) at org.apache.carbondata.hadoop.streaming.CarbonStreamRecordReader.nextKeyValue(CarbonStreamRecordReader.java:309) at org.apache.carbondata.spark.rdd.CarbonScanRDD$$anon$1.hasNext(CarbonScanRDD.scala:390) at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.scan_nextBatch$(Unknown Source) at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source) at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43) at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377) at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:231) at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:225) at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:826) at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:826) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) at org.apache.spark.rdd.RDD.iterator(RDD.scala:287) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87) at org.apache.spark.scheduler.Task.run(Task.scala:99) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745)Caused by: org.apache.carbondata.core.scan.expression.exception.FilterUnsupportedException: [B cannot be cast to org.apache.spark.unsafe.types.UTF8String at org.apache.spark.sql.SparkUnknownExpression.evaluate(SparkUnknownExpression.scala:50) at org.apache.carbondata.core.scan.filter.executer.RowLevelFilterExecuterImpl.applyFilter(RowLevelFilterExecuterImpl.java:279) at org.apache.carbondata.hadoop.streaming.CarbonStreamRecordReader.scanBlockletAndFillVector(CarbonStreamRecordReader.java:444) ... 20 more 
issueID:CARBONDATA-2201
type:Improvement
changed files:
texts:firing the LoadTablePreExecutionEvent before streaming causes NPE

issueID:CARBONDATA-2202
type:Improvement
changed files:
texts:Introduce local dictionary encoding for dimensions
Currently Carbondata will generate global dictionary for columns with 'dictionary_include' attribute.A dimension column without that attribute will only be stored after some simple compression. These columns can also be dictionary encoded in file level (called ‘local dictionary’) to reduce data size.
issueID:CARBONDATA-2203
type:Improvement
changed files:
texts:record detailed metrics information during carbon processing
We need to record more detailed information during carbondata processing (query & load).These information can be used: for figuring out the possible performance problems in carbondata for tuning as input for some adaptive strategyHere are some examples:For data query: Time costs for sql parse/optimize/plan Time to load&read metadata Time to schedule Time to execute ...For data loading: Time to schedule Time to execute Disk spill amount ...
issueID:CARBONDATA-2204
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/statusmanager/SegmentStatusManager.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonTableInputFormat.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
core/src/main/java/org/apache/carbondata/core/util/path/CarbonTablePath.java
texts:Access tablestatus file too many times during query
ProblemsCurrently in carbondata, a single query will access tablestatus file 7 times, which will definitely slow down the query performance especially when this file is in remote cluster since reading this file is purely client side operation.   Steps to reproduce1. Add logger in `AtomicFileOperationsImpl.openForRead` and printout the file name to read.2. Run a query on carbondata table. Here I ran `TestLoadDataGeneral.test("test data loading CSV file without extension name")`.3. Observe the output log and search the keyword 'tablestatus'.   
issueID:CARBONDATA-2206
type:Sub-task
changed files:core/src/main/java/org/apache/carbondata/core/scan/expression/MatchExpression.java
core/src/main/java/org/apache/carbondata/core/datamap/DataMapStoreManager.java
processing/src/main/java/org/apache/carbondata/processing/store/writer/AbstractFactDataWriter.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/scan/filter/intf/ExpressionType.java
integration/spark2/src/main/java/org/apache/carbondata/datamap/IndexDataMapProvider.java
processing/src/main/java/org/apache/carbondata/processing/datamap/DataMapWriterListener.java
processing/src/main/java/org/apache/carbondata/processing/store/writer/v3/CarbonFactDataWriterImplV3.java
core/src/main/java/org/apache/carbondata/core/datamap/dev/cgdatamap/CoarseGrainDataMapFactory.java
datamap/lucene/src/main/java/org/apache/carbondata/datamap/lucene/LuceneFineGrainDataMap.java
datamap/lucene/src/main/java/org/apache/carbondata/datamap/lucene/LuceneFineGrainDataMapFactory.java
core/src/main/java/org/apache/carbondata/core/datamap/dev/DataMap.java
core/src/main/java/org/apache/carbondata/core/datamap/dev/DataMapFactory.java
datamap/lucene/src/main/java/org/apache/carbondata/datamap/lucene/LuceneDataMapDistributable.java
datamap/lucene/src/main/java/org/apache/carbondata/datamap/lucene/LuceneDataMapFactoryBase.java
core/src/main/java/org/apache/carbondata/core/datamap/DataMapChooser.java
core/src/main/java/org/apache/carbondata/core/datamap/dev/DataMapWriter.java
datamap/lucene/src/main/java/org/apache/carbondata/datamap/lucene/LuceneDataMapWriter.java
core/src/main/java/org/apache/carbondata/core/datamap/DataMapProvider.java
texts:Integrate lucene as datamap
Implement using lucene as one DataMap and adapt to DataMapFactory
issueID:CARBONDATA-2207
type:Bug
changed files:
texts:TestCase Fails using Hive Metastore
Run All the Cabon TestCases using hive metastore out of which some test cases were failing because of not getting carbon table.
issueID:CARBONDATA-2208
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/util/path/CarbonTablePath.java
texts:Pre aggregate datamap creation is failing when count(*) present in query
Pre aggregate data map creation is failing with parsing error create datamap agg on table maintable using 'preaggregate' as select name, count from maintable group by name  
issueID:CARBONDATA-2209
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/util/path/CarbonTablePath.java
core/src/main/java/org/apache/carbondata/core/metadata/SegmentFileStore.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/SegmentIndexFileStore.java
core/src/main/java/org/apache/carbondata/core/writer/CarbonIndexFileMergeWriter.java
processing/src/main/java/org/apache/carbondata/processing/loading/DataLoadProcessBuilder.java
processing/src/main/java/org/apache/carbondata/processing/util/CarbonLoaderUtil.java
core/src/main/java/org/apache/carbondata/core/datamap/Segment.java
texts:Rename table with partitions not working issue and batch_sort and no_sort with partition table issue
After table rename on partitions table, it returns empty data upon querying. Batch sort and no sort loading is not working on partition table
issueID:CARBONDATA-2211
type:Bug
changed files:
texts:Alter Table Streaming DDL should blocking DDL like other DDL ( All DDL are blocking DDL)
DDL displayed output immediately.  But compaction took more time.it should be Blocking so that if any issue comes in the Alter (compaction ) result of DDL can be error (exception) so that use can check ERROR why  compaction is failed. 
issueID:CARBONDATA-2212
type:Bug
changed files:
texts:Event should be fired from Stream before and after updating the status

issueID:CARBONDATA-2213
type:Bug
changed files:datamap/lucene/src/main/java/org/apache/carbondata/datamap/lucene/LuceneFineGrainDataMap.java
datamap/examples/src/minmaxdatamap/main/java/org/apache/carbondata/datamap/examples/MinMaxIndexDataMapFactory.java
datamap/lucene/src/main/java/org/apache/carbondata/datamap/lucene/LuceneFineGrainDataMapFactory.java
datamap/lucene/src/main/java/org/apache/carbondata/datamap/lucene/LuceneDataMapFactoryBase.java
datamap/lucene/src/main/java/org/apache/carbondata/datamap/lucene/LuceneDataMapWriter.java
texts:Wrong version in datamap example module cause compilation failure
The version of Module ‘carbondata-datamap-example’ should be 1.4.0-snapshot instead of 1.3.0-snapshot, otherwise compilation will failed.
issueID:CARBONDATA-2214
type:Task
changed files:
texts:Remove config &#39;spark.sql.hive.thriftServer.singleSession&#39; from installation-guide.md
Remove config 'spark.sql.hive.thriftServer.singleSession' from installation-guide.md
issueID:CARBONDATA-2215
type:Task
changed files:
texts:Add the description of Carbon Stream Parser into streaming-guide.md
Add the description of Carbon Stream Parser into streaming-guide.md
issueID:CARBONDATA-2216
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/datamap/dev/expr/AndDataMapExprWrapper.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonTableInputFormat.java
core/src/main/java/org/apache/carbondata/core/datamap/dev/expr/DataMapExprWrapper.java
core/src/main/java/org/apache/carbondata/core/datamap/dev/expr/OrDataMapExprWrapper.java
core/src/main/java/org/apache/carbondata/core/datamap/dev/expr/DataMapExprWrapperImpl.java
texts:Error in compilation and execution in sdvtest
Missing imports in code files results in compilation error Error in tests results in test execution error
issueID:CARBONDATA-2217
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/metadata/SegmentFileStore.java
texts:nullpointer issue  drop partition where column does not exists and clean files issue after second level of compaction
1)when drop partition is fired for a column which does not exists , it throws null pointer exception2)select * is not working when clean files operation is fired after second level of compactioncreate table comp_dt2(id int,name string) partitioned by (dt date,c4 int) stored by 'carbondata';insert into comp_dt2 select 1,'A','2001-01-01',1;insert into comp_dt2 select 2,'B','2001-01-01',1;insert into comp_dt2 select 3,'C','2002-01-01',2;insert into comp_dt2 select 4,'D','2002-01-01',null;insert into comp_dt2 select 5,'E','2003-01-01',3;insert into comp_dt2 select 6,'F','2003-01-01',3;insert into comp_dt2 select 7,'G','2003-01-01',4;insert into comp_dt2 select 8,'H','2004-01-01','';insert into comp_dt2 select 9,'H','2001-01-01',1;insert into comp_dt2 select 10,'I','2002-01-01',null;insert into comp_dt2 select 11,'J','2003-01-01',4;insert into comp_dt2 select 12,'K','2003-01-01',5; clean files for table comp_dt2;select * from comp_dt2  
issueID:CARBONDATA-2219
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/metadata/SegmentFileStore.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/SegmentIndexFileStore.java
texts:Add validation for external partition location to use same schema

issueID:CARBONDATA-222
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/scan/expression/logical/FalseExpression.java
core/src/main/java/org/apache/carbondata/core/scan/filter/FilterExpressionProcessor.java
core/src/main/java/org/apache/carbondata/core/scan/filter/intf/ExpressionType.java
texts:Query issue for all dimensions are no dictionary columns
step 1:CREATE TABLE uniqdata_no (CUST_ID int,CUST_NAME String,ACTIVE_EMUI_VERSION string, DOB timestamp, DOJ timestamp, BIGINT_COLUMN1 bigint,BIGINT_COLUMN2 bigint,DECIMAL_COLUMN1 decimal(30,10), DECIMAL_COLUMN2 decimal(36,10),Double_COLUMN1 double, Double_COLUMN2 double,INTEGER_COLUMN1 int) STORED BY 'org.apache.carbondata.format' TBLPROPERTIES('DICTIONARY_EXCLUDE'='CUST_NAME,ACTIVE_EMUI_VERSION');step 2:LOAD DATA INPATH 'D:/download/3lakh_3.csv' into table uniqdata_no OPTIONS('DELIMITER'=',' , 'QUOTECHAR'='"','FILEHEADER'='CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1,Double_COLUMN2,INTEGER_COLUMN1');step 3:select * from uniqdata_no limit 5;the fact file is:,,,,,,,,,,,0query failed, catch exception:Caused by: java.lang.ArrayIndexOutOfBoundsException: 4 at org.apache.carbondata.core.util.ByteUtil$UnsafeComparer.compareTo(ByteUtil.java:197) at org.apache.carbondata.core.carbon.datastore.impl.btree.BTreeDataRefNodeFinder.compareIndexes(BTreeDataRefNodeFinder.java:243) at org.apache.carbondata.core.carbon.datastore.impl.btree.BTreeDataRefNodeFinder.findFirstLeafNode(BTreeDataRefNodeFinder.java:121) at org.apache.carbondata.core.carbon.datastore.impl.btree.BTreeDataRefNodeFinder.findFirstDataBlock(BTreeDataRefNodeFinder.java:80) at org.apache.carbondata.hadoop.CarbonInputFormat.getDataBlocksOfIndex(CarbonInputFormat.java:546) at org.apache.carbondata.hadoop.CarbonInputFormat.getDataBlocksOfSegment(CarbonInputFormat.java:473) at org.apache.carbondata.hadoop.CarbonInputFormat.getSplits(CarbonInputFormat.java:342) at org.apache.carbondata.hadoop.CarbonInputFormat.getSplitsNonFilter(CarbonInputFormat.java:304) at org.apache.carbondata.hadoop.CarbonInputFormat.getSplits(CarbonInputFormat.java:277)
issueID:CARBONDATA-2220
type:Improvement
changed files:processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/holder/UnsafeInmemoryMergeHolder.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/holder/UnsafeInmemoryHolder.java
processing/src/main/java/org/apache/carbondata/processing/util/CarbonDataProcessorUtil.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/holder/UnsafeSortTempFileChunkHolder.java
core/src/main/java/org/apache/carbondata/core/util/CarbonLoadStatisticsImpl.java
core/src/main/java/org/apache/carbondata/core/util/CarbonProperties.java
core/src/main/java/org/apache/carbondata/core/scan/filter/FilterUtil.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/holder/UnsafeFinalMergePageHolder.java
texts:Reduce unnecessary audit log

issueID:CARBONDATA-2221
type:Bug
changed files:
texts:Drop table should throw exception when metastore operation failed

issueID:CARBONDATA-2222
type:Bug
changed files:
texts:Update the FAQ doc for some mistakes

issueID:CARBONDATA-2223
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/metadata/SegmentFileStore.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonOutputCommitter.java
core/src/main/java/org/apache/carbondata/core/indexstore/BlockletDataMapIndexStore.java
processing/src/main/java/org/apache/carbondata/processing/loading/events/LoadEvents.java
processing/src/main/java/org/apache/carbondata/processing/util/CarbonLoaderUtil.java
core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
texts:Adding Listener Support for Partition
Remove unused listeners
issueID:CARBONDATA-2226
type:Improvement
changed files:
texts:Refactor UT&#39;s to remove duplicate test scenarios to improve CI time for PreAggregate create and drop feature

issueID:CARBONDATA-2227
type:Improvement
changed files:
texts:Add Partition Values and Location information in describe formatted for Standard partition feature

issueID:CARBONDATA-2229
type:Bug
changed files:
texts:Unable to save dataframe as carbontable with specified external database path
dataFrame.write .format("carbondata") .option("tableName", "carbon4") .option("compress", "true") .option("dataBasePath", "/tmp") .mode(SaveMode.Overwrite) .save()sql("show tables").show(false) but database is getting created at default database ------------------------databasetableNameisTemporary------------------------default carbon4 false ------------------------
issueID:CARBONDATA-223
type:Improvement
changed files:
texts:Remove unused code from Carbon Parser
Remove unused code from Carbon Parser
issueID:CARBONDATA-2230
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/datastore/impl/FileFactory.java
core/src/main/java/org/apache/carbondata/core/locks/HdfsFileLock.java
core/src/main/java/org/apache/carbondata/core/util/path/CarbonTablePath.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
core/src/main/java/org/apache/carbondata/core/locks/S3FileLock.java
core/src/main/java/org/apache/carbondata/core/statusmanager/SegmentStatusManager.java
core/src/main/java/org/apache/carbondata/core/util/CarbonProperties.java
core/src/main/java/org/apache/carbondata/core/locks/CarbonLockUtil.java
core/src/main/java/org/apache/carbondata/core/locks/ZooKeeperLocking.java
core/src/main/java/org/apache/carbondata/core/locks/LocalFileLock.java
texts:Add a path into table path to store lock files and delete useless segment lock files before loading
After PR1984 merged, it doesn't delete the lock files when unlock, there are many useless lock files in table path, especially segment lock files, they grow after every batch loading.Solution :1. add a child path into table path, called Locks, all lock files will be stored in this path;2. Before loading, get all useless segment lock files and delete them, because just segment lock files will grow, other lock files dosen't grow.
issueID:CARBONDATA-2231
type:Improvement
changed files:
texts:Refactor FT&#39;s to remove duplicate test scenarios to improve CI time for Streaming feature

issueID:CARBONDATA-2232
type:Bug
changed files:processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/UnsafeSortDataRows.java
texts:Wrong logic in spilling unsafe pages to disk
The logic to spill unsafe carbon row page to disk is incorrect, it just reversed.
issueID:CARBONDATA-2234
type:Improvement
changed files:processing/src/main/java/org/apache/carbondata/processing/loading/csvinput/CSVInputFormat.java
texts:Support UTF-8 with BOM encoding in CSVInputFormat
If CSV file is encoded on UTF-8 with BOM , first row first column data is not read correctly during data load.If First column is integer & header is false, first row of CSV will be considered as bad record. 
issueID:CARBONDATA-2235
type:Bug
changed files:
texts:add system configuration to filter datamaps from show tables command
carbon.query.show.datamapsthis property is a system configuration, if it is set to true, show tables will list all the tables including datamaps(ex: Preaggregate) and if it is false, show tables will filter the datamaps and only will show the main tables
issueID:CARBONDATA-2236
type:Bug
changed files:
texts:Add SDV Test Cases for Standard Partition
Add SDV Test Cases for Standard Partition
issueID:CARBONDATA-2237
type:Bug
changed files:
texts:Scala Parser failures are accumulated into memory form thread local

issueID:CARBONDATA-2238
type:Improvement
changed files:processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/merger/UnsafeIntermediateMerger.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonLoadOptionConstants.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/impl/UnsafeParallelReadMergeSorterWithColumnRangeImpl.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/merger/UnsafeInMemoryIntermediateDataMerger.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/impl/UnsafeParallelReadMergeSorterImpl.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/UnsafeSortDataRows.java
core/src/main/java/org/apache/carbondata/core/memory/UnsafeSortMemoryManager.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/merger/UnsafeSingleThreadFinalSortFilesMerger.java
texts:Optimization in unsafe sort during data loading
Inspired by batch_sort, if we have enough memory, in local_sort with unsafe property, we can hold all the row pages in memory if possible and only spill the pages to disk as sort temp file if the memory is unavailable.Before spilling the pages, we can do in-memory merge sort of the pages.Each time we request an unsafe row page, if the memory is unavailable, we can trigger a merge sort for the in-memory pages and spill the result to disk as a sort temp file. So the incoming pages will be held into the memory instead of spilling to disk directly.After this implementation, the data size during each spilling will be bigger than that of before and will benefit the disk IO.
issueID:CARBONDATA-224
type:Bug
changed files:
texts:Fixed data mismatch issue in case of Dictionary Exclude column for Numeric data type
Problem: In case of greater than query on dictionary exclude column of for numeric data typeThis is because data is sorted based on string because of this if data is 1,10,2,3 , data will be sorted like 1,10,2,3 but if we search greater than 3 then while applying min max will return false as last value is 3Solution:we need to sort based on actual data type for this we should have chain comparator based on data type while loading the data, currently disabling DictionaryExclude column for numeric data type and will throw exception. Will raise jira issue to for sorting the based on actual data type
issueID:CARBONDATA-2241
type:Bug
changed files:
texts:Wrong Query written in Preaggregation Document
Below query is written in document: SELECT sum(price), country from sales GROUP BY countryand it is said that it will execute on datamap, but it will execute with main table and not datamap. Fix: Correct the query so that it will execute using datamap.
issueID:CARBONDATA-2242
type:New Feature
changed files:
texts:Support materialized view
Support query optimization using materialized view
issueID:CARBONDATA-2244
type:Bug
changed files:
texts:When there are some invisibility INSERT_IN_PROGRESS/INSERT_OVERWRITE_IN_PROGRESS segments on main table, it can not create preaggregate table on it.
When there are some invisibility INSERT_IN_PROGRESS/INSERT_OVERWRITE_IN_PROGRESS segments on main table, it can not create preaggregate table on it.
issueID:CARBONDATA-2246
type:Bug
changed files:
texts:Fix not-enough-memory bugs in unsafe data loading
Currently in carbon data loading, if we enable the unsafe loading and specify corresponding properties, data loading will end in OOM.The key properties to reproduce the bug are as following:```01: CarbonProperties.getInstance().addProperty(CarbonCommonConstants.ENABLE_INMEMORY_MERGE_SORT, "true") CarbonProperties.getInstance().addProperty(CarbonCommonConstants.ENABLE_UNSAFE_SORT, "true") 02:  03: // unsafe sort memory manager 04: CarbonProperties.getInstance().addProperty(CarbonCommonConstants.IN_MEMORY_STORAGE_FOR_SORTED_DATA_IN_MB, "1024") 05:  06: // unsafe working memory manager 07: CarbonProperties.getInstance().addProperty(CarbonCommonConstants.UNSAFE_WORKING_MEMORY_IN_MB, "512") 08:  09: // one unsafe page, better if loading_cores * this < memory 10: CarbonProperties.getInstance().addProperty(CarbonCommonConstants.OFFHEAP_SORT_CHUNK_SIZE_IN_MB, "512")``` Notice that the `OFFHEAP_SORT_CHUNK_SIZE_IN_MB` are exactly the same as `UNSAFE_WORKING_MEMORY_IN_MB` which will cause problem
issueID:CARBONDATA-2247
type:Sub-task
changed files:processing/src/main/java/org/apache/carbondata/processing/loading/model/CarbonLoadModelBuilder.java
processing/src/main/java/org/apache/carbondata/processing/loading/model/LoadOption.java
store/sdk/src/main/java/org/apache/carbondata/sdk/file/CarbonWriterBuilder.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/TableSchemaBuilder.java
texts:Support writing index in CarbonWriter

issueID:CARBONDATA-2248
type:Bug
changed files:
texts:Removing parsers thread local objects after parsing of carbon query
In some scenarios where more sessions are created, there are many parser failure objects are accumulated in memory inside thread locals.Solution: Remove the parser object from thread local after parsing of the query
issueID:CARBONDATA-2249
type:Bug
changed files:
texts:Not able to query data through presto with local carbondata-store
I tried to query carbondata through presto, when I tried the following command: show tablesNote: The exception is thrown when my carbondata-store is on my local machineIt shows the following exception on the presto server:java.lang.NoClassDefFoundError: net/jpountz/lz4/LZ4BlockInputStreamat org.apache.carbondata.core.datastore.impl.DefaultFileTypeProvider.getCarbonFile(DefaultFileTypeProvider.java:48)at org.apache.carbondata.core.datastore.impl.FileFactory.getCarbonFile(FileFactory.java:89)at org.apache.carbondata.presto.impl.CarbonTableReader.updateCarbonFile(CarbonTableReader.java:167)at org.apache.carbondata.presto.impl.CarbonTableReader.updateSchemaList(CarbonTableReader.java:181)at org.apache.carbondata.presto.impl.CarbonTableReader.getSchemaNames(CarbonTableReader.java:154)at org.apache.carbondata.presto.CarbondataMetadata.listSchemaNamesInternal(CarbondataMetadata.java:58)at org.apache.carbondata.presto.CarbondataMetadata.listSchemaNames(CarbondataMetadata.java:54)at com.facebook.presto.spi.connector.ConnectorMetadata.schemaExists(ConnectorMetadata.java:64)at com.facebook.presto.spi.connector.classloader.ClassLoaderSafeConnectorMetadata.schemaExists(ClassLoaderSafeConnectorMetadata.java:104)at com.facebook.presto.metadata.MetadataManager.lambda$schemaExists$0(MetadataManager.java:254)at java.util.stream.MatchOps$1MatchSink.accept(MatchOps.java:90)at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)at java.util.Spliterators$ArraySpliterator.tryAdvance(Spliterators.java:958)at java.util.stream.ReferencePipeline.forEachWithCancel(ReferencePipeline.java:126)at java.util.stream.AbstractPipeline.copyIntoWithCancel(AbstractPipeline.java:498)at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:485)at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471)at java.util.stream.MatchOps$MatchOp.evaluateSequential(MatchOps.java:230)at java.util.stream.MatchOps$MatchOp.evaluateSequential(MatchOps.java:196)at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)at java.util.stream.ReferencePipeline.anyMatch(ReferencePipeline.java:449)at com.facebook.presto.metadata.MetadataManager.schemaExists(MetadataManager.java:254)at com.facebook.presto.sql.rewrite.ShowQueriesRewrite$Visitor.visitShowTables(ShowQueriesRewrite.java:191)at com.facebook.presto.sql.rewrite.ShowQueriesRewrite$Visitor.visitShowTables(ShowQueriesRewrite.java:151)at com.facebook.presto.sql.tree.ShowTables.accept(ShowTables.java:64)at com.facebook.presto.sql.tree.AstVisitor.process(AstVisitor.java:27)at com.facebook.presto.sql.rewrite.ShowQueriesRewrite.rewrite(ShowQueriesRewrite.java:148)at com.facebook.presto.sql.rewrite.StatementRewrite.rewrite(StatementRewrite.java:51)at com.facebook.presto.sql.analyzer.Analyzer.analyze(Analyzer.java:69)at com.facebook.presto.sql.analyzer.Analyzer.analyze(Analyzer.java:64)at com.facebook.presto.execution.SqlQueryExecution.doAnalyzeQuery(SqlQueryExecution.java:356)at com.facebook.presto.execution.SqlQueryExecution.analyzeQuery(SqlQueryExecution.java:342)at com.facebook.presto.execution.SqlQueryExecution.start(SqlQueryExecution.java:274)at com.facebook.presto.execution.QueuedExecution.lambda$start$1(QueuedExecution.java:62)at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)at java.lang.Thread.run(Thread.java:748)Caused by: java.lang.ClassNotFoundException: net.jpountz.lz4.LZ4BlockInputStreamat java.net.URLClassLoader.findClass(URLClassLoader.java:381)at java.lang.ClassLoader.loadClass(ClassLoader.java:424)at com.facebook.presto.server.PluginClassLoader.loadClass(PluginClassLoader.java:76)at java.lang.ClassLoader.loadClass(ClassLoader.java:357)... 37 more
issueID:CARBONDATA-2250
type:Improvement
changed files:processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/NewRowComparatorForNormalDims.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/NewRowComparator.java
texts:Reduce massive object generation in global sort

issueID:CARBONDATA-2251
type:Improvement
changed files:processing/src/main/java/org/apache/carbondata/processing/util/CarbonLoaderUtil.java
texts:Refactored sdv failures running on different environment
MergeIndex testcase in sdv fails if executed with different number of executors or in standalone spark.  Changes testcase having Hive UDAF like histogram_numeric having unexpected behaviour. so recommended way to write testcase using aggregation.  
issueID:CARBONDATA-2253
type:New Feature
changed files:store/sdk/src/main/java/org/apache/carbondata/sdk/file/AvroCarbonWriter.java
store/sdk/src/main/java/org/apache/carbondata/sdk/file/CarbonWriterBuilder.java
texts:Support write JSON/Avro data to carbon files
In SDK, user should be able to write JSON or Avro data to carbon files
issueID:CARBONDATA-2254
type:Improvement
changed files:
texts:Optimize CarbonData documentation
Optimize CarbonData documentation
issueID:CARBONDATA-2255
type:Improvement
changed files:
texts:Should rename the streaming examples to make it easy to understand

issueID:CARBONDATA-2256
type:Improvement
changed files:
texts:Adding sdv Testcases for SET_Parameter_Dynamically_Feature

issueID:CARBONDATA-2257
type:Test
changed files:
texts:Add SDV test cases for Partition with Global Sort
Add SDV test cases for Partition with Global Sort
issueID:CARBONDATA-2258
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/statusmanager/SegmentStatusManager.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
core/src/main/java/org/apache/carbondata/core/util/path/CarbonTablePath.java
core/src/main/java/org/apache/carbondata/core/util/CarbonProperties.java
texts:Separate visible and invisible segments info into two files to reduce the size of tablestatus file.
The size of the tablestatus file is getting larger, there are many places will scan this file and it will impact the performance of reading this file.According to the discussion on thread, it can append the invisible segment list to the file called 'tablestatus.history' when execute command 'CLEAN FILES FOR TABLE' (in method 'SegmentStatusManager.deleteLoadsAndUpdateMetadata') every time, separate visible and invisible segments into two files(tablestatus file and tablestatus.history file). If later it needs to support listing all segments(include visible and invisible) list when execute 'SHOW SEGMENTS FOR TABLE', it just need to read from two files.
issueID:CARBONDATA-2259
type:Task
changed files:
texts:Add auto CI for examples
The examples are guiding new users how to use CarbonData, but usually some examples can't work due to some code changed.1.Add auto CI for examples code.2.Merge "DataFrameAPIExample.scala, CarbonDataFrameExample.scala" into one. 
issueID:CARBONDATA-226
type:Bug
changed files:
texts:Delete load by ID message when the compacted segment is present is wrong.
Delete load by ID message when the compacted segment is present is wrong. Need to change the message.
issueID:CARBONDATA-2260
type:Improvement
changed files:
texts:CarbonThriftServer  should support S3 carbon table
CarbonThriftServer  should support S3 carbon table: store carbon table on S3 by CarbonThriftServer
issueID:CARBONDATA-2261
type:Bug
changed files:hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonTableInputFormat.java
texts:Support Set segment command for Streaming Table
Currently Set Segment  is not supported for Streaming Segments . Detail -----------------------------------------------------------------------------------------+SegmentSequenceIdStatus Load Start Time Load End Time Merged ToFile Format-----------------------------------------------------------------------------------------+2 Success 2018-03-18 00:32:16.7832018-03-18 00:32:17.036NA COLUMNAR_V31 Streaming Finish2018-03-18 00:31:59.7972018-03-18 00:32:24.016NA ROW_V1 0 Compacted 2018-03-18 00:31:53.1252018-03-18 00:31:59.7972 ROW_V1 -----------------------------------------------------------------------------------------+ Currently  "Streaming Finished" Or "Streaming "  status segments are not filtered  even these segments are not mentioned in the set segment command . fif user run below querysql("set carbon.input.segments.streaming.stream_table_handoff = 2") it should give data based on segment 2 but query includes segment 1 also .   
issueID:CARBONDATA-2262
type:New Feature
changed files:
texts:Create table should support using carbondata and stored as carbondata
Create table should support using carbondata and stored as carbondata, which is compatibility with Hive and gives better user experience .
issueID:CARBONDATA-2263
type:Bug
changed files:
texts:Date data is loaded incorrectly.
When we set :CarbonProperties.getInstance().addProperty(CarbonCommonConstants.CARBON_DATE_FORMAT, "yyyy/mm/dd")and run below commands: spark.sql("DROP TABLE IF EXISTS t3")spark.sql(s""" CREATE TABLE IF NOT EXISTS t3( ID Int, date Date, country String, name String, phonetype String, serialname String, salary Int, floatField float ) STORED BY 'carbondata'""".stripMargin)spark.sql(s"""LOAD DATA LOCAL INPATH '$testData' into table t3options('ALL_DICTIONARY_PATH'='$allDictFile', 'SINGLE_PASS'='true')""")spark.sql("""SELECT * FROM t3""").show()spark.sql("""SELECT * FROM t3 where floatField=3.5""").show()spark.sql("DROP TABLE IF EXISTS t3")Date data is loaded as below: ----------------------------------------------+ id datecountry namephonetypeserialnamesalaryfloatfield----------------------------------------------+ 92015-01-18 china aaa9 phone706 ASD86717 15008 2.34 102015-01-19 usaaaa10 phone685 ASD30505 15009 2.34 12015-01-23 china aaa1 phone197 ASD69643 15000 2.34 22015-01-24 china aaa2 phone756 ASD42892 15001 2.34 32015-01-25 china aaa3phone1904 ASD37014 15002 2.34 42015-01-26 china aaa4phone2435 ASD66902 15003 2.34 52015-01-27 china aaa5phone2441 ASD90633 15004 2.34 62015-01-28 china aaa6 phone294 ASD59961 15005 3.5 72015-01-29 china aaa7 phone610 ASD14875 15006 2.34 82015-01-30 china aaa8phone1848 ASD57308 15007 2.34----------------------------------------------+ However correct data is : ID,date,country,name,phonetype,serialname,salary,floatField1,2015/7/23,china,aaa1,phone197,ASD69643,15000,2.342,2015/7/24,china,aaa2,phone756,ASD42892,15001,2.343,2015/7/25,china,aaa3,phone1904,ASD37014,15002,2.344,2015/7/26,china,aaa4,phone2435,ASD66902,15003,2.345,2015/7/27,china,aaa5,phone2441,ASD90633,15004,2.346,2015/7/28,china,aaa6,phone294,ASD59961,15005,3.57,2015/7/29,china,aaa7,phone610,ASD14875,15006,2.348,2015/7/30,china,aaa8,phone1848,ASD57308,15007,2.349,2015/7/18,china,aaa9,phone706,ASD86717,15008,2.3410,2015/7/19,usa,aaa10,phone685,ASD30505,15009,2.34 which says Month data is loaded incorrectly. Similarly, if we use :CarbonProperties.getInstance().addProperty(CarbonCommonConstants.CARBON_DATE_FORMAT, "YYYY/mm/dd")it again store incorrect data for date.
issueID:CARBONDATA-2264
type:Bug
changed files:
texts:There is error when we create table using CarbonSource
There is error when we create table using CarbonSource .Test code:  test("test create table with complex datatype withour tablename in options") {    sql("drop table if exists create_source")    sql("create table create_source(intField int, stringField string, complexField array<string>) USING org.apache.spark.sql.CarbonSource")    sql("drop table create_source")  }Error:Table creation failed. Table name is not specified;org.apache.spark.sql.AnalysisException: Table creation failed. Table name is not specified; at org.apache.spark.sql.util.CarbonException$.analysisException(CarbonException.scala:23) at org.apache.spark.sql.CarbonSource.createRelation(CarbonSource.scala:141) at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:305) at org.apache.spark.sql.execution.command.CreateDataSourceTableCommand.run(createDataSourceTables.scala:78) at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58) at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56) at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:67) at org.apache.spark.sql.Dataset.<init>(Dataset.scala:183) at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:68) at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:632) at org.apache.spark.sql.test.Spark2TestQueryExecutor.sql(Spark2TestQueryExecutor.scala:35) at org.apache.spark.sql.test.util.QueryTest.sql(QueryTest.scala:103) at org.apache.spark.carbondata.CarbonDataSourceSuite$$anonfun$8.apply$mcV$sp(CarbonDataSourceSuite.scala:207) at org.apache.spark.carbondata.CarbonDataSourceSuite$$anonfun$8.apply(CarbonDataSourceSuite.scala:205) at org.apache.spark.carbondata.CarbonDataSourceSuite$$anonfun$8.apply(CarbonDataSourceSuite.scala:205) at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22) at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85) at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104) at org.scalatest.Transformer.apply(Transformer.scala:22) at org.scalatest.Transformer.apply(Transformer.scala:20) at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166) at org.apache.spark.sql.test.util.CarbonFunSuite.withFixture(CarbonFunSuite.scala:41) at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:163) at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175) at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175) at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306) at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:175) at org.scalatest.FunSuite.runTest(FunSuite.scala:1555) at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208) at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208) at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413) at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401) at scala.collection.immutable.List.foreach(List.scala:381) at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401) at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396) at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483) at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:208) at org.scalatest.FunSuite.runTests(FunSuite.scala:1555) at org.scalatest.Suite$class.run(Suite.scala:1424) at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1555) at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212) at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212) at org.scalatest.SuperEngine.runImpl(Engine.scala:545) at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:212) at org.apache.spark.carbondata.CarbonDataSourceSuite.org$scalatest$BeforeAndAfterAll$$super$run(CarbonDataSourceSuite.scala:29) at org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:257) at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:256) at org.apache.spark.carbondata.CarbonDataSourceSuite.run(CarbonDataSourceSuite.scala:29) at org.scalatest.tools.SuiteRunner.run(SuiteRunner.scala:55) at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2563) at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2557) at scala.collection.immutable.List.foreach(List.scala:381) at org.scalatest.tools.Runner$.doRunRunRunDaDoRunRun(Runner.scala:2557) at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1044) at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1043) at org.scalatest.tools.Runner$.withClassLoaderAndDispatchReporter(Runner.scala:2722) at org.scalatest.tools.Runner$.runOptionallyWithPassFailReporter(Runner.scala:1043) at org.scalatest.tools.Runner$.run(Runner.scala:883) at org.scalatest.tools.Runner.run(Runner.scala) at org.jetbrains.plugins.scala.testingSupport.scalaTest.ScalaTestRunner.runScalaTest2(ScalaTestRunner.java:138) at org.jetbrains.plugins.scala.testingSupport.scalaTest.ScalaTestRunner.main(ScalaTestRunner.java:28)
issueID:CARBONDATA-2265
type:Bug
changed files:processing/src/main/java/org/apache/carbondata/processing/store/writer/v3/CarbonFactDataWriterImplV3.java
processing/src/main/java/org/apache/carbondata/processing/store/writer/v3/BlockletDataHolder.java
texts:[DFX]-Load]: Load job fails if 1 folder contains 1000 files
Load job fails if 1 folder contains 1000 files.  【Precondition】：Thrift server should be running 【Test step】：  1: Create a carbon table 2: Start a load where 1 folder contains 1000 files 3: Observe that load fails Observe that Out of Memory exception is thrown.
issueID:CARBONDATA-2266
type:Bug
changed files:
texts:All Examples are throwing NoSuchElement Exception in current master branch
running all examples in current master branch is throwing this exceptionException in thread "main" java.util.NoSuchElementException: spark.carbon.common.listener.register.classname
issueID:CARBONDATA-2267
type:New Feature
changed files:integration/presto/src/main/java/org/apache/carbondata/presto/PrestoFilterUtil.java
integration/presto/src/main/java/org/apache/carbondata/presto/impl/CarbonTableReader.java
integration/presto/src/main/java/org/apache/carbondata/presto/CarbondataSplitManager.java
texts:Implement Reading Of Carbon Partition From Presto

issueID:CARBONDATA-227
type:Bug
changed files:
texts:In block distribution parralelism is decided initially and not re initialized after requesting new executors
In block distribution parralelism is decided initially and not re initialized after requesting new executors.Due to this task per node initialization is getting wrong.
issueID:CARBONDATA-2271
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/stats/QueryStatisticsRecorder.java
core/src/main/java/org/apache/carbondata/core/scan/expression/conditional/EqualToExpression.java
core/src/main/java/org/apache/carbondata/core/scan/expression/conditional/ListExpression.java
core/src/main/java/org/apache/carbondata/core/scan/expression/conditional/InExpression.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonFileInputFormat.java
core/src/main/java/org/apache/carbondata/core/scan/expression/conditional/LessThanExpression.java
core/src/main/java/org/apache/carbondata/core/util/CarbonProperties.java
core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
core/src/main/java/org/apache/carbondata/core/scan/expression/BinaryExpression.java
core/src/main/java/org/apache/carbondata/core/scan/expression/logical/OrExpression.java
core/src/main/java/org/apache/carbondata/core/scan/expression/conditional/GreaterThanExpression.java
core/src/main/java/org/apache/carbondata/core/scan/expression/logical/RangeExpression.java
core/src/main/java/org/apache/carbondata/core/scan/model/QueryModel.java
core/src/main/java/org/apache/carbondata/core/scan/expression/conditional/LessThanEqualToExpression.java
core/src/main/java/org/apache/carbondata/core/scan/expression/conditional/StartsWithExpression.java
hadoop/src/main/java/org/apache/carbondata/hadoop/CarbonMultiBlockSplit.java
core/src/main/java/org/apache/carbondata/core/scan/expression/MatchExpression.java
core/src/main/java/org/apache/carbondata/core/scan/expression/conditional/NotInExpression.java
core/src/main/java/org/apache/carbondata/core/scan/expression/ColumnExpression.java
core/src/main/java/org/apache/carbondata/core/stats/DriverQueryStatisticsRecorderImpl.java
core/src/main/java/org/apache/carbondata/core/stats/QueryStatisticsRecorderImpl.java
core/src/main/java/org/apache/carbondata/core/scan/expression/logical/FalseExpression.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonInputFormat.java
core/src/main/java/org/apache/carbondata/core/stats/TaskStatistics.java
core/src/main/java/org/apache/carbondata/core/scan/expression/logical/TrueExpression.java
core/src/main/java/org/apache/carbondata/core/stats/DriverQueryStatisticsRecorderDummy.java
core/src/main/java/org/apache/carbondata/core/stats/QueryStatisticsRecorderDummy.java
core/src/main/java/org/apache/carbondata/core/stats/QueryStatistic.java
core/src/main/java/org/apache/carbondata/core/scan/expression/LiteralExpression.java
core/src/main/java/org/apache/carbondata/core/scan/expression/Expression.java
core/src/main/java/org/apache/carbondata/core/scan/expression/logical/AndExpression.java
core/src/main/java/org/apache/carbondata/core/scan/expression/conditional/GreaterThanEqualToExpression.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonTableInputFormat.java
core/src/main/java/org/apache/carbondata/core/scan/expression/conditional/NotEqualsExpression.java
texts:Collect SQL execution information to driver side

issueID:CARBONDATA-2274
type:Bug
changed files:
texts:Partition table having more than 4 column giving zero record
 Partition table having more than 4 column giving zero record.Steps to reproduce :spark.sql("drop table if exists stats") spark.sql(s""" |CREATE TABLE stats ( | ad STRING, | impressions INT |) PARTITIONED BY (clicks INT,country STRING, year INT, month INT, day INT) |stored by 'carbondata' """.stripMargin) spark.sql( s""" | LOAD DATA LOCAL INPATH '${color}path' | INTO TABLE stats | OPTIONS('HEADER'='false') """.stripMargin)  spark.sql( s""" | SELECT * from stats """.stripMargin).show()  
issueID:CARBONDATA-2275
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/mutate/DeleteDeltaBlockDetails.java
core/src/main/java/org/apache/carbondata/core/datastore/filesystem/AbstractDFSCarbonFile.java
core/src/main/java/org/apache/carbondata/core/writer/CarbonDeleteDeltaWriterImpl.java
texts:Query Failed for 0 byte deletedelta file
When delete is failed on write step because of any exception from hdfs . Currently 0 bye deletedelta file is created and not getting cleaned up . So when any Select Query is triggered , Select Query is failed with Exception Problem in loading segment blocks. Caused by: java.lang.ArrayIndexOutOfBoundsException: 0   at org.apache.carbondata.core.datastore.filesystem.AbstractDFSCarbonFile.getLocations(AbstractDFSCarbonFile.java:514)   at org.apache.carbondata.core.indexstore.BlockletDataMapIndexStore.getAll(BlockletDataMapIndexStore.java:142)   ... 109 more  
issueID:CARBONDATA-2276
type:New Feature
changed files:core/src/main/java/org/apache/carbondata/core/reader/CarbonHeaderReader.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/SchemaReader.java
core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
store/sdk/src/main/java/org/apache/carbondata/sdk/file/CarbonReader.java
texts:Support SDK API to read schema in data file and schema file

issueID:CARBONDATA-2277
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelRangeGrtrThanEquaToFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelRangeTypeExecuterFactory.java
core/src/main/java/org/apache/carbondata/core/util/DataTypeUtil.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelRangeGrtThanFiterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/scan/executor/util/RestructureUtil.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RestructureEvaluatorImpl.java
core/src/main/java/org/apache/carbondata/core/scan/filter/FilterUtil.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelRangeLessThanFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/RowLevelRangeFilterResolverImpl.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelRangeLessThanEqualFilterExecuterImpl.java
texts:Filter on default values are not working
0: jdbc:hive2://localhost:10000> create table testFilter(data int) stored by 'carbondata';---------+ Result ---------+---------+No rows selected (1.231 seconds)0: jdbc:hive2://localhost:10000> insert into testFilter values(22);---------+ Result ---------+---------+No rows selected (3.726 seconds)0: jdbc:hive2://localhost:10000> alter table testFilter add columns(c1 int) TBLPROPERTIES('DEFAULT.VALUE.c1' = '25');---------+ Result ---------+---------+No rows selected (1.761 seconds)0: jdbc:hive2://localhost:10000> select * from testFilter;---------- data  c1 ---------- 22  25 ----------1 row selected (0.85 seconds)0: jdbc:hive2://localhost:10000> select * from testFilter where c1=25;Error: java.nio.BufferUnderflowException (state=,code=0)Stack Trace : 18/03/25 13:34:08 INFO CarbonLateDecodeRule: pool-20-thread-8 skip CarbonOptimizer18/03/25 13:34:08 INFO CarbonLateDecodeRule: pool-20-thread-8 Skip CarbonOptimizer18/03/25 13:34:08 INFO TableInfo: pool-20-thread-8 Table block size not specified for default_testfilter. Therefore considering the default value 1024 MB18/03/25 13:34:08 ERROR SparkExecuteStatementOperation: Error executing query, currentState RUNNING, java.nio.BufferUnderflowException at java.nio.Buffer.nextGetIndex(Buffer.java:506) at java.nio.HeapByteBuffer.getLong(HeapByteBuffer.java:412) at org.apache.carbondata.core.util.DataTypeUtil.getMeasureObjectFromDataType(DataTypeUtil.java:117) at org.apache.carbondata.core.scan.filter.executer.RestructureEvaluatorImpl.isMeasureDefaultValuePresentInFilterValues(RestructureEvaluatorImpl.java:113) at org.apache.carbondata.core.scan.filter.executer.RestructureExcludeFilterExecutorImpl.<init>(RestructureExcludeFilterExecutorImpl.java:43) at org.apache.carbondata.core.scan.filter.FilterUtil.getExcludeFilterExecuter(FilterUtil.java:281) at org.apache.carbondata.core.scan.filter.FilterUtil.createFilterExecuterTree(FilterUtil.java:147) at org.apache.carbondata.core.scan.filter.FilterUtil.createFilterExecuterTree(FilterUtil.java:158) at org.apache.carbondata.core.scan.filter.FilterUtil.getFilterExecuterTree(FilterUtil.java:1296) at org.apache.carbondata.core.indexstore.blockletindex.BlockletDataMap.prune(BlockletDataMap.java:644) at org.apache.carbondata.core.indexstore.blockletindex.BlockletDataMap.prune(BlockletDataMap.java:685) at org.apache.carbondata.core.datamap.TableDataMap.prune(TableDataMap.java:74) at org.apache.carbondata.hadoop.api.CarbonTableInputFormat.getDataBlocksOfSegment(CarbonTableInputFormat.java:739) at org.apache.carbondata.hadoop.api.CarbonTableInputFormat.getSplits(CarbonTableInputFormat.java:666) at org.apache.carbondata.hadoop.api.CarbonTableInputFormat.getSplits(CarbonTableInputFormat.java:426) at org.apache.carbondata.spark.rdd.CarbonScanRDD.getPartitions(CarbonScanRDD.scala:96) at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:252) at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:250) at scala.Option.getOrElse(Option.scala:121) at org.apache.spark.rdd.RDD.partitions(RDD.scala:250) at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35) at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:252) at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:250) at scala.Option.getOrElse(Option.scala:121) at org.apache.spark.rdd.RDD.partitions(RDD.scala:250) at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35) at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:252) at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:250) at scala.Option.getOrElse(Option.scala:121) at org.apache.spark.rdd.RDD.partitions(RDD.scala:250) at org.apache.spark.SparkContext.runJob(SparkContext.scala:1958) at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:935) at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151) at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112) at org.apache.spark.rdd.RDD.withScope(RDD.scala:362) at org.apache.spark.rdd.RDD.collect(RDD.scala:934) at org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:275) at org.apache.spark.sql.Dataset$$anonfun$org$apache$spark$sql$Dataset$$execute$1$1.apply(Dataset.scala:2371) at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57) at org.apache.spark.sql.Dataset.withNewExecutionId(Dataset.scala:2765) at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$execute$1(Dataset.scala:2370) at org.apache.spark.sql.Dataset$$anonfun$org$apache$spark$sql$Dataset$$collect$1.apply(Dataset.scala:2375) at org.apache.spark.sql.Dataset$$anonfun$org$apache$spark$sql$Dataset$$collect$1.apply(Dataset.scala:2375) at org.apache.spark.sql.Dataset.withCallback(Dataset.scala:2778) at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collect(Dataset.scala:2375) at org.apache.spark.sql.Dataset.collect(Dataset.scala:2351) at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:235) at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1$$anon$2.run(SparkExecuteStatementOperation.scala:163) at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1$$anon$2.run(SparkExecuteStatementOperation.scala:160) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1698) at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1.run(SparkExecuteStatementOperation.scala:173) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748)18/03/25 13:34:08 ERROR SparkExecuteStatementOperation: Error running hive query: org.apache.hive.service.cli.HiveSQLException: java.nio.BufferUnderflowException at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:258) at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1$$anon$2.run(SparkExecuteStatementOperation.scala:163) at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1$$anon$2.run(SparkExecuteStatementOperation.scala:160) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1698) at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1.run(SparkExecuteStatementOperation.scala:173) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748) 
issueID:CARBONDATA-2278
type:New Feature
changed files:integration/spark2/src/main/java/org/apache/carbondata/datamap/IndexDataMapProvider.java
core/src/main/java/org/apache/carbondata/core/util/CarbonProperties.java
core/src/main/java/org/apache/carbondata/core/datamap/DataMapStoreManager.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/datamap/DataMapClassProvider.java
integration/spark2/src/main/java/org/apache/carbondata/datamap/DataMapManager.java
core/src/main/java/org/apache/carbondata/core/datamap/DataMapCatalog.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/DataMapSchemaFactory.java
common/src/main/java/org/apache/carbondata/common/exceptions/sql/NoSuchDataMapException.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/DataMapSchema.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/DataMapSchemaStorageProvider.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/DiskBasedDMSchemaStorageProvider.java
core/src/main/java/org/apache/carbondata/core/datamap/DataMapProvider.java
texts:Save the datamaps to system folder of warehouse
ProblemCurrently, datamap schema is saved inside the main table schema itself. This approach cannot satisfy if a datamap belongs to more than one table. For suppose if we need to create a datamap joining 2 tables then we cannot keep the datamap schema under any one table.And also accessing the datamaps required to read the main table schema every time, it is not well optimized. And if we need to create multiple datamaps for a table then all datamaps need to store under the schema of that table so the size of main table schema grows and impacts the performance.SolutionMake the datamap schema independent of main table schema. And store the schema under _system}}folder location. This location is configurable by using carbon property {{carbon.system.folder.location , by default, it stores under the store location.Created datamap schema in JSON format for better readability. And has the interfaces to store it in database.Made on table <tablename> for datamap DDL as optional , so now user can create/drop or show datamaps without on table option.
issueID:CARBONDATA-228
type:Bug
changed files:
texts:Describe formatted command shows unsupported features
Remove unsupported features from Describe formatted command description
issueID:CARBONDATA-2285
type:Improvement
changed files:hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonTableInputFormat.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
texts:spark integration code refactor
Spark integration code refactor to improve extensible and re-usability
issueID:CARBONDATA-2287
type:Bug
changed files:processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactDataHandlerModel.java
core/src/main/java/org/apache/carbondata/core/datastore/filesystem/AbstractDFSCarbonFile.java
core/src/main/java/org/apache/carbondata/core/util/CarbonProperties.java
processing/src/main/java/org/apache/carbondata/processing/loading/converter/impl/MeasureFieldConverterImpl.java
core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
texts:Add event to alter partition table
During add partition , drop partition and split partition, event should be fired from Event framework.
issueID:CARBONDATA-2288
type:Improvement
changed files:
texts:Compaction should be able to run concurrently with data loading
Currently in carbondata, compaction can be triggered in two ways：1. Manually trigger compaction using ALTER statement.2. Atomically trigger compaction when doing data loading.In both ways, compaction and data loading cannot run concurrently. In way 1, compation will fail if data load is processing. In way 2, the compaction will only start after the main data loading finished and the user has to wait until the compaction is finished.In my option, data loading will work on a new segment, whereas compaction works on the existed segments, so we can let them run concurrently.For the 1st way, compaction will succeed even data loading is processing;For the 2nd way, compaction will run concurrently with the data loading, or after the data loading (we can configure it). And user will not have to wait the compaction finished.
issueID:CARBONDATA-2289
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/indexstore/BlockletDataMapIndexStore.java
texts:If carbon merge index is enabled then after IUD operation if some blocks of a segment is deleted, then during query and IUD operation the driver is throwing FileNotFoundException while preparing BlockMetaInfo.
Root cause:The carbonindexmerge file is containing deleted blocks (.carbondata) meta i.e. driver tries to load BlockletDataMap for deleted blocks and fails while populating BlockMetaInfo. Steps to reproduce:sql("drop table if exists mitable")sql("create table mitable(id int, issue date) stored by 'carbondata'")sql("insert into table mitable select '1','2000-02-01'")val table = CarbonMetadata.getInstance().getCarbonTable("default", "mitable")new CarbonIndexFileMergeWriter() .mergeCarbonIndexFilesOfSegment("0", table.getTablePath, false)sql("update mitable set(id)=(2) where issue = '2000-02-01'").show()sql("clean files for table mitable")sql("select * from mitable").show()
issueID:CARBONDATA-229
type:Bug
changed files:
texts:Array Index of bound exception thrown from dictionary look up while writing sort index file
Whenever we load dictionary data into memory, then in case of populating reverse dictionary object sometimes a chunk which has no value is also getting added to the dictionary chunk list. This is happening because the logic for dictionary chunk distribution in case of forward dictionary is not implemented for reverse dictionary and 0 size dictionary chunks are not getting removed while adding to the list of dictionary chunks.java.lang.IndexOutOfBoundsException: Index: 0, Size: 0 at java.util.ArrayList.rangeCheck(ArrayList.java:653) at java.util.ArrayList.get(ArrayList.java:429) at org.apache.carbondata.core.cache.dictionary.DictionaryChunksWrapper.next(DictionaryChunksWrapper.java:92) at org.apache.carbondata.core.writer.sortindex.CarbonDictionarySortInfoPreparator.prepareDictionarySortModels(CarbonDictionarySortInfoPreparator.java:120) at org.apache.carbondata.core.writer.sortindex.CarbonDictionarySortInfoPreparator.getDictionarySortInfo(CarbonDictionarySortInfoPreparator.java:51) at org.apache.carbondata.spark.tasks.SortIndexWriterTask.execute(SortIndexWriterTask.scala:44) at org.apache.carbondata.spark.rdd.CarbonGlobalDictionaryGenerateRDD$$anon$1.<init>(CarbonGlobalDictionaryRDD.scala:387) at org.apache.carbondata.spark.rdd.CarbonGlobalDictionaryGenerateRDD.compute(CarbonGlobalDictionaryRDD.scala:294)
issueID:CARBONDATA-2291
type:New Feature
changed files:integration/spark2/src/main/java/org/apache/carbondata/datamap/IndexDataMapProvider.java
core/src/main/java/org/apache/carbondata/core/locks/LockUsage.java
core/src/main/java/org/apache/carbondata/core/datamap/status/DataMapStatus.java
core/src/main/java/org/apache/carbondata/core/datamap/status/DataMapStatusStorageProvider.java
core/src/main/java/org/apache/carbondata/core/datamap/DataMapStoreManager.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonOutputCommitter.java
core/src/main/java/org/apache/carbondata/core/datamap/status/DataMapStatusDetail.java
core/src/main/java/org/apache/carbondata/core/datamap/status/DataMapStatusManager.java
core/src/main/java/org/apache/carbondata/core/datamap/status/DiskBasedDataMapStatusProvider.java
core/src/main/java/org/apache/carbondata/core/locks/CarbonLockUtil.java
processing/src/main/java/org/apache/carbondata/processing/loading/model/CarbonLoadModel.java
core/src/main/java/org/apache/carbondata/core/datamap/DataMapProvider.java
texts:Add datamap status and refresh command to sync data manually to datamaps
Currently, there is no way to synchronize datamaps manually to the fact table. So at first added the functionality to load the datamaps manually by using command refresh{{REFRESH DATAMAP <datamapName> }}Above command syncs the data between the datamap and parent tables.In order maintain the data consistency we require to enable or disable datamaps when data is not synchronized between fact and datamap. So added the new status called datamapstatus file under the system folder. whenever the data is out of sync between parent tables and datamap it just updates the status as disabled in datamapstatus file. After user manually refreshes the datamap it will update the status to enable.Only the enabled datamaps are considered during query.
issueID:CARBONDATA-2292
type:Bug
changed files:
texts:Different module dependencies different version spark jar
Different module dependencies different version spark jar, which maybe lead to conflict
issueID:CARBONDATA-2294
type:Sub-task
changed files:core/src/main/java/org/apache/carbondata/core/util/CarbonSessionInfo.java
core/src/main/java/org/apache/carbondata/core/metadata/SegmentFileStore.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonOutputCommitter.java
core/src/main/java/org/apache/carbondata/core/mutate/CarbonUpdateUtil.java
core/src/main/java/org/apache/carbondata/core/util/SessionParams.java
texts:Support preaggregate table creation on partition tables
Allow creation of aggregate table on partition and non-partition columns  Support to load data into aggregate table after insertion into parent table is complete. Support to compact aggregate tables if parent table is compacted. If parent table partition is dropped then drop the same partition in child tables also.
issueID:CARBONDATA-2295
type:Improvement
changed files:integration/presto/src/main/java/org/apache/carbondata/presto/impl/CarbonTableConfig.java
integration/presto/src/main/java/org/apache/carbondata/presto/impl/CarbonTableReader.java
texts:Add UNSAFE_WORKING_MEMORY_IN_MB  as a configuration parameter in presto integration
Add UNSAFE_WORKING_MEMORY_IN_MB as a configuration parameter for presto integration so we can tackle the unsafe memory related issues when querying carbon data from presto
issueID:CARBONDATA-2296
type:New Feature
changed files:integration/spark2/src/main/java/org/apache/carbondata/datamap/IndexDataMapProvider.java
texts:Test famework should take the location of local module target folder if not integrtion module
Current test framework QueryTest will not take the location and it always generates the store and metastore at spark-commons/target location. 
issueID:CARBONDATA-2297
type:New Feature
changed files:core/src/main/java/org/apache/carbondata/core/scan/executor/impl/VectorDetailQueryExecutor.java
core/src/main/java/org/apache/carbondata/core/scan/processor/BlockScan.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
core/src/main/java/org/apache/carbondata/core/util/SessionParams.java
core/src/main/java/org/apache/carbondata/core/scan/executor/impl/AbstractQueryExecutor.java
integration/spark-datasource/src/main/scala/org/apache/carbondata/spark/vectorreader/VectorizedCarbonRecordReader.java
core/src/main/java/org/apache/carbondata/core/scan/executor/impl/DetailQueryExecutor.java
core/src/main/java/org/apache/carbondata/core/scan/result/iterator/AbstractDetailQueryResultIterator.java
texts:Support SEARCH_MODE for basic filter query
Currently, tasks could be jammed subjected to available core number if query generates many tasks. For basic filter query(with limited result rows), we could add a new strategy to assign only one task with multiple splits each node and scan all the splits in parallel(get the result immediately and instead of using iterator)
issueID:CARBONDATA-2298
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/statusmanager/SegmentStatusManager.java
texts:Delete segment lock files before update metadata
If there are some COMPACTED segments and their last modified time is within one hour, the segment lock files deletion operation will not be executed.
issueID:CARBONDATA-2299
type:Improvement
changed files:
texts:Support showing all segment information(include visible and invisible segments)
Use command 'SHOW HISTORY SEGMENTS' to show all segment information(include visible and invisible segments)
issueID:CARBONDATA-23
type:Bug
changed files:
texts:Filter query issue for >, <, <= than filter
1. select count from a12 where dob > '2014-07-01 12:07:28'   throwing runtime exception2. select count from a12 where dob < '2014-07-01 12:07:28'   is including the null values also.3. select count from a12 where dob <=  '2014-07-01 12:07:28'   is including the null value Null should not considered in less than filter.create cube command: create table a12(empid String,ename String,sal double,deptno int,mgr string,gender string," +        "dob timestamp,comm decimal(4,2),desc string) stored by 'org.apache.carbondata.formatdata:empid,ename,sal,deptno,mgr,gender,dob,comm,desc1,abc,1233,10,2,,2014-07-01 12:07:28,1234.191,string_null2,bcd,1322,,3,f,2014-07-01 12:07:28,19.99,int_null3,cde,4322,,4,m,,16.996,date_null4,    ,43243,,5,m,,999.117,string_space5,,43242,20,6,m,2017-07-01 12:07:28,99.999,string_null6,ijk,,20,6,m,2017-07-01 12:07:28,50089,double_null7,pqr,2422,20,6,m,2017-07-01 12:07:28,32.339,decimal_null8
issueID:CARBONDATA-2300
type:Task
changed files:integration/presto/src/main/java/org/apache/carbondata/presto/impl/CarbonTableConfig.java
integration/presto/src/main/java/org/apache/carbondata/presto/impl/CarbonTableReader.java
texts:Add ENABLE_UNSAFE_IN_QUERY_EXECUTION as a configuration parameter in presto integration
Add ENABLE_UNSAFE_IN_QUERY_EXECUTION as a configuration parameter in presto integration.The default is true, provide this configuration parameter for users to disable the unsafe in query execution.
issueID:CARBONDATA-2301
type:Sub-task
changed files:store/sdk/src/main/java/org/apache/carbondata/store/LocalCarbonStore.java
store/sdk/src/main/java/org/apache/carbondata/store/MetaCachedCarbonStore.java
core/src/main/java/org/apache/carbondata/core/util/DataTypeUtil.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonInputFormat.java
store/sdk/src/main/java/org/apache/carbondata/store/CarbonStore.java
store/sdk/src/main/java/org/apache/carbondata/store/CarbonRowReadSupport.java
texts:Support query interface in CarbonStore
User should be able to query carbondata using CarbonStore API.1. Get API: It can be used for filter query. It accepts projection column names and filter expression, and returns matched rows.2. SQL API: it accepts SQL statement and return query result set.
issueID:CARBONDATA-2302
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/statusmanager/SegmentStatusManager.java
core/src/main/java/org/apache/carbondata/core/util/DeleteLoadFolders.java
texts:Fix some bugs when separate visible and invisible segments info into two files
There are some bugs when separate visible and invisible segments info into two files: It will not delete physical data of history segments after separating Generate duplicated segment id.
issueID:CARBONDATA-2303
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/datastore/filesystem/LocalCarbonFile.java
core/src/main/java/org/apache/carbondata/core/util/path/CarbonTablePath.java
core/src/main/java/org/apache/carbondata/core/datastore/filesystem/AbstractDFSCarbonFile.java
core/src/main/java/org/apache/carbondata/core/datastore/filesystem/AlluxioCarbonFile.java
core/src/main/java/org/apache/carbondata/core/datastore/filesystem/CarbonFile.java
core/src/main/java/org/apache/carbondata/core/datastore/filesystem/HDFSCarbonFile.java
core/src/main/java/org/apache/carbondata/core/datastore/filesystem/ViewFSCarbonFile.java
texts:If dataload is failed for parition table then cleanup is not working.
Test Step : 1. create table 2. load data (make sure data load is failed either manually or other)3. clean files for tableExpected Output: after clean files data from HDFS should be delete for segments which is Marked for delete. Actual Output: Alter cleanup ,data are not deleted from HDFS 
issueID:CARBONDATA-2304
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/scan/result/iterator/RawResultIterator.java
processing/src/main/java/org/apache/carbondata/processing/merger/CarbonCompactionExecutor.java
processing/src/main/java/org/apache/carbondata/processing/merger/RowResultMergerProcessor.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
processing/src/main/java/org/apache/carbondata/processing/merger/CompactionResultSortProcessor.java
texts:Enhance compaction performance by enabling prefetch
During compaction, carbondata will query on the segments and retrieve a row， then it will sort the rows and produce the final carbondata file.Currently we find the poor performance in retrieving the rows, so adding prefetch for the rows will surely improve the compaction performance.In my local tests, compacting 4 segments each with 100 thousand rows costs 30s with prefetch and 50s without prefetch.In my tests in a larger cluster, compacting 6 segments each with 18GB raw data costs 45min with prefetch and 57min without prefetch.
issueID:CARBONDATA-2307
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/datastore/chunk/impl/DimensionRawColumnChunk.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/impl/AbstractDimensionColumnPage.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/impl/MeasureRawColumnChunk.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/store/impl/safe/SafeAbsractDimensionDataChunkStore.java
core/src/main/java/org/apache/carbondata/core/scan/result/BlockletScannedResult.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/store/impl/safe/SafeVariableLengthDimensionDataChunkStore.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/AbstractRawColumnChunk.java
texts:OOM when using DataFrame.coalesce
TaskContext object holds reader’s reference until the task finished and coalesce combines a lot of CarbonSparkPartition into one task.
issueID:CARBONDATA-2308
type:Bug
changed files:
texts:Compaction should be allow when loading is in progress
When data loading (or insert into) is in progress, user should be able to do compaction on same table
issueID:CARBONDATA-2309
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonLoadOptionConstants.java
processing/src/main/java/org/apache/carbondata/processing/util/CarbonLoaderUtil.java
core/src/main/java/org/apache/carbondata/core/util/CarbonProperties.java
processing/src/main/java/org/apache/carbondata/processing/loading/model/CarbonLoadModelBuilder.java
processing/src/main/java/org/apache/carbondata/processing/loading/model/LoadOption.java
processing/src/main/java/org/apache/carbondata/processing/loading/model/CarbonLoadModel.java
texts:Add strategy to generate bigger carbondata files in case of small amount of data
In some scenario, the input amount of loading data is small, but carbondata still distribute them to each executors (nodes) to do local-sort, thus resulting to small carbondata files generated by each executor. In  some extreme conditions, if the cluster is big enough or if the amount of data is small enough, the carbondata file contains only one blocklet or page.I  think a new strategy should be introduced to solve the above problem.The new strategy should: be able to control the minimum amount of input data for each node ignore data locality otherwise it may always choose a small portion of particular nodes
issueID:CARBONDATA-231
type:Improvement
changed files:
texts:Rename repeared table names in same test file and add drop tables.

issueID:CARBONDATA-2310
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/readcommitter/TableStatusReadCommittedScope.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/TableInfo.java
core/src/main/java/org/apache/carbondata/core/util/BlockletDataMapUtil.java
core/src/main/java/org/apache/carbondata/core/datamap/DataMapJob.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockletDataMapFactory.java
core/src/main/java/org/apache/carbondata/core/indexstore/BlockletDataMapIndexStore.java
core/src/main/java/org/apache/carbondata/core/indexstore/TableBlockIndexUniqueIdentifier.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockletDataMapDistributable.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonFileInputFormat.java
core/src/main/java/org/apache/carbondata/core/datastore/filesystem/AbstractDFSCarbonFile.java
core/src/main/java/org/apache/carbondata/core/datastore/filesystem/CarbonFile.java
core/src/main/java/org/apache/carbondata/core/indexstore/SafeMemoryDMStore.java
core/src/main/java/org/apache/carbondata/core/util/path/CarbonTablePath.java
core/src/main/java/org/apache/carbondata/core/indexstore/schema/CarbonRowSchema.java
core/src/main/java/org/apache/carbondata/core/metadata/SegmentFileStore.java
core/src/main/java/org/apache/carbondata/core/cache/Cache.java
core/src/main/java/org/apache/carbondata/core/indexstore/row/DataMapRow.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockletDataMapModel.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/SegmentIndexFileStore.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockletDataMap.java
core/src/main/java/org/apache/carbondata/core/indexstore/UnsafeMemoryDMStore.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonInputFormat.java
core/src/main/java/org/apache/carbondata/core/indexstore/BlockletDetailInfo.java
core/src/main/java/org/apache/carbondata/core/datamap/AbstractDataMapJob.java
core/src/main/java/org/apache/carbondata/core/datastore/filesystem/LocalCarbonFile.java
core/src/main/java/org/apache/carbondata/core/indexstore/BlockletDataMapIndexWrapper.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonTableInputFormat.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
hadoop/src/main/java/org/apache/carbondata/hadoop/util/CarbonInputFormatUtil.java
core/src/main/java/org/apache/carbondata/core/indexstore/AbstractMemoryDMStore.java
core/src/main/java/org/apache/carbondata/core/indexstore/row/UnsafeDataMapRow.java
core/src/main/java/org/apache/carbondata/core/util/SessionParams.java
core/src/main/java/org/apache/carbondata/core/datamap/dev/CacheableDataMap.java
texts:Refactored code to improve Distributable interface

issueID:CARBONDATA-2311
type:Bug
changed files:
texts:Should avoid to append data to "streaming finish" streaming

issueID:CARBONDATA-2312
type:New Feature
changed files:
texts:Support In Memory catalog
Support Storing Catalog in memory(not in hive) for each session, after session restart user can create eternal table and run select query
issueID:CARBONDATA-2314
type:Bug
changed files:
texts:Data mismatch in Pre-Aggregate table after Streaming load due to threadset issue
Wrong data insertion into Pre-Aggregate table on Streaming table due to ThreadSet issue
issueID:CARBONDATA-2315
type:Improvement
changed files:
texts:DataLoad is showing success and failure message in log,when no data is loaded into table during LOAD

issueID:CARBONDATA-2316
type:Improvement
changed files:processing/src/main/java/org/apache/carbondata/processing/merger/AbstractResultProcessor.java
processing/src/main/java/org/apache/carbondata/processing/merger/CompactionResultSortProcessor.java
processing/src/main/java/org/apache/carbondata/processing/merger/RowResultMergerProcessor.java
texts:Even though one of the Compaction task failed at executor. All the executor task is showing success in UI and Job fails from driver.
At UI: At Executor :  At driver 
issueID:CARBONDATA-2317
type:Improvement
changed files:
texts:concurrent datamap with same name and schema creation throws exception
Steps to reproduce :  From Beeline user creates a table.  From 4 concurrent terminals user tries to create datamaps.Query used to reproduce:  CREATE TABLE uniqdata(CUST_ID int,CUST_NAME String,ACTIVE_EMUI_VERSION string, DOB timestamp, DOJ timestamp, BIGINT_COLUMN1 bigint,BIGINT_COLUMN2 bigint,DECIMAL_COLUMN1 decimal(30,10), DECIMAL_COLUMN2 decimal(36,10),Double_COLUMN1 double, Double_COLUMN2 double,INTEGER_COLUMN1 int) STORED BY 'org.apache.carbondata.format' TBLPROPERTIES('DICTIONARY_INCLUDE'='CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1,Double_COLUMN2,INTEGER_COLUMN1'); create datamap uniqdata_agg on table uniqdata using 'preaggregate' as select cust_name, avg(cust_id) from uniqdata group by cust_id, cust_name; create datamap uniqdata_agg_sum on table uniqdata using 'preaggregate' as select cust_name, sum(cust_id) from uniqdata group by cust_id, cust_name; create datamap uniqdata_agg_count on table uniqdata using 'preaggregate' as select cust_name, count(cust_id) from uniqdata group by cust_id, cust_name; create datamap uniqdata_agg_min on table uniqdata using 'preaggregate' as select cust_name, min(cust_id) from uniqdata group by cust_id, cust_name; create datamap uniqdata_agg_max on table uniqdata using 'preaggregate' as select cust_name, max(cust_id) from uniqdata group by cust_id, cust_name;   --->The datamaps are tried to be created from 4 concurrent terminals.2 of the datamaps fails with below error0: jdbc:hive2://ha-cluster/default> create datamap uniqdata_agg_min on table uniqdata using 'preaggregate' as select cust_name, min(cust_id) from uniqdata group by cust_id, cust_name;Error: org.apache.carbondata.spark.exception.ProcessMetaDataException: operation failed for default.uniqdata_uniqdata_agg_min: Create table'uniqdata_uniqdata_agg_min' in database 'default' failed, File does not exist: /user/hive/warehouse/carbon.store/default/uniqdata_uniqdata_agg_min/Metadata/schema.write (inode 20219) Holder DFSClient_NONMAPREDUCE_1307577692_216 does not have any open files.at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2686)at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.completeFileInternal(FSDirWriteFileOp.java:625)at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.completeFile(FSDirWriteFileOp.java:605)at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:2731)at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:883)at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:561)at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447)at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:847)at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:790)at java.security.AccessController.doPrivileged(Native Method)at javax.security.auth.Subject.doAs(Subject.java:422)at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1836)at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2486) (state=,code=0) 
issueID:CARBONDATA-2319
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/stats/TaskStatistics.java
texts:carbon_scan_time and carbon_IO_time are incorrect in task statistics
carbon_scan_time, carbon_IO_time are incorrect. query_idtask_idstart_timetotal_timeload_blocks_timeload_dictionary_timecarbon_scan_timecarbon_IO_timescan_blocks_numtotal_blockletsvalid_blockletstotal_pagesscanned_pagesvalid_pagesresult_size538574946428102018-04-08 10:52:09.01347ms0ms0ms-1ms-1ms11110110
issueID:CARBONDATA-2320
type:Bug
changed files:
texts:Fix error in lucene coarse grain datamap suite

issueID:CARBONDATA-2321
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/writer/CarbonIndexFileMergeWriter.java
texts:Selecton after a Concurrent Load Failing for Partition columns
selection after a Concurrent load fails randomly for partition column.
issueID:CARBONDATA-2322
type:Bug
changed files:
texts:Data mismatch in Aggregate query after compaction on Pre-Agg table on Partition table

issueID:CARBONDATA-2323
type:New Feature
changed files:integration/hive/src/main/java/org/apache/carbondata/hive/MapredCarbonInputFormat.java
core/src/main/java/org/apache/carbondata/core/scan/executor/QueryExecutorFactory.java
hadoop/src/main/java/org/apache/carbondata/hadoop/CarbonMultiBlockSplit.java
processing/src/main/java/org/apache/carbondata/processing/merger/CarbonCompactionExecutor.java
core/src/main/java/org/apache/carbondata/core/scan/model/QueryModelBuilder.java
hadoop/src/main/java/org/apache/carbondata/hadoop/util/CarbonInputFormatUtil.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
integration/spark2/src/main/scala/org/apache/carbondata/stream/CarbonStreamRecordReader.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/CarbonTable.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonInputFormat.java
core/src/main/java/org/apache/carbondata/hadoop/CarbonInputSplit.java
hadoop/src/main/java/org/apache/carbondata/hadoop/readsupport/impl/CarbonRowReadSupport.java
core/src/main/java/org/apache/carbondata/core/indexstore/Blocklet.java
core/src/main/java/org/apache/carbondata/core/util/CarbonProperties.java
texts:Distributed search mode using gRPC
When user gives SQL statement that only includes projection and filter, we can use RPC calls to do distributed scan on the carbon files directly instead of using RDD to do the query. In this mode, RDD overhead like RDD construction and DAG scheduling is avoided.
issueID:CARBONDATA-2324
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
texts:Support config ExecutorService in search mode
search mode should support to choose CachedThreadPool(instead of FixedThreadPool) to maximize utilization
issueID:CARBONDATA-2325
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/datastore/chunk/impl/DimensionRawColumnChunk.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/impl/MeasureRawColumnChunk.java
core/src/main/java/org/apache/carbondata/core/scan/scanner/impl/BlockletFullScanner.java
core/src/main/java/org/apache/carbondata/core/scan/scanner/impl/BlockletFilterScanner.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/store/impl/unsafe/UnsafeVariableLengthDimensionDataChunkStore.java
texts:Page level uncompress and Query performance improvement for Unsafe No Dictionary
Page Level Decoder for queryAdd page level on demand decoding, in current code, all pages of blocklet is getting uncompressed, because of this memory footprint is too high and causing OOM, Now added code to support page level decoding, one page will be decoding when all the records are processed next page data will be decoded. It will improve query performance for example limit query.Unsafe No Dictionary(Unsafe variable length)Optimized getRow(for Vector processing) And putArray method
issueID:CARBONDATA-2326
type:Bug
changed files:
texts:Statistics feature throw NPE  when spark.sql.execution.id is null

issueID:CARBONDATA-2327
type:Bug
changed files:integration/presto/src/main/java/org/apache/carbondata/presto/impl/CarbonTableReader.java
texts:invalid schema name  _system shows when executed show schemas in presto
presto> show schemas; Schema -------------------- _system  default  information_schema (3 rows)Query 20180410_101915_00010_sidw4, FINISHED, 1 nodeSplits: 18 total, 18 done (100.00%)0:00 &#91;3 rows, 47B&#93; &#91;25 rows/s, 395B/s&#93;
issueID:CARBONDATA-2329
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/util/CarbonSessionInfo.java
texts:Non Serializable extra info in session is overwritten by values from thread
1. Non Serializable extra info is copied from thread which causes stale data from old session when the thread is reused by spark. 2. CarboSessionInfo clone is not copying Non serializable info to new object which can damage session level values if local query thread updates values.
issueID:CARBONDATA-233
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
processing/src/main/java/org/apache/carbondata/processing/surrogatekeysgenerator/csvbased/BadRecordsLogger.java
processing/src/main/java/org/apache/carbondata/processing/model/CarbonLoadModel.java
integration/spark-common/src/main/java/org/apache/carbondata/spark/load/CarbonLoaderUtil.java
common/src/main/java/org/apache/carbondata/common/constants/LoggerAction.java
texts:bad record logger support for non parseable numeric and timestamp data
bad record logger support for non parseable numeric and timestamp data
issueID:CARBONDATA-2333
type:Bug
changed files:
texts:Block insert overwrite on parent table if any of the child tables are not partitioned on the specified partition columns
Block insert overwrite on parent table if any of the child tables are not partitioned on the specified partition columns.This is required because if any aggregate table is not partitioned then overriding the data for one partition may cause data inconsistency between the parent and child 
issueID:CARBONDATA-2334
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/metadata/schema/datamap/DataMapProperty.java
texts:Add property to enable users to block partitioning of Pre-Aggregate table

issueID:CARBONDATA-2335
type:Bug
changed files:
texts:Autohandoff is failing when preaggregate is created on streaming table
Auto hand off is failing with NullPointerException when preaggregate table is present in the streaming table.
issueID:CARBONDATA-2337
type:Bug
changed files:
texts:Fix duplicately acquiring &#39;streaming.lock&#39; error when integrating with spark-streaming
After merged [PR2135|https://github.com/apache/carbondata/pull/2135] it will acquire 'streaming.lock' duplicately when integrating with spark-streaming.
issueID:CARBONDATA-2338
type:Improvement
changed files:
texts:Add example to upload data to S3 by using SDK
Add example to upload data to S3 by using SDK
issueID:CARBONDATA-2339
type:Bug
changed files:
texts:下标越界
java.lang.ArrayIndexOutOfBoundsException
issueID:CARBONDATA-234
type:Bug
changed files:integration/spark-common/src/main/java/org/apache/carbondata/spark/merger/CarbonCompactionUtil.java
texts:wrong message is printed in the logs each time when the compaction is done.
wrong message is printed in the logs each time when the compaction is done.
issueID:CARBONDATA-2340
type:Bug
changed files:
texts:load数据超过32000byte
INFO storage.BlockManagerMasterEndpoint: Registering block manager spark1:12603 with 5.2 GB RAM, BlockManagerId(1, spark1, 12603, None)18/04/11 14:24:23 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on spark1:12603 (size: 34.9 KB, free: 5.2 GB)18/04/11 14:24:34 WARN scheduler.TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0, spark1, executor 1): org.apache.carbondata.processing.loading.exception.CarbonDataLoadingException: Dataload failed, String size cannot exceed 32000 bytes at org.apache.carbondata.processing.loading.converter.impl.NonDictionaryFieldConverterImpl.convert(NonDictionaryFieldConverterImpl.java:75) at org.apache.carbondata.processing.loading.converter.impl.RowConverterImpl.convert(RowConverterImpl.java:162) at org.apache.carbondata.processing.loading.steps.DataConverterProcessorStepImpl.processRowBatch(DataConverterProcessorStepImpl.java:104) at org.apache.carbondata.processing.loading.steps.DataConverterProcessorStepImpl$1.next(DataConverterProcessorStepImpl.java:91) at org.apache.carbondata.processing.loading.steps.DataConverterProcessorStepImpl$1.next(DataConverterProcessorStepImpl.java:77) at org.apache.carbondata.processing.loading.sort.impl.ParallelReadMergeSorterImpl$SortIteratorThread.run(ParallelReadMergeSorterImpl.java:214) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:748) 
issueID:CARBONDATA-2341
type:Improvement
changed files:
texts:Add CleanUp for Pre-Aggregate table

issueID:CARBONDATA-2343
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/datamap/DataMapChooser.java
texts:Improper filter resolver cause more filter scan on data that could be skipped
In DataMapChooser, Carbondata try to choose and combine datamap for expressions. In some scenario, it will generate `TrueConditionalResolverImpl` to wrap the sub-expression, which will cause data scan on the blocklet which can be skipped (For `TrueConditionalResolverImpl`, the  `TrueFilterExecutor` will always cause scanning the data even it simply wraps a range expression).
issueID:CARBONDATA-2344
type:Bug
changed files:
texts:Fix bugs in BlockletDataMap
DMStore stores DataMapRows for each blocklet.Currently carbondata access the DMStore by blockletId, which is not unique and will cause problems.
issueID:CARBONDATA-2345
type:Bug
changed files:
texts:"Task failed while writing rows" error occuers when streaming ingest into carbondata table
carbondata version:1.3.1。spark:2.2.1When using spark structured streaming ingest data into carbondata table , such error occurs:warning: there was one deprecation warning; re-run with -deprecation for detailsqry: org.apache.spark.sql.streaming.StreamingQuery = org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@7ddf193a&#91;Stage 1:> (0 + 2) / 5&#93;18/04/13 18:03:56 WARN TaskSetManager: Lost task 1.0 in stage 1.0 (TID 2, sz-pg-entanalytics-research-004.tendcloud.com, executor 1): org.apache.carbondata.streaming.CarbonStreamException: Task failed while writing rows at org.apache.spark.sql.execution.streaming.CarbonAppendableStreamSink$.writeDataFileTask(CarbonAppendableStreamSink.scala:345) at org.apache.spark.sql.execution.streaming.CarbonAppendableStreamSink$$anonfun$writeDataFileJob$1$$anonfun$apply$mcV$sp$1.apply(CarbonAppendableStreamSink.scala:247) at org.apache.spark.sql.execution.streaming.CarbonAppendableStreamSink$$anonfun$writeDataFileJob$1$$anonfun$apply$mcV$sp$1.apply(CarbonAppendableStreamSink.scala:246) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87) at org.apache.spark.scheduler.Task.run(Task.scala:108) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745)Caused by: java.lang.NullPointerException at org.apache.carbondata.processing.loading.BadRecordsLogger.addBadRecordsToBuilder(BadRecordsLogger.java:126) at org.apache.carbondata.processing.loading.converter.impl.RowConverterImpl.convert(RowConverterImpl.java:164) at org.apache.carbondata.hadoop.streaming.CarbonStreamRecordWriter.write(CarbonStreamRecordWriter.java:186) at org.apache.carbondata.streaming.segment.StreamSegment.appendBatchData(StreamSegment.java:244) at org.apache.spark.sql.execution.streaming.CarbonAppendableStreamSink$$anonfun$writeDataFileTask$1.apply$mcV$sp(CarbonAppendableStreamSink.scala:336) at org.apache.spark.sql.execution.streaming.CarbonAppendableStreamSink$$anonfun$writeDataFileTask$1.apply(CarbonAppendableStreamSink.scala:326) at org.apache.spark.sql.execution.streaming.CarbonAppendableStreamSink$$anonfun$writeDataFileTask$1.apply(CarbonAppendableStreamSink.scala:326) at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1371) at org.apache.spark.sql.execution.streaming.CarbonAppendableStreamSink$.writeDataFileTask(CarbonAppendableStreamSink.scala:338) ... 8 more&#91;Stage 1:===========> (1 + 2) / 5&#93;18/04/13 18:03:57 ERROR TaskSetManager: Task 0 in stage 1.0 failed 4 times; aborting job18/04/13 18:03:57 ERROR CarbonAppendableStreamSink$: stream execution thread for &#91;id = 3abdadea-65f6-4d94-8686-306fccae4559, runId = 689adf7e-a617-41d9-96bc-de075ce4dd73&#93; Aborting job job_20180413180354_0000.org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 4 times, most recent failure: Lost task 0.3 in stage 1.0 (TID 11, sz-pg-entanalytics-research-004.tendcloud.com, executor 1): org.apache.carbondata.streaming.CarbonStreamException: Task failed while writing rows at org.apache.spark.sql.execution.streaming.CarbonAppendableStreamSink$.writeDataFileTask(CarbonAppendableStreamSink.scala:345) at org.apache.spark.sql.execution.streaming.CarbonAppendableStreamSink$$anonfun$writeDataFileJob$1$$anonfun$apply$mcV$sp$1.apply(CarbonAppendableStreamSink.scala:247) at org.apache.spark.sql.execution.streaming.CarbonAppendableStreamSink$$anonfun$writeDataFileJob$1$$anonfun$apply$mcV$sp$1.apply(CarbonAppendableStreamSink.scala:246) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87) at org.apache.spark.scheduler.Task.run(Task.scala:108) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745)Caused by: java.lang.NullPointerException at org.apache.carbondata.processing.loading.BadRecordsLogger.addBadRecordsToBuilder(BadRecordsLogger.java:126) at org.apache.carbondata.processing.loading.converter.impl.RowConverterImpl.convert(RowConverterImpl.java:164) at org.apache.carbondata.hadoop.streaming.CarbonStreamRecordWriter.write(CarbonStreamRecordWriter.java:186) at org.apache.carbondata.streaming.segment.StreamSegment.appendBatchData(StreamSegment.java:244) at org.apache.spark.sql.execution.streaming.CarbonAppendableStreamSink$$anonfun$writeDataFileTask$1.apply$mcV$sp(CarbonAppendableStreamSink.scala:336) at org.apache.spark.sql.execution.streaming.CarbonAppendableStreamSink$$anonfun$writeDataFileTask$1.apply(CarbonAppendableStreamSink.scala:326) at org.apache.spark.sql.execution.streaming.CarbonAppendableStreamSink$$anonfun$writeDataFileTask$1.apply(CarbonAppendableStreamSink.scala:326) at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1371) at org.apache.spark.sql.execution.streaming.CarbonAppendableStreamSink$.writeDataFileTask(CarbonAppendableStreamSink.scala:338) ... 8 more
issueID:CARBONDATA-2346
type:Bug
changed files:
texts:Dropping partition failing with null error for Partition table with Pre-Aggregate tables

issueID:CARBONDATA-2347
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/datamap/DataMapStoreManager.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockletDataMapFactory.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/datamap/DataMapClassProvider.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/CarbonTable.java
datamap/examples/src/minmaxdatamap/main/java/org/apache/carbondata/datamap/examples/MinMaxIndexDataMapFactory.java
processing/src/main/java/org/apache/carbondata/processing/store/writer/AbstractFactDataWriter.java
core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
integration/spark2/src/main/java/org/apache/carbondata/datamap/IndexDataMapProvider.java
processing/src/main/java/org/apache/carbondata/processing/datamap/DataMapWriterListener.java
datamap/bloom/src/main/java/org/apache/carbondata/datamap/bloom/BloomCoarseGrainDataMapFactory.java
core/src/main/java/org/apache/carbondata/core/metadata/SegmentFileStore.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
datamap/lucene/src/main/java/org/apache/carbondata/datamap/lucene/LuceneFineGrainDataMap.java
datamap/lucene/src/main/java/org/apache/carbondata/datamap/lucene/LuceneFineGrainDataMapFactory.java
core/src/main/java/org/apache/carbondata/core/features/TableOperation.java
core/src/main/java/org/apache/carbondata/core/datamap/dev/DataMapFactory.java
datamap/lucene/src/main/java/org/apache/carbondata/datamap/lucene/LuceneDataMapDistributable.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/DataMapSchemaStorageProvider.java
core/src/main/java/org/apache/carbondata/core/datamap/TableDataMap.java
datamap/examples/src/minmaxdatamap/main/java/org/apache/carbondata/datamap/examples/MinMaxDataWriter.java
datamap/lucene/src/main/java/org/apache/carbondata/datamap/lucene/LuceneDataMapFactoryBase.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/DataMapSchema.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/DiskBasedDMSchemaStorageProvider.java
core/src/main/java/org/apache/carbondata/core/datamap/dev/DataMapWriter.java
datamap/lucene/src/main/java/org/apache/carbondata/datamap/lucene/LuceneDataMapWriter.java
texts:Fix Functional issues in LuceneDatamap in load and query and make stable
1) The index write location for the lucene is same, and to IndexWriter will take a lock file called write.lock in write location while writing the index files. In carbon loading the writer tasks are launched parallel and those many writers are opened,Since the write.lock file is acquired by one writer, all other tasks will fail and dataloading will fail.2)in query side, read index path for lucene was in single path, but after load fix, there will be multiple index directories after loadfunctional issues in drop table, drop datamap, show datamap  
issueID:CARBONDATA-235
type:Bug
changed files:integration/spark-common/src/main/java/org/apache/carbondata/spark/load/CarbonLoaderUtil.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
processing/src/main/java/org/apache/carbondata/processing/surrogatekeysgenerator/csvbased/BadRecordsLogger.java
texts:Some no-used constants still exist in CarbonCommonConstants
such as WRITE_ALL_NODE_IN_SINGLE_TIME = "carbon.datawriter.write.all.node";CARBON_BIGDATA_ACL_ENABLED = "carbon.bigdata.acl.enable";
issueID:CARBONDATA-2350
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/datastore/page/ColumnPage.java
datamap/examples/src/minmaxdatamap/main/java/org/apache/carbondata/datamap/examples/MinMaxIndexDataMapFactory.java
datamap/examples/src/minmaxdatamap/main/java/org/apache/carbondata/datamap/examples/MinMaxDataWriter.java
datamap/examples/src/minmaxdatamap/main/java/org/apache/carbondata/datamap/examples/MinMaxIndexDataMap.java
datamap/lucene/src/main/java/org/apache/carbondata/datamap/lucene/LuceneDataMapWriter.java
texts:Fix bugs in minmax datamap example
Current implementation of minmax datamap example doesn't work as intended and has the following problems: The example cannot run functionally. The minmax datamap doesn't prune blocklet as intended. 
issueID:CARBONDATA-2351
type:Bug
changed files:
texts:CarbonData Select null
Carbondata1.3.1 with Spark2.1.2,after insert into values,SELECT is right.but next day,data of SELECT is null but the line number is right. create table carbon01(id int,name int,age int,sex int) stored by 'carbondata';insert into carbon01 values(1,1,1,1);select * from carbon01;1 1 1 1then,i exit spark-sql.but,next day,select * from carbon01;null null null null .it may not because of time.after one line turn to null,i insert into again.insert into carbon01 values(2,2,2,2);then i select,the result is:null null null null2 2 2 2the log is:ResultCollectorFactory:&#91;Executor task launch worker for task 1&#93;&#91;partition...&#93; Restructure dictionary vector collector is used to scan and collect the dataResultCollectorFactory:&#91;Executor task launch worker for task 2&#93;&#91;partition...&#93; Vector based dictionary collector is used to scan and collect the dataand i follow source code,in task1, tableBlockInfo.getDetailInfo.columnSchemas.get(0).isDimensionColumn=true;in task2, tableBlockInfo.getDetailInfo.columnSchemas.get(0).isDimensionColumn=false;  
issueID:CARBONDATA-2352
type:Test
changed files:
texts:Add SDV Test Cases for Partition with Pre-Aggregate table

issueID:CARBONDATA-2353
type:Improvement
changed files:integration/spark2/src/main/java/org/apache/carbondata/datamap/IndexDataMapProvider.java
processing/src/main/java/org/apache/carbondata/processing/datamap/DataMapWriterListener.java
core/src/main/java/org/apache/carbondata/core/datamap/DataMapStoreManager.java
integration/spark2/src/main/java/org/apache/carbondata/datamap/DataMapManager.java
core/src/main/java/org/apache/carbondata/core/datamap/status/DataMapStatusManager.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/DataMapSchemaStorageProvider.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/DataMapSchema.java
core/src/main/java/org/apache/carbondata/core/datamap/DataMapChooser.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/DiskBasedDMSchemaStorageProvider.java
texts:Add cache for DataMap schema provider to avoid IO for each read
Add cache for DataMap schema provider to avoid IO for each read. 
issueID:CARBONDATA-2355
type:Improvement
changed files:
texts:Support run SQL on carbon files directly, which is generated by SDk
Spark support run sql on files directly:https://spark.apache.org/docs/latest/sql-programming-guide.html#run-sql-on-files-directlyCarbonData also should support.This is for carbonfile generated by SDKNow only support directly read the file directory with one carbonfile.For example:select * FROM carbonfile.$filePath`No need to create table and user can read data firectly.
issueID:CARBONDATA-2356
type:Test
changed files:datamap/lucene/src/main/java/org/apache/carbondata/datamap/lucene/LuceneFineGrainDataMap.java
core/src/main/java/org/apache/carbondata/core/scan/expression/MatchExpression.java
texts:Adding UT for Lucene DataMap

issueID:CARBONDATA-2357
type:Improvement
changed files:datamap/lucene/src/main/java/org/apache/carbondata/datamap/lucene/LuceneDataMapWriter.java
texts:Add column name and index mapping in lucene datamap writer

issueID:CARBONDATA-2358
type:Improvement
changed files:
texts:Dataframe overwrite does not work properly if the table is already created and has deleted segments
Dataframe overwrite does not work properly if the table is already created and has deleted segments. 
issueID:CARBONDATA-2359
type:Sub-task
changed files:core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/TableSchemaBuilder.java
store/sdk/src/main/java/org/apache/carbondata/sdk/file/CarbonWriterBuilder.java
texts:Support applicable load options and table properties for Non Transactional table

issueID:CARBONDATA-236
type:Wish
changed files:
texts:maven compile warning
1. maven-shade-plugin omit version2. &#91;WARNING&#93; 'dependencies.dependency.(groupId:artifactId:type:classifier)' must be unique: org.apache.carbondata:carbondata-format:jar -> duplicate declaration of version ${project.version} @ line 99, column 17
issueID:CARBONDATA-2360
type:Sub-task
changed files:core/src/main/java/org/apache/carbondata/core/metadata/schema/table/TableInfo.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/CarbonTable.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonInputFormat.java
core/src/main/java/org/apache/carbondata/core/scan/executor/util/RestructureUtil.java
core/src/main/java/org/apache/carbondata/core/scan/executor/impl/AbstractQueryExecutor.java
processing/src/main/java/org/apache/carbondata/processing/util/CarbonLoaderUtil.java
processing/src/main/java/org/apache/carbondata/processing/loading/model/CarbonLoadModelBuilder.java
processing/src/main/java/org/apache/carbondata/processing/loading/model/LoadOption.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/CarbonTableBuilder.java
core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonTableOutputFormat.java
processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactDataHandlerModel.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/SchemaReader.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonTableInputFormat.java
hadoop/src/main/java/org/apache/carbondata/hadoop/util/CarbonInputFormatUtil.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonOutputCommitter.java
processing/src/main/java/org/apache/carbondata/processing/loading/model/CarbonLoadModel.java
processing/src/main/java/org/apache/carbondata/processing/loading/DataLoadProcessBuilder.java
processing/src/main/java/org/apache/carbondata/processing/loading/CarbonDataLoadConfiguration.java
store/sdk/src/main/java/org/apache/carbondata/sdk/file/CarbonWriterBuilder.java
texts:Insert into and Insert Into overwrite support for Non Transactional table
Insert into and Insert Into overwrite support for unmanaged table
issueID:CARBONDATA-2361
type:Sub-task
changed files:core/src/main/java/org/apache/carbondata/core/readcommitter/TableStatusReadCommittedScope.java
core/src/main/java/org/apache/carbondata/core/readcommitter/ReadCommittedIndexFileSnapShot.java
core/src/main/java/org/apache/carbondata/core/datamap/dev/expr/AndDataMapExprWrapper.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockletDataMapFactory.java
datamap/examples/src/minmaxdatamap/main/java/org/apache/carbondata/datamap/examples/MinMaxIndexDataMapFactory.java
core/src/main/java/org/apache/carbondata/core/datamap/dev/expr/DataMapExprWrapper.java
core/src/main/java/org/apache/carbondata/core/mutate/CarbonUpdateUtil.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonInputFormat.java
core/src/main/java/org/apache/carbondata/core/readcommitter/ReadCommittedScope.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonFileInputFormat.java
processing/src/main/java/org/apache/carbondata/processing/util/CarbonLoaderUtil.java
core/src/main/java/org/apache/carbondata/core/datamap/dev/expr/OrDataMapExprWrapper.java
core/src/main/java/org/apache/carbondata/core/datamap/Segment.java
core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
core/src/main/java/org/apache/carbondata/core/datamap/dev/expr/DataMapExprWrapperImpl.java
core/src/main/java/org/apache/carbondata/core/datamap/DistributableDataMapFormat.java
processing/src/main/java/org/apache/carbondata/processing/datamap/DataMapWriterListener.java
core/src/main/java/org/apache/carbondata/core/indexstore/SegmentPropertiesFetcher.java
processing/src/main/java/org/apache/carbondata/processing/merger/CarbonDataMergerUtil.java
core/src/main/java/org/apache/carbondata/core/metadata/SegmentFileStore.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonTableInputFormat.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonOutputCommitter.java
datamap/lucene/src/main/java/org/apache/carbondata/datamap/lucene/LuceneFineGrainDataMapFactory.java
core/src/main/java/org/apache/carbondata/core/indexstore/BlockletDetailsFetcher.java
core/src/main/java/org/apache/carbondata/core/statusmanager/SegmentStatusManager.java
core/src/main/java/org/apache/carbondata/core/datamap/dev/DataMapFactory.java
core/src/main/java/org/apache/carbondata/core/datamap/TableDataMap.java
texts:Refactor Read Committed Scope implementation.
Refactor Read Committed Scope Implementation.
issueID:CARBONDATA-2363
type:Bug
changed files:
texts:Should add CarbonStreamingQueryListener to SparkSession

issueID:CARBONDATA-2364
type:Bug
changed files:processing/src/main/java/org/apache/carbondata/processing/loading/steps/DataWriterProcessorStepImpl.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/impl/UnsafeParallelReadMergeSorterWithColumnRangeImpl.java
texts:Remove useless and time consuming code block
Remove useless and time consuming code block
issueID:CARBONDATA-2365
type:Improvement
changed files:integration/spark-datasource/src/main/scala/org/apache/carbondata/spark/vectorreader/VectorizedCarbonRecordReader.java
core/src/main/java/org/apache/carbondata/core/scan/executor/QueryExecutorFactory.java
core/src/main/java/org/apache/carbondata/core/scan/result/iterator/AbstractDetailQueryResultIterator.java
core/src/main/java/org/apache/carbondata/core/scan/processor/BlockScan.java
texts:Add QueryExecutor in SearchMode for row-based CarbonRecordReader

issueID:CARBONDATA-2366
type:Bug
changed files:
texts:Concurrent Datamap creation is failing when using hive metastore
Issue1: On  firing 60 concurrent datamap create commands only 1 datamap is getting registered to maintable. Issue2: If datamap creation fails then the datamap is not dropped from metastore due to which it goes into stale state.
issueID:CARBONDATA-2369
type:Sub-task
changed files:
texts:Add a document for Non Transactional table with SDK writer guide

issueID:CARBONDATA-237
type:Test
changed files:
texts:Add testcase for auto load merge
add a testcase for property "ENABLE_AUTO_LOAD_MERGE"
issueID:CARBONDATA-2370
type:Task
changed files:
texts:Document for Presto cluster setup for carbondata
Create a document related to instructions to be followed for setting up carbondata on multinode presto cluster.
issueID:CARBONDATA-2371
type:New Feature
changed files:core/src/main/java/org/apache/carbondata/core/scan/expression/MatchExpression.java
core/src/main/java/org/apache/carbondata/core/datamap/dev/expr/AndDataMapExprWrapper.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockletDataMap.java
core/src/main/java/org/apache/carbondata/core/datamap/dev/expr/DataMapExprWrapper.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonInputFormat.java
core/src/main/java/org/apache/carbondata/core/profiler/ExplainCollector.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/resolverinfo/visitor/RangeNoDictionaryTypeVisitor.java
core/src/main/java/org/apache/carbondata/core/datamap/dev/expr/OrDataMapExprWrapper.java
core/src/main/java/org/apache/carbondata/core/profiler/TablePruningInfo.java
core/src/main/java/org/apache/carbondata/core/datamap/DataMapChooser.java
core/src/main/java/org/apache/carbondata/core/datamap/dev/expr/DataMapExprWrapperImpl.java
texts:Add Profiler output in EXPLAIN command
More information should give in EXPLAIN command to show the effeteness of datamap
issueID:CARBONDATA-2373
type:New Feature
changed files:datamap/bloom/src/main/java/org/apache/carbondata/datamap/bloom/BloomDataMapWriter.java
datamap/bloom/src/main/java/org/apache/carbondata/datamap/bloom/BloomCoarseGrainDataMap.java
core/src/main/java/org/apache/carbondata/core/datamap/DataMapMeta.java
datamap/bloom/src/main/java/org/apache/carbondata/datamap/bloom/BloomCoarseGrainDataMapFactory.java
texts:Add bloom filter datamap to support precise query

issueID:CARBONDATA-2374
type:Bug
changed files:
texts:Fix bugs in minmax datamap example

issueID:CARBONDATA-2375
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/datamap/DataMapChooser.java
core/src/main/java/org/apache/carbondata/core/datamap/Segment.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonInputFormat.java
texts:Add CG prune before FG prune
CG prune before FG prune, and passes the pruned segments and indexfiles to FG DataMap for further pruning.
issueID:CARBONDATA-2376
type:Improvement
changed files:datamap/bloom/src/main/java/org/apache/carbondata/datamap/bloom/BloomDataMapWriter.java
core/src/main/java/org/apache/carbondata/core/datamap/dev/fgdatamap/FineGrainBlocklet.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockletDataMapFactory.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonInputFormat.java
processing/src/main/java/org/apache/carbondata/processing/store/writer/AbstractFactDataWriter.java
core/src/main/java/org/apache/carbondata/core/indexstore/ExtendedBlocklet.java
core/src/main/java/org/apache/carbondata/core/datamap/Segment.java
datamap/bloom/src/main/java/org/apache/carbondata/datamap/bloom/BloomCoarseGrainDataMap.java
processing/src/main/java/org/apache/carbondata/processing/datamap/DataMapWriterListener.java
core/src/main/java/org/apache/carbondata/core/util/path/CarbonTablePath.java
datamap/bloom/src/main/java/org/apache/carbondata/datamap/bloom/BloomCoarseGrainDataMapFactory.java
datamap/lucene/src/main/java/org/apache/carbondata/datamap/lucene/LuceneFineGrainDataMap.java
core/src/main/java/org/apache/carbondata/core/indexstore/Blocklet.java
datamap/examples/src/minmaxdatamap/main/java/org/apache/carbondata/datamap/examples/MinMaxDataWriter.java
datamap/lucene/src/main/java/org/apache/carbondata/datamap/lucene/LuceneDataMapFactoryBase.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/DiskBasedDMSchemaStorageProvider.java
core/src/main/java/org/apache/carbondata/core/datamap/dev/DataMapWriter.java
datamap/lucene/src/main/java/org/apache/carbondata/datamap/lucene/LuceneDataMapWriter.java
texts:Improve Lucene datamap performance by eliminating blockid while writing and reading index.
Currently DataMap interface implementations use blockid and blockletid while writing index files, Actually blockid is not needed to store in index files as it only requires blockletid. So it adds more memory and disk size to write index files.
issueID:CARBONDATA-2377
type:New Feature
changed files:core/src/main/java/org/apache/carbondata/core/scan/model/QueryModel.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
core/src/main/java/org/apache/carbondata/core/util/CarbonProperties.java
texts:Support message throttling in search mode
In search mode concurrent query scenario, we should control the number of request sent to Worker to prevent it from overloaded
issueID:CARBONDATA-2378
type:New Feature
changed files:
texts:Support enable/disable search mode in ThriftServer
User should be able to enable or disable search mode when using ThriftServer
issueID:CARBONDATA-2379
type:Improvement
changed files:
texts:Support Search mode run in the cluster and fix some error
Error 1 JVM crashed## A fatal error has been detected by the Java Runtime Environment:##  SIGSEGV (0xb) at pc=0x00007fa3ae68c0c7, pid=28382, tid=0x00007fa281f8e700## JRE version: Java(TM) SE Runtime Environment (8.0_161-b12) (build 1.8.0_161-b12)# Java VM: Java HotSpot(TM) 64-Bit Server VM (25.161-b12 mixed mode linux-amd64 compressed oops)# Problematic frame:# V  [libjvm.so+0xa9b0c7]  Unsafe_GetInt+0x47## Failed to write core dump. Core dumps have been disabled. To enable core dumping, try "ulimit -c unlimited" before starting Java again## An error report file with more information is saved as:# /huawei/xubo/git/carbondata2/hs_err_pid28382.logCompiled method (nm)  106412 8135     n 0       sun.misc.Unsafe::getInt (native) total in heap  [0x00007fa399a84b10,0x00007fa399a84e70] = 864 relocation     [0x00007fa399a84c38,0x00007fa399a84c80] = 72 main code      [0x00007fa399a84c80,0x00007fa399a84e70] = 496Compiled method (nm)  106412 8135     n 0       sun.misc.Unsafe::getInt (native) total in heap  [0x00007fa399a84b10,0x00007fa399a84e70] = 864 relocation     [0x00007fa399a84c38,0x00007fa399a84c80] = 72 main code      [0x00007fa399a84c80,0x00007fa399a84e70] = 496## If you would like to submit a bug report, please visit:#   http://bugreport.java.com/bugreport/crash.jsp# Error 2 It will error that concurrentQueryBenchmark read data from HDFS in clster
issueID:CARBONDATA-238
type:Bug
changed files:
texts:CarbonOptimizer shouldn&#39;t add CarbonDictionaryCatalystDecoder for HiveTable

issueID:CARBONDATA-2380
type:New Feature
changed files:core/src/main/java/org/apache/carbondata/core/datamap/DataMapStoreManager.java
core/src/main/java/org/apache/carbondata/core/util/SessionParams.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
core/src/main/java/org/apache/carbondata/core/datamap/DataMapChooser.java
texts:Support visible/invisible datamap for performance tuning
Invisible datamap will not be used during the query, which can be used to verify whether to remove this datamap in the future. This feature is similar to `Invisible indexed` in mysql (https://dev.mysql.com/doc/refman/8.0/en/invisible-indexes.html).
issueID:CARBONDATA-2381
type:Improvement
changed files:
texts:Improve compaction performance by filling batch result in columnar format and performing IO at blocklet level
Problem: Compaction performance is slow as compared to data load. If compaction threshold is set to 6,6 then on minor compaction after 6 loads compaction performance is almost 6-7 times of the total load performance for 6 loads.Analysis: During compaction result filling is done in row format. Due to this as the number of columns increases the dimension and measure data filling time increases. This happens because in row filling we are not able to take advantage of OS cacheable buffers as we continuously read data for next column. As compaction uses a page level reader flow wherein both IO and uncompression is done at page level, the IO and uncompression time increases in this model.
issueID:CARBONDATA-2384
type:Improvement
changed files:store/sdk/src/main/java/org/apache/carbondata/sdk/file/CarbonReaderBuilder.java
examples/spark2/src/main/java/org/apache/carbondata/examples/sdk/SDKS3Example.java
store/sdk/src/main/java/org/apache/carbondata/sdk/file/CarbonWriterBuilder.java
texts:SDK support write/read data into/from S3

issueID:CARBONDATA-2386
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/statusmanager/SegmentStatusManager.java
core/src/main/java/org/apache/carbondata/core/datamap/Segment.java
texts:Query on Pre-Aggregate table is slower
Problem: Query on pre aggregate table is consuming too much time.Root cause: Time consumption to calculate size of selecting the smallest Pre-Aggregate table is approximately 76 seconds. This is index file is being read when segment file is present to compute the size of Pre-Aggregate table Solution: Read table status and get the size of data file and index file for valid segments. For older segments were datasize and indexsize is not present calculate the size of store folder
issueID:CARBONDATA-2388
type:Sub-task
changed files:processing/src/main/java/org/apache/carbondata/processing/loading/converter/impl/FieldEncoderFactory.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/SortParameters.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/column/CarbonColumn.java
store/sdk/src/main/java/org/apache/carbondata/sdk/file/AvroCarbonWriter.java
processing/src/main/java/org/apache/carbondata/processing/loading/steps/CarbonRowDataWriterProcessorStepImpl.java
processing/src/main/java/org/apache/carbondata/processing/datatypes/PrimitiveDataType.java
core/src/main/java/org/apache/carbondata/core/scan/collector/impl/DictionaryBasedResultCollector.java
processing/src/main/java/org/apache/carbondata/processing/datatypes/StructDataType.java
processing/src/main/java/org/apache/carbondata/processing/datatypes/GenericDataType.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/scan/executor/util/QueryUtil.java
core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactDataHandlerModel.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/SchemaReader.java
store/sdk/src/main/java/org/apache/carbondata/sdk/file/Field.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/TableSchemaBuilder.java
core/src/main/java/org/apache/carbondata/core/scan/result/BlockletScannedResult.java
processing/src/main/java/org/apache/carbondata/processing/util/CarbonDataProcessorUtil.java
processing/src/main/java/org/apache/carbondata/processing/datatypes/ArrayDataType.java
integration/spark2/src/main/scala/org/apache/carbondata/stream/CarbonStreamRecordReader.java
processing/src/main/java/org/apache/carbondata/processing/loading/DataLoadProcessBuilder.java
processing/src/main/java/org/apache/carbondata/processing/store/TablePage.java
core/src/main/java/org/apache/carbondata/core/metadata/datatype/DataTypes.java
core/src/main/java/org/apache/carbondata/core/scan/complextypes/ArrayQueryType.java
processing/src/main/java/org/apache/carbondata/processing/loading/CarbonDataLoadConfiguration.java
hadoop/src/main/java/org/apache/carbondata/hadoop/stream/CarbonStreamInputFormat.java
core/src/main/java/org/apache/carbondata/core/scan/filter/GenericQueryType.java
core/src/main/java/org/apache/carbondata/core/scan/complextypes/StructQueryType.java
processing/src/main/java/org/apache/carbondata/processing/loading/DataField.java
store/sdk/src/main/java/org/apache/carbondata/sdk/file/CarbonWriterBuilder.java
core/src/main/java/org/apache/carbondata/core/scan/complextypes/PrimitiveQueryType.java
texts:Avro Nested Datatype Support
Support AVRO Non Primitive Complex Datatype in Carbon. 
issueID:CARBONDATA-2389
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/readcommitter/TableStatusReadCommittedScope.java
core/src/main/java/org/apache/carbondata/core/datamap/DataMapStoreManager.java
core/src/main/java/org/apache/carbondata/core/datamap/dev/expr/AndDataMapExprWrapper.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
core/src/main/java/org/apache/carbondata/core/datamap/dev/expr/DataMapExprWrapper.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonInputFormat.java
core/src/main/java/org/apache/carbondata/core/readcommitter/ReadCommittedScope.java
hadoop/src/main/java/org/apache/carbondata/hadoop/CarbonRecordReader.java
core/src/main/java/org/apache/carbondata/core/datamap/dev/expr/OrDataMapExprWrapper.java
datamap/lucene/src/main/java/org/apache/carbondata/datamap/lucene/LuceneDataMapFactoryBase.java
core/src/main/java/org/apache/carbondata/core/datamap/Segment.java
core/src/main/java/org/apache/carbondata/core/datamap/dev/expr/DataMapExprWrapperImpl.java
texts:Search mode support lucene datamap
Carbon doesn's support now18/04/23 06:12:14 ERROR CarbonSession: Exception when executing search mode: Error while resolving filter expression, fallback to SparkSQL18/04/23 06:12:14 ERROR CarbonSession: Exception when executing search mode: Error while resolving filter expression, fallback to SparkSQLCarbon should support it.
issueID:CARBONDATA-239
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/stats/QueryStatisticsModel.java
core/src/main/java/org/apache/carbondata/core/stats/QueryStatisticsConstants.java
core/src/main/java/org/apache/carbondata/core/stats/QueryStatisticsRecorderImpl.java
integration/spark-common/src/main/java/org/apache/carbondata/spark/merger/CarbonCompactionUtil.java
core/src/main/java/org/apache/carbondata/core/scan/result/iterator/AbstractDetailQueryResultIterator.java
core/src/main/java/org/apache/carbondata/core/scan/scanner/impl/FilterScanner.java
texts:Failure of one compaction in queue should not affect the others.
Failure of one compaction in queue should not affect the others.If a compaction is triggered by the user on table1 , and other requests will go to queue.  and if the compaction is failed for table1 then the requests in queue should continue and at the end the beeline will show the failure message to the user.if any compaction gets failed for a table which is other than the user requested table then the error in the beeline should not appear.
issueID:CARBONDATA-2390
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/scan/collector/impl/RowIdBasedResultCollector.java
core/src/main/java/org/apache/carbondata/core/scan/executor/infos/BlockExecutionInfo.java
core/src/main/java/org/apache/carbondata/core/scan/collector/ResultCollectorFactory.java
core/src/main/java/org/apache/carbondata/core/scan/result/BlockletScannedResult.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/CarbonTable.java
core/src/main/java/org/apache/carbondata/core/scan/collector/impl/DictionaryBasedResultCollector.java
core/src/main/java/org/apache/carbondata/core/scan/model/QueryModel.java
core/src/main/java/org/apache/carbondata/core/scan/executor/impl/AbstractQueryExecutor.java
datamap/lucene/src/main/java/org/apache/carbondata/datamap/lucene/LuceneDataMapFactoryBase.java
datamap/lucene/src/main/java/org/apache/carbondata/datamap/lucene/LuceneDataMapWriter.java
datamap/lucene/src/main/java/org/apache/carbondata/datamap/lucene/LuceneDataMapRefresher.java
texts:Refresh Lucene data map for the exists table with data
if the table has old data before the creation of the Lucene data map, we should use Refresh command to build data map incrementally.
issueID:CARBONDATA-2391
type:Bug
changed files:processing/src/main/java/org/apache/carbondata/processing/merger/CarbonCompactionExecutor.java
processing/src/main/java/org/apache/carbondata/processing/merger/RowResultMergerProcessor.java
texts:Thread leak in compaction operation if prefetch is enabled and compaction process is killed
ProblemThread leak in compaction operation if prefetch is enabled and compaction process is killedAnalysisDuring compaction if prefetch is enabled RawResultIterator launches an executor service for prefetching the data.If compaction fails or the process is killed it can lead to thread leak due to executor service still in running state.
issueID:CARBONDATA-2392
type:Improvement
changed files:examples/spark2/src/main/java/org/apache/carbondata/examples/sdk/CarbonReaderExample.java
store/sdk/src/main/java/org/apache/carbondata/sdk/file/CarbonReader.java
examples/spark2/src/main/java/org/apache/carbondata/examples/sdk/SDKS3Example.java
texts:Add close method for CarbonReader
CarbonReader haven't close method, it need about one miniute to stop when we invoke the carbonReader
issueID:CARBONDATA-2393
type:Bug
changed files:hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonTableOutputFormat.java
texts:TaskNo is not working for SDK
set Task No. to SDK interface as below val fields: Array&#91;Field&#93; = new Array&#91;Field&#93;(2)fields(0) = new Field("stringField", DataTypes.STRING)fields(1) = new Field("intField", DataTypes.INT)val builder: CarbonWriterBuilder = CarbonWriter.builder.withSchema(new Schema(fields)) .isTransactionalTable(true).outputPath("D:/data/yyy").taskNo("5").isTransactionalTable(false)val writer: CarbonWriter = builder.buildWriterForCSVInputwriter.write(Array("babu","12"));writer.close() check the files index file name is = 328748098884003_batchno0-0-0.carbonindexdata file is =part-0-328748098884003_batchno0-0-0.carbondata Expected index file name is = 5_batchno0-0-0.carbonindexdata file is =part-0-5_batchno0-0-0.carbondata 
issueID:CARBONDATA-2394
type:Bug
changed files:
texts:Setting segments in thread local space but not getting reflected in the driver

issueID:CARBONDATA-2396
type:Improvement
changed files:
texts:Add CTAS support for using DataSource Syntax

issueID:CARBONDATA-24
type:Bug
changed files:
texts:Filter query issue for >, <, <= than filter
1. select count from a12 where dob > '2014-07-01 12:07:28'   throwing runtime exception2. select count from a12 where dob < '2014-07-01 12:07:28'   is including the null values also.3. select count from a12 where dob <=  '2014-07-01 12:07:28'   is including the null value Null should not considered in less than filter.create cube command: create table a12(empid String,ename String,sal double,deptno int,mgr string,gender string," +        "dob timestamp,comm decimal(4,2),desc string) stored by 'org.apache.carbondata.formatdata:empid,ename,sal,deptno,mgr,gender,dob,comm,desc1,abc,1233,10,2,,2014-07-01 12:07:28,1234.191,string_null2,bcd,1322,,3,f,2014-07-01 12:07:28,19.99,int_null3,cde,4322,,4,m,,16.996,date_null4,    ,43243,,5,m,,999.117,string_space5,,43242,20,6,m,2017-07-01 12:07:28,99.999,string_null6,ijk,,20,6,m,2017-07-01 12:07:28,50089,double_null7,pqr,2422,20,6,m,2017-07-01 12:07:28,32.339,decimal_null8
issueID:CARBONDATA-240
type:Improvement
changed files:
texts:Use SQLContext to query CarbonData directly without creating table
When using Spark 1.6, user can use SQLContext to query CarbonData files directly without creating table
issueID:CARBONDATA-2401
type:Bug
changed files:processing/src/main/java/org/apache/carbondata/processing/loading/model/CarbonLoadModelBuilder.java
texts:Date and Timestamp options are not working in SDK
Run Below Program val fields: Array&#91;Field&#93; = new Array&#91;Field&#93;(3) fields(0) = new Field("stringField", DataTypes.STRING) fields(1) = new Field("intField", DataTypes.INT) fields(2) = new Field("mydate", DataTypes.DATE) var loaoption = Map("bad_records_logger_enable" -> "true","bad_records_action" -> "FAIL","bad_record_path" -> "D:/data/bardrecords","dateformat" -> "dd-MM-yyyy").asJava val builder: CarbonWriterBuilder = CarbonWriter.builder.withSchema(new Schema(fields)) .isTransactionalTable(true).outputPath("D:/data/yyy").taskNo("5").isTransactionalTable(false).withLoadOptions(loaoption) val writer: CarbonWriter = builder.buildWriterForCSVInput val startime=System.currentTimeMillis(); writer.write(Array("babu","1","02-01-2002"));writer.close()  Actual:- Data loading is failed Expected :- data loading should be success as date is correct .  
issueID:CARBONDATA-2402
type:Improvement
changed files:
texts:Optimize allocated buffer size while converting object to byte array
Currently in carbondata, while converting object to byte array, it use 8 byte space for int and short datatype, which cause memory wasted.Code can be found in `CarbonUtil.getValueAsBytes` and `DataTypeUtil.getMeasureObjectFromDataType` 
issueID:CARBONDATA-2404
type:Improvement
changed files:
texts:Add documentation for using carbondata and stored as carbondata
Add documentation for using carbondata and stored as carbondata
issueID:CARBONDATA-2406
type:Bug
changed files:
texts:Dictionary Server and Dictionary Client  MD5 Validation failed with hive.server2.enable.doAs = true
With conf hive.server2.enable.doAs = true, the dictionary server is started with the user who submit the load request. But the dictionary client run as the user who started the executor process. Due to this dictionary client can not successfully communicate with the dictionary server.
issueID:CARBONDATA-2407
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/cache/CacheProvider.java
core/src/main/java/org/apache/carbondata/core/scan/executor/impl/AbstractQueryExecutor.java
core/src/main/java/org/apache/carbondata/core/scan/filter/FilterExpressionProcessor.java
core/src/main/java/org/apache/carbondata/core/scan/filter/FilterProcessor.java
texts:Removed All Unused Executor BTree code
After adding datamap, executor btree is not used as driver is loading blocklet information. 
issueID:CARBONDATA-2408
type:Bug
changed files:
texts:Before register to master, the master maybe not finished the start service.
Before register to master, the master maybe not finished the start service.Error1: java.lang.RuntimeException: javax.security.sasl.SaslException: DIGEST-MD5: digest response format violation. Mismatched response. at org.spark_project.guava.base.Throwables.propagate(Throwables.java:160) at org.apache.spark.network.sasl.SparkSaslServer.response(SparkSaslServer.java:122) at org.apache.spark.network.sasl.SaslRpcHandler.receive(SaslRpcHandler.java:101) at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:159) at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:107) at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343) at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336) at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:287) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343) at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336) at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343) at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336) at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343) at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336) at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1294) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343) at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:911) at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131) at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:643) at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:566) at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:480) at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442) at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131) at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144) at java.lang.Thread.run(Thread.java:748)Caused by: javax.security.sasl.SaslException: DIGEST-MD5: digest response format violation. Mismatched response. at com.sun.security.sasl.digest.DigestMD5Server.validateClientResponse(DigestMD5Server.java:627) at com.sun.security.sasl.digest.DigestMD5Server.evaluateResponse(DigestMD5Server.java:244) at org.apache.spark.network.sasl.SparkSaslServer.response(SparkSaslServer.java:120) ... 31 more2018-04-26 19:57:28,801 | WARN  | [task-result-getter-0] | Lost task 0.0 in stage 15.0 (TID 1046, BLR1000014269, executor 6): org.apache.spark.SparkException: Exception thrown in awaitResult at org.apache.spark.rpc.RpcTimeout$$anonfun$1.applyOrElse(RpcTimeout.scala:77) at org.apache.spark.rpc.RpcTimeout$$anonfun$1.applyOrElse(RpcTimeout.scala:75) at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36) at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:59) at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:59) at scala.PartialFunction$OrElse.apply(PartialFunction.scala:167) at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:83) at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:100) at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:108) at org.apache.spark.rpc.Worker$.registerToMaster(Worker.scala:96) at org.apache.spark.rpc.Worker$.init(Worker.scala:45) at org.apache.carbondata.store.SparkCarbonStore$$anonfun$1.apply(SparkCarbonStore.scala:143) at org.apache.carbondata.store.SparkCarbonStore$$anonfun$1.apply(SparkCarbonStore.scala:141) at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:798) at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:798) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87) at org.apache.spark.scheduler.Task.run(Task.scala:99) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:325) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748)Caused by: org.apache.spark.SparkException: Message is dropped because Outbox is stopped at org.apache.spark.rpc.netty.Outbox.stop(Outbox.scala:271) at org.apache.spark.rpc.netty.NettyRpcEnv.removeOutbox(NettyRpcEnv.scala:107) at org.apache.spark.rpc.netty.NettyRpcHandler.channelInactive(NettyRpcEnv.scala:624) at org.apache.spark.network.server.TransportRequestHandler.channelInactive(TransportRequestHandler.java:99) at org.apache.spark.network.server.TransportChannelHandler.channelInactive(TransportChannelHandler.java:103) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:241) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:227) at io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:220) at io.netty.channel.ChannelInboundHandlerAdapter.channelInactive(ChannelInboundHandlerAdapter.java:75) at io.netty.handler.timeout.IdleStateHandler.channelInactive(IdleStateHandler.java:278) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:241) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:227) at io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:220) at io.netty.channel.ChannelInboundHandlerAdapter.channelInactive(ChannelInboundHandlerAdapter.java:75) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:241) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:227) at io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:220) at io.netty.channel.ChannelInboundHandlerAdapter.channelInactive(ChannelInboundHandlerAdapter.java:75) at org.apache.spark.network.util.TransportFrameDecoder.channelInactive(TransportFrameDecoder.java:182) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:241) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:227) at io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:220) at io.netty.channel.DefaultChannelPipeline$HeadContext.channelInactive(DefaultChannelPipeline.java:1289) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:241) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:227) at io.netty.channel.DefaultChannelPipeline.fireChannelInactive(DefaultChannelPipeline.java:893) at io.netty.channel.AbstractChannel$AbstractUnsafe$7.run(AbstractChannel.java:691) at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:399) at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:446) at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131) at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144) ... 1 more | org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)Error2:18/04/27 10:52:43 ERROR Executor: Exception in task 0.0 in stage 3.0 (TID 3)org.apache.spark.SparkException: Exception thrown in awaitResult:  at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:205) at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75) at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:100) at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:108) at org.apache.spark.rpc.Worker$.registerToMaster(Worker.scala:96) at org.apache.spark.rpc.Worker$.init(Worker.scala:45) at org.apache.carbondata.store.SparkCarbonStore$$anonfun$1.apply(SparkCarbonStore.scala:143) at org.apache.carbondata.store.SparkCarbonStore$$anonfun$1.apply(SparkCarbonStore.scala:141) at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797) at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) at org.apache.spark.rdd.RDD.iterator(RDD.scala:287) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87) at org.apache.spark.scheduler.Task.run(Task.scala:108) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748)Caused by: java.io.IOException: Failed to connect to /127.0.0.1:10020 at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:232) at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:182) at org.apache.spark.rpc.netty.NettyRpcEnv.createClient(NettyRpcEnv.scala:197) at org.apache.spark.rpc.netty.Outbox$$anon$1.call(Outbox.scala:194) at org.apache.spark.rpc.netty.Outbox$$anon$1.call(Outbox.scala:190) at java.util.concurrent.FutureTask.run(FutureTask.java:266) ... 3 moreCaused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: /127.0.0.1:10020 at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717) at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:257) at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:291) at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:631) at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:566) at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:480) at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442) at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131) at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144) ... 1 more18/04/27 10:52:43 ERROR TaskSetManager: Task 0 in stage 3.0 failed 1 times; aborting jobException in thread "main" org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 3, localhost, executor driver): org.apache.spark.SparkException: Exception thrown in awaitResult:  at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:205) at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75) at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:100) at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:108) at org.apache.spark.rpc.Worker$.registerToMaster(Worker.scala:96) at org.apache.spark.rpc.Worker$.init(Worker.scala:45) at org.apache.carbondata.store.SparkCarbonStore$$anonfun$1.apply(SparkCarbonStore.scala:143) at org.apache.carbondata.store.SparkCarbonStore$$anonfun$1.apply(SparkCarbonStore.scala:141) at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797) at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) at org.apache.spark.rdd.RDD.iterator(RDD.scala:287) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87) at org.apache.spark.scheduler.Task.run(Task.scala:108) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748)Caused by: java.io.IOException: Failed to connect to /127.0.0.1:10020 at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:232) at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:182) at org.apache.spark.rpc.netty.NettyRpcEnv.createClient(NettyRpcEnv.scala:197) at org.apache.spark.rpc.netty.Outbox$$anon$1.call(Outbox.scala:194) at org.apache.spark.rpc.netty.Outbox$$anon$1.call(Outbox.scala:190) at java.util.concurrent.FutureTask.run(FutureTask.java:266) ... 3 moreCaused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: /127.0.0.1:10020 at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717) at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:257) at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:291) at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:631) at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:566) at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:480) at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442) at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131) at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144) ... 1 moreDriver stacktrace: at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517) at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505) at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504) at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59) at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48) at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504) at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814) at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814) at scala.Option.foreach(Option.scala:257) at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814) at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732) at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687) at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676) at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48) at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630) at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029) at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050) at org.apache.spark.SparkContext.runJob(SparkContext.scala:2069) at org.apache.spark.SparkContext.runJob(SparkContext.scala:2094) at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936) at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151) at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112) at org.apache.spark.rdd.RDD.withScope(RDD.scala:362) at org.apache.spark.rdd.RDD.collect(RDD.scala:935) at org.apache.carbondata.store.SparkCarbonStore.startAllWorkers(SparkCarbonStore.scala:148) at org.apache.carbondata.store.SparkCarbonStore.startSearchMode(SparkCarbonStore.scala:113) at org.apache.spark.sql.CarbonSession.startSearchMode(CarbonSession.scala:186) at org.apache.carbondata.examples.SearchModeExample$.exampleBody(SearchModeExample.scala:79) at org.apache.carbondata.examples.SearchModeExample$.main(SearchModeExample.scala:37) at org.apache.carbondata.examples.SearchModeExample.main(SearchModeExample.scala)Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult:  at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:205) at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75) at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:100) at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:108) at org.apache.spark.rpc.Worker$.registerToMaster(Worker.scala:96) at org.apache.spark.rpc.Worker$.init(Worker.scala:45) at org.apache.carbondata.store.SparkCarbonStore$$anonfun$1.apply(SparkCarbonStore.scala:143) at org.apache.carbondata.store.SparkCarbonStore$$anonfun$1.apply(SparkCarbonStore.scala:141) at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797) at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) at org.apache.spark.rdd.RDD.iterator(RDD.scala:287) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87) at org.apache.spark.scheduler.Task.run(Task.scala:108) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748)Caused by: java.io.IOException: Failed to connect to /127.0.0.1:10020 at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:232) at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:182) at org.apache.spark.rpc.netty.NettyRpcEnv.createClient(NettyRpcEnv.scala:197) at org.apache.spark.rpc.netty.Outbox$$anon$1.call(Outbox.scala:194) at org.apache.spark.rpc.netty.Outbox$$anon$1.call(Outbox.scala:190) at java.util.concurrent.FutureTask.run(FutureTask.java:266) ... 3 moreCaused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: /127.0.0.1:10020 at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717) at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:257) at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:291) at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:631) at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:566) at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:480) at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442) at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131) at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144) ... 1 more
issueID:CARBONDATA-241
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/scan/model/QueryModel.java
core/src/main/java/org/apache/carbondata/core/scan/executor/impl/AbstractQueryExecutor.java
core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
texts:OOM error during query execution in long run
*Problem:* During long run query execution is taking more time and it is throwing out of memory issue.*Reason*: In compaction we are compacting segments and each segment metadata is loaded in memory. So after compaction compacted segments are invalid but its meta data is not removed from memory because of this duplicate metadata is pile up and it is taking more memory and after few days query exeution is throwing OOM*Solution*: Need to remove invalid blocks from memory
issueID:CARBONDATA-2410
type:Bug
changed files:processing/src/main/java/org/apache/carbondata/processing/util/CarbonDataProcessorUtil.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
processing/src/main/java/org/apache/carbondata/processing/loading/csvinput/CSVInputFormat.java
texts:Error message correction when column value length exceeds 320000 charactor

issueID:CARBONDATA-2411
type:Bug
changed files:store/sdk/src/main/java/org/apache/carbondata/sdk/file/AvroCarbonWriter.java
store/sdk/src/main/java/org/apache/carbondata/sdk/file/CSVCarbonWriter.java
store/sdk/src/main/java/org/apache/carbondata/sdk/file/CarbonWriterBuilder.java
texts:infinite loop when sdk writer throws Exception
When SDK CSVWriter throws Error (Cast String Exception). application is stuck and data loading Threads are in waiting state. Use below code to reproduce issue. val fields: Array&#91;Field&#93; = new Array&#91;Field&#93;(2) fields(0) = new Field("stringField", DataTypes.STRING) fields(1) = new Field("intField", DataTypes.INT)val builder: CarbonWriterBuilder = CarbonWriter.builder.withSchema(new Schema(fields)) .isTransactionalTable(false).outputPath("D:/data/yyy").taskNo("5").isTransactionalTable(false) val writer: CarbonWriter = builder.buildWriterForCSVInputwriter.write(Array("xyz",1))writer.close() 
issueID:CARBONDATA-2413
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/datastore/chunk/reader/MeasureColumnChunkReader.java
core/src/main/java/org/apache/carbondata/core/scan/result/iterator/RawResultIterator.java
store/sdk/src/main/java/org/apache/carbondata/sdk/file/CarbonWriterBuilder.java
processing/src/main/java/org/apache/carbondata/processing/loading/DataLoadProcessBuilder.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/DiskBasedDMSchemaStorageProvider.java
core/src/main/java/org/apache/carbondata/core/scan/filter/FilterUtil.java
processing/src/main/java/org/apache/carbondata/processing/loading/model/CarbonLoadModel.java
texts:After running CarbonWriter, there is null directory about datamap
After running CarbonWriter, there is null directory:root@ecs-909c:/huawei/xubo/git/carbondata2/null/_system# lsdatamap.mdtfileroot@ecs-909c:/huawei/xubo/git/carbondata2/null/_system# cd ../../root@ecs-909c:/huawei/xubo/git/carbondata2# git statusOn branch CARBONDATA-2392-CarbonReaderCloseUntracked files:  (use "git add <file>..." to include in what will be committed)        null/Another error: java.io.IOException: No such file or directory at java.io.UnixFileSystem.createFileExclusively(Native Method) at java.io.File.createNewFile(File.java:1012) at org.apache.carbondata.core.datastore.filesystem.LocalCarbonFile.createNewFile(LocalCarbonFile.java:416) at org.apache.carbondata.core.datastore.impl.FileFactory.createNewFile(FileFactory.java:221) at org.apache.carbondata.core.metadata.schema.table.DiskBasedDMSchemaStorageProvider.touchMDTFile(DiskBasedDMSchemaStorageProvider.java:198) at org.apache.carbondata.core.metadata.schema.table.DiskBasedDMSchemaStorageProvider.checkAndReloadDataMapSchemas(DiskBasedDMSchemaStorageProvider.java:186) at org.apache.carbondata.core.metadata.schema.table.DiskBasedDMSchemaStorageProvider.retrieveSchemas(DiskBasedDMSchemaStorageProvider.java:106) at org.apache.carbondata.core.datamap.DataMapStoreManager.getDataMapSchemasOfTable(DataMapStoreManager.java:121) at org.apache.carbondata.core.datamap.DataMapStoreManager.getAllDataMap(DataMapStoreManager.java:101) at org.apache.carbondata.processing.datamap.DataMapWriterListener.registerAllWriter(DataMapWriterListener.java:57) at org.apache.carbondata.processing.store.CarbonFactDataHandlerModel.createCarbonFactDataHandlerModel(CarbonFactDataHandlerModel.java:264) at org.apache.carbondata.processing.loading.steps.DataWriterProcessorStepImpl.processRange(DataWriterProcessorStepImpl.java:166) at org.apache.carbondata.processing.loading.steps.DataWriterProcessorStepImpl.access$100(DataWriterProcessorStepImpl.java:53) at org.apache.carbondata.processing.loading.steps.DataWriterProcessorStepImpl$WriterForwarder.call(DataWriterProcessorStepImpl.java:156) at org.apache.carbondata.processing.loading.steps.DataWriterProcessorStepImpl$WriterForwarder.call(DataWriterProcessorStepImpl.java:141) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745)2018-05-04 10:23:57,353 ERROR [org.apache.carbondata.common.logging.impl.StandardLogService.logErrorMessage(StandardLogService.java:143)] CarbonRecordWriter:_tempTable Failed for table: _tempTable in DataWriterProcessorStepImplorg.apache.carbondata.core.datastore.exception.CarbonDataWriterException:  at org.apache.carbondata.processing.loading.steps.DataWriterProcessorStepImpl.execute(DataWriterProcessorStepImpl.java:121) at org.apache.carbondata.processing.loading.DataLoadExecutor.execute(DataLoadExecutor.java:51) at org.apache.carbondata.hadoop.api.CarbonTableOutputFormat$1.run(CarbonTableOutputFormat.java:251) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745)Caused by: java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.io.IOException: No such file or directory at java.util.concurrent.FutureTask.report(FutureTask.java:122) at java.util.concurrent.FutureTask.get(FutureTask.java:192) at org.apache.carbondata.processing.loading.steps.DataWriterProcessorStepImpl.execute(DataWriterProcessorStepImpl.java:118) ... 7 moreCaused by: java.lang.RuntimeException: java.io.IOException: No such file or directory at org.apache.carbondata.processing.datamap.DataMapWriterListener.registerAllWriter(DataMapWriterListener.java:60) at org.apache.carbondata.processing.store.CarbonFactDataHandlerModel.createCarbonFactDataHandlerModel(CarbonFactDataHandlerModel.java:264) at org.apache.carbondata.processing.loading.steps.DataWriterProcessorStepImpl.processRange(DataWriterProcessorStepImpl.java:166) at org.apache.carbondata.processing.loading.steps.DataWriterProcessorStepImpl.access$100(DataWriterProcessorStepImpl.java:53) at org.apache.carbondata.processing.loading.steps.DataWriterProcessorStepImpl$WriterForwarder.call(DataWriterProcessorStepImpl.java:156) at org.apache.carbondata.processing.loading.steps.DataWriterProcessorStepImpl$WriterForwarder.call(DataWriterProcessorStepImpl.java:141) ... 4 moreCaused by: java.io.IOException: No such file or directory at java.io.UnixFileSystem.createFileExclusively(Native Method) at java.io.File.createNewFile(File.java:1012) at org.apache.carbondata.core.datastore.filesystem.LocalCarbonFile.createNewFile(LocalCarbonFile.java:416) at org.apache.carbondata.core.datastore.impl.FileFactory.createNewFile(FileFactory.java:221) at org.apache.carbondata.core.metadata.schema.table.DiskBasedDMSchemaStorageProvider.touchMDTFile(DiskBasedDMSchemaStorageProvider.java:198) at org.apache.carbondata.core.metadata.schema.table.DiskBasedDMSchemaStorageProvider.checkAndReloadDataMapSchemas(DiskBasedDMSchemaStorageProvider.java:186) at org.apache.carbondata.core.metadata.schema.table.DiskBasedDMSchemaStorageProvider.retrieveSchemas(DiskBasedDMSchemaStorageProvider.java:106) at org.apache.carbondata.core.datamap.DataMapStoreManager.getDataMapSchemasOfTable(DataMapStoreManager.java:121) at org.apache.carbondata.core.datamap.DataMapStoreManager.getAllDataMap(DataMapStoreManager.java:101) at org.apache.carbondata.processing.datamap.DataMapWriterListener.registerAllWriter(DataMapWriterListener.java:57) ... 9 more
issueID:CARBONDATA-2414
type:Improvement
changed files:
texts:Optimize documents for sort_column_bounds
Optimize documents for sort column bounds for better use
issueID:CARBONDATA-2415
type:New Feature
changed files:core/src/main/java/org/apache/carbondata/core/datamap/DataMapStoreManager.java
processing/src/main/java/org/apache/carbondata/processing/loading/steps/CarbonRowDataWriterProcessorStepImpl.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockletDataMapFactory.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/datamap/DataMapClassProvider.java
integration/spark2/src/main/java/org/apache/carbondata/datamap/DataMapManager.java
datamap/examples/src/minmaxdatamap/main/java/org/apache/carbondata/datamap/examples/MinMaxIndexDataMapFactory.java
core/src/main/java/org/apache/carbondata/core/datamap/status/DataMapStatusManager.java
core/src/main/java/org/apache/carbondata/core/scan/scanner/impl/BlockletFilterScanner.java
core/src/main/java/org/apache/carbondata/core/datamap/dev/DataMapBuilder.java
datamap/bloom/src/main/java/org/apache/carbondata/datamap/bloom/BloomCoarseGrainDataMap.java
integration/spark2/src/main/java/org/apache/carbondata/datamap/IndexDataMapProvider.java
core/src/main/java/org/apache/carbondata/core/util/path/CarbonTablePath.java
datamap/bloom/src/main/java/org/apache/carbondata/datamap/bloom/BloomCoarseGrainDataMapFactory.java
datamap/lucene/src/main/java/org/apache/carbondata/datamap/lucene/LuceneFineGrainDataMap.java
core/src/main/java/org/apache/carbondata/core/indexstore/Blocklet.java
datamap/lucene/src/main/java/org/apache/carbondata/datamap/lucene/LuceneDataMapBuilder.java
datamap/lucene/src/main/java/org/apache/carbondata/datamap/lucene/LuceneDataMapFactoryBase.java
core/src/main/java/org/apache/carbondata/core/datamap/DataMapRegistry.java
core/src/main/java/org/apache/carbondata/core/datamap/DataMapMeta.java
datamap/lucene/src/main/java/org/apache/carbondata/datamap/lucene/LuceneDataMapWriter.java
processing/src/main/java/org/apache/carbondata/processing/datamap/DataMapWriterException.java
datamap/bloom/src/main/java/org/apache/carbondata/datamap/bloom/BloomDataMapWriter.java
core/src/main/java/org/apache/carbondata/core/datamap/dev/fgdatamap/FineGrainBlocklet.java
core/src/main/java/org/apache/carbondata/core/datamap/dev/fgdatamap/FineGrainDataMapFactory.java
datamap/examples/src/minmaxdatamap/main/java/org/apache/carbondata/datamap/examples/MinMaxDataWriter.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/CarbonTable.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonInputFormat.java
processing/src/main/java/org/apache/carbondata/processing/store/writer/AbstractFactDataWriter.java
datamap/bloom/src/main/java/org/apache/carbondata/datamap/bloom/BloomDataMapBuilder.java
core/src/main/java/org/apache/carbondata/core/datamap/Segment.java
core/src/main/java/org/apache/carbondata/core/datamap/dev/expr/DataMapExprWrapperImpl.java
processing/src/main/java/org/apache/carbondata/processing/datamap/DataMapWriterListener.java
processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactDataHandlerModel.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
core/src/main/java/org/apache/carbondata/core/datamap/dev/cgdatamap/CoarseGrainDataMapFactory.java
datamap/lucene/src/main/java/org/apache/carbondata/datamap/lucene/LuceneFineGrainDataMapFactory.java
core/src/main/java/org/apache/carbondata/core/datamap/dev/DataMapFactory.java
core/src/main/java/org/apache/carbondata/core/datamap/TableDataMap.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/DataMapSchema.java
core/src/main/java/org/apache/carbondata/core/datamap/DataMapChooser.java
core/src/main/java/org/apache/carbondata/core/indexstore/ExtendedBlocklet.java
core/src/main/java/org/apache/carbondata/core/datamap/dev/DataMapWriter.java
core/src/main/java/org/apache/carbondata/core/datamap/DataMapProvider.java
texts:All DataMap should support REFRESH command
Currently, only LuceneDataMap supports REFRESH command. If user create a BloomFilter DataMap, and trigger REFRESH DATAMAP, it is ignoring the command. The makes the datamap usage a lot of confusion.
issueID:CARBONDATA-2416
type:New Feature
changed files:core/src/main/java/org/apache/carbondata/core/datamap/DataMapStoreManager.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockletDataMapFactory.java
datamap/examples/src/minmaxdatamap/main/java/org/apache/carbondata/datamap/examples/MinMaxIndexDataMapFactory.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonInputFormat.java
core/src/main/java/org/apache/carbondata/core/datamap/status/DataMapStatusDetail.java
core/src/main/java/org/apache/carbondata/core/datamap/status/DataMapStatusManager.java
core/src/main/java/org/apache/carbondata/core/datamap/dev/DataMapBuilder.java
datamap/lucene/src/main/java/org/apache/carbondata/datamap/lucene/LuceneDataMapBuilder.java
integration/spark2/src/main/java/org/apache/carbondata/datamap/IndexDataMapProvider.java
processing/src/main/java/org/apache/carbondata/processing/datamap/DataMapWriterListener.java
datamap/bloom/src/main/java/org/apache/carbondata/datamap/bloom/BloomCoarseGrainDataMapFactory.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/datamap/DataMapProperty.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonOutputCommitter.java
core/src/main/java/org/apache/carbondata/core/datamap/dev/DataMapFactory.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/DataMapSchema.java
datamap/bloom/src/main/java/org/apache/carbondata/datamap/bloom/BloomDataMapBuilder.java
datamap/lucene/src/main/java/org/apache/carbondata/datamap/lucene/LuceneDataMapFactoryBase.java
core/src/main/java/org/apache/carbondata/core/datamap/DataMapChooser.java
core/src/main/java/org/apache/carbondata/core/datamap/DataMapProvider.java
texts:Index DataMap should support immediate load and deferred load when creating the DataMap
For 'preaggregate' and 'timeseries' datamap, carbon is loading the datamap as soon as user creates it. But for 'lucene' and 'bloomfilter' it is not. This behavior should be aligned, otherwise user will be confused. A better option is that when creating datamap, let user to choose whether load the datamap immediately or deferred (manually refresh later)
issueID:CARBONDATA-2417
type:Bug
changed files:processing/src/main/java/org/apache/carbondata/processing/loading/iterator/CarbonOutputIteratorWrapper.java
store/sdk/src/main/java/org/apache/carbondata/sdk/file/CarbonWriter.java
store/sdk/src/main/java/org/apache/carbondata/sdk/file/CSVCarbonWriter.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonTableOutputFormat.java
texts:SDK writer goes to infinite wait when consumer thread goes dead
problem: SDK writer goes to infinite wait when cosumer thread is deadroot cause: due to bad record when exception happens at consumer thread during write, this message  is not reached producer (sdk writer).So, SDK keeps writing data assuming consumer will consume it. But as consumer is dead. Queue becomes full and queue.put() will be blocked forever.Solution: If cannot be added to queue, check whether consumer is alive or not after every 10 seconds. If not alive throw exception, if alive try again
issueID:CARBONDATA-2418
type:Bug
changed files:integration/presto/src/main/java/org/apache/carbondata/presto/impl/CarbonTableConfig.java
integration/presto/src/main/java/org/apache/carbondata/presto/impl/CarbonTableReader.java
texts:Presto can&#39;t query Carbon table when carbonstore is created at s3
anubhav@anubhav-Vostro-3559:~/Downloads/softwares/presto-server-0.187$ ./presto-cli-0.187-executable.jar --server localhost:9000 --catalog carbondata --schema defaultpresto:default> show tables;Query 20180430_084314_00002_xpaf9 failed: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not foundpresto:default> show tables;Query 20180430_084815_00003_xpaf9 failed: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not foundpresto:default>
issueID:CARBONDATA-2419
type:Bug
changed files:store/sdk/src/main/java/org/apache/carbondata/sdk/file/CarbonWriterBuilder.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/TableSchemaBuilder.java
texts:sortColumns Order we are getting wrong as we set for external table is fixed

issueID:CARBONDATA-242
type:Bug
changed files:
texts:NOT IN with Null filter results are not compatible With Hive
When user provides Null member inside NOT IN filter condition the resultset is not compatible with hive result.Eg: select country from t3 where country not in ('china','france',null) group by country
issueID:CARBONDATA-2420
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/metadata/blocklet/BlockletInfo.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/SortParameters.java
core/src/main/java/org/apache/carbondata/core/datastore/page/VarLengthColumnPageBase.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/store/impl/safe/SafeVariableShortLengthDimensionDataChunkStore.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/reader/dimension/v3/DimensionChunkReaderV3.java
core/src/main/java/org/apache/carbondata/core/metadata/datatype/DataType.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/store/impl/safe/SafeVariableLengthDimensionDataChunkStore.java
core/src/main/java/org/apache/carbondata/core/util/DataTypeUtil.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/store/DimensionChunkStoreFactory.java
core/src/main/java/org/apache/carbondata/core/metadata/encoder/Encoding.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/compress/DirectCompressCodec.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/SortStepRowHandler.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/store/impl/unsafe/UnsafeVariableLengthDimensionDataChunkStore.java
core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
core/src/main/java/org/apache/carbondata/core/datastore/page/statistics/LVStringStatsCollector.java
core/src/main/java/org/apache/carbondata/core/indexstore/schema/CarbonRowSchema.java
core/src/main/java/org/apache/carbondata/core/datastore/page/ColumnPage.java
processing/src/main/java/org/apache/carbondata/processing/loading/csvinput/CSVInputFormat.java
core/src/main/java/org/apache/carbondata/core/indexstore/row/DataMapRow.java
processing/src/main/java/org/apache/carbondata/processing/store/TablePage.java
core/src/main/java/org/apache/carbondata/core/metadata/datatype/DataTypes.java
processing/src/main/java/org/apache/carbondata/processing/merger/CompactionResultSortProcessor.java
core/src/main/java/org/apache/carbondata/core/util/AbstractDataFileFooterConverter.java
core/src/main/java/org/apache/carbondata/core/metadata/datatype/VarcharType.java
processing/src/main/java/org/apache/carbondata/processing/loading/converter/impl/NonDictionaryFieldConverterImpl.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/dimension/legacy/HighCardDictDimensionIndexCodec.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/impl/VariableLengthDimensionColumnPage.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/store/impl/unsafe/UnsafeVariableIntLengthDimensionDataChunkStore.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockletDataMap.java
core/src/main/java/org/apache/carbondata/core/indexstore/UnsafeMemoryDMStore.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/impl/FixedLengthDimensionColumnPage.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/TableFieldStat.java
processing/src/main/java/org/apache/carbondata/processing/loading/row/IntermediateSortTempRow.java
core/src/main/java/org/apache/carbondata/core/datastore/page/statistics/LVShortStringStatsCollector.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/store/impl/unsafe/UnsafeVariableShortLengthDimensionDataChunkStore.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/TableSchemaBuilder.java
processing/src/main/java/org/apache/carbondata/processing/util/CarbonDataProcessorUtil.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/EncodingFactory.java
core/src/main/java/org/apache/carbondata/core/indexstore/row/UnsafeDataMapRow.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/store/impl/safe/SafeVariableIntLengthDimensionDataChunkStore.java
core/src/main/java/org/apache/carbondata/core/datastore/page/statistics/LVLongStringStatsCollector.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/DefaultEncodingFactory.java
core/src/main/java/org/apache/carbondata/core/metadata/converter/ThriftWrapperSchemaConverterImpl.java
texts:Support string longer than 32000 characters
Add a property in creating table 'long_string_columns' to support string columns that will contains more than 32000 characters.Inside carbondata, it use an integer instead of short to store the length of bytes content.
issueID:CARBONDATA-2421
type:Bug
changed files:
texts:Remove unused CarbonGetTableDetailCommand
CarbonGetTableDetailCommand is unused, better to remove it
issueID:CARBONDATA-2422
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
texts:Search mode Master port should be dynamic

issueID:CARBONDATA-2423
type:Sub-task
changed files:store/sdk/src/main/java/org/apache/carbondata/sdk/file/CarbonReaderBuilder.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonFileInputFormat.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/CarbonTable.java
texts:CarbonReader Support To Read Non Transactional Table
CarbonReader Support To Read Non Transactional Table
issueID:CARBONDATA-2424
type:Task
changed files:
texts:Add documentation for properties of Pre-Aggregate tables

issueID:CARBONDATA-2426
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/metadata/SegmentFileStore.java
texts:IOException after compaction on Pre-Aggregate table on Partition table

issueID:CARBONDATA-2427
type:Bug
changed files:
texts:Fix SearchMode Serialization Issue during Load

issueID:CARBONDATA-2428
type:Improvement
changed files:processing/src/main/java/org/apache/carbondata/processing/merger/RowResultMergerProcessor.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockletDataMapFactory.java
core/src/main/java/org/apache/carbondata/core/scan/scanner/impl/BlockletFullScanner.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/CarbonTable.java
core/src/main/java/org/apache/carbondata/core/mutate/CarbonUpdateUtil.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonInputFormat.java
processing/src/main/java/org/apache/carbondata/processing/loading/AbstractDataLoadProcessorStep.java
processing/src/main/java/org/apache/carbondata/processing/store/writer/AbstractFactDataWriter.java
core/src/main/java/org/apache/carbondata/core/scan/executor/impl/AbstractQueryExecutor.java
core/src/main/java/org/apache/carbondata/core/datastore/block/TableBlockInfo.java
processing/src/main/java/org/apache/carbondata/processing/util/CarbonLoaderUtil.java
core/src/main/java/org/apache/carbondata/core/datamap/Segment.java
core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
processing/src/main/java/org/apache/carbondata/processing/datamap/DataMapWriterListener.java
processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactDataHandlerModel.java
core/src/main/java/org/apache/carbondata/core/util/path/CarbonTablePath.java
core/src/main/java/org/apache/carbondata/core/scan/result/BlockletScannedResult.java
datamap/bloom/src/main/java/org/apache/carbondata/datamap/bloom/BloomCoarseGrainDataMapFactory.java
core/src/main/java/org/apache/carbondata/core/metadata/SegmentFileStore.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonTableInputFormat.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
core/src/main/java/org/apache/carbondata/hadoop/CarbonInputSplit.java
streaming/src/main/java/org/apache/carbondata/streaming/CarbonStreamRecordWriter.java
core/src/main/java/org/apache/carbondata/core/datamap/TableDataMap.java
processing/src/main/java/org/apache/carbondata/processing/merger/CompactionResultSortProcessor.java
datamap/lucene/src/main/java/org/apache/carbondata/datamap/lucene/LuceneDataMapFactoryBase.java
processing/src/main/java/org/apache/carbondata/processing/merger/AbstractResultProcessor.java
processing/src/main/java/org/apache/carbondata/processing/loading/model/CarbonLoadModel.java
core/src/main/java/org/apache/carbondata/core/datamap/dev/DataMapWriter.java
processing/src/main/java/org/apache/carbondata/processing/loading/TableProcessingOperations.java
texts:Support Flat folder structure in carbon.
1. Flat folder makes all carbondata files store flat under table path.2. It is controlled through table property `flat_folder`. By default it is false.3. It cannot be hybrid, so user cannot change the property once table created.4. Segment file is created for each loading.And segment file is created under MetaData folder under table path.5. Segment number is added as part of carbondata and index files.6. All datamap files now create directly under table path with <tablepath>/<dmname>/<segment_number>/<task_name>/dmIUD : It supports but list files during IUD may hit performance.Compaction: Supports Delete Segment : No impactClean files : No impactAlter table : No impactPre Agg : Property need to inherited to child, so it also supports flat folder structure.Partition : No Impact on this feature as it already has flat folder structure.Streaming : Only during handoff it supports flat folder structure. Streaming segment location is no change. 
issueID:CARBONDATA-2430
type:Sub-task
changed files:store/sdk/src/main/java/org/apache/carbondata/sdk/file/Field.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/TableSchemaBuilder.java
store/sdk/src/main/java/org/apache/carbondata/sdk/file/AvroCarbonWriter.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/CarbonTable.java
core/src/main/java/org/apache/carbondata/core/metadata/datatype/StructField.java
core/src/main/java/org/apache/carbondata/core/metadata/datatype/ArrayType.java
store/sdk/src/main/java/org/apache/carbondata/sdk/file/CarbonWriterBuilder.java
texts:Reshuffling of Columns given by user in SDK
Reshuffling of Columns given by user in SDK. Order should be Sort COlumns -> Dimension -> Complex -> Measure
issueID:CARBONDATA-2431
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/readcommitter/TableStatusReadCommittedScope.java
core/src/main/java/org/apache/carbondata/core/readcommitter/ReadCommittedIndexFileSnapShot.java
core/src/main/java/org/apache/carbondata/core/datamap/DataMapStoreManager.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonTableInputFormat.java
core/src/main/java/org/apache/carbondata/core/statusmanager/SegmentRefreshInfo.java
core/src/main/java/org/apache/carbondata/core/readcommitter/ReadCommittedScope.java
core/src/main/java/org/apache/carbondata/core/datamap/Segment.java
texts:Incremental data added after table creation is not reflecting while doing select query.
steps to reproduce : 1: Write a carbon data file which is having 10 records using SDK and upload to HDFS. Create an external table with this location  2: execute the select query and observe that 10 records are written. 3: Again write a new file in same folder with 10 records and move it to HDFS in same folder. 4: Now again execute the select query and observe that only 10 records are returned instaed of 20. 5: Create a new external table on same location and observe that all 20 records are returned.
issueID:CARBONDATA-2432
type:Bug
changed files:
texts:BloomFilter DataMap should be contained in carbon assembly jar
Currently after build, the generated carbondata assembly jar does not contain bloomfilter datamap.
issueID:CARBONDATA-2433
type:Sub-task
changed files:core/src/main/java/org/apache/carbondata/core/datamap/AbstractDataMapJob.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockletDataMap.java
datamap/lucene/src/main/java/org/apache/carbondata/datamap/lucene/LuceneFineGrainDataMap.java
core/src/main/java/org/apache/carbondata/core/datamap/dev/DataMap.java
core/src/main/java/org/apache/carbondata/core/datamap/TableDataMap.java
datamap/examples/src/minmaxdatamap/main/java/org/apache/carbondata/datamap/examples/MinMaxIndexDataMap.java
datamap/bloom/src/main/java/org/apache/carbondata/datamap/bloom/BloomCoarseGrainDataMap.java
datamap/lucene/src/main/java/org/apache/carbondata/datamap/lucene/LuceneDataMapWriter.java
core/src/main/java/org/apache/carbondata/core/datamap/DistributableDataMapFormat.java
texts:Executor OOM because of GC when blocklet pruning is done using Lucene datamap
While seraching using lucene it creates a PriorityQueue to hold the documents. As size is not specified by default the PriorityQueue size is equal to the number of lucene documents. As the docuemnts start getting added to the heap the GC time increases and after some time task fails due to excessive GC and executor OOM occurs.Reference blog:*http://lucene.472066.n3.nabble.com/Optimization-of-memory-usage-in-PriorityQueue-td590355.html*
issueID:CARBONDATA-2434
type:Task
changed files:
texts:Add ExternalTableExample and LuceneDataMapExample
Add ExternalTableExample and LuceneDataMapExample
issueID:CARBONDATA-2435
type:Bug
changed files:processing/src/main/java/org/apache/carbondata/processing/loading/converter/impl/FieldEncoderFactory.java
processing/src/main/java/org/apache/carbondata/processing/loading/steps/InputProcessorStepWithNoConverterImpl.java
processing/src/main/java/org/apache/carbondata/processing/datatypes/PrimitiveDataType.java
processing/src/main/java/org/apache/carbondata/processing/loading/converter/impl/RowConverterImpl.java
texts:SDK dependency Spark jar
SDK dependency Spark jar, we should remove it.java.lang.ClassNotFoundException: org.apache.spark.util.SizeEstimator at org.apache.catalina.loader.WebappClassLoaderBase.loadClass(WebappClassLoaderBase.java:1291) at org.apache.catalina.loader.WebappClassLoaderBase.loadClass(WebappClassLoaderBase.java:1119) at java.lang.Class.forName0(Native Method) at java.lang.Class.forName(Class.java:264) at org.apache.carbondata.core.util.ObjectSizeCalculator.estimate(ObjectSizeCalculator.java:56) at org.apache.carbondata.core.cache.dictionary.ReverseDictionaryCache.<clinit>(ReverseDictionaryCache.java:53) at org.apache.carbondata.core.cache.CacheProvider.createDictionaryCacheForGivenType(CacheProvider.java:115) at org.apache.carbondata.core.cache.CacheProvider.createCache(CacheProvider.java:100) at org.apache.carbondata.processing.loading.converter.impl.RowConverterImpl.initialize(RowConverterImpl.java:89) at org.apache.carbondata.processing.loading.steps.DataConverterProcessorStepImpl.initialize(DataConverterProcessorStepImpl.java:81) at org.apache.carbondata.processing.loading.steps.SortProcessorStepImpl.initialize(SortProcessorStepImpl.java:53) at org.apache.carbondata.processing.loading.steps.DataWriterProcessorStepImpl.initialize(DataWriterProcessorStepImpl.java:75) at org.apache.carbondata.processing.loading.DataLoadExecutor.execute(DataLoadExecutor.java:48) at org.apache.carbondata.hadoop.api.CarbonTableOutputFormat$1.run(CarbonTableOutputFormat.java:251) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
issueID:CARBONDATA-2436
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockletDataMap.java
texts:Block pruning problem post the carbon schema restructure.
Currently datamap is pruning with segmentproperties from the 0th blcok of BlockletDataMap is not correct. As post restructure if the table is updated then all the block will not have symetric schema within the same segments.Fix: It must be ensured the block could be pruned with the same schema.
issueID:CARBONDATA-2437
type:Bug
changed files:processing/src/main/java/org/apache/carbondata/processing/datatypes/StructDataType.java
processing/src/main/java/org/apache/carbondata/processing/datatypes/ArrayDataType.java
processing/src/main/java/org/apache/carbondata/processing/datatypes/PrimitiveDataType.java
hadoop/src/main/java/org/apache/carbondata/hadoop/stream/CarbonStreamInputFormat.java
texts:Complex Type data loading is failing is for null values
Problem:Complex Type data loading is failing is for null values Rootcause: This is failing because for null values Primitive type it is not writing length and during converting to columnar format it is throwing BufferUnderFlowExceptionSolution:Write null values in LV formatAdded code to support No Dictionary for complex type column, now default complex type column will be No dictionary
issueID:CARBONDATA-2438
type:Bug
changed files:
texts:Remove spark/hadoop related classes in carbon assembly
Currently, carbon-assembly contains spark/hadoop related classes which should be removed in case of class conflicts.
issueID:CARBONDATA-2439
type:Bug
changed files:
texts:Update guava version for bloom datamap
Spark2.1 and spark2.2 provide guava version 14.0.1, we should use this version instead of hadoop guava version 11.0.2.
issueID:CARBONDATA-244
type:Bug
changed files:integration/spark-common/src/main/java/org/apache/carbondata/spark/load/CarbonLoaderUtil.java
texts:Load and delete segment by id queries giving inconsistent results when we execute parallely
Delete segment by id behavior is inconsistent when  we Execute load and delete segment by id queries parallely,
issueID:CARBONDATA-2440
type:Bug
changed files:
texts:In SDK user can not specified the Unsafe memory , so it should take complete from Heap , and it should not be sorted using unsafe.

issueID:CARBONDATA-2441
type:Bug
changed files:datamap/bloom/src/main/java/org/apache/carbondata/datamap/bloom/BloomDataMapDistributable.java
datamap/bloom/src/main/java/org/apache/carbondata/datamap/bloom/BloomCoarseGrainDataMapFactory.java
texts:Implement distribute interface for bloom datamap
Implement distribute interface for bloom datamap to accelerate blocklet pruning.
issueID:CARBONDATA-2442
type:Bug
changed files:hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonTableInputFormat.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/column/ColumnSchema.java
core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
texts:Reading two sdk writer output with differnt schema should prompt exception

issueID:CARBONDATA-2443
type:Sub-task
changed files:processing/src/main/java/org/apache/carbondata/processing/loading/converter/impl/FieldEncoderFactory.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/TableSchemaBuilder.java
processing/src/main/java/org/apache/carbondata/processing/util/CarbonDataProcessorUtil.java
store/sdk/src/main/java/org/apache/carbondata/sdk/file/AvroCarbonWriter.java
processing/src/main/java/org/apache/carbondata/processing/loading/steps/InputProcessorStepWithNoConverterImpl.java
processing/src/main/java/org/apache/carbondata/processing/loading/DataLoadProcessBuilder.java
processing/src/main/java/org/apache/carbondata/processing/loading/model/CarbonLoadModel.java
store/sdk/src/main/java/org/apache/carbondata/sdk/file/CarbonWriterBuilder.java
texts:Multi Level Complex Type Support for AVRO SDK
Multi Level Complex Type Support for AVRO SDK
issueID:CARBONDATA-2446
type:Bug
changed files:
texts:load data from parquet table always failed
 I have a parquet table  and a carbon table. This table have 1 billion rows.parquet table :============CREATE TABLE mc_idx3(COL_1 integer,COL_2  integer,COL_3  string,COL_4  integer,COL_5  string,COL_6  string,COL_7   string,COL_8   string,COL_9   integer,COL_10 long,COL_11 string,COL_12 string,COL_13 string,COL_14 string,COL_15 integer,COL_16 string,COL_17 Timestamp )STORED AS PARQUET;==============carbon table:===============CREATE TABLE mc_idxok_cd1(COL_1 integer,COL_2  integer,COL_3  string,COL_4  integer,COL_5  string,COL_6  string,COL_7   string,COL_8   string,COL_9   integer,COL_10 long,COL_11 string,COL_12 string,COL_13 string,COL_14 string,COL_15 integer,COL_16 string,COL_17 Timestamp )STORED BY 'carbondata'TBLPROPERTIES ('SORT_COLUMNS'='COL_17,COL_1');=============when I using insert into table mc_idxok_cd1 select * from mc_idx3.It always failed.ERROR LOG:org.apache.carbondata.processing.loading.exception.CarbonDataLoadingException: There is an unexpected error: org.apache.carbondata.core.datastore.exception.CarbonDataWriterException: Problem while copying file from local store to carbon storeat org.apache.carbondata.processing.loading.steps.DataWriterProcessorStepImpl.execute(DataWriterProcessorStepImpl.java:123)at org.apache.carbondata.processing.loading.DataLoadExecutor.execute(DataLoadExecutor.java:51)at org.apache.carbondata.spark.rdd.NewDataFrameLoaderRDD$$anon$2.<init>(NewCarbonDataLoadRDD.scala:390)at org.apache.carbondata.spark.rdd.NewDataFrameLoaderRDD.internalCompute(NewCarbonDataLoadRDD.scala:353)at org.apache.carbondata.spark.rdd.CarbonRDD.compute(CarbonRDD.scala:60)at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)at org.apache.spark.scheduler.Task.run(Task.scala:108)at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)at java.lang.Thread.run(Thread.java:745)Caused by: org.apache.carbondata.processing.loading.exception.CarbonDataLoadingException: org.apache.carbondata.core.datastore.exception.CarbonDataWriterException: Problem while copying file from local store to carbon storeat org.apache.carbondata.processing.loading.steps.DataWriterProcessorStepImpl.processingComplete(DataWriterProcessorStepImpl.java:162)at org.apache.carbondata.processing.loading.steps.DataWriterProcessorStepImpl.finish(DataWriterProcessorStepImpl.java:148)at org.apache.carbondata.processing.loading.steps.DataWriterProcessorStepImpl.execute(DataWriterProcessorStepImpl.java:112)
issueID:CARBONDATA-2447
type:Bug
changed files:
texts:Range Partition Table。When the update operation is performed, the data will be lost.
Range Partition Table。When the update operation is performed, the data will be lost.As shown in the picture。如下面图片所示，数据丢失必现。    
issueID:CARBONDATA-2448
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockletDataMap.java
processing/src/main/java/org/apache/carbondata/processing/loading/model/CarbonLoadModel.java
texts:Adding compacted segments to load and alter events

issueID:CARBONDATA-245
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/writer/ThriftWriter.java
texts:Actual Exception is getting lost in case of data dictionary file generation.
Actual Exception is getting lost in case of data dictionary file generation.getting the null pointer exception as in the finally block we are trying to write data to a null stream.
issueID:CARBONDATA-2454
type:Improvement
changed files:datamap/bloom/src/main/java/org/apache/carbondata/datamap/bloom/BloomDataMapWriter.java
datamap/bloom/src/main/java/org/apache/carbondata/datamap/bloom/BloomCoarseGrainDataMapFactory.java
datamap/bloom/src/main/java/org/apache/carbondata/datamap/bloom/BloomDataMapBuilder.java
texts:Add false positive probability property for bloom filter datamap
Add false positive probability property for bloom filter datamap
issueID:CARBONDATA-2455
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/metadata/schema/table/DiskBasedDMSchemaStorageProvider.java
datamap/lucene/src/main/java/org/apache/carbondata/datamap/lucene/LuceneFineGrainDataMap.java
datamap/lucene/src/main/java/org/apache/carbondata/datamap/lucene/LuceneDataMapWriter.java
texts:Fix _System Folder creation and lucene AND,OR,NOT Filter fix

issueID:CARBONDATA-2457
type:Sub-task
changed files:store/sdk/src/main/java/org/apache/carbondata/sdk/file/AvroCarbonWriter.java
texts:Add converter to get Carbon SDK Schema from Avro schema directly.
In the current implementation, SDK users have to manually create carbon schema of fields from avro schema. This is time consuming and error prone. Also usere should not be worried about this logic.So, abstract the carbon schema creation from avro schema by exposing a method to user. 
issueID:CARBONDATA-2458
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/metadata/schema/table/CarbonTable.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/resolverinfo/FalseConditionalResolverImpl.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/resolverinfo/TrueConditionalResolverImpl.java
core/src/main/java/org/apache/carbondata/core/scan/executor/impl/AbstractQueryExecutor.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/LogicalFilterResolverImpl.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonFileInputFormat.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/RowLevelFilterResolverImpl.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/metadata/FilterResolverMetadata.java
core/src/main/java/org/apache/carbondata/core/scan/executor/util/QueryUtil.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/FilterResolverIntf.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/resolverinfo/visitor/DictionaryColumnVisitor.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/resolverinfo/visitor/RangeDictionaryColumnVisitor.java
core/src/main/java/org/apache/carbondata/core/scan/filter/FilterProcessor.java
core/src/main/java/org/apache/carbondata/core/scan/model/QueryModelBuilder.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonTableInputFormat.java
core/src/main/java/org/apache/carbondata/core/scan/filter/FilterExpressionProcessor.java
core/src/main/java/org/apache/carbondata/core/scan/filter/FilterUtil.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/ConditionalFilterResolverImpl.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/RowLevelRangeFilterResolverImpl.java
texts:Remove unnecessary TableProvider interface

issueID:CARBONDATA-2459
type:Improvement
changed files:datamap/bloom/src/main/java/org/apache/carbondata/datamap/bloom/BloomDataMapWriter.java
datamap/bloom/src/main/java/org/apache/carbondata/datamap/bloom/BloomCoarseGrainDataMapFactory.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
datamap/bloom/src/main/java/org/apache/carbondata/datamap/bloom/BloomDataMapCache.java
datamap/bloom/src/main/java/org/apache/carbondata/datamap/bloom/BloomCoarseGrainDataMap.java
texts:Support cache for bloom datamap
Currently query using bloom filter datamap is slow. The root cause is that loading bloom filter index costs too much time. We can implement a driver side cache to accelerate the loading index procedure.
issueID:CARBONDATA-246
type:Bug
changed files:
texts:compaction is wrong in case if last segment is not assigned to an executor.
if during compaction of 4 loads, for any executor if only first 3 loads task is assigned then the col cardinality calculation based on the last segment info will become wrong.in this case the cardinality will go wrong for that executor.
issueID:CARBONDATA-2463
type:Bug
changed files:store/sdk/src/main/java/org/apache/carbondata/sdk/file/CarbonWriterBuilder.java
texts:if two insert operations are running concurrently 1 task fails and causes wrong no of records in select
If two insert operations are running concurrently then 1 task fails for one of the job. However both jobs are successs. Below is the exception:org.apache.carbondata.processing.loading.exception.CarbonDataLoadingException: Error while initializing data handler :
issueID:CARBONDATA-2464
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/util/ByteUtil.java
texts:Fixed OOM in case of complex type
Problem: Query with Complex type is failing with OOMRoot Cause: Complex type child column(No-dictionary) values are written in LV format, while reading the data it will read length then based on length it is reading the data. Converting byte array to int is giving wrong length value, because of this it's trying to create big memory chunk and as memory is not available in Unsafe it is failing with OOM.Code issue: While converting byte array to int it is not masking the the byte values and because of this is giving wrong integer value.Solution: Mask each byte and then left shift the bits  
issueID:CARBONDATA-2465
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
processing/src/main/java/org/apache/carbondata/processing/store/writer/AbstractFactDataWriter.java
texts:Improve the carbondata file reliability in data load when direct hdfs write is enabled
At present if we enable direct write on HDFS, file is written with replication of 1. As the setReplication is asyn operation, it can cause data loss if any DN is down before NN finishes the replication.
issueID:CARBONDATA-2467
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/metadata/datatype/ArrayType.java
store/sdk/src/main/java/org/apache/carbondata/sdk/file/CarbonWriterBuilder.java
texts:Null is printed in the SDK writer logs for operations logged
Expected Output：Null should not be printed in the SDK writer logs Actual Output：Null is printed in the SDK writer logs for operations logged as shown below. This is confusing for the user.  
issueID:CARBONDATA-2468
type:Bug
changed files:store/sdk/src/main/java/org/apache/carbondata/sdk/file/CarbonWriterBuilder.java
texts:sortcolumns considers all dimension also if few columns specified for sort_columns prop

issueID:CARBONDATA-2469
type:Bug
changed files:hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonTableInputFormat.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/column/ColumnSchema.java
core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
texts:External Table must show its location instead of default store path in describe formatted

issueID:CARBONDATA-247
type:Bug
changed files:
texts:Higher MAXCOLUMNS value in load DML options is leading to out of memory error
When a higher value lets say Integer max value is configured for maxcolumns option in load DML and executor memory is less, then in that case UnivocityCsvParser throws an out of memory error when it tries to create an array of size of maxColumns option value.java.lang.OutOfMemoryError: Java heap space at com.univocity.parsers.common.ParserOutput.<init>(ParserOutput.java:86) at com.univocity.parsers.common.AbstractParser.<init>(AbstractParser.java:66) at com.univocity.parsers.csv.CsvParser.<init>(CsvParser.java:50) at org.apache.carbondata.processing.csvreaderstep.UnivocityCsvParser.initialize(UnivocityCsvParser.java:114) at org.apache.carbondata.processing.csvreaderstep.CsvInput.doProcessUnivocity(CsvInput.java:427) at org.apache.carbondata.processing.csvreaderstep.CsvInput.access$100(CsvInput.java:60) at org.apache.carbondata.processing.csvreaderstep.CsvInput$1.call(CsvInput.java:389)
issueID:CARBONDATA-2470
type:Improvement
changed files:
texts:Refactor AlterTableCompactionPostStatusUpdateEvent usage in compaction flow
AlterTableCompactionPostStatusUpdateEvent in compaction flow is controlled only by the preaggregate listener. If the CommitPreAggregateListener sets the commitComplete property to true, this event will not be fired for the next iteration
issueID:CARBONDATA-2472
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/util/BlockletDataMapUtil.java
examples/spark2/src/main/java/org/apache/carbondata/examples/sdk/CarbonReaderExample.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockletDataMapFactory.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/SegmentIndexFileStore.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/CarbonTable.java
core/src/main/java/org/apache/carbondata/core/indexstore/BlockletDataMapIndexStore.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonInputFormat.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonFileInputFormat.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/column/ColumnSchema.java
store/sdk/src/main/java/org/apache/carbondata/sdk/file/CarbonReaderBuilder.java
core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonTableInputFormat.java
core/src/main/java/org/apache/carbondata/core/indexstore/TableBlockIndexUniqueIdentifierWrapper.java
hadoop/src/main/java/org/apache/carbondata/hadoop/CarbonRecordReader.java
core/src/main/java/org/apache/carbondata/core/datamap/dev/CacheableDataMap.java
texts:Refactor NonTransactional table code for Index file IO performance

issueID:CARBONDATA-2473
type:Improvement
changed files:
texts:Support Materialized View as enhanced Preaggregate DataMap
Carbon DataMap is a framework to accelerate certain type of analysis workload. in OLAP domain, traditionally there is a technique called Materialized View to accelerate OLAP queries. Currently carbon supports preaggregate datamap, as the preaggregate is only on single table, Materialized View enhance it by adding join capability.
issueID:CARBONDATA-2474
type:Sub-task
changed files:core/src/main/java/org/apache/carbondata/core/datamap/DataMapStoreManager.java
integration/spark2/src/main/java/org/apache/carbondata/datamap/DataMapManager.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/datamap/DataMapClassProvider.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/DataMapSchema.java
texts:Support Modular Plan
Modular Plan is the basic structure for query plan in Materialized View. Carbon should support converting Spark Logical Plan to Modular Plan
issueID:CARBONDATA-2475
type:Sub-task
changed files:core/src/main/java/org/apache/carbondata/core/datamap/status/DataMapStatusManager.java
texts:Support Materialized View query rewrite

issueID:CARBONDATA-2476
type:Bug
changed files:datamap/bloom/src/main/java/org/apache/carbondata/datamap/bloom/BloomDataMapCache.java
texts:Fix bug in bloom datamap cache
Currently there is a bug that the cache for datamap will be evicted after it is loaded.
issueID:CARBONDATA-2477
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/datastore/page/LazyColumnPage.java
core/src/main/java/org/apache/carbondata/core/datastore/page/VarLengthColumnPageBase.java
core/src/main/java/org/apache/carbondata/core/util/ByteUtil.java
processing/src/main/java/org/apache/carbondata/processing/datatypes/PrimitiveDataType.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/ColumnPageEncoder.java
processing/src/main/java/org/apache/carbondata/processing/datatypes/StructDataType.java
processing/src/main/java/org/apache/carbondata/processing/datatypes/GenericDataType.java
core/src/main/java/org/apache/carbondata/core/datastore/page/UnsafeFixLengthColumnPage.java
core/src/main/java/org/apache/carbondata/core/datastore/page/SafeVarLengthColumnPage.java
core/src/main/java/org/apache/carbondata/core/datastore/ColumnType.java
processing/src/main/java/org/apache/carbondata/processing/datatypes/ArrayDataType.java
core/src/main/java/org/apache/carbondata/core/datastore/page/ColumnPage.java
core/src/main/java/org/apache/carbondata/core/datastore/page/ComplexColumnPage.java
core/src/main/java/org/apache/carbondata/core/datastore/page/SafeFixLengthColumnPage.java
processing/src/main/java/org/apache/carbondata/processing/store/TablePage.java
core/src/main/java/org/apache/carbondata/core/scan/complextypes/StructQueryType.java
store/sdk/src/main/java/org/apache/carbondata/sdk/file/CarbonWriterBuilder.java
core/src/main/java/org/apache/carbondata/core/scan/complextypes/PrimitiveQueryType.java
texts:No dictionary Complex type with double/date/decimal data type table creation is failing
Problem: SDK create table with No Dictionary complex type is failing when complex type child contain double/date/decimal data type Solution: In complex type validation , it is not allowing double/date/decimal data , need to remove the sameChanged no dictionary complex type storage format, instead of storing length in int , now storing in short to reduce storage space
issueID:CARBONDATA-2478
type:Bug
changed files:
texts:Add datamap-developer-guide.md file in readme
Add datamap-developer-guide.md file in readme 
issueID:CARBONDATA-2479
type:Bug
changed files:store/sdk/src/main/java/org/apache/carbondata/sdk/file/AvroCarbonWriter.java
texts:Multiple issue in sdk writer and external table flow
    Multiple issues:    fixed external table path display    fixed default value for array in AVRO    fixed NPE when delete folder before the second select query    fixed primetive time stamp sdk load Issue fix 
issueID:CARBONDATA-248
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/scan/result/iterator/AbstractDetailQueryResultIterator.java
core/src/main/java/org/apache/carbondata/core/carbon/querystatistics/DriverQueryStatisticsRecorderImpl.java
texts:There was no header in driver statistics table and scan block time was always zero
Column header of total query cost was missing in driver statistics table while recording statistics and scan block time was always zero while recording query statistics.
issueID:CARBONDATA-2480
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/metadata/schema/table/CarbonTable.java
datamap/lucene/src/main/java/org/apache/carbondata/datamap/lucene/LuceneFineGrainDataMapFactory.java
texts:Search mode RuntimeException: Error while resolving filter expression
There are some error when running org.apache.carbondata.examples.SearchModeExample or org.apache.carbondata.spark.testsuite.detailquery.SearchModeTestCasejava.lang.RuntimeException: Error while resolving filter expression at org.apache.carbondata.core.metadata.schema.table.CarbonTable.resolveFilter(CarbonTable.java:924) at org.apache.spark.rpc.Master.chooseFGDataMap(Master.scala:261) at org.apache.spark.rpc.Master.search(Master.scala:218) at org.apache.carbondata.store.SparkCarbonStore.search(SparkCarbonStore.scala:144) at org.apache.spark.sql.CarbonSession.runSearch(CarbonSession.scala:225) at org.apache.spark.sql.CarbonSession.org$apache$spark$sql$CarbonSession$$trySearchMode(CarbonSession.scala:180) at org.apache.spark.sql.CarbonSession$$anonfun$sql$1.apply(CarbonSession.scala:100) at org.apache.spark.sql.CarbonSession$$anonfun$sql$1.apply(CarbonSession.scala:97) at org.apache.spark.sql.CarbonSession.withProfiler(CarbonSession.scala:156) at org.apache.spark.sql.CarbonSession.sql(CarbonSession.scala:95) at org.apache.carbondata.examples.SearchModeExample$$anonfun$3$$anon$1.run(SearchModeExample.scala:168) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.run$$$capture(FutureTask.java:266) at java.util.concurrent.FutureTask.run(FutureTask.java) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748)Caused by: java.lang.NullPointerException at org.apache.carbondata.core.scan.filter.resolver.RowLevelFilterResolverImpl.resolve(RowLevelFilterResolverImpl.java:74) at org.apache.carbondata.core.scan.filter.FilterExpressionProcessor.traverseAndResolveTree(FilterExpressionProcessor.java:252) at org.apache.carbondata.core.scan.filter.FilterExpressionProcessor.traverseAndResolveTree(FilterExpressionProcessor.java:251) at org.apache.carbondata.core.scan.filter.FilterExpressionProcessor.getFilterResolvertree(FilterExpressionProcessor.java:232) at org.apache.carbondata.core.scan.filter.FilterExpressionProcessor.getFilterResolver(FilterExpressionProcessor.java:84) at org.apache.carbondata.core.metadata.schema.table.CarbonTable.resolveFilter(CarbonTable.java:921) ... 16 more18/05/15 11:27:00 ERROR CarbonSession: Exception when executing search mode: Error while resolving filter expression, fallback to SparkSQLsearch mode asynchronous queryjava.lang.RuntimeException: Error while resolving filter expression at org.apache.carbondata.core.metadata.schema.table.CarbonTable.resolveFilter(CarbonTable.java:924) at org.apache.spark.rpc.Master.chooseFGDataMap(Master.scala:261) at org.apache.spark.rpc.Master.search(Master.scala:218) at org.apache.carbondata.store.SparkCarbonStore.search(SparkCarbonStore.scala:144) at org.apache.spark.sql.CarbonSession.runSearch(CarbonSession.scala:225) at org.apache.spark.sql.CarbonSession.org$apache$spark$sql$CarbonSession$$trySearchMode(CarbonSession.scala:180) at org.apache.spark.sql.CarbonSession$$anonfun$sql$1.apply(CarbonSession.scala:100) at org.apache.spark.sql.CarbonSession$$anonfun$sql$1.apply(CarbonSession.scala:97) at org.apache.spark.sql.CarbonSession.withProfiler(CarbonSession.scala:156) at org.apache.spark.sql.CarbonSession.sql(CarbonSession.scala:95) at org.apache.carbondata.examples.SearchModeExample$$anonfun$3$$anon$1.run(SearchModeExample.scala:168) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.run$$$capture(FutureTask.java:266) at java.util.concurrent.FutureTask.run(FutureTask.java) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748)Caused by: java.lang.NullPointerException at org.apache.carbondata.core.scan.filter.resolver.RowLevelFilterResolverImpl.resolve(RowLevelFilterResolverImpl.java:74) at org.apache.carbondata.core.scan.filter.FilterExpressionProcessor.traverseAndResolveTree(FilterExpressionProcessor.java:252) at org.apache.carbondata.core.scan.filter.FilterExpressionProcessor.traverseAndResolveTree(FilterExpressionProcessor.java:251) at org.apache.carbondata.core.scan.filter.FilterExpressionProcessor.getFilterResolvertree(FilterExpressionProcessor.java:232) at org.apache.carbondata.core.scan.filter.FilterExpressionProcessor.getFilterResolver(FilterExpressionProcessor.java:84) at org.apache.carbondata.core.metadata.schema.table.CarbonTable.resolveFilter(CarbonTable.java:921) ... 16 more18/05/15 11:27:03 ERROR CarbonSession: Exception when executing search mode: Error while resolving filter expression, fallback to SparkSQLjava.lang.RuntimeException: Error while resolving filter expression at org.apache.carbondata.core.metadata.schema.table.CarbonTable.resolveFilter(CarbonTable.java:924) at org.apache.spark.rpc.Master.chooseFGDataMap(Master.scala:261) at org.apache.spark.rpc.Master.search(Master.scala:218) at org.apache.carbondata.store.SparkCarbonStore.search(SparkCarbonStore.scala:144) at org.apache.spark.sql.CarbonSession.runSearch(CarbonSession.scala:225) at org.apache.spark.sql.CarbonSession.org$apache$spark$sql$CarbonSession$$trySearchMode(CarbonSession.scala:180) at org.apache.spark.sql.CarbonSession$$anonfun$sql$1.apply(CarbonSession.scala:100) at org.apache.spark.sql.CarbonSession$$anonfun$sql$1.apply(CarbonSession.scala:97) at org.apache.spark.sql.CarbonSession.withProfiler(CarbonSession.scala:156) at org.apache.spark.sql.CarbonSession.sql(CarbonSession.scala:95) at org.apache.carbondata.examples.SearchModeExample$$anonfun$3$$anon$1.run(SearchModeExample.scala:168) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.run$$$capture(FutureTask.java:266) at java.util.concurrent.FutureTask.run(FutureTask.java) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748)Caused by: java.lang.NullPointerException at org.apache.carbondata.core.scan.filter.resolver.RowLevelFilterResolverImpl.resolve(RowLevelFilterResolverImpl.java:74) at org.apache.carbondata.core.scan.filter.FilterExpressionProcessor.traverseAndResolveTree(FilterExpressionProcessor.java:252) at org.apache.carbondata.core.scan.filter.FilterExpressionProcessor.traverseAndResolveTree(FilterExpressionProcessor.java:251) at org.apache.carbondata.core.scan.filter.FilterExpressionProcessor.getFilterResolvertree(FilterExpressionProcessor.java:232) at org.apache.carbondata.core.scan.filter.FilterExpressionProcessor.getFilterResolver(FilterExpressionProcessor.java:84) at org.apache.carbondata.core.metadata.schema.table.CarbonTable.resolveFilter(CarbonTable.java:921) ... 16 more18/05/15 11:27:04 ERROR CarbonSession: Exception when executing search mode: Error while resolving filter expression, fallback to SparkSQLjava.lang.RuntimeException: Error while resolving filter expression at org.apache.carbondata.core.metadata.schema.table.CarbonTable.resolveFilter(CarbonTable.java:924) at org.apache.spark.rpc.Master.chooseFGDataMap(Master.scala:261) at org.apache.spark.rpc.Master.search(Master.scala:218) at org.apache.carbondata.store.SparkCarbonStore.search(SparkCarbonStore.scala:144) at org.apache.spark.sql.CarbonSession.runSearch(CarbonSession.scala:225) at org.apache.spark.sql.CarbonSession.org$apache$spark$sql$CarbonSession$$trySearchMode(CarbonSession.scala:180) at org.apache.spark.sql.CarbonSession$$anonfun$sql$1.apply(CarbonSession.scala:100) at org.apache.spark.sql.CarbonSession$$anonfun$sql$1.apply(CarbonSession.scala:97) at org.apache.spark.sql.CarbonSession.withProfiler(CarbonSession.scala:156) at org.apache.spark.sql.CarbonSession.sql(CarbonSession.scala:95) at org.apache.carbondata.examples.SearchModeExample$$anonfun$3$$anon$1.run(SearchModeExample.scala:168) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.run$$$capture(FutureTask.java:266) at java.util.concurrent.FutureTask.run(FutureTask.java) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748)Caused by: java.lang.NullPointerException at org.apache.carbondata.core.scan.filter.resolver.RowLevelFilterResolverImpl.resolve(RowLevelFilterResolverImpl.java:74) at org.apache.carbondata.core.scan.filter.FilterExpressionProcessor.traverseAndResolveTree(FilterExpressionProcessor.java:252) at org.apache.carbondata.core.scan.filter.FilterExpressionProcessor.traverseAndResolveTree(FilterExpressionProcessor.java:251) at org.apache.carbondata.core.scan.filter.FilterExpressionProcessor.getFilterResolvertree(FilterExpressionProcessor.java:232) at org.apache.carbondata.core.scan.filter.FilterExpressionProcessor.getFilterResolver(FilterExpressionProcessor.java:84) at org.apache.carbondata.core.metadata.schema.table.CarbonTable.resolveFilter(CarbonTable.java:921) ... 16 more18/05/15 11:27:04 ERROR CarbonSession: Exception when executing search mode: Error while resolving filter expression, fallback to SparkSQLjava.lang.RuntimeException: Error while resolving filter expression at org.apache.carbondata.core.metadata.schema.table.CarbonTable.resolveFilter(CarbonTable.java:924) at org.apache.spark.rpc.Master.chooseFGDataMap(Master.scala:261) at org.apache.spark.rpc.Master.search(Master.scala:218) at org.apache.carbondata.store.SparkCarbonStore.search(SparkCarbonStore.scala:144) at org.apache.spark.sql.CarbonSession.runSearch(CarbonSession.scala:225) at org.apache.spark.sql.CarbonSession.org$apache$spark$sql$CarbonSession$$trySearchMode(CarbonSession.scala:180) at org.apache.spark.sql.CarbonSession$$anonfun$sql$1.apply(CarbonSession.scala:100) at org.apache.spark.sql.CarbonSession$$anonfun$sql$1.apply(CarbonSession.scala:97) at org.apache.spark.sql.CarbonSession.withProfiler(CarbonSession.scala:156) at org.apache.spark.sql.CarbonSession.sql(CarbonSession.scala:95) at org.apache.carbondata.examples.SearchModeExample$$anonfun$3$$anon$1.run(SearchModeExample.scala:168) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.run$$$capture(FutureTask.java:266) at java.util.concurrent.FutureTask.run(FutureTask.java) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748)Caused by: java.lang.NullPointerException at org.apache.carbondata.core.scan.filter.resolver.RowLevelFilterResolverImpl.resolve(RowLevelFilterResolverImpl.java:74) at org.apache.carbondata.core.scan.filter.FilterExpressionProcessor.traverseAndResolveTree(FilterExpressionProcessor.java:252) at org.apache.carbondata.core.scan.filter.FilterExpressionProcessor.traverseAndResolveTree(FilterExpressionProcessor.java:251) at org.apache.carbondata.core.scan.filter.FilterExpressionProcessor.getFilterResolvertree(FilterExpressionProcessor.java:232) at org.apache.carbondata.core.scan.filter.FilterExpressionProcessor.getFilterResolver(FilterExpressionProcessor.java:84) at org.apache.carbondata.core.metadata.schema.table.CarbonTable.resolveFilter(CarbonTable.java:921) ... 16 moreThe second error :18/05/14 20:51:20 ERROR TransportRequestHandler: Error while invoking RpcHandler#receive() on RPC id 6929993627477870468java.io.InvalidClassException: scala.collection.convert.Wrappers$MutableSetWrapper; no valid constructor at java.io.ObjectStreamClass$ExceptionInfo.newInvalidClassException(ObjectStreamClass.java:157) at java.io.ObjectStreamClass.checkDeserialize(ObjectStreamClass.java:862) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2041) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1571) at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2285) at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2209) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2067) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1571) at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431) at java.util.ArrayList.readObject(ArrayList.java:797) at sun.reflect.GeneratedMethodAccessor53.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1158) at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2176) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2067) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1571) at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2285) at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2209) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2067) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1571) at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2285) at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2209) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2067) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1571) at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431) at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75) at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108) at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:270) at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58) at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:319) at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:269) at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58) at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:268) at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:603) at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:654) at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:639) at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:157) at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:105) at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:367) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:353) at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:346) at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:266) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:367) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:353) at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:346) at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:367) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:353) at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:346) at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:367) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:353) at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:346) at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1294) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:367) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:353) at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:911) at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131) at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:652) at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:575) at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:489) at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:451) at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:140) at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144) at java.lang.Thread.run(Thread.java:748)org.apache.spark.SparkException: Exception thrown in awaitResult:  at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:205) at org.apache.spark.rpc.Master$$anonfun$search$1.apply(Master.scala:236) at org.apache.spark.rpc.Master$$anonfun$search$1.apply(Master.scala:231) at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99) at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99) at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230) at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40) at scala.collection.mutable.HashMap.foreach(HashMap.scala:99) at org.apache.spark.rpc.Master.search(Master.scala:231) at org.apache.carbondata.store.SparkCarbonStore.search(SparkCarbonStore.scala:144) at org.apache.spark.sql.CarbonSession.runSearch(CarbonSession.scala:225) at org.apache.spark.sql.CarbonSession.org$apache$spark$sql$CarbonSession$$trySearchMode(CarbonSession.scala:186) at org.apache.spark.sql.CarbonSession$$anonfun$sql$1.apply(CarbonSession.scala:100) at org.apache.spark.sql.CarbonSession$$anonfun$sql$1.apply(CarbonSession.scala:97) at org.apache.spark.sql.CarbonSession.withProfiler(CarbonSession.scala:156) at org.apache.spark.sql.CarbonSession.sql(CarbonSession.scala:95) at org.apache.spark.sql.test.Spark2TestQueryExecutor.sql(Spark2TestQueryExecutor.scala:35) at org.apache.spark.sql.test.util.QueryTest.sql(QueryTest.scala:113) at org.apache.carbondata.spark.testsuite.detailquery.SearchModeTestCase.org$apache$carbondata$spark$testsuite$detailquery$SearchModeTestCase$$checkSearchAnswer(SearchModeTestCase.scala:58) at org.apache.carbondata.spark.testsuite.detailquery.SearchModeTestCase$$anonfun$10$$anonfun$apply$mcV$sp$1.apply$mcV$sp(SearchModeTestCase.scala:108) at org.apache.carbondata.spark.testsuite.detailquery.SearchModeTestCase$$anonfun$10$$anonfun$apply$mcV$sp$1.apply(SearchModeTestCase.scala:105) at org.apache.carbondata.spark.testsuite.detailquery.SearchModeTestCase$$anonfun$10$$anonfun$apply$mcV$sp$1.apply(SearchModeTestCase.scala:105) at org.apache.spark.sql.catalyst.util.package$.benchmark(package.scala:129) at org.apache.carbondata.spark.testsuite.detailquery.SearchModeTestCase$$anonfun$10.apply$mcV$sp(SearchModeTestCase.scala:105) at org.apache.carbondata.spark.testsuite.detailquery.SearchModeTestCase$$anonfun$10.apply(SearchModeTestCase.scala:103) at org.apache.carbondata.spark.testsuite.detailquery.SearchModeTestCase$$anonfun$10.apply(SearchModeTestCase.scala:103) at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22) at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85) at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104) at org.scalatest.Transformer.apply(Transformer.scala:22) at org.scalatest.Transformer.apply(Transformer.scala:20) at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166) at org.apache.spark.sql.test.util.CarbonFunSuite.withFixture(CarbonFunSuite.scala:41) at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:163) at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175) at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175) at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306) at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:175) at org.scalatest.FunSuite.runTest(FunSuite.scala:1555) at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208) at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208) at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413) at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401) at scala.collection.immutable.List.foreach(List.scala:381) at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401) at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396) at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483) at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:208) at org.scalatest.FunSuite.runTests(FunSuite.scala:1555) at org.scalatest.Suite$class.run(Suite.scala:1424) at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1555) at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212) at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212) at org.scalatest.SuperEngine.runImpl(Engine.scala:545) at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:212) at org.apache.carbondata.spark.testsuite.detailquery.SearchModeTestCase.org$scalatest$BeforeAndAfterAll$$super$run(SearchModeTestCase.scala:31) at org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:257) at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:256) at org.apache.carbondata.spark.testsuite.detailquery.SearchModeTestCase.run(SearchModeTestCase.scala:31) at org.scalatest.tools.SuiteRunner.run(SuiteRunner.scala:55) at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2563) at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2557) at scala.collection.immutable.List.foreach(List.scala:381) at org.scalatest.tools.Runner$.doRunRunRunDaDoRunRun(Runner.scala:2557) at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1044) at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1043) at org.scalatest.tools.Runner$.withClassLoaderAndDispatchReporter(Runner.scala:2722) at org.scalatest.tools.Runner$.runOptionallyWithPassFailReporter(Runner.scala:1043) at org.scalatest.tools.Runner$.run(Runner.scala:883) at org.scalatest.tools.Runner.run(Runner.scala) at org.jetbrains.plugins.scala.testingSupport.scalaTest.ScalaTestRunner.runScalaTest2(ScalaTestRunner.java:131) at org.jetbrains.plugins.scala.testingSupport.scalaTest.ScalaTestRunner.main(ScalaTestRunner.java:28)Caused by: java.lang.RuntimeException: java.io.InvalidClassException: scala.collection.convert.Wrappers$MutableSetWrapper; no valid constructor at java.io.ObjectStreamClass$ExceptionInfo.newInvalidClassException(ObjectStreamClass.java:157) at java.io.ObjectStreamClass.checkDeserialize(ObjectStreamClass.java:862) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2041) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1571) at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2285) at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2209) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2067) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1571) at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431) at java.util.ArrayList.readObject(ArrayList.java:797) at sun.reflect.GeneratedMethodAccessor53.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1158) at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2176) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2067) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1571) at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2285) at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2209) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2067) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1571) at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2285) at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2209) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2067) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1571) at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431) at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75) at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108) at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:270) at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58) at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:319) at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:269) at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58) at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:268) at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:603) at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:654) at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:639) at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:157) at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:105) at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:367) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:353) at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:346) at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:266) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:367) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:353) at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:346) at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:367) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:353) at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:346) at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:367) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:353) at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:346) at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1294) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:367) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:353) at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:911) at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131) at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:652) at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:575) at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:489) at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:451) at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:140) at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144) at java.lang.Thread.run(Thread.java:748) at org.apache.spark.network.client.TransportResponseHandler.handle(TransportResponseHandler.java:207) at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:120) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:367) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:353) at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:346) at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:266) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:367) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:353) at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:346) at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:367) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:353) at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:346) at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:367) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:353) at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:346) at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1294) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:367) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:353) at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:911) at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131) at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:652) at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:575) at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:489) at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:451) at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:140) at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144) at java.lang.Thread.run(Thread.java:748)18/05/14 20:51:20 ERROR CarbonSession: Exception when executing search mode: Exception thrown in awaitResult: , fallback to SparkSQL18/05/14 20:51:20 INFO CarbonSparkSqlParser: Parsing command: select id from main where id = '3' limit 1018/05/14 20:51:20 INFO HiveMetaStore: 0: get_table : db=default tbl=main18/05/14 20:51:20 INFO audit: ugi=xubo ip=unknown-ip-addr cmd=get_table : db=default tbl=main 18/05/14 20:51:20 INFO CatalystSqlParser: Parsing command: array<string>18/05/14 20:51:20 INFO CarbonLateDecodeRule: ScalaTest-run-running-SearchModeTestCase skip CarbonOptimizer18/05/14 20:51:20 INFO CarbonLateDecodeRule: ScalaTest-run-running-SearchModeTestCase Skip CarbonOptimizer18/05/14 20:51:20 INFO CodeGenerator: Code generated in 4.758886 ms18/05/14 20:51:20 INFO DistributionUtil$: ScalaTest-run-running-SearchModeTestCase Executors configured : 118/05/14 20:51:20 INFO DistributionUtil$: ScalaTest-run-running-SearchModeTestCase Total Time taken to ensure the required executors : 118/05/14 20:51:20 INFO DistributionUtil$: ScalaTest-run-running-SearchModeTestCase Time elapsed to allocate the required executors: 018/05/14 20:51:20 INFO CarbonScanRDD:  Identified no.of.blocks: 1, no.of.tasks: 1, no.of.nodes: 1, parallelism: 0       18/05/14 20:51:20 INFO SparkContext: Starting job: collect at SearchModeTestCase.scala:5418/05/14 20:51:20 INFO DAGScheduler: Got job 17 (collect at SearchModeTestCase.scala:54) with 1 output partitions18/05/14 20:51:20 INFO DAGScheduler: Final stage: ResultStage 23 (collect at SearchModeTestCase.scala:54)18/05/14 20:51:20 INFO DAGScheduler: Parents of final stage: List()18/05/14 20:51:20 INFO DAGScheduler: Missing parents: List()18/05/14 20:51:20 INFO DAGScheduler: Submitting ResultStage 23 (MapPartitionsRDD[80] at collect at SearchModeTestCase.scala:54), which has no missing parents18/05/14 20:51:20 INFO MemoryStore: Block broadcast_23 stored as values in memory (estimated size 32.1 KB, free 2004.0 MB)18/05/14 20:51:20 INFO MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 26.5 KB, free 2004.0 MB)18/05/14 20:51:20 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on 192.168.44.90:63398 (size: 26.5 KB, free: 2004.3 MB)18/05/14 20:51:20 INFO SparkContext: Created broadcast 23 from broadcast at DAGScheduler.scala:100618/05/14 20:51:20 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 23 (MapPartitionsRDD[80] at collect at SearchModeTestCase.scala:54) (first 15 tasks are for partitions Vector(0))18/05/14 20:51:20 INFO TaskSchedulerImpl: Adding task set 23.0 with 1 tasks18/05/14 20:51:20 INFO TaskSetManager: Starting task 0.0 in stage 23.0 (TID 822, localhost, executor driver, partition 0, PROCESS_LOCAL, 5955 bytes)18/05/14 20:51:20 INFO Executor: Running task 0.0 in stage 23.0 (TID 822)18/05/14 20:51:20 INFO AbstractQueryExecutor: [Executor task launch worker for task 822][partitionID:main;queryID:65652430146138] Query will be executed on table: main18/05/14 20:51:20 INFO ResultCollectorFactory: pool-36-thread-7 Row based dictionary collector is used to scan and collect the data18/05/14 20:51:20 INFO UnsafeMemoryManager: [Executor task launch worker for task 822][partitionID:main;queryID:65652430146138] Total memory used after task 65652473133085 is 3687 Current tasks running now are : [65647314993698, 65639014191902, 65647481932757, 65644793091292, 65639048545043, 65641096541779]18/05/14 20:51:20 INFO Executor: Finished task 0.0 in stage 23.0 (TID 822). 1162 bytes result sent to driver18/05/14 20:51:20 INFO TaskSetManager: Finished task 0.0 in stage 23.0 (TID 822) in 27 ms on localhost (executor driver) (1/1)18/05/14 20:51:20 INFO TaskSchedulerImpl: Removed TaskSet 23.0, whose tasks have all completed, from pool 18/05/14 20:51:20 INFO DAGScheduler: ResultStage 23 (collect at SearchModeTestCase.scala:54) finished in 0.027 s18/05/14 20:51:20 INFO DAGScheduler: Job 17 finished: collect at SearchModeTestCase.scala:54, took 0.034065 s18/05/14 20:51:20 INFO CarbonLateDecodeRule: ScalaTest-run-running-SearchModeTestCase skip CarbonOptimizer18/05/14 20:51:20 INFO CarbonLateDecodeRule: ScalaTest-run-running-SearchModeTestCase Skip CarbonOptimizer18/05/14 20:51:20 INFO DistributionUtil$: ScalaTest-run-running-SearchModeTestCase Executors configured : 118/05/14 20:51:20 INFO DistributionUtil$: ScalaTest-run-running-SearchModeTestCase Total Time taken to ensure the required executors : 018/05/14 20:51:20 INFO DistributionUtil$: ScalaTest-run-running-SearchModeTestCase Time elapsed to allocate the required executors: 018/05/14 20:51:20 INFO CarbonScanRDD:  Identified no.of.blocks: 1, no.of.tasks: 1, no.of.nodes: 1, parallelism: 0       18/05/14 20:51:20 INFO SparkContext: Starting job: checkAnswer at SearchModeTestCase.scala:5818/05/14 20:51:20 INFO DAGScheduler: Got job 18 (checkAnswer at SearchModeTestCase.scala:58) with 1 output partitions18/05/14 20:51:20 INFO DAGScheduler: Final stage: ResultStage 24 (checkAnswer at SearchModeTestCase.scala:58)18/05/14 20:51:20 INFO DAGScheduler: Parents of final stage: List()18/05/14 20:51:20 INFO DAGScheduler: Missing parents: List()18/05/14 20:51:20 INFO DAGScheduler: Submitting ResultStage 24 (MapPartitionsRDD[83] at checkAnswer at SearchModeTestCase.scala:58), which has no missing parents18/05/14 20:51:20 INFO MemoryStore: Block broadcast_24 stored as values in memory (estimated size 32.1 KB, free 2003.9 MB)18/05/14 20:51:20 INFO MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 26.5 KB, free 2003.9 MB)18/05/14 20:51:20 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on 192.168.44.90:63398 (size: 26.5 KB, free: 2004.3 MB)18/05/14 20:51:20 INFO SparkContext: Created broadcast 24 from broadcast at DAGScheduler.scala:100618/05/14 20:51:20 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 24 (MapPartitionsRDD[83] at checkAnswer at SearchModeTestCase.scala:58) (first 15 tasks are for partitions Vector(0))18/05/14 20:51:20 INFO TaskSchedulerImpl: Adding task set 24.0 with 1 tasks18/05/14 20:51:20 INFO TaskSetManager: Starting task 0.0 in stage 24.0 (TID 823, localhost, executor driver, partition 0, PROCESS_LOCAL, 5955 bytes)18/05/14 20:51:20 INFO Executor: Running task 0.0 in stage 24.0 (TID 823)18/05/14 20:51:20 INFO AbstractQueryExecutor: [Executor task launch worker for task 823][partitionID:main;queryID:65652512653847] Query will be executed on table: main18/05/14 20:51:20 INFO ResultCollectorFactory: pool-36-thread-6 Row based dictionary collector is used to scan and collect the data18/05/14 20:51:20 INFO UnsafeMemoryManager: [Executor task launch worker for task 823][partitionID:main;queryID:65652512653847] Total memory used after task 65652542497559 is 3687 Current tasks running now are : [65647314993698, 65639014191902, 65647481932757, 65644793091292, 65639048545043, 65641096541779]18/05/14 20:51:20 INFO Executor: Finished task 0.0 in stage 24.0 (TID 823). 1162 bytes result sent to driver18/05/14 20:51:20 INFO TaskSetManager: Finished task 0.0 in stage 24.0 (TID 823) in 29 ms on localhost (executor driver) (1/1)18/05/14 20:51:20 INFO TaskSchedulerImpl: Removed TaskSet 24.0, whose tasks have all completed, from pool 18/05/14 20:51:20 INFO DAGScheduler: ResultStage 24 (checkAnswer at SearchModeTestCase.scala:58) finished in 0.030 s18/05/14 20:51:20 INFO DAGScheduler: Job 18 finished: checkAnswer at SearchModeTestCase.scala:58, took 0.033859 s18/05/14 20:51:20 INFO CarbonSparkSqlParser: Parsing command: set carbon.search.enabled = false18/05/14 20:51:20 INFO CarbonLateDecodeRule: ScalaTest-run-running-SearchModeTestCase skip CarbonOptimizer18/05/14 20:51:20 INFO CarbonLateDecodeRule: ScalaTest-run-running-SearchModeTestCase Skip CarbonOptimizer18/05/14 20:51:20 AUDIT CacheProvider: [localhost][xubo][Thread-1]The key carbon.search.enabled with value false added in the session param18/05/14 20:51:20 INFO SparkCarbonStore: ScalaTest-run-running-SearchModeTestCase Shutting down all workers...18/05/14 20:51:20 INFO SearchRequestHandler: [dispatcher-event-loop-0][partitionID:main;queryID:65643146221485] Shutting down worker...18/05/14 20:51:20 INFO SearchRequestHandler: [dispatcher-event-loop-0][partitionID:main;queryID:65643146221485] Worker shutted down18/05/14 20:51:20 INFO SparkCarbonStore: ScalaTest-run-running-SearchModeTestCase All workers are shutted down18/05/14 20:51:20 INFO SparkCarbonStore: ScalaTest-run-running-SearchModeTestCase Stopping master...18/05/14 20:51:20 INFO Registry: dispatcher-event-loop-1 Registry Endpoint stopped18/05/14 20:51:20 INFO SparkCarbonStore: ScalaTest-run-running-SearchModeTestCase Master stopped18/05/14 20:51:20 INFO SearchModeTestCase: ScalaTest-run-running-SearchModeTestCase ===== FINISHED org.apache.carbondata.spark.testsuite.detailquery.SearchModeTestCase: 'set search mode' =====248.566341ms
issueID:CARBONDATA-2481
type:Test
changed files:
texts:Adding SDV testcases for SDK Writer

issueID:CARBONDATA-2482
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/datamap/DataMapStoreManager.java
processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactDataHandlerModel.java
processing/src/main/java/org/apache/carbondata/processing/util/CarbonLoaderUtil.java
core/src/main/java/org/apache/carbondata/core/writer/CarbonIndexFileMergeWriter.java
texts:Pass uuid while writing segment file if possible

issueID:CARBONDATA-2484
type:Sub-task
changed files:integration/hive/src/main/java/org/apache/carbondata/hive/MapredCarbonInputFormat.java
core/src/main/java/org/apache/carbondata/core/datamap/DataMapJob.java
core/src/main/java/org/apache/carbondata/core/datamap/DataMapStoreManager.java
core/src/main/java/org/apache/carbondata/core/datamap/dev/expr/AndDataMapExprWrapper.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockletDataMapFactory.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/CarbonTable.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonInputFormat.java
core/src/main/java/org/apache/carbondata/core/datamap/DataMapUtil.java
core/src/main/java/org/apache/carbondata/core/datamap/dev/expr/DataMapExprWrapperImpl.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonTableOutputFormat.java
core/src/main/java/org/apache/carbondata/core/datamap/DistributableDataMapFormat.java
core/src/main/java/org/apache/carbondata/core/datamap/AbstractDataMapJob.java
datamap/bloom/src/main/java/org/apache/carbondata/datamap/bloom/BloomCoarseGrainDataMapFactory.java
hadoop/src/main/java/org/apache/carbondata/hadoop/util/CarbonInputFormatUtil.java
core/src/main/java/org/apache/carbondata/core/util/ObjectSerializationUtil.java
streaming/src/main/java/org/apache/carbondata/streaming/CarbonStreamOutputFormat.java
core/src/main/java/org/apache/carbondata/core/indexstore/BlockletDetailsFetcher.java
core/src/main/java/org/apache/carbondata/core/datamap/TableDataMap.java
datamap/lucene/src/main/java/org/apache/carbondata/datamap/lucene/LuceneDataMapFactoryBase.java
core/src/main/java/org/apache/carbondata/core/datamap/DataMapChooser.java
texts:Refactor the datamap code and clear the datamap from executor on table drop
During query, blockletDataMapFactory maintains a segmentMap which has mapping ofsegmentId -> list of index file, and this will be used while getting the extended blockletby checking whether the blocklet present in the index or not.In case of Lucene, the datamap job will be launched and during pruning the segmentMap will be addedin executor and this map will be cleared in driver when drop table is called, but it will not be cleared in executor.so when the query is fired after table or datamap is dropped, the lucene query fails.
issueID:CARBONDATA-2486
type:Bug
changed files:
texts:set search mode information is not updated in the documentation
set search mode (set carbon.search.enabled parameter)information is not updated in the Carbondata documentation.
issueID:CARBONDATA-2487
type:Bug
changed files:
texts:Block filters for lucene with more than one text_match udf

issueID:CARBONDATA-2489
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/datamap/DataMapStoreManager.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/merger/UnsafeIntermediateFileMerger.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockletDataMapFactory.java
integration/presto/src/main/java/org/apache/carbondata/presto/readers/IntegerStreamReader.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/merger/UnsafeIntermediateMerger.java
core/src/main/java/org/apache/carbondata/core/scan/scanner/impl/BlockletFilterScanner.java
integration/presto/src/main/java/org/apache/carbondata/presto/readers/DoubleStreamReader.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RestructureEvaluatorImpl.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelFilterExecuterImpl.java
integration/presto/src/main/java/org/apache/carbondata/presto/impl/CarbonLocalInputSplit.java
integration/presto/src/main/java/org/apache/carbondata/presto/readers/BooleanStreamReader.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelRangeGrtrThanEquaToFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/datastore/page/UnsafeDecimalColumnPage.java
core/src/main/java/org/apache/carbondata/core/util/path/CarbonTablePath.java
core/src/main/java/org/apache/carbondata/core/locks/ZookeeperInit.java
processing/src/main/java/org/apache/carbondata/processing/loading/BadRecordsLogger.java
store/sdk/src/main/java/org/apache/carbondata/store/LocalCarbonStore.java
integration/presto/src/main/java/org/apache/carbondata/presto/readers/ObjectStreamReader.java
integration/spark-datasource/src/main/scala/org/apache/carbondata/spark/vectorreader/VectorizedCarbonRecordReader.java
core/src/main/java/org/apache/carbondata/core/metadata/datatype/DecimalType.java
integration/presto/src/main/java/org/apache/carbondata/presto/readers/ShortStreamReader.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/merger/UnsafeInMemoryIntermediateDataMerger.java
core/src/main/java/org/apache/carbondata/core/util/AbstractDataFileFooterConverter.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelRangeLessThanFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RangeValueFilterExecuterImpl.java
processing/src/main/java/org/apache/carbondata/processing/merger/RowResultMergerProcessor.java
core/src/main/java/org/apache/carbondata/core/datamap/dev/BlockletSerializer.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/SegmentIndexFileStore.java
core/src/main/java/org/apache/carbondata/core/cache/dictionary/ColumnDictionaryInfo.java
processing/src/main/java/org/apache/carbondata/processing/store/writer/AbstractFactDataWriter.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/column/ColumnSchema.java
integration/presto/src/main/java/org/apache/carbondata/presto/readers/LongStreamReader.java
processing/src/main/java/org/apache/carbondata/processing/loading/steps/InputProcessorStepImpl.java
processing/src/main/java/org/apache/carbondata/processing/merger/CarbonDataMergerUtil.java
core/src/main/java/org/apache/carbondata/core/util/CarbonMetadataUtil.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/EncodingFactory.java
core/src/main/java/org/apache/carbondata/core/memory/UnsafeMemoryManager.java
core/src/main/java/org/apache/carbondata/core/preagg/TimeSeriesUDF.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelRangeGrtThanFiterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/datamap/DataMapChooser.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/merger/UnsafeSingleThreadFinalSortFilesMerger.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/impl/AbstractDimensionColumnPage.java
common/src/main/java/org/apache/carbondata/common/logging/impl/ExtendedRollingFileAppender.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/reader/dimension/v3/DimensionChunkReaderV3.java
core/src/main/java/org/apache/carbondata/core/util/DataTypeUtil.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/SingleThreadFinalSortFilesMerger.java
core/src/main/java/org/apache/carbondata/core/datastore/filesystem/AbstractDFSCarbonFile.java
core/src/main/java/org/apache/carbondata/core/scan/result/iterator/AbstractDetailQueryResultIterator.java
integration/hive/src/main/java/org/apache/carbondata/hive/CarbonDictionaryDecodeReadSupport.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/impl/ThreadStatusObserver.java
core/src/main/java/org/apache/carbondata/core/datastore/page/statistics/LVStringStatsCollector.java
core/src/main/java/org/apache/carbondata/core/datastore/page/UnsafeFixLengthColumnPage.java
integration/presto/src/main/java/org/apache/carbondata/presto/CarbondataSplitManager.java
core/src/main/java/org/apache/carbondata/core/scan/filter/FilterExpressionProcessor.java
integration/presto/src/main/java/org/apache/carbondata/presto/readers/SliceStreamReader.java
core/src/main/java/org/apache/carbondata/core/scan/filter/FilterUtil.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/UnsafeSortDataRows.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/ConditionalFilterResolverImpl.java
core/src/main/java/org/apache/carbondata/core/statusmanager/LoadMetadataDetails.java
integration/hive/src/main/java/org/apache/carbondata/hive/MapredCarbonInputFormat.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/impl/VariableLengthDimensionColumnPage.java
core/src/main/java/org/apache/carbondata/core/datastore/page/statistics/KeyPageStatsCollector.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/impl/FixedLengthDimensionColumnPage.java
core/src/main/java/org/apache/carbondata/core/metadata/datatype/StructType.java
core/src/main/java/org/apache/carbondata/core/scan/collector/impl/RestructureBasedRawResultCollector.java
store/sdk/src/main/java/org/apache/carbondata/sdk/file/CarbonReaderBuilder.java
processing/src/main/java/org/apache/carbondata/processing/util/CarbonQueryUtil.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelRangeLessThanEqualFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/datamap/AbstractDataMapJob.java
core/src/main/java/org/apache/carbondata/core/datastore/compression/Compressor.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
integration/spark2/src/main/scala/org/apache/carbondata/stream/CarbonStreamRecordReader.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonOutputCommitter.java
core/src/main/java/org/apache/carbondata/core/datastore/compression/SnappyCompressor.java
integration/presto/src/main/java/org/apache/carbondata/presto/readers/TimestampStreamReader.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/IntermediateFileMerger.java
core/src/main/java/org/apache/carbondata/core/metadata/datatype/ArrayType.java
texts:Fix coverity reported warnings
Fix the coverity warnings that are validhttps://scan.coverity.com/projects/carbondata?tab=overview
issueID:CARBONDATA-249
type:Improvement
changed files:
texts:As LongType.simpleString in spark is "bigint", Carbon will convert Long to BigInt
Describe command will show DataType.simpleString in datatype column,for LongType Spark DataType, simpleString is "bigint".We are internally using long as name for bigint, which needs to be changed to bigint in Carbon DataTypes.
issueID:CARBONDATA-2491
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/datamap/DataMapStoreManager.java
core/src/main/java/org/apache/carbondata/core/scan/result/iterator/ChunkRowIterator.java
store/sdk/src/main/java/org/apache/carbondata/sdk/file/CarbonReader.java
hadoop/src/main/java/org/apache/carbondata/hadoop/CarbonRecordReader.java
texts:There are some error when reader twice with SDK carbonReader
Test code in org.apache.carbondata.sdk.file.CarbonReaderTest:  @Test  public void testWriteAndReadFiles() throws IOException, InterruptedException {    String path = "./testWriteFiles";    FileUtils.deleteDirectory(new File(path));    Field[] fields = new Field[2];    fields[0] = new Field("name", DataTypes.STRING);    fields[1] = new Field("age", DataTypes.INT);    TestUtil.writeFilesAndVerify(new Schema(fields), path, true);    CarbonReader reader = CarbonReader.builder(path, "_temp")        .projection(new String[]{"name", "age"}).build();    // expected output after sorting    String[] name = new String[100];    int[] age = new int[100];    for (int i = 0; i < 100; i++) {      name[i] = "robot" + (i / 10);      age[i] = (i % 10) * 10 + i / 10;    }    int i = 0;    while (reader.hasNext()) {      Object[] row = (Object[]) reader.readNextRow();      // Default sort column is applied for dimensions. So, need  to validate accordingly      Assert.assertEquals(name[i], row[0]);      Assert.assertEquals(age[i], row[1]);      i++;    }    Assert.assertEquals(i, 100);    reader.close();    // Read again    CarbonReader reader2 = CarbonReader        .builder(path, "_temp")        .projection(new String[]{"name", "age"})        .build();    i = 0;    while (reader2.hasNext()) {      Object[] row = (Object[]) reader2.readNextRow();      // Default sort column is applied for dimensions. So, need  to validate accordingly      Assert.assertEquals(name[i], row[0]);      Assert.assertEquals(age[i], row[1]);      i++;    }    Assert.assertEquals(i, 100);    reader2.close();    FileUtils.deleteDirectory(new File(path));  }There are some error when build reader in the second tie with SDK carbonReader:When run this case first time:Error1java.lang.StringIndexOutOfBoundsException: String index out of range: -1 at java.lang.String.substring(String.java:1967) at org.apache.carbondata.core.util.path.CarbonTablePath$DataFileUtil.getTaskNo(CarbonTablePath.java:510) at org.apache.carbondata.hadoop.api.CarbonInputFormat.getDataBlocksOfSegment(CarbonInputFormat.java:372) at org.apache.carbondata.hadoop.api.CarbonFileInputFormat.getSplits(CarbonFileInputFormat.java:197) at org.apache.carbondata.hadoop.api.CarbonFileInputFormat.getSplits(CarbonFileInputFormat.java:166) at org.apache.carbondata.sdk.file.CarbonReaderBuilder.build(CarbonReaderBuilder.java:160) at org.apache.carbondata.sdk.file.CarbonReaderTest.testWriteAndReadFiles(CarbonReaderTest.java:64) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at junit.framework.TestCase.runTest(TestCase.java:176) at junit.framework.TestCase.runBare(TestCase.java:141) at junit.framework.TestResult$1.protect(TestResult.java:122) at junit.framework.TestResult.runProtected(TestResult.java:142) at junit.framework.TestResult.run(TestResult.java:125) at junit.framework.TestCase.run(TestCase.java:129) at junit.framework.TestSuite.runTest(TestSuite.java:255) at junit.framework.TestSuite.run(TestSuite.java:250) at org.junit.internal.runners.JUnit38ClassRunner.run(JUnit38ClassRunner.java:84) at org.junit.runner.JUnitCore.run(JUnitCore.java:160) at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:68) at com.intellij.rt.execution.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:47) at com.intellij.rt.execution.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:242) at com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:70)sometimes has the second exception:Error2:java.lang.NegativeArraySizeException at org.apache.carbondata.core.indexstore.row.UnsafeDataMapRow.convertToSafeRow(UnsafeDataMapRow.java:234) at org.apache.carbondata.core.indexstore.row.UnsafeDataMapRow.convertToSafeRow(UnsafeDataMapRow.java:245) at org.apache.carbondata.core.indexstore.blockletindex.BlockletDataMap.prune(BlockletDataMap.java:648) at org.apache.carbondata.core.indexstore.blockletindex.BlockletDataMap.prune(BlockletDataMap.java:705) at org.apache.carbondata.core.indexstore.blockletindex.BlockletDataMapFactory.getAllBlocklets(BlockletDataMapFactory.java:297) at org.apache.carbondata.core.datamap.TableDataMap.prune(TableDataMap.java:92) at org.apache.carbondata.core.datamap.dev.expr.DataMapExprWrapperImpl.prune(DataMapExprWrapperImpl.java:52) at org.apache.carbondata.hadoop.api.CarbonInputFormat.getPrunedBlocklets(CarbonInputFormat.java:425) at org.apache.carbondata.hadoop.api.CarbonInputFormat.getDataBlocksOfSegment(CarbonInputFormat.java:362) at org.apache.carbondata.hadoop.api.CarbonFileInputFormat.getSplits(CarbonFileInputFormat.java:197) at org.apache.carbondata.hadoop.api.CarbonFileInputFormat.getSplits(CarbonFileInputFormat.java:166) at org.apache.carbondata.sdk.file.CarbonReaderBuilder.build(CarbonReaderBuilder.java:160) at org.apache.carbondata.sdk.file.CarbonReaderTest.testWriteAndReadFiles(CarbonReaderTest.java:64) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at junit.framework.TestCase.runTest(TestCase.java:176) at junit.framework.TestCase.runBare(TestCase.java:141) at junit.framework.TestResult$1.protect(TestResult.java:122) at junit.framework.TestResult.runProtected(TestResult.java:142) at junit.framework.TestResult.run(TestResult.java:125) at junit.framework.TestCase.run(TestCase.java:129) at junit.framework.TestSuite.runTest(TestSuite.java:255) at junit.framework.TestSuite.run(TestSuite.java:250) at org.junit.internal.runners.JUnit38ClassRunner.run(JUnit38ClassRunner.java:84) at org.junit.runner.JUnitCore.run(JUnitCore.java:160) at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:68) at com.intellij.rt.execution.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:47) at com.intellij.rt.execution.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:242) at com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:70)
issueID:CARBONDATA-2492
type:Bug
changed files:processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/UnsafeSortDataRows.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/impl/UnsafeParallelReadMergeSorterImpl.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonTableOutputFormat.java
texts:Thread leak issue in case of any data load failure
Problem: Executor service is not getting shutdown after data load failure.root cause: When sort step is failing because of any exception it is not updating its parent thread about the failure because of this executor service is not getting shutdownsolution: update the parent thread about the failure
issueID:CARBONDATA-2493
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/metadata/datatype/MapType.java
core/src/main/java/org/apache/carbondata/core/metadata/datatype/DataType.java
core/src/main/java/org/apache/carbondata/core/metadata/datatype/StructType.java
core/src/main/java/org/apache/carbondata/core/metadata/datatype/StructField.java
core/src/main/java/org/apache/carbondata/core/metadata/datatype/DecimalType.java
core/src/main/java/org/apache/carbondata/core/metadata/datatype/ArrayType.java
texts:DataType.equals() failes for complex types
Only object comparision happens for  DataTYpe.equals complex type are not singleton objects,  So, even the data types are same, compare returns falseOne of place I found issue is in ColumnSchema.equals() } else if (!dataType.equals(other.dataType)) { return false;} That check will fail.
issueID:CARBONDATA-2494
type:Improvement
changed files:datamap/lucene/src/main/java/org/apache/carbondata/datamap/lucene/LuceneFineGrainDataMap.java
datamap/lucene/src/main/java/org/apache/carbondata/datamap/lucene/LuceneFineGrainDataMapFactory.java
datamap/lucene/src/main/java/org/apache/carbondata/datamap/lucene/LuceneDataMapFactoryBase.java
datamap/lucene/src/main/java/org/apache/carbondata/datamap/lucene/LuceneDataMapBuilder.java
datamap/lucene/src/main/java/org/apache/carbondata/datamap/lucene/LuceneDataMapWriter.java
texts:Improve Lucene datamap size and performnace.
Improved lucene datamap size and performance by using the following parameters.New DM properties `flush_cache`: size of the cache to maintain in Lucene writer, if specified then it tries to aggregate the unique data till the cache limit and flush to Lucene. It is best suitable for low cardinality dimensions.`split_blocklet`: when made as true then store the data in blocklet wise in lucene , it means new folder will be created for each blocklet thus it eliminates storing on blockletid in lucene. And also it makes lucene small chunks of data.
issueID:CARBONDATA-2495
type:Improvement
changed files:
texts:Add document for bloomfilter datamap

issueID:CARBONDATA-2496
type:Improvement
changed files:datamap/bloom/src/main/java/org/apache/carbondata/datamap/bloom/BloomDataMapWriter.java
datamap/bloom/src/main/java/org/apache/hadoop/util/bloom/CarbonBloomFilter.java
datamap/bloom/src/main/java/org/apache/carbondata/datamap/bloom/BloomCoarseGrainDataMapFactory.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockletDataMapFactory.java
datamap/bloom/src/main/java/org/apache/carbondata/datamap/bloom/BloomDataMapCache.java
datamap/bloom/src/main/java/org/apache/carbondata/datamap/bloom/BloomDataMapBuilder.java
datamap/bloom/src/main/java/org/apache/carbondata/datamap/bloom/BloomCoarseGrainDataMap.java
texts:Chnage the bloom implementation to hadoop for better performance and compression
The current implementation of bloom does not give better performance and compression, And also it adds new guava dependency to carbon. So remove the guava dependency and add hadoop bloom.
issueID:CARBONDATA-2497
type:Bug
changed files:
texts:There are some error log when running CGDataMapTestCase
When running org.apache.carbondata.spark.testsuite.datamap.CGDataMapTestCase,there are some error log:ERROR DataMapStoreManager: ScalaTest-run-running-CGDataMapTestCase failed to get carbon table from table Path18/05/20 20:43:04 ERROR ColumnSchema: column name is name but other column name is city18/05/20 20:43:04 ERROR ColumnSchema: column name is age but other column name is idDetail:18/05/20 20:43:00 AUDIT CarbonCreateTableCommand: [localhost][xubo][Thread-1]Creating Table with Database name [default] and Table name [normal_test]18/05/20 20:43:00 ERROR DataMapStoreManager: ScalaTest-run failed to get carbon table from table Path18/05/20 20:43:01 AUDIT CarbonCreateTableCommand: [localhost][xubo][Thread-1]Table created with Database name [default] and Table name [normal_test]18/05/20 20:43:01 AUDIT CarbonDataRDDFactory$: [localhost][xubo][Thread-1]Data load request has been received for table default.normal_test18/05/20 20:43:02 ERROR ColumnSchema: column name is name but other column name is city18/05/20 20:43:02 ERROR ColumnSchema: column name is age but other column name is id18/05/20 20:43:04 AUDIT CarbonDataRDDFactory$: [localhost][xubo][Thread-1]Data load is successful for default.normal_test18/05/20 20:43:04 AUDIT CarbonCreateTableCommand: [localhost][xubo][Thread-1]Creating Table with Database name [default] and Table name [datamap_test_cg]18/05/20 20:43:04 ERROR DataMapStoreManager: ScalaTest-run-running-CGDataMapTestCase failed to get carbon table from table Path18/05/20 20:43:04 AUDIT CarbonCreateTableCommand: [localhost][xubo][Thread-1]Table created with Database name [default] and Table name [datamap_test_cg]18/05/20 20:43:04 AUDIT CarbonCreateDataMapCommand: [localhost][xubo][Thread-1]DataMap cgdatamap successfully added18/05/20 20:43:04 AUDIT CarbonDataRDDFactory$: [localhost][xubo][Thread-1]Data load request has been received for table default.datamap_test_cg18/05/20 20:43:04 ERROR ColumnSchema: column name is name but other column name is city18/05/20 20:43:04 ERROR ColumnSchema: column name is age but other column name is id18/05/20 20:43:06 AUDIT CarbonDataRDDFactory$: [localhost][xubo][Thread-1]Data load is successful for default.datamap_test_cg18/05/20 20:43:07 AUDIT CarbonCreateTableCommand: [localhost][xubo][Thread-1]Creating Table with Database name [default] and Table name [datamap_test]18/05/20 20:43:07 ERROR DataMapStoreManager: ScalaTest-run-running-CGDataMapTestCase failed to get carbon table from table Path18/05/20 20:43:07 AUDIT CarbonCreateTableCommand: [localhost][xubo][Thread-1]Table created with Database name [default] and Table name [datamap_test]18/05/20 20:43:07 AUDIT CarbonCreateDataMapCommand: [localhost][xubo][Thread-1]DataMap ggdatamap1 successfully added18/05/20 20:43:07 AUDIT CarbonCreateDataMapCommand: [localhost][xubo][Thread-1]DataMap ggdatamap2 successfully added18/05/20 20:43:07 AUDIT CarbonDataRDDFactory$: [localhost][xubo][Thread-1]Data load request has been received for table default.datamap_test18/05/20 20:43:07 ERROR ColumnSchema: column name is name but other column name is city18/05/20 20:43:07 ERROR ColumnSchema: column name is age but other column name is id18/05/20 20:43:09 AUDIT CarbonDataRDDFactory$: [localhost][xubo][Thread-1]Data load is successful for default.datamap_test18/05/20 20:43:09 AUDIT CarbonDropTableCommand: [localhost][xubo][Thread-1]Deleting table [datamap_test] under database [default]18/05/20 20:43:10 ERROR DataMapStoreManager: ScalaTest-run-running-CGDataMapTestCase failed to get carbon table from table Path18/05/20 20:43:10 ERROR DataMapStoreManager: ScalaTest-run-running-CGDataMapTestCase failed to get carbon table from table Path18/05/20 20:43:10 AUDIT CarbonDropTableCommand: [localhost][xubo][Thread-1]Deleted table [datamap_test] under database [default]18/05/20 20:43:10 AUDIT CarbonCreateTableCommand: [localhost][xubo][Thread-1]Creating Table with Database name [default] and Table name [datamap_test]18/05/20 20:43:10 AUDIT CarbonCreateTableCommand: [localhost][xubo][Thread-1]Table created with Database name [default] and Table name [datamap_test]18/05/20 20:43:10 AUDIT CarbonCreateDataMapCommand: [localhost][xubo][Thread-1]DataMap datamap1 successfully added18/05/20 20:43:10 AUDIT CarbonCreateDataMapCommand: [localhost][xubo][Thread-1]DataMap datamap2 successfully added18/05/20 20:43:10 AUDIT CarbonDataRDDFactory$: [localhost][xubo][Thread-1]Data load request has been received for table default.datamap_test18/05/20 20:43:10 ERROR ColumnSchema: column name is name but other column name is city18/05/20 20:43:10 ERROR ColumnSchema: column name is age but other column name is id18/05/20 20:43:11 AUDIT CarbonDataRDDFactory$: [localhost][xubo][Thread-1]Data load is successful for default.datamap_test18/05/20 20:43:11 AUDIT CacheProvider: [localhost][xubo][Thread-1]The key carbon.datamap.visible.default.datamap_test.datamap1 with value false added in the session param18/05/20 20:43:12 AUDIT CacheProvider: [localhost][xubo][Thread-1]The key carbon.datamap.visible.default.datamap_test.datamap2 with value false added in the session param18/05/20 20:43:12 AUDIT CacheProvider: [localhost][xubo][Thread-1]The key carbon.datamap.visible.default.datamap_test.datamap1 with value true added in the session param18/05/20 20:43:12 AUDIT CacheProvider: [localhost][xubo][Thread-1]The key carbon.datamap.visible.default.datamap_test.datamap1 with value true added in the session param18/05/20 20:43:12 AUDIT CarbonCreateTableCommand: [localhost][xubo][Thread-1]Creating Table with Database name [default] and Table name [datamap_store_test]18/05/20 20:43:12 ERROR DataMapStoreManager: ScalaTest-run-running-CGDataMapTestCase failed to get carbon table from table Path18/05/20 20:43:12 AUDIT CarbonCreateTableCommand: [localhost][xubo][Thread-1]Table created with Database name [default] and Table name [datamap_store_test]18/05/20 20:43:12 AUDIT CarbonCreateDataMapCommand: [localhost][xubo][Thread-1]DataMap test_cg_datamap successfully added18/05/20 20:43:13 AUDIT CarbonCreateTableCommand: [localhost][xubo][Thread-1]Creating Table with Database name [default] and Table name [datamap_store_test1]18/05/20 20:43:13 ERROR DataMapStoreManager: ScalaTest-run-running-CGDataMapTestCase failed to get carbon table from table Path18/05/20 20:43:13 AUDIT CarbonCreateTableCommand: [localhost][xubo][Thread-1]Table created with Database name [default] and Table name [datamap_store_test1]18/05/20 20:43:13 AUDIT CarbonCreateDataMapCommand: [localhost][xubo][Thread-1]DataMap test_cg_datamap1 successfully added18/05/20 20:43:13 AUDIT CarbonDropDataMapCommand: [localhost][xubo][Thread-1]Deleting datamap [test_cg_datamap1] under table [datamap_store_test1]18/05/20 20:43:13 AUDIT CarbonCreateTableCommand: [localhost][xubo][Thread-1]Creating Table with Database name [default] and Table name [datamap_store_test2]18/05/20 20:43:13 ERROR DataMapStoreManager: ScalaTest-run-running-CGDataMapTestCase failed to get carbon table from table Path18/05/20 20:43:13 AUDIT CarbonCreateTableCommand: [localhost][xubo][Thread-1]Table created with Database name [default] and Table name [datamap_store_test2]18/05/20 20:43:13 AUDIT CarbonCreateDataMapCommand: [localhost][xubo][Thread-1]DataMap test_cg_datamap2 successfully added18/05/20 20:43:13 AUDIT CarbonDropDataMapCommand: [localhost][xubo][Thread-1]Deleting datamap [test_cg_datamap2] under table [datamap_store_test2]18/05/20 20:43:13 AUDIT CarbonDropTableCommand: [localhost][xubo][Thread-1]Deleting table [normal_test] under database [default]18/05/20 20:43:13 AUDIT CarbonDropTableCommand: [localhost][xubo][Thread-1]Deleted table [normal_test] under database [default]18/05/20 20:43:13 AUDIT CarbonDropTableCommand: [localhost][xubo][Thread-1]Deleting table [datamap_test] under database [default]18/05/20 20:43:13 ERROR DataMapStoreManager: ScalaTest-run failed to get carbon table from table Path18/05/20 20:43:13 ERROR DataMapStoreManager: ScalaTest-run failed to get carbon table from table Path18/05/20 20:43:13 AUDIT CarbonDropTableCommand: [localhost][xubo][Thread-1]Deleted table [datamap_test] under database [default]18/05/20 20:43:13 AUDIT CarbonDropTableCommand: [localhost][xubo][Thread-1]Deleting table [datamap_test_cg] under database [default]18/05/20 20:43:13 ERROR DataMapStoreManager: ScalaTest-run failed to get carbon table from table Path18/05/20 20:43:13 AUDIT CarbonDropTableCommand: [localhost][xubo][Thread-1]Deleted table [datamap_test_cg] under database [default]18/05/20 20:43:13 AUDIT CarbonDropTableCommand: [localhost][xubo][Thread-1]Deleting table [datamap_store_test] under database [default]18/05/20 20:43:13 ERROR DataMapStoreManager: ScalaTest-run failed to get carbon table from table Path18/05/20 20:43:13 AUDIT CarbonDropTableCommand: [localhost][xubo][Thread-1]Deleted table [datamap_store_test] under database [default]18/05/20 20:43:13 AUDIT CarbonDropTableCommand: [localhost][xubo][Thread-1]Deleting table [datamap_store_test1] under database [default]18/05/20 20:43:13 AUDIT CarbonDropTableCommand: [localhost][xubo][Thread-1]Deleted table [datamap_store_test1] under database [default]18/05/20 20:43:13 AUDIT CarbonDropTableCommand: [localhost][xubo][Thread-1]Deleting table [datamap_store_test2] under database [default]18/05/20 20:43:13 AUDIT CarbonDropTableCommand: [localhost][xubo][Thread-1]Deleted table [datamap_store_test2] under database [default]
issueID:CARBONDATA-2498
type:Bug
changed files:store/sdk/src/main/java/org/apache/carbondata/sdk/file/AvroCarbonWriter.java
examples/spark2/src/main/java/org/apache/carbondata/examples/sdk/CarbonReaderExample.java
examples/spark2/src/main/java/org/apache/carbondata/examples/sdk/SDKS3Example.java
store/sdk/src/main/java/org/apache/carbondata/sdk/file/CarbonWriterBuilder.java
texts:Change CarbonWriterBuilder interface to take schema while creating writer

issueID:CARBONDATA-2499
type:Improvement
changed files:
texts:Validate the visible/invisible status of datamap
This jira https://issues.apache.org/jira/browse/CARBONDATA-2380 not check the visible/invisible status of datamap.  There will not thro exception when the related code changed and affect visible/invisible status of datamap.  So we should add some test case for this function.
issueID:CARBONDATA-25
type:Bug
changed files:
texts:Filter query issue for >, <, <= than filter
1. select count from a12 where dob > '2014-07-01 12:07:28'   throwing runtime exception2. select count from a12 where dob < '2014-07-01 12:07:28'   is including the null values also.3. select count from a12 where dob <=  '2014-07-01 12:07:28'   is including the null value Null should not considered in less than filter.create cube command: create table a12(empid String,ename String,sal double,deptno int,mgr string,gender string," +        "dob timestamp,comm decimal(4,2),desc string) stored by 'org.apache.carbondata.formatdata:empid,ename,sal,deptno,mgr,gender,dob,comm,desc1,abc,1233,10,2,,2014-07-01 12:07:28,1234.191,string_null2,bcd,1322,,3,f,2014-07-01 12:07:28,19.99,int_null3,cde,4322,,4,m,,16.996,date_null4,    ,43243,,5,m,,999.117,string_space5,,43242,20,6,m,2017-07-01 12:07:28,99.999,string_null6,ijk,,20,6,m,2017-07-01 12:07:28,50089,double_null7,pqr,2422,20,6,m,2017-07-01 12:07:28,32.339,decimal_null8
issueID:CARBONDATA-250
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/scan/filter/FilterUtil.java
core/src/main/java/org/apache/carbondata/core/scan/expression/conditional/EqualToExpression.java
texts:Throw exception and fail the data load if provided MAXCOLUMNS value is not proper
If the provided MAXCOLUMNS value in load query is not proper, then throw exception and fail the data load.Impact Area : Load query with maxcolumns option value
issueID:CARBONDATA-2500
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/metadata/schema/table/TableInfo.java
core/src/main/java/org/apache/carbondata/core/util/DataFileFooterConverter2.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/TableSchemaBuilder.java
core/src/main/java/org/apache/carbondata/core/util/DataFileFooterConverterV3.java
core/src/main/java/org/apache/carbondata/core/reader/CarbonHeaderReader.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/CarbonTable.java
core/src/main/java/org/apache/carbondata/core/util/DataFileFooterConverter.java
core/src/main/java/org/apache/carbondata/core/util/AbstractDataFileFooterConverter.java
store/sdk/src/main/java/org/apache/carbondata/sdk/file/CarbonReader.java
core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
core/src/main/java/org/apache/carbondata/core/metadata/converter/ThriftWrapperSchemaConverterImpl.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/TableSchema.java
texts:The order is different between write and read data type of schema in SDK
The order is different between write and read data type of schema in SDK
issueID:CARBONDATA-2503
type:Bug
changed files:store/sdk/src/main/java/org/apache/carbondata/sdk/file/CarbonWriterBuilder.java
texts:Data write fails if empty value is provided for sort columns in sdk
Reproduce step : Use SDK to write data where empty value is provided for sort columns 
issueID:CARBONDATA-2504
type:New Feature
changed files:integration/spark-datasource/src/main/scala/org/apache/carbondata/converter/SparkDataTypeConverterImpl.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/CarbonTable.java
store/sdk/src/main/java/org/apache/carbondata/store/LocalCarbonStore.java
common/src/main/java/org/apache/carbondata/common/exceptions/NoSuchStreamException.java
texts:Support StreamSQL for streaming job
Currently carbon supports creating streaming job via Spark Streaming API, this requires user to use spark-submit to create the streaming job. To make it easier for SQL users, carbon should support StreamSQL to manage the streaming job.
issueID:CARBONDATA-2506
type:Bug
changed files:
texts:Tests failed occasionally with ConcurrentModificationException
```java.util.ConcurrentModificationException at java.util.HashMap$HashIterator.nextNode(HashMap.java:1442) at java.util.HashMap$KeyIterator.next(HashMap.java:1466) at java.util.AbstractCollection.toArray(AbstractCollection.java:196) at org.apache.carbondata.core.indexstore.blockletindex.BlockletDataMapFactory.clear(BlockletDataMapFactory.java:284) at org.apache.carbondata.core.datamap.DataMapStoreManager.clearDataMaps(DataMapStoreManager.java:423) at org.apache.carbondata.core.datamap.DistributableDataMapFormat$1.initialize(DistributableDataMapFormat.java:123)```
issueID:CARBONDATA-2507
type:Task
changed files:core/src/main/java/org/apache/carbondata/core/util/SessionParams.java
core/src/main/java/org/apache/carbondata/core/util/CarbonProperties.java
texts:Some properties not validate in CarbonData, like enable.offheap.sort
Some properties not validate in CarbonData, like enable.offheap.sort Refer:org.apache.carbondata.core.util.CarbonProperties#validateAndLoadDefaultProperties(java.lang.String)
issueID:CARBONDATA-2508
type:Bug
changed files:
texts:There are some errors when I running SearchModeExample
There are some errors when I running org.apache.carbondata.examples.SearchModeExample:org.apache.carbondata.examples.SearchModeExamplelog4j:WARN No appenders could be found for logger (org.apache.carbondata.core.util.CarbonProperties).log4j:WARN Please initialize the log4j system properly.log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties18/05/22 16:12:42 INFO SparkContext: Running Spark version 2.2.118/05/22 16:12:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable18/05/22 16:12:42 WARN Utils: Your hostname, localhost resolves to a loopback address: 127.0.0.1; using 192.168.44.90 instead (on interface en3)18/05/22 16:12:42 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address18/05/22 16:12:42 INFO SparkContext: Submitted application: SearchModeExample18/05/22 16:12:42 INFO SecurityManager: Changing view acls to: xubo18/05/22 16:12:42 INFO SecurityManager: Changing modify acls to: xubo18/05/22 16:12:42 INFO SecurityManager: Changing view acls groups to: 18/05/22 16:12:42 INFO SecurityManager: Changing modify acls groups to: 18/05/22 16:12:42 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(xubo); groups with view permissions: Set(); users  with modify permissions: Set(xubo); groups with modify permissions: Set()18/05/22 16:12:43 INFO Utils: Successfully started service 'sparkDriver' on port 64124.18/05/22 16:12:43 INFO SparkEnv: Registering MapOutputTracker18/05/22 16:12:43 INFO SparkEnv: Registering BlockManagerMaster18/05/22 16:12:43 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information18/05/22 16:12:43 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up18/05/22 16:12:43 INFO DiskBlockManager: Created local directory at /private/var/folders/lw/4y5plg0x7rq45h38m4sfxlbm0000gn/T/blockmgr-0ed23439-9e4f-4798-b197-0681f40e9fa518/05/22 16:12:43 INFO MemoryStore: MemoryStore started with capacity 2004.6 MB18/05/22 16:12:43 INFO SparkEnv: Registering OutputCommitCoordinator18/05/22 16:12:43 INFO Utils: Successfully started service 'SparkUI' on port 4040.18/05/22 16:12:43 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.44.90:404018/05/22 16:12:43 INFO Executor: Starting executor ID driver on host localhost18/05/22 16:12:43 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 64125.18/05/22 16:12:43 INFO NettyBlockTransferService: Server created on 192.168.44.90:6412518/05/22 16:12:43 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy18/05/22 16:12:43 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.44.90, 64125, None)18/05/22 16:12:43 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.44.90:64125 with 2004.6 MB RAM, BlockManagerId(driver, 192.168.44.90, 64125, None)18/05/22 16:12:43 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.44.90, 64125, None)18/05/22 16:12:43 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.44.90, 64125, None)18/05/22 16:12:43 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/Users/xubo/Desktop/xubo/git/carbondata1/spark-warehouse').18/05/22 16:12:43 INFO SharedState: Warehouse path is 'file:/Users/xubo/Desktop/xubo/git/carbondata1/spark-warehouse'.18/05/22 16:12:44 INFO HiveUtils: Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.18/05/22 16:12:45 INFO HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore18/05/22 16:12:45 INFO ObjectStore: ObjectStore, initialize called18/05/22 16:12:45 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored18/05/22 16:12:45 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored18/05/22 16:12:46 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"18/05/22 16:12:47 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.18/05/22 16:12:47 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.18/05/22 16:12:47 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.18/05/22 16:12:47 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.18/05/22 16:12:47 INFO Query: Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing18/05/22 16:12:47 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY18/05/22 16:12:47 INFO ObjectStore: Initialized ObjectStore18/05/22 16:12:47 INFO HiveMetaStore: Added admin role in metastore18/05/22 16:12:47 INFO HiveMetaStore: Added public role in metastore18/05/22 16:12:47 INFO HiveMetaStore: No user is added in admin role, since config is empty18/05/22 16:12:48 INFO HiveMetaStore: 0: get_all_databases18/05/22 16:12:48 INFO audit: ugi=xubo ip=unknown-ip-addr cmd=get_all_databases 18/05/22 16:12:48 INFO HiveMetaStore: 0: get_functions: db=default pat=*18/05/22 16:12:48 INFO audit: ugi=xubo ip=unknown-ip-addr cmd=get_functions: db=default pat=* 18/05/22 16:12:48 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MResourceUri" is tagged as "embedded-only" so does not have its own datastore table.18/05/22 16:12:48 INFO SessionState: Created local directory: /var/folders/lw/4y5plg0x7rq45h38m4sfxlbm0000gn/T/af495878-1838-415e-98e8-83d405d8cac4_resources18/05/22 16:12:48 INFO SessionState: Created HDFS directory: /tmp/hive/xubo/af495878-1838-415e-98e8-83d405d8cac418/05/22 16:12:48 INFO SessionState: Created local directory: /var/folders/lw/4y5plg0x7rq45h38m4sfxlbm0000gn/T/xubo/af495878-1838-415e-98e8-83d405d8cac418/05/22 16:12:48 INFO SessionState: Created HDFS directory: /tmp/hive/xubo/af495878-1838-415e-98e8-83d405d8cac4/_tmp_space.db18/05/22 16:12:48 INFO HiveClientImpl: Warehouse location for Hive client (version 1.2.1) is file:/Users/xubo/Desktop/xubo/git/carbondata1/spark-warehouse18/05/22 16:12:48 INFO HiveMetaStore: 0: get_database: default18/05/22 16:12:48 INFO audit: ugi=xubo ip=unknown-ip-addr cmd=get_database: default 18/05/22 16:12:48 INFO HiveMetaStore: 0: get_database: global_temp18/05/22 16:12:48 INFO audit: ugi=xubo ip=unknown-ip-addr cmd=get_database: global_temp 18/05/22 16:12:48 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException18/05/22 16:12:48 INFO SessionState: Created local directory: /var/folders/lw/4y5plg0x7rq45h38m4sfxlbm0000gn/T/b17d0cc6-c9ad-48f5-8df5-df9b88fe3736_resources18/05/22 16:12:48 INFO SessionState: Created HDFS directory: /tmp/hive/xubo/b17d0cc6-c9ad-48f5-8df5-df9b88fe373618/05/22 16:12:48 INFO SessionState: Created local directory: /var/folders/lw/4y5plg0x7rq45h38m4sfxlbm0000gn/T/xubo/b17d0cc6-c9ad-48f5-8df5-df9b88fe373618/05/22 16:12:48 INFO SessionState: Created HDFS directory: /tmp/hive/xubo/b17d0cc6-c9ad-48f5-8df5-df9b88fe3736/_tmp_space.db18/05/22 16:12:48 INFO HiveClientImpl: Warehouse location for Hive client (version 1.2.1) is file:/Users/xubo/Desktop/xubo/git/carbondata1/spark-warehouse18/05/22 16:12:48 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint18/05/22 16:12:49 AUDIT CarbonDropTableCommand: [localhost][xubo][Thread-1]Deleting table [carbonsession_table] under database [default]18/05/22 16:12:50 AUDIT CarbonDropTableCommand: [localhost][xubo][Thread-1]Deleted table [carbonsession_table] under database [default]18/05/22 16:12:50 AUDIT CarbonCreateTableCommand: [localhost][xubo][Thread-1]Creating Table with Database name [default] and Table name [carbonsession_table]18/05/22 16:12:51 AUDIT CarbonCreateTableCommand: [localhost][xubo][Thread-1]Table created with Database name [default] and Table name [carbonsession_table]18/05/22 16:12:52 AUDIT CarbonDataRDDFactory$: [localhost][xubo][Thread-1]Data load request has been received for table default.carbonsession_table18/05/22 16:12:52 AUDIT CarbonDataRDDFactory$: [localhost][xubo][Thread-1]Data load is successful for default.carbonsession_tablesearch mode asynchronous query18/05/22 16:12:53 ERROR CarbonSession: Exception when executing search mode: null, fallback to SparkSQLException in thread "main" java.util.concurrent.ExecutionException: java.lang.NullPointerException at java.util.concurrent.FutureTask.report(FutureTask.java:122) at java.util.concurrent.FutureTask.get(FutureTask.java:192) at org.apache.carbondata.examples.SearchModeExample$$anonfun$org$apache$carbondata$examples$SearchModeExample$$runAsynchrousSQL$1.apply(SearchModeExample.scala:179) at org.apache.carbondata.examples.SearchModeExample$$anonfun$org$apache$carbondata$examples$SearchModeExample$$runAsynchrousSQL$1.apply(SearchModeExample.scala:179) at scala.collection.Iterator$class.foreach(Iterator.scala:893) at scala.collection.AbstractIterator.foreach(Iterator.scala:1336) at scala.collection.IterableLike$class.foreach(IterableLike.scala:72) at scala.collection.AbstractIterable.foreach(Iterable.scala:54) at org.apache.carbondata.examples.SearchModeExample$.org$apache$carbondata$examples$SearchModeExample$$runAsynchrousSQL(SearchModeExample.scala:179) at org.apache.carbondata.examples.SearchModeExample$$anonfun$exampleBody$1.apply$mcV$sp(SearchModeExample.scala:113) at org.apache.carbondata.examples.SearchModeExample$$anonfun$exampleBody$1.apply(SearchModeExample.scala:113) at org.apache.carbondata.examples.SearchModeExample$$anonfun$exampleBody$1.apply(SearchModeExample.scala:113) at org.apache.spark.sql.catalyst.util.package$.benchmark(package.scala:129) at org.apache.carbondata.examples.SearchModeExample$.exampleBody(SearchModeExample.scala:112) at org.apache.carbondata.examples.SearchModeExample$.main(SearchModeExample.scala:70) at org.apache.carbondata.examples.SearchModeExample.main(SearchModeExample.scala)Caused by: java.lang.NullPointerException at org.apache.carbondata.core.indexstore.blockletindex.BlockletDataMap.prune(BlockletDataMap.java:659) at org.apache.carbondata.core.indexstore.blockletindex.BlockletDataMap.prune(BlockletDataMap.java:705) at org.apache.carbondata.core.datamap.TableDataMap.prune(TableDataMap.java:101) at org.apache.carbondata.core.datamap.dev.expr.DataMapExprWrapperImpl.prune(DataMapExprWrapperImpl.java:52) at org.apache.carbondata.hadoop.api.CarbonInputFormat.getPrunedBlocklets(CarbonInputFormat.java:409) at org.apache.carbondata.hadoop.api.CarbonInputFormat.getDataBlocksOfSegment(CarbonInputFormat.java:346) at org.apache.carbondata.hadoop.api.CarbonTableInputFormat.getSplits(CarbonTableInputFormat.java:525) at org.apache.carbondata.hadoop.api.CarbonTableInputFormat.getSplits(CarbonTableInputFormat.java:249) at org.apache.carbondata.spark.rdd.CarbonScanRDD.getPartitions(CarbonScanRDD.scala:121) at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:252) at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:250) at scala.Option.getOrElse(Option.scala:121) at org.apache.spark.rdd.RDD.partitions(RDD.scala:250) at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35) at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:252) at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:250) at scala.Option.getOrElse(Option.scala:121) at org.apache.spark.rdd.RDD.partitions(RDD.scala:250) at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35) at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:252) at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:250) at scala.Option.getOrElse(Option.scala:121) at org.apache.spark.rdd.RDD.partitions(RDD.scala:250) at org.apache.spark.SparkContext.runJob(SparkContext.scala:2094) at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936) at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151) at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112) at org.apache.spark.rdd.RDD.withScope(RDD.scala:362) at org.apache.spark.rdd.RDD.collect(RDD.scala:935) at org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:278) at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:2861) at org.apache.spark.sql.Dataset$$anonfun$collect$1.apply(Dataset.scala:2387) at org.apache.spark.sql.Dataset$$anonfun$collect$1.apply(Dataset.scala:2387) at org.apache.spark.sql.Dataset$$anonfun$55.apply(Dataset.scala:2842) at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65) at org.apache.spark.sql.Dataset.withAction(Dataset.scala:2841) at org.apache.spark.sql.Dataset.collect(Dataset.scala:2387) at org.apache.carbondata.examples.SearchModeExample$$anonfun$3$$anon$1.run(SearchModeExample.scala:174) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748)error2:18/05/30 15:48:41 INFO SessionState: Created HDFS directory: /tmp/hive/xubo/125f7359-dbef-42f6-99a2-12a3ad07d2e9/_tmp_space.db18/05/30 15:48:41 INFO HiveClientImpl: Warehouse location for Hive client (version 1.2.1) is file:/Users/xubo/Desktop/xubo/git/carbondata2/spark-warehouse/18/05/30 15:48:41 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint18/05/30 15:48:42 AUDIT CarbonCreateTableCommand: [localhost][xubo][Thread-1]Creating Table with Database name [default] and Table name [carbonsession_table]18/05/30 15:48:42 ERROR DataMapStoreManager: main failed to get carbon table from table Path18/05/30 15:48:43 AUDIT CarbonCreateTableCommand: [localhost][xubo][Thread-1]Table created with Database name [default] and Table name [carbonsession_table]18/05/30 15:48:44 AUDIT CarbonDataRDDFactory$: [localhost][xubo][Thread-1]Data load request has been received for table default.carbonsession_table18/05/30 15:48:44 AUDIT CarbonDataRDDFactory$: [localhost][xubo][Thread-1]Data load is successful for default.carbonsession_tablesearch mode asynchronous query2605.495751mssearch mode synchronous query3627.856844mssparksql asynchronous query2951.804416mssparksql synchronous query9028.520219mssearch mode asynchronous queryjava.lang.NullPointerException at org.apache.carbondata.core.indexstore.blockletindex.BlockletDataMap.prune(BlockletDataMap.java:679) at org.apache.carbondata.core.datamap.TableDataMap.prune(TableDataMap.java:101) at org.apache.carbondata.core.datamap.dev.expr.DataMapExprWrapperImpl.prune(DataMapExprWrapperImpl.java:52) at org.apache.carbondata.hadoop.api.CarbonInputFormat.getPrunedBlocklets(CarbonInputFormat.java:436) at org.apache.carbondata.hadoop.api.CarbonInputFormat.getDataBlocksOfSegment(CarbonInputFormat.java:373) at org.apache.carbondata.hadoop.api.CarbonTableInputFormat.getSplits(CarbonTableInputFormat.java:525) at org.apache.carbondata.hadoop.api.CarbonTableInputFormat.getSplits(CarbonTableInputFormat.java:249) at org.apache.spark.rpc.Master.pruneBlock(Master.scala:271) at org.apache.spark.rpc.Master.search(Master.scala:217) at org.apache.carbondata.store.SparkCarbonStore.search(SparkCarbonStore.scala:144) at org.apache.spark.sql.CarbonSession.runSearch(CarbonSession.scala:225) at org.apache.spark.sql.CarbonSession.org$apache$spark$sql$CarbonSession$$trySearchMode(CarbonSession.scala:180) at org.apache.spark.sql.CarbonSession$$anonfun$sql$1.apply(CarbonSession.scala:100) at org.apache.spark.sql.CarbonSession$$anonfun$sql$1.apply(CarbonSession.scala:97) at org.apache.spark.sql.CarbonSession.withProfiler(CarbonSession.scala:156) at org.apache.spark.sql.CarbonSession.sql(CarbonSession.scala:95) at org.apache.carbondata.examples.SearchModeExample$$anonfun$3$$anon$1.run(SearchModeExample.scala:168) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748)18/05/30 15:49:04 ERROR CarbonSession: Exception when executing search mode: null, fallback to SparkSQLjava.lang.NullPointerException at org.apache.carbondata.core.indexstore.blockletindex.BlockletDataMap.prune(BlockletDataMap.java:659) at org.apache.carbondata.core.indexstore.blockletindex.BlockletDataMap.prune(BlockletDataMap.java:705) at org.apache.carbondata.core.datamap.TableDataMap.prune(TableDataMap.java:101) at org.apache.carbondata.core.datamap.dev.expr.DataMapExprWrapperImpl.prune(DataMapExprWrapperImpl.java:52) at org.apache.carbondata.hadoop.api.CarbonInputFormat.getPrunedBlocklets(CarbonInputFormat.java:436) at org.apache.carbondata.hadoop.api.CarbonInputFormat.getDataBlocksOfSegment(CarbonInputFormat.java:373) at org.apache.carbondata.hadoop.api.CarbonTableInputFormat.getSplits(CarbonTableInputFormat.java:525) at org.apache.carbondata.hadoop.api.CarbonTableInputFormat.getSplits(CarbonTableInputFormat.java:249) at org.apache.spark.rpc.Master.pruneBlock(Master.scala:271) at org.apache.spark.rpc.Master.search(Master.scala:217) at org.apache.carbondata.store.SparkCarbonStore.search(SparkCarbonStore.scala:144) at org.apache.spark.sql.CarbonSession.runSearch(CarbonSession.scala:225) at org.apache.spark.sql.CarbonSession.org$apache$spark$sql$CarbonSession$$trySearchMode(CarbonSession.scala:180) at org.apache.spark.sql.CarbonSession$$anonfun$sql$1.apply(CarbonSession.scala:100) at org.apache.spark.sql.CarbonSession$$anonfun$sql$1.apply(CarbonSession.scala:97) at org.apache.spark.sql.CarbonSession.withProfiler(CarbonSession.scala:156) at org.apache.spark.sql.CarbonSession.sql(CarbonSession.scala:95) at org.apache.carbondata.examples.SearchModeExample$$anonfun$3$$anon$1.run(SearchModeExample.scala:168) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748)18/05/30 15:49:04 ERROR CarbonSession: Exception when executing search mode: null, fallback to SparkSQL815.116785mssearch mode synchronous query2672.414774mssparksql asynchronous query1798.036148mssparksql synchronous query7715.027916ms18/05/30 15:49:16 AUDIT CarbonDropTableCommand: [localhost][xubo][Thread-1]Deleting table [carbonsession_table] under database [default]18/05/30 15:49:17 AUDIT CarbonDropTableCommand: [localhost][xubo][Thread-1]Deleted table [carbonsession_table] under database [default]Finished!Process finished with exit code 0
issueID:CARBONDATA-2509
type:Sub-task
changed files:
texts:long string columns should not be sort column

issueID:CARBONDATA-251
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
texts:making the auto compaction as blocking call.
making the auto compaction as blocking call.disabling the system level compaction lock feature.
issueID:CARBONDATA-2510
type:Sub-task
changed files:
texts:long string columns should not be dictionary columns

issueID:CARBONDATA-2511
type:Sub-task
changed files:
texts:long_string_columns should be string columns

issueID:CARBONDATA-2512
type:Sub-task
changed files:store/sdk/src/main/java/org/apache/carbondata/sdk/file/Field.java
store/sdk/src/main/java/org/apache/carbondata/sdk/file/CarbonWriterBuilder.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/TableSchemaBuilder.java
texts:Support long_string_columns in sdk

issueID:CARBONDATA-2513
type:Sub-task
changed files:
texts:Support long_string_columns property in dataframe writer

issueID:CARBONDATA-2514
type:Bug
changed files:store/sdk/src/main/java/org/apache/carbondata/sdk/file/CarbonWriterBuilder.java
core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
core/src/main/java/org/apache/carbondata/core/util/DataTypeUtil.java
texts:Duplicate columns in CarbonWriter is throwing NullPointerException

issueID:CARBONDATA-2515
type:Bug
changed files:integration/presto/src/main/java/org/apache/carbondata/presto/PrestoFilterUtil.java
texts:Filter OR Expression not working properly in Presto integration
Scenario - carbon-data Table 'load_table' with columns 'integer', 'datetime'//table creation and load code (spark) val random = new Random() val df = spark.sparkContext.parallelize(1 to (365 * 24 * 360)) .map(x => (random.nextInt(200), new Timestamp(currentMillis - (x * 1000l)))) .toDF("integer", "datetime")// Saves dataframe to carbondata file df.write.format("carbondata") .option("tableName", "load_table") .option("compress", "true") .option("tempCSV", "false") .mode(SaveMode.Overwrite) .save()SQL (through Presto CLI) - select * from load_table where integer < 10 or integer > 50;Actual result - 0 rows. Expected result - rows with integer value less than 10 and greater than 50.cause - PrestoFilterUtil is creating AND Expressions.
issueID:CARBONDATA-2516
type:Bug
changed files:integration/presto/src/main/java/org/apache/carbondata/presto/PrestoFilterUtil.java
texts:Filter Greater-than for timestamp datatype not generating Expression in PrestoFilterUtil
Scenario - carbon-data Table 'load_table' with columns 'integer', 'datetime'//table creation and load code (spark)val random = new Random()val df = spark.sparkContext.parallelize(1 to (365 * 24 * 360)).map(x => (random.nextInt(200), new Timestamp(currentMillis - (x * 1000l)))).toDF("integer", "datetime")// Saves dataframe to carbondata filedf.write.format("carbondata").option("tableName", "load_table").option("compress", "true").option("tempCSV", "false").mode(SaveMode.Overwrite).save()SQL (through Presto CLI) - select * from load_table where datetime > date_parse('2018-05-10 18:22:15', '%Y-%m-%d %T');Issue - Carbondata is having full scan over the files although we have passed greater than expression filter on timestamp.cause - PrestoFilterUtil is not creating greater than Expression for timestamp.
issueID:CARBONDATA-2519
type:Improvement
changed files:store/sdk/src/main/java/org/apache/carbondata/sdk/file/CarbonReaderBuilder.java
store/sdk/src/main/java/org/apache/carbondata/sdk/file/CarbonReader.java
store/sdk/src/main/java/org/apache/carbondata/sdk/file/CarbonWriterBuilder.java
texts:Add document for CarbonReader
Add document for CarbonReader, and change the carbon writer guide document
issueID:CARBONDATA-252
type:Bug
changed files:
texts:Filter result is not proper when Double data type values with 0.0 and -0.0 will be used
Filter result is not proper when Double data type values with 0.0 and -0.0 will be used in filter model
issueID:CARBONDATA-2520
type:Bug
changed files:datamap/bloom/src/main/java/org/apache/carbondata/datamap/bloom/BloomDataMapWriter.java
processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactDataHandlerModel.java
processing/src/main/java/org/apache/carbondata/processing/loading/steps/CarbonRowDataWriterProcessorStepImpl.java
processing/src/main/java/org/apache/carbondata/processing/loading/steps/DataWriterProcessorStepImpl.java
processing/src/main/java/org/apache/carbondata/processing/loading/AbstractDataLoadProcessorStep.java
core/src/main/java/org/apache/carbondata/core/datamap/dev/DataMapWriter.java
datamap/lucene/src/main/java/org/apache/carbondata/datamap/lucene/LuceneDataMapWriter.java
texts:datamap writers are not getting closed on task failure
Problem: The datamap writers registered to listener are closed or finished only in case of load success case and not in any failure case. So when tesing lucene, it is found that, after task is failed and the writer is not closed, so the write.lock file written in the index folder of lucene is still exists, so when next task comes to write index in same directory, it fails with the error lock file already exists.
issueID:CARBONDATA-2521
type:Improvement
changed files:store/sdk/src/main/java/org/apache/carbondata/sdk/file/CarbonReader.java
texts:Support create carbonReader without tableName
Support create carbonReader without tableName
issueID:CARBONDATA-2522
type:Bug
changed files:
texts:MV dataset when created with Joins, then it is not pointing towards the MV, while executing that join query.
When MV is created on Joining tables, then the explain of that join query points to the maintable, instead of the created MV datamap.  Queries:drop table if exists fact_table1;CREATE TABLE fact_table1 (empno int, empname String, designation String, doj Timestamp,workgroupcategory int, workgroupcategoryname String, deptno int, deptname String,projectcode int, projectjoindate Timestamp, projectenddate Timestamp,attendance int,utilization int,salary int)STORED BY 'org.apache.carbondata.format';LOAD DATA local inpath 'hdfs://hacluster/user/prasanna/data_mv.csv' INTO TABLE fact_table1 OPTIONS('DELIMITER'= ',', 'QUOTECHAR'= '"','timestampformat'='dd-MM-yyyy');LOAD DATA local inpath 'hdfs://hacluster/user/prasanna/data_mv.csv' INTO TABLE fact_table1 OPTIONS('DELIMITER'= ',', 'QUOTECHAR'= '"','timestampformat'='dd-MM-yyyy');drop table if exists fact_table2;CREATE TABLE fact_table2 (empno int, empname String, designation String, doj Timestamp,workgroupcategory int, workgroupcategoryname String, deptno int, deptname String,projectcode int, projectjoindate Timestamp, projectenddate Timestamp,attendance int,utilization int,salary int)STORED BY 'org.apache.carbondata.format';LOAD DATA local inpath 'hdfs://hacluster/user/prasanna/data_mv.csv' INTO TABLE fact_table2 OPTIONS('DELIMITER'= ',', 'QUOTECHAR'= '"','timestampformat'='dd-MM-yyyy');LOAD DATA local inpath 'hdfs://hacluster/user/prasanna/data_mv.csv' INTO TABLE fact_table2 OPTIONS('DELIMITER'= ',', 'QUOTECHAR'= '"','timestampformat'='dd-MM-yyyy');drop table if exists fact_table3;CREATE TABLE fact_table3 (empno int, empname String, designation String, doj Timestamp,workgroupcategory int, workgroupcategoryname String, deptno int, deptname String,projectcode int, projectjoindate Timestamp, projectenddate Timestamp,attendance int,utilization int,salary int)STORED BY 'org.apache.carbondata.format';LOAD DATA local inpath 'hdfs://hacluster/user/prasanna/data_mv.csv' INTO TABLE fact_table2 OPTIONS('DELIMITER'= ',', 'QUOTECHAR'= '"','timestampformat'='dd-MM-yyyy');LOAD DATA local inpath 'hdfs://hacluster/user/prasanna/data_mv.csv' INTO TABLE fact_table2 OPTIONS('DELIMITER'= ',', 'QUOTECHAR'= '"','timestampformat'='dd-MM-yyyy');create datamap datamap25 using 'mv' as select t1.empname as c1, t2.designation from fact_table1 t1,fact_table2 t2,fact_table3 t3  where t1.empname = t2.empname and t1.empname=t3.empname;explain create datamap datamap25 using 'mv' as select t1.empname as c1, t2.designation from fact_table1 t1,fact_table2 t2,fact_table3 t3  where t1.empname = t2.empname and t1.empname=t3.empname; 
issueID:CARBONDATA-2524
type:Improvement
changed files:store/sdk/src/main/java/org/apache/carbondata/sdk/file/CarbonReaderBuilder.java
texts:Support create carbonReader with default projection
Support create carbonReader with default projection
issueID:CARBONDATA-2526
type:Bug
changed files:
texts:MV datamap - When the MV datamap is created for the operators: sum(col1)+sum(col2) then when we execute a query of sum(col1+col2) it is not accessing the data from the MV.
When the MV datamap is created for the operators like: sum(col1)+sum(col2) then when we execute a query like sum(col1+col2), then it is not accessing the data from the created MV.Test queries:CREATE TABLE originTable (empno int, empname String, designation String, doj Timestamp, workgroupcategory int, workgroupcategoryname String, deptno int, deptname String, projectcode int, projectjoindate Timestamp, projectenddate Timestamp,attendance int, utilization int,salary int) STORED BY 'org.apache.carbondata.format';LOAD DATA local inpath 'hdfs://hacluster/user/prasanna/data.csv' INTO TABLE originTable OPTIONS('DELIMITER'= ',', 'QUOTECHAR'= '"','timestampformat'='dd-MM-yyyy');create datamap arithmetic_op using 'mv' as select empno,sum(salary)+sum(utilization) as total , sum(salary)/sum(utilization) as updownratio from originTable where empno>10 group by empno;rebuild datamap arithmetic_op;  explain select empno,sum(salary)+sum(utilization) as total , sum(salary)/sum(utilization) as updownratio from originTable where empno>10 group by empno;explain select empno,sum(salary+utilization) as total from originTable where empno>10 group by empno;As   sum(col1)+sum(col2) = sum(col1+col2) are equal, it should point to the same datamap.
issueID:CARBONDATA-2528
type:Bug
changed files:
texts:MV Datamap - When the MV is created with the order by, then when we execute the corresponding query defined in MV with order by, then the data is not accessed from the MV.
When the MV is created with the order by condition, then when we execute the corresponding query defined in MV along with order by, then the data is not accessed from the MV. The data is being accessed from the maintable only. Test queries:create datamap MV_order using 'mv' as select empno,sum(salary)+sum(utilization) as total from originTable group by empno order by empno;create datamap MV_desc_order using 'mv' as select empno,sum(salary)+sum(utilization) as total from originTable group by empno order by empno DESC;rebuild datamap MV_order;rebuild datamap MV_desc_order;explain select empno,sum(salary)+sum(utilization) as total from originTable group by empno order by empno;explain select empno,sum(salary)+sum(utilization) as total from originTable group by empno order by empno DESC;Expected result: MV with order by condition should access data from the MV table only. Please see the attached document for more details.
issueID:CARBONDATA-2529
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/datastore/filesystem/HDFSCarbonFile.java
texts:S3 Example not working with Hadoop 2.8.3
S3 Example not working with Hadoop 2.8.3
issueID:CARBONDATA-253
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/datastore/block/BlockInfo.java
core/src/main/java/org/apache/carbondata/core/util/DataFileFooterConverter.java
core/src/main/java/org/apache/carbondata/core/metadata/blocklet/DataFileFooter.java
core/src/main/java/org/apache/carbondata/core/datastore/block/TableBlockInfo.java
texts:Duplicate block loading when distribution is based on blocklet
In case of query execution when distribution is based on blocklet same blocks are getting loaded multiple times this is because hash code and equals method contract is not same
issueID:CARBONDATA-2530
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/datamap/status/DataMapStatusManager.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/DiskBasedDMSchemaStorageProvider.java
texts:[MV] Wrong data displayed when parent table data are loaded
Spark Release:- Spark2.2.1Create table and load data to it create MV rebuild datamaprun query (used during mv creation) which hits MV and get dataNow load data to main table again run query (used during mv creation) which hits MV and get dataResult:- data shown is from MV which is wrong  if increament load happens then data should get from Main table(parent table) Commands0: jdbc:hive2://10.18.222.231:23040> create table tt13 ( name string, age int) stored by 'carbondata';0: jdbc:hive2://10.18.222.231:23040> insert into tt13 select 'babu',12;---------+ Result ---------+---------–+0: jdbc:hive2://10.18.222.231:23040> create datamap datamap43 using 'mv' as select name from tt13 ;---------+ Result ---------+---------+No rows selected (0.51 seconds)0: jdbc:hive2://10.18.222.231:23040> rebuild datamap datamap43;---------+ Result ---------+---------+No rows selected (8.747 seconds)0: jdbc:hive2://10.18.222.231:23040> explain select name from tt13;------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ plan ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ == CarbonData Profiler ==Table Scan on datamap43_table total blocklets: 1 filter: none pruned by Main DataMap skipped blocklets: 0 == Physical Plan ==*BatchedScan CarbonDatasourceHadoopRelation [ Database name :default, Table name :datamap43_table, Schema :Some(StructType(StructField(tt13_name,StringType,true))) ] default.datamap43_tablett13_name#1311 ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+2 rows selected (0.525 seconds)0: jdbc:hive2://10.18.222.231:23040> select name from tt13;------------+ tt13_name ------------+ babu ------------+1 row selected (0.483 seconds)0: jdbc:hive2://10.18.222.231:23040> insert into tt13 select 'lal',13;---------+ Result ---------+---------+No rows selected (11.819 seconds)0: jdbc:hive2://10.18.222.231:23040> select name from tt13;------------+ tt13_name ------------+ babu ------------+1 row selected (0.349 seconds)0: jdbc:hive2://10.18.222.231:23040> explain select name from tt13;------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ plan ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ == CarbonData Profiler ==Table Scan on datamap43_table total blocklets: 1 filter: none pruned by Main DataMap skipped blocklets: 0 == Physical Plan ==*BatchedScan CarbonDatasourceHadoopRelation [ Database name :default, Table name :datamap43_table, Schema :Some(StructType(StructField(tt13_name,StringType,true))) ] default.datamap43_tablett13_name#1311 ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+2 rows selected (0.391 seconds)  Expected output should display 2 records . (check insert into command)   
issueID:CARBONDATA-2531
type:Bug
changed files:
texts:[MV] MV not hit when alias is in use
#testcase class  MVCreateTestCase  CREATE TABLE fact5 (empname String, designation String, doj Timestamp, workgroupcategory int, workgroupcategoryname String, deptno int, deptname String, projectcode int, projectjoindate Timestamp, projectenddate Timestamp,attendance int, utilization int,salary int) STORED BY 'org.apache.carbondata.format'LOAD DATA local inpath '/tmp/babu/data_big.csv' INTO TABLE fact5 OPTIONS('DELIMITER'= ',', 'QUOTECHAR'= '"','timestampformat'='dd-MM-yyyy');create datamap datamap53 using 'mv' as select deptname, sum(salary) from fact4 group by deptname 0: jdbc:hive2://10.18.222.231:23040> explain select deptname, sum(salary) from fact4 as tt group by deptname;------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ plan ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ == Physical Plan ==*BatchedScan CarbonDatasourceHadoopRelation [ Database name :default, Table name :datamap53_table, Schema :Some(StructType(StructField(fact4_deptname,StringType,true), StructField(sum_salary,LongType,true))) ] default.datamap53_tablefact4_deptname#1539,sum_salary#5961L ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------–+  Now use alias for column name 0: jdbc:hive2://10.18.222.231:23040> explain select deptname as babu, sum(salary) from fact4 as tt group by deptname; plan ------------------------------------------------------------------------------------------------------------------+ == Physical Plan ==*HashAggregate(keys=deptname#1539, functions=sum(cast(salary#1545 as bigint)))+- Exchange hashpartitioning(deptname#1539, 200) +- *HashAggregate(keys=deptname#1539, functions=partial_sum(cast(salary#1545 as bigint))) +- *BatchedScan CarbonDatasourceHadoopRelation [ Database name :default, Table name :fact4, Schema :Some(StructType(StructField(empname,StringType,true), StructField(designation,StringType,true), StructField(doj,TimestampType,true), StructField(workgroupcategory,IntegerType,true), StructField(workgroupcategoryname,StringType,true), StructField(deptno,IntegerType,true), StructField(deptname,StringType,true), StructField(projectcode,IntegerType,true), StructField(projectjoindate,TimestampType,true), StructField(projectenddate,TimestampType,true), StructField(attendance,IntegerType,true), StructField(utilization,IntegerType,true), StructField(salary,IntegerType,true))) ] default.fact4deptname#1539,salary#1545 +----------------------------------------------------------------------------------------------------------------- Alias should be ignored data data should be fetched from MV table datamap53_table. 
issueID:CARBONDATA-2532
type:Improvement
changed files:hadoop/src/main/java/org/apache/carbondata/hadoop/stream/CarbonStreamUtils.java
integration/spark2/src/main/scala/org/apache/carbondata/stream/CarbonStreamRecordReader.java
hadoop/src/main/java/org/apache/carbondata/hadoop/stream/StreamBlockletReader.java
integration/spark-datasource/src/main/spark2.1andspark2.2/org/apache/spark/sql/CarbonVectorProxy.java
integration/spark-datasource/src/main/spark2.3plus/org/apache/spark/sql/ColumnVectorFactory.java
integration/spark-datasource/src/main/scala/org/apache/carbondata/spark/vectorreader/VectorizedCarbonRecordReader.java
hadoop/src/main/java/org/apache/carbondata/hadoop/stream/CarbonStreamInputFormat.java
integration/spark-datasource/src/main/spark2.3plus/org/apache/spark/sql/CarbonVectorProxy.java
integration/spark-datasource/src/main/scala/org/apache/carbondata/spark/vectorreader/ColumnarVectorWrapper.java
texts:Carbon to support spark 2.3 version
1) Column vector and Columnar Batch interface compatibility issue2) Other compatibility issues related to spark 2.3 version with carbon as few methods parameters types has been changed and classes got renamed.
issueID:CARBONDATA-2534
type:Bug
changed files:
texts:MV Dataset - MV creation is not working with the substring()
MV creation is not working with the sub string function. We are getting the spark.sql.AnalysisException while trying to create a MV with the substring and aggregate function. Spark -shell test queries: scala> carbon.sql("create datamap mv_substr using 'mv' as select sum(salary),substring(empname,2,5),designation from originTable group by substring(empname,2,5),designation").show(200,false)org.apache.spark.sql.AnalysisException: Cannot create a table having a column whose name contains commas in Hive metastore. Table: `default`.`mv_substr_table`; Column: substring_empname,_2,_5; at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$org$apache$spark$sql$hive$HiveExternalCatalog$$verifyDataSchema$2.apply(HiveExternalCatalog.scala:150) at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$org$apache$spark$sql$hive$HiveExternalCatalog$$verifyDataSchema$2.apply(HiveExternalCatalog.scala:148) at scala.collection.immutable.List.foreach(List.scala:381) at org.apache.spark.sql.hive.HiveExternalCatalog.org$apache$spark$sql$hive$HiveExternalCatalog$$verifyDataSchema(HiveExternalCatalog.scala:148) at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$doCreateTable$1.apply$mcV$sp(HiveExternalCatalog.scala:222) at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$doCreateTable$1.apply(HiveExternalCatalog.scala:216) at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$doCreateTable$1.apply(HiveExternalCatalog.scala:216) at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97) at org.apache.spark.sql.hive.HiveExternalCatalog.doCreateTable(HiveExternalCatalog.scala:216) at org.apache.spark.sql.catalyst.catalog.ExternalCatalog.createTable(ExternalCatalog.scala:110) at org.apache.spark.sql.catalyst.catalog.SessionCatalog.createTable(SessionCatalog.scala:316) at org.apache.spark.sql.execution.command.CreateDataSourceTableCommand.run(createDataSourceTables.scala:119) at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58) at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56) at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:67) at org.apache.spark.sql.Dataset.<init>(Dataset.scala:183) at org.apache.spark.sql.CarbonSession$$anonfun$sql$1.apply(CarbonSession.scala:108) at org.apache.spark.sql.CarbonSession$$anonfun$sql$1.apply(CarbonSession.scala:97) at org.apache.spark.sql.CarbonSession.withProfiler(CarbonSession.scala:155) at org.apache.spark.sql.CarbonSession.sql(CarbonSession.scala:95) at org.apache.spark.sql.execution.command.table.CarbonCreateTableCommand.processMetadata(CarbonCreateTableCommand.scala:126) at org.apache.spark.sql.execution.command.MetadataCommand.run(package.scala:68) at org.apache.carbondata.mv.datamap.MVHelper$.createMVDataMap(MVHelper.scala:103) at org.apache.carbondata.mv.datamap.MVDataMapProvider.initMeta(MVDataMapProvider.scala:53) at org.apache.spark.sql.execution.command.datamap.CarbonCreateDataMapCommand.processMetadata(CarbonCreateDataMapCommand.scala:118) at org.apache.spark.sql.execution.command.AtomicRunnableCommand.run(package.scala:90) at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58) at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56) at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:67) at org.apache.spark.sql.Dataset.<init>(Dataset.scala:183) at org.apache.spark.sql.CarbonSession$$anonfun$sql$1.apply(CarbonSession.scala:108) at org.apache.spark.sql.CarbonSession$$anonfun$sql$1.apply(CarbonSession.scala:97) at org.apache.spark.sql.CarbonSession.withProfiler(CarbonSession.scala:155) at org.apache.spark.sql.CarbonSession.sql(CarbonSession.scala:95) ... 48 elided
issueID:CARBONDATA-2537
type:Bug
changed files:
texts:MV Dataset - User queries with &#39;having&#39; condition is not accessing the data from the MV datamap.
User queries with 'having' condition is not accessing the data from the MV datamap. It is accessing the data from the Main table.Test queries - spark shell:scala>carbon.sql("CREATE TABLE originTable (empno int, empname String, designation String, doj Timestamp, workgroupcategory int, workgroupcategoryname String, deptno int, deptname String, projectcode int, projectjoindate Timestamp, projectenddate Timestamp,attendance int, utilization int,salary int) STORED BY 'org.apache.carbondata.format'").show()++++++scala>carbon.sql("LOAD DATA local inpath 'hdfs://hacluster/user/prasanna/data.csv' INTO TABLE originTable OPTIONS('DELIMITER'= ',', 'QUOTECHAR'= '\"','timestampformat'='dd-MM-yyyy')").show()++++++scala> carbon.sql("select empno from originTable having salary>10000").show(200,false)-----empno-----14 15 20 19 -----scala> carbon.sql("create datamap mv_hav using 'mv' as select empno from originTable having salary>10000").show(200,false)++++++scala> carbon.sql("explain select empno from originTable having salary>10000").show(200,false)---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------plan ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------== CarbonData Profiler ==Table Scan on origintable total blocklets: 1 filter: (salary <> null and salary > 10000) pruned by Main DataMap skipped blocklets: 0== Physical Plan ==*Project empno#1131+- *BatchedScan CarbonDatasourceHadoopRelation [ Database name :default, Table name :origintable, Schema :Some(StructType(StructField(empno,IntegerType,true), StructField(empname,StringType,true), StructField(designation,StringType,true), StructField(doj,TimestampType,true), StructField(workgroupcategory,IntegerType,true), StructField(workgroupcategoryname,StringType,true), StructField(deptno,IntegerType,true), StructField(deptname,StringType,true), StructField(projectcode,IntegerType,true), StructField(projectjoindate,TimestampType,true), StructField(projectenddate,TimestampType,true), StructField(attendance,IntegerType,true), StructField(utilization,IntegerType,true), StructField(salary,IntegerType,true))) ] default.origintableempno#1131 PushedFilters: &#91;IsNotNull(salary), GreaterThan(salary,10000)&#93;---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------  
issueID:CARBONDATA-2538
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/metadata/schema/table/CarbonTable.java
core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
texts:No exception is thrown if writer path has only lock files
Steps to reproduce: Create external table  Manually delete the index and carbon files Describe table (lock files would be created) Select from table
issueID:CARBONDATA-2539
type:Bug
changed files:
texts:MV Dataset - Subqueries is not accessing the data from the MV datamap.
Inner subquery is not accessing the data from the MV datamap. It is accessing the data from the main table.Test queries - Spark shell:scala> carbon.sql("drop table if exists origintable").show()++++++ scala> carbon.sql("CREATE TABLE originTable (empno int, empname String, designation String, doj Timestamp, workgroupcategory int, workgroupcategoryname String, deptno int, deptname String, projectcode int, projectjoindate Timestamp, projectenddate Timestamp,attendance int, utilization int,salary int) STORED BY 'org.apache.carbondata.format'").show(200,false)++++++scala> carbon.sql("LOAD DATA local inpath 'hdfs://hacluster/user/prasanna/data.csv' INTO TABLE originTable OPTIONS('DELIMITER'= ',', 'QUOTECHAR'= '\"','timestampformat'='dd-MM-yyyy')").show(200,false)++++++ scala> carbon.sql("drop datamap datamap_subqry").show(200,false)++++++scala> carbon.sql("create datamap datamap_subqry using 'mv' as select min(salary) from originTable group by empno").show(200,false)++++++scala> carbon.sql("explain SELECT max(empno) FROM originTable WHERE salary IN (select min(salary) from originTable group by empno ) group by empname").show(200,false)------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------plan ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------== CarbonData Profiler ==Table Scan on origintable total blocklets: 2 filter: none pruned by Main DataMap skipped blocklets: 0Table Scan on origintable total blocklets: 2 filter: none pruned by Main DataMap skipped blocklets: 0== Physical Plan ==*HashAggregate(keys=empname#2132, functions=max(empno#2131))+- Exchange hashpartitioning(empname#2132, 200) +- *HashAggregate(keys=empname#2132, functions=partial_max(empno#2131)) +- *Project empno#2131, empname#2132 +- *BroadcastHashJoin salary#2144, *min(salary*)#2219, LeftSemi, BuildRight :- BatchedScan CarbonDatasourceHadoopRelation [ *Database name :default, Table name :origintable, Schema :Some(StructType(StructField(empno,IntegerType,true), StructField(empname,StringType,true), StructField(designation,StringType,true), StructField(doj,TimestampType,true), StructField(workgroupcategory,IntegerType,true), StructField(workgroupcategoryname,StringType,true), StructField(deptno,IntegerType,true), StructField(deptname,StringType,true), StructField(projectcode,IntegerType,true), StructField(projectjoindate,TimestampType,true), StructField(projectenddate,TimestampType,true), StructField(attendance,IntegerType,true), StructField(utilization,IntegerType,true), StructField(salary,IntegerType,true))) ] default.origintableempno#2131,empname#2132,designation#2133,doj#2134,workgroupcategory#2135,workgroupcategoryname#2136,deptno#2137,deptname#2138,projectcode#2139,projectjoindate#2140,projectenddate#2141,attendance#2142,utilization#2143,salary#2144 +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input&#91;0, int, true&#93; as bigint))) +- *HashAggregate(keys=empno#2131, functions=min(salary#2144)) +- Exchange hashpartitioning(empno#2131, 200) +- *HashAggregate(keys=empno#2131, functions=partial_min(salary#2144)) +- *BatchedScan CarbonDatasourceHadoopRelation [ Database name :default, Table name :origintable, Schema :Some(StructType(StructField(empno,IntegerType,true), StructField(empname,StringType,true), StructField(designation,StringType,true), StructField(doj,TimestampType,true), StructField(workgroupcategory,IntegerType,true), StructField(workgroupcategoryname,StringType,true), StructField(deptno,IntegerType,true), StructField(deptname,StringType,true), StructField(projectcode,IntegerType,true), StructField(projectjoindate,TimestampType,true), StructField(projectenddate,TimestampType,true), StructField(attendance,IntegerType,true), StructField(utilization,IntegerType,true), StructField(salary,IntegerType,true))) ] default.origintableempno#2131,salary#2144------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ 
issueID:CARBONDATA-254
type:Improvement
changed files:processing/src/main/java/org/apache/carbondata/processing/util/CarbonDataProcessorUtil.java
texts:Code Inspection Optiminization
Code Inspection Optiminization
issueID:CARBONDATA-2540
type:Bug
changed files:
texts:MV Dataset - Unionall queries are not fetching data from MV dataset.
Unionall queries are not fetching data from MV dataset. Test queries:scala> carbon.sql("drop table if exists fact_table1").show(200,false)++++++scala> carbon.sql("CREATE TABLE fact_table1 (empno int, empname String, designation String, doj Timestamp,workgroupcategory int, workgroupcategoryname String, deptno int, deptname String,projectcode int, projectjoindate Timestamp, projectenddate Timestamp,attendance int,utilization int,salary int)STORED BY 'org.apache.carbondata.format'").show(200,false)++++++scala> carbon.sql("LOAD DATA local inpath 'hdfs://hacluster/user/prasanna/data_mv.csv' INTO TABLE fact_table1 OPTIONS('DELIMITER'= ',', 'QUOTECHAR'= '"','timestampformat'='dd-MM-yyyy')").show(200,false)++++++scala> carbon.sql("LOAD DATA local inpath 'hdfs://hacluster/user/prasanna/data_mv.csv' INTO TABLE fact_table1 OPTIONS('DELIMITER'= ',', 'QUOTECHAR'= '\"','timestampformat'='dd-MM-yyyy')").show(200,false)++++++scala> carbon.sql("drop table if exists fact_table2").show(200,false)++++++scala> carbon.sql("CREATE TABLE fact_table2 (empno int, empname String, designation String, doj Timestamp,workgroupcategory int, workgroupcategoryname String, deptno int, deptname String,projectcode int, projectjoindate Timestamp, projectenddate Timestamp,attendance int,utilization int,salary int)STORED BY 'org.apache.carbondata.format'").show(200,false)++++++scala> carbon.sql("LOAD DATA local inpath 'hdfs://hacluster/user/prasanna/data_mv.csv' INTO TABLE fact_table2 OPTIONS('DELIMITER'= ',', 'QUOTECHAR'= '\"','timestampformat'='dd-MM-yyyy')").show(200,false)++++++scala> carbon.sql("LOAD DATA local inpath 'hdfs://hacluster/user/prasanna/data_mv.csv' INTO TABLE fact_table2 OPTIONS('DELIMITER'= ',', 'QUOTECHAR'= '\"','timestampformat'='dd-MM-yyyy')").show(200,false)++++++ scala> carbon.sql("create datamap mv_unional using 'mv' as Select Z.empno From (Select empno,empname From fact_table1 Union All Select empno,empname from fact_table2) As Z Group By Z.empno").show(200,false)++++++ scala> carbon.sql("rebuild datamap mv_unional").show()++++++scala> carbon.sql("explain Select Z.empno From (Select empno,empname From fact_table1 Union All Select empno,empname from fact_table2) As Z Group By Z.empno").show(200,false)-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------plan -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------== CarbonData Profiler ==Table Scan on fact_table1 total blocklets: 4 filter: none pruned by Main DataMap skipped blocklets: 2== Physical Plan ==*HashAggregate(keys=empno#2588, functions=[])+- Exchange hashpartitioning(empno#2588, 200) +- *HashAggregate(keys=empno#2588, functions=[]) +- Union :- *BatchedScan CarbonDatasourceHadoopRelation [ Database name :default, Table name :fact_table1, Schema :Some(StructType(StructField(empno,IntegerType,true), StructField(empname,StringType,true), StructField(designation,StringType,true), StructField(doj,TimestampType,true), StructField(workgroupcategory,IntegerType,true), StructField(workgroupcategoryname,StringType,true), StructField(deptno,IntegerType,true), StructField(deptname,StringType,true), StructField(projectcode,IntegerType,true), StructField(projectjoindate,TimestampType,true), StructField(projectenddate,TimestampType,true), StructField(attendance,IntegerType,true), StructField(utilization,IntegerType,true), StructField(salary,IntegerType,true))) ] default.fact_table1empno#2588 +- *BatchedScan CarbonDatasourceHadoopRelation [ Database name :default, Table name :fact_table2, Schema :Some(StructType(StructField(empno,IntegerType,true), StructField(empname,StringType,true), StructField(designation,StringType,true), StructField(doj,TimestampType,true), StructField(workgroupcategory,IntegerType,true), StructField(workgroupcategoryname,StringType,true), StructField(deptno,IntegerType,true), StructField(deptname,StringType,true), StructField(projectcode,IntegerType,true), StructField(projectjoindate,TimestampType,true), StructField(projectenddate,TimestampType,true), StructField(attendance,IntegerType,true), StructField(utilization,IntegerType,true), StructField(salary,IntegerType,true))) ] default.fact_table2empno#2514------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- It is accessing data only from the main tables and not from the created MV for the unionall.
issueID:CARBONDATA-2542
type:Bug
changed files:
texts:MV creation is failed for other than default database
0: jdbc:hive2://10.18.222.231:23040> CREATE TABLE fact10 (empname String, designation String, doj Timestamp, workgroupcategory int, workgroupcategoryname String, deptno int, deptname String, projectcode int, projectjoindate Timestamp, projectenddate Timestamp,attendance int, utilization int,salary int) STORED BY 'org.apache.carbondata.format' TBLPROPERTIES('table_blocksize'='256');---------+ Result ---------+---------+No rows selected (0.962 seconds)0: jdbc:hive2://10.18.222.231:23040> LOAD DATA local inpath '/tmp/babu/data_big_1.csv' INTO TABLE fact10 OPTIONS('DELIMITER'= ',', 'QUOTECHAR'= '"','timestampformat'='dd-MM-yyyy','FILEHEADER'='empno,empname,designation,doj,workgroupcategory,workgroupcategoryname,deptno,deptname,projectcode,projectjoindate,projectenddate,attendance,utilization,salary');---------+ Result ---------+---------+No rows selected (6.188 seconds)0: jdbc:hive2://10.18.222.231:23040> create datamap datamap66 using 'mv' as select doj,sum(salary) from babu.fact10 group by doj;---------+ Result ---------+---------+No rows selected (0.893 seconds)0: jdbc:hive2://10.18.222.231:23040> create datamap datamap68 using 'mv' as select doj,sum(salary) from fact10 group by doj;Error: org.apache.spark.sql.AnalysisException: Table or view not found: fact10; line 1 pos 49 (state=,code=0)0: jdbc:hive2://10.18.222.231:23040> 
issueID:CARBONDATA-2545
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/datastore/impl/FileFactory.java
core/src/main/java/org/apache/carbondata/core/datastore/impl/DefaultFileTypeProvider.java
core/src/main/java/org/apache/carbondata/core/keygenerator/columnar/impl/MultiDimKeyVarLengthEquiSplitGenerator.java
core/src/main/java/org/apache/carbondata/core/datamap/DataMapStoreManager.java
datamap/lucene/src/main/java/org/apache/carbondata/datamap/lucene/LuceneFineGrainDataMap.java
core/src/main/java/org/apache/carbondata/core/datastore/impl/FileTypeInterface.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/DataMapSchema.java
core/src/main/java/org/apache/carbondata/core/scan/filter/FilterUtil.java
texts:Fix some spell error in CarbonData
Fix some spell error in CarbonData
issueID:CARBONDATA-2546
type:Bug
changed files:store/sdk/src/main/java/org/apache/carbondata/sdk/file/CarbonReaderBuilder.java
hadoop/src/main/java/org/apache/carbondata/hadoop/CarbonRecordReader.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonInputFormat.java
texts:It will throw exception when give same column twice in projection and tries to print it.
It will throw exception when give same column twice in projection and tries to print it.
issueID:CARBONDATA-2547
type:Bug
changed files:
texts:CarbonData loads a null character between the two separators, and the loaded result value is &#39;&#39; , actual need null
CarbonData loads a null character between the two separators, and the loaded result value is '' and cannot be modified by a parameter.but Parquet loads null characters between two separators, and the loaded value is NULL.Causes CarbonData to not filter query results by is not null condition。Detailed test information：the table ex_testnull is a external table,the source data is csv,another table testnull is Paruet table， testnullCarbon is CarbonData tale 
issueID:CARBONDATA-2549
type:Improvement
changed files:datamap/bloom/src/main/java/org/apache/carbondata/datamap/bloom/BloomDataMapWriter.java
datamap/bloom/src/main/java/org/apache/carbondata/datamap/bloom/BloomCoarseGrainDataMapFactory.java
core/src/main/java/org/apache/carbondata/core/cache/CacheProvider.java
core/src/main/java/org/apache/carbondata/core/cache/CacheType.java
datamap/bloom/src/main/java/org/apache/carbondata/datamap/bloom/BloomDataMapModel.java
datamap/bloom/src/main/java/org/apache/carbondata/datamap/bloom/BloomDataMapCache.java
datamap/bloom/src/main/java/org/apache/hadoop/util/bloom/CarbonBloomFilter.java
datamap/bloom/src/main/java/org/apache/carbondata/datamap/bloom/BloomCacheKeyValue.java
datamap/bloom/src/main/java/org/apache/carbondata/datamap/bloom/BloomCoarseGrainDataMap.java
texts:Implement LRU cache in Bloom filter based on Carbon LRU cache interface
Currently bloom cache is implemented using guava cache, carbon has its own lru cache interfaces and complete sysytem it controls the cache intstead of controlling feature wise. So replace guava cache with carbon lru cache 
issueID:CARBONDATA-255
type:Bug
changed files:
texts:keyword SEGMENT should be used instead of LOAD  In data management dml because LOAD is not supported now
keyword SEGMENT should be used instead of LOAD  In data management dml because LOAD is not supported now
issueID:CARBONDATA-2550
type:Bug
changed files:
texts:[MV] Limit is ignored when data fetched from MV, Query rewrite is Wrong
0: jdbc:hive2://10.18.222.231:23040> create table mvtable1(name string,age int,salary int) stored by 'carbondata';---------+ Result ---------+---------+No rows selected (0.279 seconds)0: jdbc:hive2://10.18.222.231:23040> insert into mvtable1 select 'n1',12,12;---------+ Result ---------+---------+No rows selected (11.973 seconds)0: jdbc:hive2://10.18.222.231:23040> insert into mvtable1 select 'n1',12,12;---------+ Result ---------+---------+No rows selected (9.92 seconds)0: jdbc:hive2://10.18.222.231:23040> insert into mvtable1 select 'n3',12,12;---------+ Result ---------+---------+No rows selected (9.883 seconds)0: jdbc:hive2://10.18.222.231:23040> insert into mvtable1 select 'n4',12,12;---------+ Result ---------+---------+No rows selected (10.488 seconds)0: jdbc:hive2://10.18.222.231:23040> select name,sum(salary) from mvtable1 group by name;------------------- name  sum(salary) ------------------- n3  12  n1  24  n4  12 -------------------–0: jdbc:hive2://10.18.222.231:23040> select name,sum(salary) from mvtable1 group by name limit 2;------------------- name  sum(salary) ------------------- n3  12  n1  24 -------------------2 rows selected (4.175 seconds)0: jdbc:hive2://10.18.222.231:23040> create datamap map1 using 'mv' as select name,sum(salary) from mvtable1 group by name;---------+ Result ---------+---------+No rows selected (0.396 seconds)0: jdbc:hive2://10.18.222.231:23040> rebuild datamap map1;---------+ Result ---------+---------+No rows selected (13.246 seconds) 0: jdbc:hive2://10.18.222.231:23040> select name,sum(salary) from mvtable1 group by name limit 2;--------------------------- mvtable1_name  sum_salary --------------------------- n3  12  n1  24  n4  12 ---------------------------3 rows selected (2.453 seconds)0: jdbc:hive2://10.18.222.231:23040> select name,sum(salary) from mvtable1 group by name limit 1;--------------------------- mvtable1_name  sum_salary --------------------------- n3  12  n1  24  n4  12 ---------------------------3 rows selected (0.347 seconds)0: jdbc:hive2://10.18.222.231:23040>  Even limit is given MV returns all the records from MV table.Cause:- When Rewriting MV query ,limit is ignored,0: jdbc:hive2://10.18.222.231:23040> explain select name,sum(salary) from mvtable1 group by name limit 2; plan  == CarbonData Profiler ==Table Scan on map1_table total blocklets: 2 filter: none pruned by Main DataMap skipped blocklets: 0 == Physical Plan ==*BatchedScan CarbonDatasourceHadoopRelation [ Database name :default, Table name :map1_table, Schema :Some(StructType(StructField(mvtable1_name,StringType,true), StructField(sum_salary,LongType,true))) ] default.map1_tablemvtable1_name#4438,sum_salary#4614L ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+2 rows selected (0.36  
issueID:CARBONDATA-2552
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/scan/complextypes/PrimitiveQueryType.java
texts:Fix Data Mismatch for Complex Data type Array of Timestamp with Dictionary Include

issueID:CARBONDATA-2553
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
core/src/main/java/org/apache/carbondata/core/datastore/filesystem/LocalCarbonFile.java
core/src/main/java/org/apache/carbondata/core/util/CarbonProperties.java
texts:support ZSTD compression for sort temp file
Using zstd compression could reduce the amount of data written to disk by as much as 50% than snappy.
issueID:CARBONDATA-2554
type:Improvement
changed files:store/sdk/src/main/java/org/apache/carbondata/sdk/file/AvroCarbonWriter.java
core/src/main/java/org/apache/carbondata/core/keygenerator/directdictionary/timestamp/TimeStampDirectDictionaryGenerator.java
processing/src/main/java/org/apache/carbondata/processing/loading/dictionary/DirectDictionary.java
processing/src/main/java/org/apache/carbondata/processing/datatypes/PrimitiveDataType.java
core/src/main/java/org/apache/carbondata/core/keygenerator/directdictionary/timestamp/DateDirectDictionaryGenerator.java
core/src/main/java/org/apache/carbondata/core/keygenerator/directdictionary/DirectDictionaryGenerator.java
processing/src/main/java/org/apache/carbondata/processing/loading/steps/InputProcessorStepWithNoConverterImpl.java
texts:Support Logical types(date and timestamp) for AvroCarbonWriter.

issueID:CARBONDATA-2555
type:Bug
changed files:store/sdk/src/main/java/org/apache/carbondata/sdk/file/CarbonReaderBuilder.java
examples/spark2/src/main/java/org/apache/carbondata/examples/sdk/CarbonReaderExample.java
examples/spark2/src/main/java/org/apache/carbondata/examples/sdk/SDKS3Example.java
texts:SDK Reader should have isTransactionalTable = false by default, to be inline with SDK writer

issueID:CARBONDATA-2557
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/util/BlockletDataMapUtil.java
examples/spark2/src/main/java/org/apache/carbondata/examples/sdk/CarbonReaderExample.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockletDataMapFactory.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/SegmentIndexFileStore.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/CarbonTable.java
core/src/main/java/org/apache/carbondata/core/indexstore/BlockletDataMapIndexStore.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonInputFormat.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonFileInputFormat.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/column/ColumnSchema.java
store/sdk/src/main/java/org/apache/carbondata/sdk/file/CarbonReaderBuilder.java
core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonTableInputFormat.java
core/src/main/java/org/apache/carbondata/core/indexstore/TableBlockIndexUniqueIdentifierWrapper.java
hadoop/src/main/java/org/apache/carbondata/hadoop/CarbonRecordReader.java
core/src/main/java/org/apache/carbondata/core/datamap/dev/CacheableDataMap.java
texts:Improve Carbon Reader Schema reading performance on S3
On S3 reader performance is slow as current schema fetching API is reading schema from carobndatafile. If the carbonData file is huge, this can be slow.Should use index file for getting schema
issueID:CARBONDATA-2558
type:Improvement
changed files:store/sdk/src/main/java/org/apache/carbondata/sdk/file/Field.java
store/sdk/src/main/java/org/apache/carbondata/store/MetaCachedCarbonStore.java
store/sdk/src/main/java/org/apache/carbondata/sdk/file/Schema.java
store/sdk/src/main/java/org/apache/carbondata/sdk/file/CarbonSchemaReader.java
store/sdk/src/main/java/org/apache/carbondata/sdk/file/CarbonReader.java
texts:Optimize carbon schema reader interface of SDK
Optimize carbon schema reader interface of SDK
issueID:CARBONDATA-2559
type:Bug
changed files:
texts:task id is not being set for CarbonReader

issueID:CARBONDATA-256
type:Improvement
changed files:common/src/main/java/org/apache/carbondata/common/logging/impl/ExtendedRollingFileAppender.java
texts:Replace StringBuffer with StringBuilder in case of local variables
Even with local variables of Stringbuffer type, unnecessary synchronization overhead will be added in string operation.Replacing StringBuffer with StringBuilder in such  cases can avoid synchronization overhead
issueID:CARBONDATA-2560
type:Bug
changed files:
texts:[MV] Exception in console during MV creation but MV registered successfully
0: jdbc:hive2://10.18.222.231:23040> create table mvtest4(unit int,y_year int,country int,b_country int,imei int,salary double) stored by 'carbondata' TBLPROPERTIES('sort_columns'='y_year,country,imei,b_country,unit');---------+ Result ---------+---------+No rows selected (0.45 seconds)0: jdbc:hive2://10.18.222.231:23040> create datamap map1mvtest4 using 'mv' as select unit,sum(salary) from mvtest4 group by unit;---------+ Result ---------+---------+No rows selected (0.505 seconds)0: jdbc:hive2://10.18.222.2310: jdbc:hive2://10.18.222.231:23040> create datamap map2mvtest4 using 'mv' as select unit,sum(salary) from mvtest4 group by unit;Error: java.lang.RuntimeException: Asked to register already registered. (state=,code=0)0: jdbc:hive2://10.18.222.231:23040> show datamap on table mvtest4;--------------------------------------------------+ DataMapName  ClassName  Associated Table --------------------------------------------------+ map1mvtest4  mv  default.map1mvtest4_table  map2mvtest4  mv  default.map2mvtest4_table --------------------------------------------------+2 rows selected (0.338 seconds) So even we got error in data map creation but datamap is registered._system folder is also have datamap file.
issueID:CARBONDATA-2563
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/profiler/ExplainCollector.java
texts:Explain query with Order by operator is fired Spark Job which is increase explain query time
Create Table (hive or carbon )create table justtesthive( name string,age int)insert into justtesthive select 'babu',12; WithCarbonSession means startCarbonThriftserver  Without CarbonSession (with SparkSession)0: jdbc:hive2://10.18.222.231:23040> explain select name from justtesthive order by name;----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ plan ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ == Physical Plan ==*Sort name#15 ASC NULLS FIRST, true, 0+- Exchange rangepartitioning(name#15 ASC NULLS FIRST, 200) +- HiveTableScan name#15, HiveTableRelation `default`.`justtesthive`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, name#15, age#16 ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+1 row selected (0.089 seconds)0: jdbc:hive2://10.18.222.231:23040>    With CarbonSession. 0: jdbc:hive2://10.18.222.231:23040> explain select name from justtesthive order by name;--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ plan --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ == CarbonData Profiler == == Physical Plan ==*Sort name#1867 ASC NULLS FIRST, true, 0+- Exchange rangepartitioning(name#1867 ASC NULLS FIRST, 200) +- HiveTableScan name#1867, HiveTableRelation `default`.`justtesthive`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, name#1867, age#1868 --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+2 rows selected (11.609 seconds) Time taken by explain is 10 sec for CarbonSession because SparkJob is fired , but for Explain Job should not fired . 
issueID:CARBONDATA-2566
type:Improvement
changed files:examples/spark2/src/main/java/org/apache/carbondata/examples/sdk/CarbonReaderExample.java
texts:Optimize CarbonReaderExample
Optimize CarbonReaderExample
issueID:CARBONDATA-2568
type:Bug
changed files:
texts:[MV]  MV datamap is not hit when ,column is in group by but not in projection
0: jdbc:hive2://10.18.222.231:23040> create table mvtestcarbon10 (name_t1 string,age_t1 int,salray_t1 float,dob date,doj timestamp) stored by 'carbondata';---------+ Result ---------+---------+No rows selected (0.252 seconds)0: jdbc:hive2://10.18.222.231:23040> insert into mvtestcarbon10 select 'babu',12,25.25,'2014-01-01','2014-05-05 02:03:02';---------+ Result ---------+---------+No rows selected (21.893 seconds)0: jdbc:hive2://10.18.222.231:23040> create datamap mvtestcarbon10_map2 using 'mv' as select sum(salray_t1) from mvtestcarbon10 group by name_t1;---------+ Result ---------+---------+No rows selected (0.525 seconds) 0: jdbc:hive2://10.18.222.231:23040> explain select sum(salray_t1) from mvtestcarbon10 group by name_t1; plan  == Physical Plan ==*HashAggregate(keys=name_t1#2422, functions=sum(salray_t1#2424))+- Exchange hashpartitioning(name_t1#2422, 200) +- *HashAggregate(keys=name_t1#2422, functions=partial_sum(salray_t1#2424)) +- BatchedScan CarbonDatasourceHadoopRelation [ Database name :default, Table name :*mvtestcarbon10, Schema :Some(StructType(StructField(name_t1,StringType,true), StructField(age_t1,IntegerType,true), StructField(salray_t1,DoubleType,true), StructField(dob,DateType,true), StructField(doj,TimestampType,true))) ] default.mvtestcarbon10name_t1#2422,salray_t1#2424   0: jdbc:hive2://10.18.222.231:23040> explain select sum(salray_t1) from mvtestcarbon10 ;+----------------------------------------------------------------------------------------------------------------- plan +----------------------------------------------------------------------------------------------------------------- == Physical Plan ==*HashAggregate(keys=[], functions=sum(salray_t1#2424))+- Exchange SinglePartition +- *HashAggregate(keys=[], functions=partial_sum(salray_t1#2424)) +- BatchedScan CarbonDatasourceHadoopRelation [ Database name :default, Table name :*mvtestcarbon10, Schema :Some(StructType(StructField(name_t1,StringType,true), StructField(age_t1,IntegerType,true), StructField(salray_t1,DoubleType,true), StructField(dob,DateType,true), StructField(doj,TimestampType,true))) ] default.mvtestcarbon10salray_t1#2424 +----------------------------------------------------------------------------------------------------------------- 
issueID:CARBONDATA-2569
type:Improvement
changed files:
texts:Search mode throw exception but test case pass
Search mode throw exception but test case passcode:org.apache.carbondata.spark.testsuite.detailquery.SearchModeTestCase  test("SearchMode Query: row result") {    CarbonProperties.getInstance().addProperty(CarbonCommonConstants.ENABLE_VECTOR_READER, "false")    checkSearchAnswer("select * from main where city = 'city3'")    CarbonProperties.getInstance().addProperty(CarbonCommonConstants.ENABLE_VECTOR_READER,      CarbonCommonConstants.ENABLE_VECTOR_READER_DEFAULT)  }error:18/05/31 20:16:37 INFO SearchModeTestCase: ScalaTest-run-running-SearchModeTestCase ===== TEST OUTPUT FOR org.apache.carbondata.spark.testsuite.detailquery.SearchModeTestCase: 'SearchMode Query: row result' =====18/05/31 20:16:37 INFO CarbonSparkSqlParser: Parsing command: select * from main where city = 'city3'18/05/31 20:16:37 INFO HiveMetaStore: 0: get_table : db=default tbl=main18/05/31 20:16:37 INFO audit: ugi=xubo ip=unknown-ip-addr cmd=get_table : db=default tbl=main 18/05/31 20:16:37 INFO CatalystSqlParser: Parsing command: array<string>18/05/31 20:16:37 INFO HiveMetaStore: 0: get_database: default18/05/31 20:16:37 INFO audit: ugi=xubo ip=unknown-ip-addr cmd=get_database: default 18/05/31 20:16:37 INFO HiveMetaStore: 0: get_database: default18/05/31 20:16:37 INFO audit: ugi=xubo ip=unknown-ip-addr cmd=get_database: default 18/05/31 20:16:37 INFO HiveMetaStore: 0: get_tables: db=default pat=*18/05/31 20:16:37 INFO audit: ugi=xubo ip=unknown-ip-addr cmd=get_tables: db=default pat=* 18/05/31 20:16:37 INFO CarbonLRUCache: ScalaTest-run-running-SearchModeTestCase Configured LRU cache size is 1024 MB18/05/31 20:16:37 INFO Scheduler: ScalaTest-run-running-SearchModeTestCase sending search request to worker 127.0.0.1:1002118/05/31 20:16:37 ERROR CarbonSession: Exception when executing search mode: (org.apache.spark.rpc.Schedulable@6b72c62,[]) (of class scala.Tuple2), fallback to SparkSQL18/05/31 20:16:37 INFO SearchModeTestCase: ScalaTest-run-running-SearchModeTestCase ===== FINISHED org.apache.carbondata.spark.testsuite.detailquery.SearchModeTestCase: 'SearchMode Query: row result' =====18/05/31 20:16:37 INFO SearchModeTestCase: ScalaTest-run-running-SearchModeTestCase ===== TEST OUTPUT FOR org.apache.carbondata.spark.testsuite.detailquery.SearchModeTestCase: 'SearchMode Query: vector result' =====18/05/31 20:16:37 INFO CarbonSparkSqlParser: Parsing command: select * from main where city = 'city3'18/05/31 20:16:37 INFO HiveMetaStore: 0: get_table : db=default tbl=main18/05/31 20:16:37 INFO audit: ugi=xubo ip=unknown-ip-addr cmd=get_table : db=default tbl=main 18/05/31 20:16:37 INFO SearchRequestHandler: dispatcher-event-loop-7 [SearchId:-220196541] receive search request18/05/31 20:16:37 INFO SearchRequestHandler: dispatcher-event-loop-7 [SearchId:-220196541] scan on table default.main, 9 projection columns with filter (org.apache.carbondata.core.scan.expression.conditional.EqualToExpression@33c93178), number of block: 4(org.apache.spark.rpc.Schedulable@6b72c62,[]) (of class scala.Tuple2)scala.MatchError: (org.apache.spark.rpc.Schedulable@6b72c62,[]) (of class scala.Tuple2) at org.apache.spark.rpc.Master$$anonfun$search$1.apply(Master.scala:308) at org.apache.spark.rpc.Master$$anonfun$search$1.apply(Master.scala:308) at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99) at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99) at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230) at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40) at scala.collection.mutable.HashMap.foreach(HashMap.scala:99) at org.apache.spark.rpc.Master.search(Master.scala:308) at org.apache.carbondata.store.SparkCarbonStore.search(SparkCarbonStore.scala:144) at org.apache.spark.sql.CarbonSession.runSearch(CarbonSession.scala:225) at org.apache.spark.sql.CarbonSession.org$apache$spark$sql$CarbonSession$$trySearchMode(CarbonSession.scala:180) at org.apache.spark.sql.CarbonSession$$anonfun$sql$1.apply(CarbonSession.scala:100) at org.apache.spark.sql.CarbonSession$$anonfun$sql$1.apply(CarbonSession.scala:97) at org.apache.spark.sql.CarbonSession.withProfiler(CarbonSession.scala:156) at org.apache.spark.sql.CarbonSession.sql(CarbonSession.scala:95) at org.apache.spark.sql.test.Spark2TestQueryExecutor.sql(Spark2TestQueryExecutor.scala:35) at org.apache.spark.sql.test.util.QueryTest.sql(QueryTest.scala:113) at org.apache.carbondata.spark.testsuite.detailquery.SearchModeTestCase.org$apache$carbondata$spark$testsuite$detailquery$SearchModeTestCase$$checkSearchAnswer(SearchModeTestCase.scala:64) at org.apache.carbondata.spark.testsuite.detailquery.SearchModeTestCase$$anonfun$1.apply$mcV$sp(SearchModeTestCase.scala:69) at org.apache.carbondata.spark.testsuite.detailquery.SearchModeTestCase$$anonfun$1.apply(SearchModeTestCase.scala:67) at org.apache.carbondata.spark.testsuite.detailquery.SearchModeTestCase$$anonfun$1.apply(SearchModeTestCase.scala:67) at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22) at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85) at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104) at org.scalatest.Transformer.apply(Transformer.scala:22) at org.scalatest.Transformer.apply(Transformer.scala:20) at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166) at org.apache.spark.sql.test.util.CarbonFunSuite.withFixture(CarbonFunSuite.scala:41) at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:163) at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175) at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175) at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306) at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:175) at org.scalatest.FunSuite.runTest(FunSuite.scala:1555) at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208) at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208) at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413) at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401) at scala.collection.immutable.List.foreach(List.scala:381) at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401) at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396) at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483) at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:208) at org.scalatest.FunSuite.runTests(FunSuite.scala:1555) at org.scalatest.Suite$class.run(Suite.scala:1424) at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1555) at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212) at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212) at org.scalatest.SuperEngine.runImpl(Engine.scala:545) at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:212) at org.apache.carbondata.spark.testsuite.detailquery.SearchModeTestCase.org$scalatest$BeforeAndAfterAll$$super$run(SearchModeTestCase.scala:33) at org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:257) at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:256) at org.apache.carbondata.spark.testsuite.detailquery.SearchModeTestCase.run(SearchModeTestCase.scala:33) at org.scalatest.tools.SuiteRunner.run(SuiteRunner.scala:55) at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2563) at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2557) at scala.collection.immutable.List.foreach(List.scala:381) at org.scalatest.tools.Runner$.doRunRunRunDaDoRunRun(Runner.scala:2557) at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1044) at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1043) at org.scalatest.tools.Runner$.withClassLoaderAndDispatchReporter(Runner.scala:2722) at org.scalatest.tools.Runner$.runOptionallyWithPassFailReporter(Runner.scala:1043) at org.scalatest.tools.Runner$.run(Runner.scala:883) at org.scalatest.tools.Runner.run(Runner.scala) at org.jetbrains.plugins.scala.testingSupport.scalaTest.ScalaTestRunner.runScalaTest2(ScalaTestRunner.java:131) at org.jetbrains.plugins.scala.testingSupport.scalaTest.ScalaTestRunner.main(ScalaTestRunner.java:28)
issueID:CARBONDATA-257
type:New Feature
changed files:core/src/main/java/org/apache/carbondata/core/metadata/schema/SchemaReader.java
core/src/main/java/org/apache/carbondata/core/metadata/AbsoluteTableIdentifier.java
texts:Make CarbonData readable through Spark/MapReduce program
User should be able to use SparkContext.newAPIHadoopFile to read CarbonData files
issueID:CARBONDATA-2571
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
texts:Calculating the carbonindex and carbondata file size of a table is wrong

issueID:CARBONDATA-2573
type:Improvement
changed files:
texts:Merge carbonstore branch to master

issueID:CARBONDATA-2575
type:Improvement
changed files:
texts:Add document to explain DataMap Management

issueID:CARBONDATA-2576
type:Bug
changed files:
texts:MV Datamap - MV is not working fine if there is more than 3 aggregate function in the same datamap.
MV is not working fine if there is more than 3 aggregate function in the same datamap. It is working fine upto 3 aggregate functions on the same MV. Please see the attached document for more details.Test queries: scala> carbon.sql("create datamap datamap_comp_maxsumminavg using 'mv' as select empno,max(projectenddate),sum(salary),min(projectjoindate),avg(attendance) from originTable group by empno").show(200,false)++   ++++  rebuild datascala> carbon.sql("rebuild datamap datamap_comp_maxsumminavg").show(200,false)++   ++++  scala> carbon.sql("explain select empno,max(projectenddate),sum(salary),min(projectjoindate),avg(attendance) from originTable group by empno").show(200,false)org.apache.spark.sql.AnalysisException: expression 'datamap_comp_maxsumminavg_table.`avg_attendance`' is neither present in the group by, nor is it an aggregate function. Add to group by or wrap in first() (or first_value) if you don't care which value you get.;;Aggregate origintable_empno#2925, origintable_empno#2925 AS empno#3002, max(max_projectenddate#2926) AS max(projectenddate)#3003, sum(sum_salary#2927L) AS sum(salary)#3004L, min(min_projectjoindate#2928) AS min(projectjoindate)#3005, avg_attendance#2929 AS avg(attendance)#3006+- SubqueryAlias datamap_comp_maxsumminavg_table   +- Relationorigintable_empno#2925,max_projectenddate#2926,sum_salary#2927L,min_projectjoindate#2928,avg_attendance#2929 CarbonDatasourceHadoopRelation [ Database name :default, Table name :datamap_comp_maxsumminavg_table, Schema :Some(StructType(StructField(origintable_empno,IntegerType,true), StructField(max_projectenddate,TimestampType,true), StructField(sum_salary,LongType,true), StructField(min_projectjoindate,TimestampType,true), StructField(avg_attendance,DoubleType,true))) ]   at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.failAnalysis(CheckAnalysis.scala:39)  at org.apache.spark.sql.catalyst.analysis.Analyzer.failAnalysis(Analyzer.scala:91)  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$class$$anonfun$$checkValidAggregateExpression$1(CheckAnalysis.scala:247)  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$org$apache$spark$sql$catalyst$analysis$CheckAnalysis$class$$anonfun$$checkValidAggregateExpression$1$5.apply(CheckAnalysis.scala:253)  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$org$apache$spark$sql$catalyst$analysis$CheckAnalysis$class$$anonfun$$checkValidAggregateExpression$1$5.apply(CheckAnalysis.scala:253)  at scala.collection.immutable.List.foreach(List.scala:381)  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$class$$anonfun$$checkValidAggregateExpression$1(CheckAnalysis.scala:253)  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$9.apply(CheckAnalysis.scala:280)  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$9.apply(CheckAnalysis.scala:280)  at scala.collection.immutable.List.foreach(List.scala:381)  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:280)  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:78)  at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:78)  at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:91)  at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:52)  at org.apache.spark.sql.CarbonSession.withProfiler(CarbonSession.scala:148)  at org.apache.spark.sql.CarbonSession.sql(CarbonSession.scala:95)  at org.apache.carbondata.mv.datamap.MVAnalyzerRule.apply(MVAnalyzerRule.scala:72)  at org.apache.carbondata.mv.datamap.MVAnalyzerRule.apply(MVAnalyzerRule.scala:38)  at org.apache.spark.sql.hive.CarbonAnalyzer.execute(CarbonAnalyzer.scala:46)  at org.apache.spark.sql.hive.CarbonAnalyzer.execute(CarbonAnalyzer.scala:27)  at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:69)  at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:67)  at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:50)  at org.apache.spark.sql.execution.QueryExecution.withCachedData$lzycompute(QueryExecution.scala:73)  at org.apache.spark.sql.execution.QueryExecution.withCachedData(QueryExecution.scala:72)  at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:78)  at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:78)  at org.apache.spark.sql.execution.QueryExecution.sparkPlan$lzycompute(QueryExecution.scala:84)  at org.apache.spark.sql.execution.QueryExecution.sparkPlan(QueryExecution.scala:80)  at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:89)  at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:89)  at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:92)  at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:92)  at org.apache.spark.sql.execution.command.table.CarbonExplainCommand.collectProfiler(CarbonExplainCommand.scala:54)  at org.apache.spark.sql.execution.command.table.CarbonExplainCommand.processMetadata(CarbonExplainCommand.scala:45)  at org.apache.spark.sql.execution.command.MetadataCommand.run(package.scala:68)  at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)  at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)  at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:67)  at org.apache.spark.sql.Dataset.<init>(Dataset.scala:183)  at org.apache.spark.sql.CarbonSession$$anonfun$sql$1.apply(CarbonSession.scala:108)  at org.apache.spark.sql.CarbonSession$$anonfun$sql$1.apply(CarbonSession.scala:97)  at org.apache.spark.sql.CarbonSession.withProfiler(CarbonSession.scala:155)  at org.apache.spark.sql.CarbonSession.sql(CarbonSession.scala:95)  ... 48 elided
issueID:CARBONDATA-2578
type:Improvement
changed files:store/sdk/src/main/java/org/apache/carbondata/sdk/file/CarbonReader.java
store/sdk/src/main/java/org/apache/carbondata/sdk/file/CarbonWriterBuilder.java
texts:RowBatch Object is present even after CarbonReader is closed.

issueID:CARBONDATA-258
type:Bug
changed files:
texts:update doc for adding information about COMMENTCHAR
After adding COMMENTCHAR option, update the doc.
issueID:CARBONDATA-2581
type:Improvement
changed files:
texts:Adding QueryStatistics in Presto Integration Code to Measure Performance

issueID:CARBONDATA-2582
type:Bug
changed files:
texts:Carbon properties does not get distributed on cluster mode
Unsafe memory related Carbon properties mentioned in carbondata catalog of presto were not getting distributed in presto cluster mode. 
issueID:CARBONDATA-2583
type:Improvement
changed files:
texts:Presto Performance Optimization - Creating a Multiblock Split to reduce network IO

issueID:CARBONDATA-2584
type:New Feature
changed files:
texts:CarbonData Local Dictionary Support
Currently CarbonData supports global dictionary or No-Dictionary (Plain-Text stored in LV format) for storing dimension column data.Bottleneck with Global DictionaryIt’s difficult for user to determine whether the column should be dictionary or not if number of columns in table is high.Global dictionary generation generally slows down the load process.Multiple IO operations are made during load even though dictionary already exists.During query, multiple IO operations done for reading dictionary files and carbondata files.Bottleneck with No-DictionaryStorage size is high as we store the data in LV formatQuery on No-Dictionary column is slower as data read/processed is moreFiltering is slower on No-Dictionary columns as number of comparison is highMemory footprint is highThe above bottlenecks can be solved by generating dictionary for low cardinality columns at each blocklet level, which will help to achieve below benefits:Reduces the extra IO operations read/write on the dictionary files generated in case of global dictionary.It will eliminate the problem for user to identify the dictionary columns when the number of columns are more in a table.It helps in getting more compression on dimension columns with less cardinality.Filter queries and full scan queries on No-dictionary columns with local dictionary will be faster as filter will be done on encoded data.It will help in reducing the store size and memory footprint as only unique values will be stored as part of local dictionary and corresponding data will be stored as encoded data.
issueID:CARBONDATA-2585
type:Sub-task
changed files:core/src/main/java/org/apache/carbondata/core/localdictionary/generator/ColumnLocalDictionaryGenerator.java
core/src/main/java/org/apache/carbondata/core/datastore/blocklet/BlockletEncodedColumnPage.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/CarbonTable.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/column/ColumnSchema.java
core/src/main/java/org/apache/carbondata/core/util/CarbonProperties.java
core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
core/src/main/java/org/apache/carbondata/core/metadata/converter/ThriftWrapperSchemaConverterImpl.java
texts:Support Adding Local Dictionary configuration in Create table statement
Allow user to pass local dictionary configuration in Create table statement.LOCAL_DICTIONARY_ENABLE :  enable or disable local dictionary generation for a table(default local dictionary generation will be true)LOCAL_DICTIONARY_THRESHOLD: configuring the threshold value for local dictionary generation(default will be 1000)LOCAL_DICTIONARY_INCLUDE**: list of columns for which user wants to generate local dictionary (default all the no dictionary string data type columns will be considered for generation) LOCAL_DICTIONARY_EXCLUDE**: list of columns for which user does not want to generate local dictionary (default no string datatype no dictionary columns are excluded unless it is configured) CREATE TABLE carbontable(column1 string,column2 string,column3 LONG )STORED BY 'carbondata'TBLPROPERTIES('LOCAL_DICTIONARY_ENABLE'='true',’LOCAL_DICTIONARY_THRESHOLD=1000','LOCAL_DICTIONARY_INCLUDE'='column1','LOCAL_DICTIONARY_EXCLUDE'='column2')
issueID:CARBONDATA-2586
type:Sub-task
changed files:core/src/main/java/org/apache/carbondata/core/localdictionary/generator/ColumnLocalDictionaryGenerator.java
core/src/main/java/org/apache/carbondata/core/datastore/blocklet/BlockletEncodedColumnPage.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/CarbonTable.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/column/ColumnSchema.java
core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
core/src/main/java/org/apache/carbondata/core/metadata/converter/ThriftWrapperSchemaConverterImpl.java
texts:Support Showing local dictionary configuration in desc formatted command
Support Showing local dictionary parameter in Desc formatted commandLOCAL_DICTIONARY_ENABLELOCAL_DICTIONARY_THRESHOLDLOCAL_DICTIONARY_INCLUDE LOCAL_DICTIONARY_EXCLUDE
issueID:CARBONDATA-2587
type:Sub-task
changed files:core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/EncodedColumnPage.java
core/src/main/java/org/apache/carbondata/core/datastore/page/VarLengthColumnPageBase.java
core/src/main/java/org/apache/carbondata/core/datastore/page/statistics/DummyStatsCollector.java
core/src/main/java/org/apache/carbondata/core/datastore/page/FallbackEncodedColumnPage.java
processing/src/main/java/org/apache/carbondata/processing/store/writer/v3/BlockletDataHolder.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/reader/dimension/v3/DimensionChunkReaderV3.java
processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactDataHandlerColumnar.java
processing/src/main/java/org/apache/carbondata/processing/datatypes/StructDataType.java
processing/src/main/java/org/apache/carbondata/processing/datatypes/GenericDataType.java
core/src/main/java/org/apache/carbondata/core/datastore/page/SafeVarLengthColumnPage.java
core/src/main/java/org/apache/carbondata/core/localdictionary/generator/ColumnLocalDictionaryGenerator.java
processing/src/main/java/org/apache/carbondata/processing/store/writer/v3/CarbonFactDataWriterImplV3.java
core/src/main/java/org/apache/carbondata/core/datastore/page/UnsafeDecimalColumnPage.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/dimension/legacy/DirectDictDimensionIndexCodec.java
core/src/main/java/org/apache/carbondata/core/datastore/blocklet/BlockletEncodedColumnPage.java
core/src/main/java/org/apache/carbondata/core/datastore/page/ColumnPage.java
core/src/main/java/org/apache/carbondata/core/datastore/blocklet/EncodedBlocklet.java
processing/src/main/java/org/apache/carbondata/processing/store/TablePage.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/dimension/legacy/HighCardDictDimensionIndexCodec.java
core/src/main/java/org/apache/carbondata/core/datastore/columnar/BlockIndexerStorageForNoInvertedIndexForShort.java
core/src/main/java/org/apache/carbondata/core/localdictionary/exception/DictionaryThresholdReachedException.java
core/src/main/java/org/apache/carbondata/core/datastore/page/LocalDictColumnPage.java
core/src/main/java/org/apache/carbondata/core/datastore/page/UnsafeVarLengthColumnPage.java
core/src/main/java/org/apache/carbondata/core/localdictionary/dictionaryholder/DictionaryStore.java
processing/src/main/java/org/apache/carbondata/processing/datatypes/PrimitiveDataType.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/CarbonTable.java
processing/src/main/java/org/apache/carbondata/processing/store/writer/AbstractFactDataWriter.java
core/src/main/java/org/apache/carbondata/core/localdictionary/PageLevelDictionary.java
core/src/main/java/org/apache/carbondata/core/localdictionary/generator/LocalDictionaryGenerator.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/ColumnPageEncoder.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/dimension/legacy/DictDimensionIndexCodec.java
core/src/main/java/org/apache/carbondata/core/datastore/ColumnType.java
processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactDataHandlerModel.java
processing/src/main/java/org/apache/carbondata/processing/datatypes/ArrayDataType.java
core/src/main/java/org/apache/carbondata/core/datastore/page/ComplexColumnPage.java
core/src/main/java/org/apache/carbondata/core/datastore/page/ActualDataBasedFallbackEncoder.java
core/src/main/java/org/apache/carbondata/core/util/CarbonMetadataUtil.java
core/src/main/java/org/apache/carbondata/core/cache/dictionary/DictionaryByteArrayWrapper.java
core/src/main/java/org/apache/carbondata/core/localdictionary/dictionaryholder/MapBasedDictionaryStore.java
texts:Support Local dictionary in data loading
Support local dictionary in data loading for low cardinality no dictionary string data type column
issueID:CARBONDATA-2588
type:Sub-task
changed files:core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/EncodedColumnPage.java
core/src/main/java/org/apache/carbondata/core/datastore/page/VarLengthColumnPageBase.java
core/src/main/java/org/apache/carbondata/core/datastore/page/statistics/DummyStatsCollector.java
core/src/main/java/org/apache/carbondata/core/datastore/page/FallbackEncodedColumnPage.java
processing/src/main/java/org/apache/carbondata/processing/store/writer/v3/BlockletDataHolder.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/reader/dimension/v3/DimensionChunkReaderV3.java
processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactDataHandlerColumnar.java
processing/src/main/java/org/apache/carbondata/processing/datatypes/StructDataType.java
processing/src/main/java/org/apache/carbondata/processing/datatypes/GenericDataType.java
core/src/main/java/org/apache/carbondata/core/datastore/page/SafeVarLengthColumnPage.java
core/src/main/java/org/apache/carbondata/core/localdictionary/generator/ColumnLocalDictionaryGenerator.java
processing/src/main/java/org/apache/carbondata/processing/store/writer/v3/CarbonFactDataWriterImplV3.java
core/src/main/java/org/apache/carbondata/core/datastore/page/UnsafeDecimalColumnPage.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/dimension/legacy/DirectDictDimensionIndexCodec.java
core/src/main/java/org/apache/carbondata/core/datastore/blocklet/BlockletEncodedColumnPage.java
core/src/main/java/org/apache/carbondata/core/datastore/page/ColumnPage.java
core/src/main/java/org/apache/carbondata/core/datastore/blocklet/EncodedBlocklet.java
processing/src/main/java/org/apache/carbondata/processing/store/TablePage.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/dimension/legacy/HighCardDictDimensionIndexCodec.java
core/src/main/java/org/apache/carbondata/core/datastore/columnar/BlockIndexerStorageForNoInvertedIndexForShort.java
core/src/main/java/org/apache/carbondata/core/localdictionary/exception/DictionaryThresholdReachedException.java
core/src/main/java/org/apache/carbondata/core/datastore/page/LocalDictColumnPage.java
core/src/main/java/org/apache/carbondata/core/datastore/page/UnsafeVarLengthColumnPage.java
core/src/main/java/org/apache/carbondata/core/localdictionary/dictionaryholder/DictionaryStore.java
processing/src/main/java/org/apache/carbondata/processing/datatypes/PrimitiveDataType.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/CarbonTable.java
processing/src/main/java/org/apache/carbondata/processing/store/writer/AbstractFactDataWriter.java
core/src/main/java/org/apache/carbondata/core/localdictionary/PageLevelDictionary.java
core/src/main/java/org/apache/carbondata/core/localdictionary/generator/LocalDictionaryGenerator.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/ColumnPageEncoder.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/dimension/legacy/DictDimensionIndexCodec.java
core/src/main/java/org/apache/carbondata/core/datastore/ColumnType.java
processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactDataHandlerModel.java
processing/src/main/java/org/apache/carbondata/processing/datatypes/ArrayDataType.java
core/src/main/java/org/apache/carbondata/core/datastore/page/ComplexColumnPage.java
core/src/main/java/org/apache/carbondata/core/datastore/page/ActualDataBasedFallbackEncoder.java
core/src/main/java/org/apache/carbondata/core/util/CarbonMetadataUtil.java
core/src/main/java/org/apache/carbondata/core/cache/dictionary/DictionaryByteArrayWrapper.java
core/src/main/java/org/apache/carbondata/core/localdictionary/dictionaryholder/MapBasedDictionaryStore.java
texts:Support Local dictionary in data loading with complex type columns
Generate local dictionary for complex type primitive columns(no dictionary low cardinality column) 
issueID:CARBONDATA-2589
type:Sub-task
changed files:core/src/main/java/org/apache/carbondata/core/datastore/page/VarLengthColumnPageBase.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/reader/dimension/v3/DimensionChunkReaderV3.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/store/impl/LocalDictDimensionDataChunkStore.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/store/DimensionChunkStoreFactory.java
core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/impl/DimensionRawColumnChunk.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelRangeGrtrThanEquaToFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/localdictionary/generator/ColumnLocalDictionaryGenerator.java
core/src/main/java/org/apache/carbondata/core/scan/result/vector/CarbonDictionary.java
core/src/main/java/org/apache/carbondata/core/datastore/blocklet/BlockletEncodedColumnPage.java
core/src/main/java/org/apache/carbondata/core/datastore/page/ColumnPage.java
core/src/main/java/org/apache/carbondata/core/scan/result/vector/impl/CarbonDictionaryImpl.java
integration/spark-datasource/src/main/scala/org/apache/carbondata/spark/vectorreader/VectorizedCarbonRecordReader.java
processing/src/main/java/org/apache/carbondata/processing/store/TablePage.java
core/src/main/java/org/apache/carbondata/core/scan/filter/FilterUtil.java
integration/spark-datasource/src/main/scala/org/apache/carbondata/spark/vectorreader/ColumnarVectorWrapper.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelRangeLessThanFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/scan/result/vector/CarbonColumnarBatch.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RangeValueFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/datastore/page/LocalDictColumnPage.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/impl/VariableLengthDimensionColumnPage.java
core/src/main/java/org/apache/carbondata/core/scan/result/vector/impl/CarbonColumnVectorImpl.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/CarbonTable.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/impl/FixedLengthDimensionColumnPage.java
core/src/main/java/org/apache/carbondata/core/localdictionary/PageLevelDictionary.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/ExcludeFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelRangeLessThanEqualFilterExecuterImpl.java
integration/spark-datasource/src/main/scala/org/apache/carbondata/spark/vectorreader/CarbonDictionaryWrapper.java
core/src/main/java/org/apache/carbondata/core/scan/result/vector/CarbonColumnVector.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
core/src/main/java/org/apache/carbondata/core/datastore/page/ComplexColumnPage.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/store/ColumnPageWrapper.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/IncludeFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelRangeGrtThanFiterExecuterImpl.java
integration/presto/src/main/java/org/apache/carbondata/presto/CarbonColumnVectorWrapper.java
texts:Support Query on Local dictionary columns
Support Query on local dictionary generated column
issueID:CARBONDATA-259
type:Bug
changed files:
texts:Log of query statistics are not present for limit queries.
Log of query statistics are not present for limit queries.
issueID:CARBONDATA-2590
type:Sub-task
changed files:core/src/main/java/org/apache/carbondata/core/datastore/page/VarLengthColumnPageBase.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/reader/dimension/v3/DimensionChunkReaderV3.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/store/impl/LocalDictDimensionDataChunkStore.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/store/DimensionChunkStoreFactory.java
core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/impl/DimensionRawColumnChunk.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelRangeGrtrThanEquaToFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/localdictionary/generator/ColumnLocalDictionaryGenerator.java
core/src/main/java/org/apache/carbondata/core/scan/result/vector/CarbonDictionary.java
core/src/main/java/org/apache/carbondata/core/datastore/blocklet/BlockletEncodedColumnPage.java
core/src/main/java/org/apache/carbondata/core/datastore/page/ColumnPage.java
core/src/main/java/org/apache/carbondata/core/scan/result/vector/impl/CarbonDictionaryImpl.java
integration/spark-datasource/src/main/scala/org/apache/carbondata/spark/vectorreader/VectorizedCarbonRecordReader.java
processing/src/main/java/org/apache/carbondata/processing/store/TablePage.java
core/src/main/java/org/apache/carbondata/core/scan/filter/FilterUtil.java
integration/spark-datasource/src/main/scala/org/apache/carbondata/spark/vectorreader/ColumnarVectorWrapper.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelRangeLessThanFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/scan/result/vector/CarbonColumnarBatch.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RangeValueFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/datastore/page/LocalDictColumnPage.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/impl/VariableLengthDimensionColumnPage.java
core/src/main/java/org/apache/carbondata/core/scan/result/vector/impl/CarbonColumnVectorImpl.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/CarbonTable.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/impl/FixedLengthDimensionColumnPage.java
core/src/main/java/org/apache/carbondata/core/localdictionary/PageLevelDictionary.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/ExcludeFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelRangeLessThanEqualFilterExecuterImpl.java
integration/spark-datasource/src/main/scala/org/apache/carbondata/spark/vectorreader/CarbonDictionaryWrapper.java
core/src/main/java/org/apache/carbondata/core/scan/result/vector/CarbonColumnVector.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
core/src/main/java/org/apache/carbondata/core/datastore/page/ComplexColumnPage.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/store/ColumnPageWrapper.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/IncludeFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelRangeGrtThanFiterExecuterImpl.java
integration/presto/src/main/java/org/apache/carbondata/presto/CarbonColumnVectorWrapper.java
texts:Support Query on Local dictionary Complex type column
Support query on local dictionary generated complex type columns
issueID:CARBONDATA-2591
type:Improvement
changed files:store/sdk/src/main/java/org/apache/carbondata/sdk/file/CarbonReaderBuilder.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/CarbonTable.java
core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
texts:SDK CarbonReader support filter
SDK CarbonReader support filterCode:  @Test  public void testReadWithFilterNonTransactional() throws IOException, InterruptedException {    String path = "./testWriteFiles";    FileUtils.deleteDirectory(new File(path));    Field[] fields = new Field[2];    fields[0] = new Field("name", DataTypes.STRING);    fields[1] = new Field("age", DataTypes.INT);    TestUtil.writeFilesAndVerify(new Schema(fields), path, false, false);    ColumnSchema columnSchema = new ColumnSchema();    columnSchema.setColumnar(true);    columnSchema.setColumnName("name");    columnSchema.setColumnUniqueId(UUID.randomUUID().toString());    columnSchema.setDataType(DataTypes.STRING);    columnSchema.setDimensionColumn(true);    List<Encoding> encodingList = new ArrayList<>();    encodingList.add(Encoding.IMPLICIT);    columnSchema.setEncodingList(encodingList);    CarbonColumn carbonColumn = new CarbonColumn(columnSchema, 0, 0);    ColumnExpression columnExpression = new ColumnExpression("name", DataTypes.STRING);    columnExpression.setCarbonColumn(carbonColumn);    EqualToExpression equalToExpression = new EqualToExpression(columnExpression,        new LiteralExpression("robot1", DataTypes.STRING));    CarbonReader reader = CarbonReader        .builder(path, "_temp")        .isTransactionalTable(false)        .projection(new String[]{"name", "age"})        .filter(equalToExpression)        .build();    // expected output after sorting    String[] name = new String[200];    Integer[] age = new Integer[200];    for (int i = 0; i < 200; i++) {      name[i] = "robot" + (i / 10);      age[i] = i;    }    int i = 0;    while (reader.hasNext()) {      Object[] row = (Object[]) reader.readNextRow();      // Default sort column is applied for dimensions. So, need  to validate accordingly      assert ("robot1".equals(row[0]));      System.out.println(row[0] + "\t" + row[1]);      i++;    }    Assert.assertEquals(i, 20);    reader.close();    FileUtils.deleteDirectory(new File(path));  }Error:log4j:WARN No appenders could be found for logger (org.apache.carbondata.core.util.CarbonProperties).log4j:WARN Please initialize the log4j system properly.log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.java.lang.RuntimeException: Error while resolving filter expression at org.apache.carbondata.core.metadata.schema.table.CarbonTable.resolveFilter(CarbonTable.java:909) at org.apache.carbondata.hadoop.api.CarbonFileInputFormat.getSplits(CarbonFileInputFormat.java:128) at org.apache.carbondata.sdk.file.CarbonReaderBuilder.build(CarbonReaderBuilder.java:200) at org.apache.carbondata.sdk.file.CarbonReaderTest.testReadWithFilterNonTransactional(CarbonReaderTest.java:189) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at junit.framework.TestCase.runTest(TestCase.java:176) at junit.framework.TestCase.runBare(TestCase.java:141) at junit.framework.TestResult$1.protect(TestResult.java:122) at junit.framework.TestResult.runProtected(TestResult.java:142) at junit.framework.TestResult.run(TestResult.java:125) at junit.framework.TestCase.run(TestCase.java:129) at junit.framework.TestSuite.runTest(TestSuite.java:255) at junit.framework.TestSuite.run(TestSuite.java:250) at org.junit.internal.runners.JUnit38ClassRunner.run(JUnit38ClassRunner.java:84) at org.junit.runner.JUnitCore.run(JUnitCore.java:160) at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:68) at com.intellij.rt.execution.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:47) at com.intellij.rt.execution.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:242) at com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:70)Caused by: java.lang.NullPointerException at org.apache.carbondata.core.scan.filter.FilterExpressionProcessor.getFilterResolverBasedOnExpressionType(FilterExpressionProcessor.java:353) at org.apache.carbondata.core.scan.filter.FilterExpressionProcessor.createFilterResolverTree(FilterExpressionProcessor.java:284) at org.apache.carbondata.core.scan.filter.FilterExpressionProcessor.getFilterResolvertree(FilterExpressionProcessor.java:233) at org.apache.carbondata.core.scan.filter.FilterExpressionProcessor.getFilterResolver(FilterExpressionProcessor.java:84) at org.apache.carbondata.core.metadata.schema.table.CarbonTable.resolveFilter(CarbonTable.java:906) ... 21 more
issueID:CARBONDATA-2593
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
core/src/main/java/org/apache/carbondata/core/util/CarbonProperties.java
texts:Add an option &#39;carbon.insert.storage.level&#39; to support configuring the storage level when insert into data with &#39;carbon.insert.persist.enable&#39;=&#39;true&#39;
When insert into data with 'carbon.insert.persist.enable'='true', the storage level of dataset is 'MEMORY_AND_DISK', it should support configuring the storage level to correspond to different environment.
issueID:CARBONDATA-2594
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/metadata/schema/table/TableSchemaBuilder.java
texts:Incorrect logic when set &#39;Encoding.INVERTED_INDEX&#39; for each dimension column
All of non-sort dimension columns are set as 'Encoding.INVERTED_INDEX' column, this is wrong, only the columns defined in 'SORT_COLUMN' and not in 'NO_INVERTED_INDEX' need to be set as  'Encoding.INVERTED_INDEX' column.
issueID:CARBONDATA-2595
type:Improvement
changed files:
texts:Reformat the output of command &#39;desc formatted table_name&#39;
According to the discussion in topic, reformat the output of command 'desc formatted table_name'. 
issueID:CARBONDATA-2599
type:Improvement
changed files:streaming/src/main/java/org/apache/carbondata/streaming/parser/CarbonStreamParser.java
texts:Use RowStreamParserImp as default value of config &#39;carbon.stream.parser&#39;
See the detailed info in topic
issueID:CARBONDATA-26
type:Bug
changed files:
texts:Filter query issue for >, <, <= than filter
1. select count from a12 where dob > '2014-07-01 12:07:28'   throwing runtime exception2. select count from a12 where dob < '2014-07-01 12:07:28'   is including the null values also.3. select count from a12 where dob <=  '2014-07-01 12:07:28'   is including the null value Null should not considered in less than filter.create cube command: create table a12(empid String,ename String,sal double,deptno int,mgr string,gender string," +        "dob timestamp,comm decimal(4,2),desc string) stored by 'org.apache.carbondata.formatdata:empid,ename,sal,deptno,mgr,gender,dob,comm,desc1,abc,1233,10,2,,2014-07-01 12:07:28,1234.191,string_null2,bcd,1322,,3,f,2014-07-01 12:07:28,19.99,int_null3,cde,4322,,4,m,,16.996,date_null4,    ,43243,,5,m,,999.117,string_space5,,43242,20,6,m,2017-07-01 12:07:28,99.999,string_null6,ijk,,20,6,m,2017-07-01 12:07:28,50089,double_null7,pqr,2422,20,6,m,2017-07-01 12:07:28,32.339,decimal_null8
issueID:CARBONDATA-260
type:Bug
changed files:
texts:Equal or lesser value of MAXCOLUMNS option than column count in CSV header results into array index of bound exception
If column count in CSV header is more or equal to MAXCOLUMNS option value then array index out of bound exception is thrown by the Univocity CSV parser. This is because while parsing the row, parser adds each row to an array and increments the index and after incrementing it performs one more operation using the incremented index value which leads to array index pf bound exceptionjava.lang.OutOfMemoryError: Java heap spaceat com.univocity.parsers.common.ParserOutput.<init>(ParserOutput.java:86)at com.univocity.parsers.common.AbstractParser.<init>(AbstractParser.java:66)at com.univocity.parsers.csv.CsvParser.<init>(CsvParser.java:50)at org.apache.carbondata.processing.csvreaderstep.UnivocityCsvParser.initialize(UnivocityCsvParser.java:114)at org.apache.carbondata.processing.csvreaderstep.CsvInput.doProcessUnivocity(CsvInput.java:427)at org.apache.carbondata.processing.csvreaderstep.CsvInput.access$100(CsvInput.java:60)at org.apache.carbondata.processing.csvreaderstep.CsvInput$1.call(CsvInput.java:389)
issueID:CARBONDATA-2600
type:New Feature
changed files:
texts:Add a command to show detailed index information for a segment
Add a command to show detailed index information for a segment, for example:show index for table table_name where segment_id = 0;
issueID:CARBONDATA-2602
type:Sub-task
changed files:core/src/main/java/org/apache/carbondata/core/datastore/page/VarLengthColumnPageBase.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/reader/dimension/v3/DimensionChunkReaderV3.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/store/impl/LocalDictDimensionDataChunkStore.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/store/DimensionChunkStoreFactory.java
core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/impl/DimensionRawColumnChunk.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelRangeGrtrThanEquaToFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/localdictionary/generator/ColumnLocalDictionaryGenerator.java
core/src/main/java/org/apache/carbondata/core/scan/result/vector/CarbonDictionary.java
core/src/main/java/org/apache/carbondata/core/datastore/blocklet/BlockletEncodedColumnPage.java
core/src/main/java/org/apache/carbondata/core/datastore/page/ColumnPage.java
core/src/main/java/org/apache/carbondata/core/scan/result/vector/impl/CarbonDictionaryImpl.java
integration/spark-datasource/src/main/scala/org/apache/carbondata/spark/vectorreader/VectorizedCarbonRecordReader.java
processing/src/main/java/org/apache/carbondata/processing/store/TablePage.java
core/src/main/java/org/apache/carbondata/core/scan/filter/FilterUtil.java
integration/spark-datasource/src/main/scala/org/apache/carbondata/spark/vectorreader/ColumnarVectorWrapper.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelRangeLessThanFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/scan/result/vector/CarbonColumnarBatch.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RangeValueFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/datastore/page/LocalDictColumnPage.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/impl/VariableLengthDimensionColumnPage.java
core/src/main/java/org/apache/carbondata/core/scan/result/vector/impl/CarbonColumnVectorImpl.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/CarbonTable.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/impl/FixedLengthDimensionColumnPage.java
core/src/main/java/org/apache/carbondata/core/localdictionary/PageLevelDictionary.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/ExcludeFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelRangeLessThanEqualFilterExecuterImpl.java
integration/spark-datasource/src/main/scala/org/apache/carbondata/spark/vectorreader/CarbonDictionaryWrapper.java
core/src/main/java/org/apache/carbondata/core/scan/result/vector/CarbonColumnVector.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
core/src/main/java/org/apache/carbondata/core/datastore/page/ComplexColumnPage.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/store/ColumnPageWrapper.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/IncludeFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelRangeGrtThanFiterExecuterImpl.java
integration/presto/src/main/java/org/apache/carbondata/presto/CarbonColumnVectorWrapper.java
texts:Support Filter Query on Local dictionary generated columns
Support Filter Query on Local dictionary generated columns
issueID:CARBONDATA-2603
type:Improvement
changed files:store/sdk/src/main/java/org/apache/carbondata/sdk/file/CarbonReaderBuilder.java
texts:if creation of one CarbonReader  for non-transactional table fails then we are not able to create new object of CarbonReader
to reproduce follow the following steps :  create a carbonReader for non transactional Table and give the wrong projection column name (so that it will fail). create another carbonReader for non-transactional table with correct values.Expectation : Second reader should be successfully createdActual : creation of second carbonReader is failing with following exception. TestCase to reproduce the issue ; Field[] fields = new Field[] { new Field("c1", "string"),{{ new Field("c2", "int") };}}Schema schema = new Schema(fields);CarbonWriterBuilder builder = CarbonWriter.builder();CarbonWriter carbonWriter ={{ builder.outputPath("D:/mydata").isTransactionalTable(false).uniqueIdentifier(12345)}}{{ .buildWriterForCSVInput(schema);}}carbonWriter.write(new String[] { "MNO", "100" });carbonWriter.close();Field[] fields1 = new Field[] { new Field("p1", "string"),{{ new Field("p2", "int") };}}Schema schema1 = new Schema(fields1);CarbonWriterBuilder builder1 = CarbonWriter.builder();CarbonWriter carbonWriter1 ={{ builder1.outputPath("D:/mydata1").isTransactionalTable(false).uniqueIdentifier(12345)}}{{ .buildWriterForCSVInput(schema1);}}carbonWriter1.write(new String[] { "PQR", "200" });carbonWriter1.close();try {{{ CarbonReader reader =}}{{ CarbonReader.builder("D:/mydata", "_temp").}}{{ projection(new String[] { "c1", "c3" })}}{{ .isTransactionalTable(false).build();}}} catch (Exception e){{{ System.out.println("Success");}}}CarbonReader reader1 ={{ CarbonReader.builder("D:/mydata1", "_temp1")}}{{ .projection(new String[] { "p1", "p2" })}}{{ .isTransactionalTable(false).build();}}while (reader1.hasNext()) {{{ Object[] row1 = (Object[]) reader1.readNextRow();}}{{ System.out.println(row1&#91;0&#93;);}}{{ System.out.println(row1&#91;1&#93;);}}}reader1.close();{{}}   
issueID:CARBONDATA-2604
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/scan/collector/impl/RawBasedResultCollector.java
texts:getting ArrayIndexOutOfBoundException during compaction after IUD in cluster
Exception : To reproduce the issue follow the following steps :  create table brinjal (imei string,AMSize string,channelsId string,ActiveCountry string, Activecity string,gamePointId double,deviceInformationId double,productionDate Timestamp,deliveryDate timestamp,deliverycharge double) STORED BY 'org.apache.carbondata.format' TBLPROPERTIES('table_blocksize'='2000','sort_columns'='imei'); LOAD DATA INPATH '/user/loader/xyz.csv' INTO TABLE brinjal OPTIONS('DELIMITER'=',', 'QUOTECHAR'= '','BAD_RECORDS_ACTION'='FORCE','FILEHEADER'= 'imei,deviceInformationId,AMSize,channelsId,ActiveCountry,Activecity,gamePointId,productionDate,deliveryDate,deliverycharge'); LOAD DATA INPATH '/user/loader/xyz.csv' INTO TABLE brinjal OPTIONS('DELIMITER'=',', 'QUOTECHAR'= '','BAD_RECORDS_ACTION'='FORCE','FILEHEADER'= 'imei,deviceInformationId,AMSize,channelsId,ActiveCountry,Activecity,gamePointId,productionDate,deliveryDate,deliverycharge'); LOAD DATA INPATH '/user/loader/xyz.csv' INTO TABLE brinjal OPTIONS('DELIMITER'=',', 'QUOTECHAR'= '','BAD_RECORDS_ACTION'='FORCE','FILEHEADER'= 'imei,deviceInformationId,AMSize,channelsId,ActiveCountry,Activecity,gamePointId,productionDate,deliveryDate,deliverycharge'); insert into brinjal select * from brinjal; update brinjal set (AMSize)= ('8RAM size') where AMSize='4RAM size'; delete from brinjal where AMSize='8RAM size'; delete from table brinjal where segment.id IN(0); clean files for table brinjal; alter table brinjal compact 'minor'; alter table brinjal compact 'major'; 
issueID:CARBONDATA-2606
type:Sub-task
changed files:processing/src/main/java/org/apache/carbondata/processing/datatypes/PrimitiveDataType.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/CarbonTable.java
core/src/main/java/org/apache/carbondata/core/util/DataTypeUtil.java
core/src/main/java/org/apache/carbondata/core/scan/collector/impl/DictionaryBasedResultCollector.java
core/src/main/java/org/apache/carbondata/core/scan/executor/util/RestructureUtil.java
processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactDataHandlerColumnar.java
core/src/main/java/org/apache/carbondata/core/scan/executor/impl/AbstractQueryExecutor.java
core/src/main/java/org/apache/carbondata/core/scan/model/QueryProjection.java
processing/src/main/java/org/apache/carbondata/processing/datatypes/StructDataType.java
core/src/main/java/org/apache/carbondata/core/scan/executor/util/QueryUtil.java
processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactDataHandlerModel.java
core/src/main/java/org/apache/carbondata/core/scan/model/ProjectionDimension.java
core/src/main/java/org/apache/carbondata/core/scan/result/BlockletScannedResult.java
core/src/main/java/org/apache/carbondata/core/scan/model/QueryModelBuilder.java
processing/src/main/java/org/apache/carbondata/processing/datatypes/ArrayDataType.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/store/ColumnPageWrapper.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/column/CarbonDimension.java
core/src/main/java/org/apache/carbondata/core/scan/complextypes/ArrayQueryType.java
core/src/main/java/org/apache/carbondata/core/scan/complextypes/StructQueryType.java
core/src/main/java/org/apache/carbondata/core/scan/filter/GenericQueryType.java
core/src/main/java/org/apache/carbondata/core/scan/complextypes/PrimitiveQueryType.java
texts:Projection push down for struct data type
Projection push down for struct data type
issueID:CARBONDATA-2607
type:Sub-task
changed files:core/src/main/java/org/apache/carbondata/core/util/ByteUtil.java
processing/src/main/java/org/apache/carbondata/processing/datatypes/PrimitiveDataType.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/reader/dimension/v3/DimensionChunkReaderV3.java
core/src/main/java/org/apache/carbondata/core/util/DataTypeUtil.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/ColumnPageEncoderMeta.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/ColumnPageEncoder.java
processing/src/main/java/org/apache/carbondata/processing/datatypes/StructDataType.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/reader/measure/v3/MeasureChunkPageReaderV3.java
core/src/main/java/org/apache/carbondata/core/scan/executor/util/QueryUtil.java
processing/src/main/java/org/apache/carbondata/processing/datatypes/GenericDataType.java
core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/adaptive/AdaptiveDeltaIntegralCodec.java
core/src/main/java/org/apache/carbondata/core/datastore/page/UnsafeFixLengthColumnPage.java
processing/src/main/java/org/apache/carbondata/processing/util/CarbonDataProcessorUtil.java
processing/src/main/java/org/apache/carbondata/processing/datatypes/ArrayDataType.java
core/src/main/java/org/apache/carbondata/core/datastore/page/ColumnPage.java
core/src/main/java/org/apache/carbondata/core/datastore/row/ComplexColumnInfo.java
core/src/main/java/org/apache/carbondata/core/datastore/page/SafeFixLengthColumnPage.java
core/src/main/java/org/apache/carbondata/core/datastore/page/ComplexColumnPage.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/store/ColumnPageWrapper.java
core/src/main/java/org/apache/carbondata/core/datastore/page/ActualDataBasedFallbackEncoder.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/EncodingFactory.java
core/src/main/java/org/apache/carbondata/core/datastore/page/statistics/PrimitivePageStatsCollector.java
processing/src/main/java/org/apache/carbondata/processing/store/TablePage.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/DefaultEncodingFactory.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/reader/measure/v3/MeasureChunkReaderV3.java
core/src/main/java/org/apache/carbondata/core/scan/complextypes/PrimitiveQueryType.java
texts:Provide Adaptive Encoding and Decoding for all data type
Provide Adaptive Encoding and Decoding for all data type
issueID:CARBONDATA-2608
type:Sub-task
changed files:processing/src/main/java/org/apache/carbondata/processing/loading/steps/JsonInputProcessorStepImpl.java
store/sdk/src/main/java/org/apache/carbondata/sdk/file/CarbonWriterBuilder.java
processing/src/main/java/org/apache/carbondata/processing/loading/jsoninput/JsonInputFormat.java
processing/src/main/java/org/apache/carbondata/processing/loading/parser/impl/JsonRowParser.java
store/sdk/src/main/java/org/apache/carbondata/sdk/file/JsonCarbonWriter.java
processing/src/main/java/org/apache/carbondata/processing/util/CarbonDataProcessorUtil.java
processing/src/main/java/org/apache/carbondata/processing/loading/jsoninput/JsonStreamReader.java
processing/src/main/java/org/apache/carbondata/processing/loading/steps/InputProcessorStepWithNoConverterImpl.java
processing/src/main/java/org/apache/carbondata/processing/loading/DataLoadProcessBuilder.java
processing/src/main/java/org/apache/carbondata/processing/loading/model/CarbonLoadModel.java
store/sdk/src/main/java/org/apache/carbondata/sdk/file/CarbonReaderBuilder.java
processing/src/main/java/org/apache/carbondata/processing/loading/steps/InputProcessorStepImpl.java
core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
texts:SDK Support JSON data loading directly without AVRO conversion
Support JSON data loading directly into Carbon table.
issueID:CARBONDATA-261
type:Bug
changed files:integration/spark-common/src/main/java/org/apache/carbondata/spark/load/CarbonLoaderUtil.java
texts:clean files is updating the stale segments to the table status.
in clean files it will read the table status first and it will delete the physical locations and update the table metadata  and then take the lock for writing and after that it will write the old read list to the file.but this is wrong as in the time between the read and write the table status would have been changed.so before writing it should read the table status again and update the changes and then write.
issueID:CARBONDATA-2610
type:Bug
changed files:
texts:DataMap creation fails on null values
Create a table load data in table having null values. Create datamap on table.Exception Details18/06/13 23:23:52 ERROR Executor: Exception in task 0.0 in stage 4.0 (TID 4)java.lang.NullPointerExceptionat org.apache.carbondata.datamap.OriginalReadSupport$$anonfun$readRow$1.apply(IndexDataMapRebuildRDD.scala:130)at org.apache.carbondata.datamap.OriginalReadSupport$$anonfun$readRow$1.apply(IndexDataMapRebuildRDD.scala:128)at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)at org.apache.carbondata.datamap.OriginalReadSupport.readRow(IndexDataMapRebuildRDD.scala:128)at org.apache.carbondata.datamap.OriginalReadSupport.readRow(IndexDataMapRebuildRDD.scala:122)at org.apache.carbondata.hadoop.CarbonRecordReader.getCurrentValue(CarbonRecordReader.java:108)at org.apache.carbondata.datamap.IndexDataMapRebuildRDD.internalCompute(IndexDataMapRebuildRDD.scala:194)at org.apache.carbondata.spark.rdd.CarbonRDD.compute(CarbonRDD.scala:76)at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)at org.apache.spark.scheduler.Task.run(Task.scala:108)at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)at java.lang.Thread.run(Thread.java:748)18/06/13 23:23:52 ERROR TaskSetManager: Task 0 in stage 4.0 failed 1 times; aborting job  
issueID:CARBONDATA-2611
type:Sub-task
changed files:
texts:Add test cases for Create table statement for Local Dictionary Support
What changes were made?Unti Test cases and SDV Test cases were added for Local Dictionary Support for Create table command and Describe formatted commandWhat scenarios were covered?Create table command with all combinations of configurations for table properties like  LOCAL_DICTIONARY_ENABLE LOCAL_DICTIONARY_THRESHOLD LOCAL_DICTIONARY_INCLUDE LOCAL_DICTIONARY_EXCLUDE****Verifying exception and error messages for all the invalid scenariosDescribe formatted command to verify the values configured 
issueID:CARBONDATA-2613
type:Sub-task
changed files:
texts:Support create/load/query on csv based carbontable

issueID:CARBONDATA-2614
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/scan/model/QueryModel.java
hadoop/src/main/java/org/apache/carbondata/hadoop/CarbonRecordReader.java
texts:There are some exception when using FG in search mode and the prune result is none
test code:  test("test lucene datamap with search mode, two column") {    sql("set carbon.search.enabled = true")    sql("drop datamap if exists dm3 ON TABLE main")    sql("CREATE DATAMAP dm3 ON TABLE main USING 'lucene' DMProperties('INDEX_COLUMNS'='city , id') ")    checkAnswer(sql("SELECT * FROM main WHERE TEXT_MATCH('city:city6')"),      sql("SELECT * FROM main WHERE city='city6'"))    checkAnswer(sql("SELECT * FROM main WHERE TEXT_MATCH('id:100000')"),      sql(s"SELECT * FROM main WHERE id='100000'"))    sql("DROP DATAMAP if exists dm3 ON TABLE main")  }error:18/06/15 03:30:45 INFO UnsafeMemoryManager: [dispatcher-event-loop-7][partitionID:main;queryID:33936819040113] Total memory used after task 33936818832698 is 0 Current tasks running now are : [33934154436368]18/06/15 03:30:45 ERROR Inbox: Ignoring errorjava.lang.RuntimeException: java.util.concurrent.ExecutionException: org.apache.carbondata.core.scan.expression.exception.FilterUnsupportedException: TEXT_MATCH is not supported on table main at org.apache.carbondata.core.scan.result.iterator.AbstractSearchModeResultIterator.hasNext(AbstractSearchModeResultIterator.java:75) at org.apache.carbondata.core.scan.result.iterator.ChunkRowIterator.<init>(ChunkRowIterator.java:40) at org.apache.carbondata.hadoop.CarbonRecordReader.initialize(CarbonRecordReader.java:89) at org.apache.carbondata.store.worker.SearchRequestHandler.handleRequest(SearchRequestHandler.java:140) at org.apache.carbondata.store.worker.SearchRequestHandler.handleSearch(SearchRequestHandler.java:71) at org.apache.spark.search.Searcher$$anonfun$receiveAndReply$1.applyOrElse(Searcher.scala:42) at org.apache.spark.rpc.netty.Inbox$$anonfun$process$1.apply$mcV$sp(Inbox.scala:105) at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:205) at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101) at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:216) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748)Caused by: java.util.concurrent.ExecutionException: org.apache.carbondata.core.scan.expression.exception.FilterUnsupportedException: TEXT_MATCH is not supported on table main at java.util.concurrent.FutureTask.report(FutureTask.java:122) at java.util.concurrent.FutureTask.get(FutureTask.java:192) at org.apache.carbondata.core.scan.result.iterator.AbstractSearchModeResultIterator.hasNext(AbstractSearchModeResultIterator.java:71) ... 12 moreCaused by: org.apache.carbondata.core.scan.expression.exception.FilterUnsupportedException: TEXT_MATCH is not supported on table main at org.apache.carbondata.core.scan.filter.executer.RowLevelFilterExecuterImpl.applyFilter(RowLevelFilterExecuterImpl.java:199) at org.apache.carbondata.core.scan.scanner.impl.BlockletFilterScanner.executeFilter(BlockletFilterScanner.java:174) at org.apache.carbondata.core.scan.scanner.impl.BlockletFilterScanner.scanBlocklet(BlockletFilterScanner.java:101) at org.apache.carbondata.core.scan.processor.BlockScan.scan(BlockScan.java:70) at org.apache.carbondata.core.scan.result.iterator.AbstractSearchModeResultIterator$1.call(AbstractSearchModeResultIterator.java:59) at org.apache.carbondata.core.scan.result.iterator.AbstractSearchModeResultIterator$1.call(AbstractSearchModeResultIterator.java:53) at java.util.concurrent.FutureTask.run(FutureTask.java:266) ... 3 more18/06/15 03:30:45 ERROR CarbonSession: Exception when executing search mode: Exception thrown in awaitResult: 18/06/15 03:30:45 INFO SearchModeTestCase: ScalaTest-run-running-SearchModeTestCase ===== FINISHED org.apache.carbondata.spark.testsuite.detailquery.SearchModeTestCase: 'test lucene datamap with search mode 2' =====Exception thrown in awaitResult: org.apache.spark.SparkException: Exception thrown in awaitResult:  at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:205) at org.apache.spark.rpc.Master$$anonfun$search$1.apply(Master.scala:239) at org.apache.spark.rpc.Master$$anonfun$search$1.apply(Master.scala:230) at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99) at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99) at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230) at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40) at scala.collection.mutable.HashMap.foreach(HashMap.scala:99) at org.apache.spark.rpc.Master.search(Master.scala:230) at org.apache.carbondata.store.SparkCarbonStore.search(SparkCarbonStore.scala:144) at org.apache.spark.sql.CarbonSession.runSearch(CarbonSession.scala:228) at org.apache.spark.sql.CarbonSession.org$apache$spark$sql$CarbonSession$$trySearchMode(CarbonSession.scala:180) at org.apache.spark.sql.CarbonSession$$anonfun$sql$1.apply(CarbonSession.scala:99) at org.apache.spark.sql.CarbonSession$$anonfun$sql$1.apply(CarbonSession.scala:96) at org.apache.spark.sql.CarbonSession.withProfiler(CarbonSession.scala:154) at org.apache.spark.sql.CarbonSession.sql(CarbonSession.scala:94) at org.apache.spark.sql.test.Spark2TestQueryExecutor.sql(Spark2TestQueryExecutor.scala:35) at org.apache.spark.sql.test.util.QueryTest.sql(QueryTest.scala:113) at org.apache.carbondata.spark.testsuite.detailquery.SearchModeTestCase$$anonfun$12.apply$mcV$sp(SearchModeTestCase.scala:129) at org.apache.carbondata.spark.testsuite.detailquery.SearchModeTestCase$$anonfun$12.apply(SearchModeTestCase.scala:122) at org.apache.carbondata.spark.testsuite.detailquery.SearchModeTestCase$$anonfun$12.apply(SearchModeTestCase.scala:122) at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22) at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85) at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104) at org.scalatest.Transformer.apply(Transformer.scala:22) at org.scalatest.Transformer.apply(Transformer.scala:20) at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166) at org.apache.spark.sql.test.util.CarbonFunSuite.withFixture(CarbonFunSuite.scala:41) at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:163) at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175) at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175) at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306) at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:175) at org.scalatest.FunSuite.runTest(FunSuite.scala:1555) at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208) at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208) at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413) at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401) at scala.collection.immutable.List.foreach(List.scala:381) at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401) at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396) at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483) at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:208) at org.scalatest.FunSuite.runTests(FunSuite.scala:1555) at org.scalatest.Suite$class.run(Suite.scala:1424) at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1555) at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212) at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212) at org.scalatest.SuperEngine.runImpl(Engine.scala:545) at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:212) at org.apache.carbondata.spark.testsuite.detailquery.SearchModeTestCase.org$scalatest$BeforeAndAfterAll$$super$run(SearchModeTestCase.scala:32) at org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:257) at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:256) at org.apache.carbondata.spark.testsuite.detailquery.SearchModeTestCase.run(SearchModeTestCase.scala:32) at org.scalatest.tools.SuiteRunner.run(SuiteRunner.scala:55) at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2563) at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2557) at scala.collection.immutable.List.foreach(List.scala:381) at org.scalatest.tools.Runner$.doRunRunRunDaDoRunRun(Runner.scala:2557) at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1044) at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1043) at org.scalatest.tools.Runner$.withClassLoaderAndDispatchReporter(Runner.scala:2722) at org.scalatest.tools.Runner$.runOptionallyWithPassFailReporter(Runner.scala:1043) at org.scalatest.tools.Runner$.run(Runner.scala:883) at org.scalatest.tools.Runner.run(Runner.scala) at org.jetbrains.plugins.scala.testingSupport.scalaTest.ScalaTestRunner.runScalaTest2(ScalaTestRunner.java:131) at org.jetbrains.plugins.scala.testingSupport.scalaTest.ScalaTestRunner.main(ScalaTestRunner.java:28)Caused by: java.lang.RuntimeException: java.util.concurrent.ExecutionException: org.apache.carbondata.core.scan.expression.exception.FilterUnsupportedException: TEXT_MATCH is not supported on table main at org.apache.carbondata.core.scan.result.iterator.AbstractSearchModeResultIterator.hasNext(AbstractSearchModeResultIterator.java:75) at org.apache.carbondata.core.scan.result.iterator.ChunkRowIterator.<init>(ChunkRowIterator.java:40) at org.apache.carbondata.hadoop.CarbonRecordReader.initialize(CarbonRecordReader.java:89) at org.apache.carbondata.store.worker.SearchRequestHandler.handleRequest(SearchRequestHandler.java:140) at org.apache.carbondata.store.worker.SearchRequestHandler.handleSearch(SearchRequestHandler.java:71) at org.apache.spark.search.Searcher$$anonfun$receiveAndReply$1.applyOrElse(Searcher.scala:42) at org.apache.spark.rpc.netty.Inbox$$anonfun$process$1.apply$mcV$sp(Inbox.scala:105) at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:205) at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101) at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:216) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748)Caused by: java.util.concurrent.ExecutionException: org.apache.carbondata.core.scan.expression.exception.FilterUnsupportedException: TEXT_MATCH is not supported on table main at java.util.concurrent.FutureTask.report(FutureTask.java:122) at java.util.concurrent.FutureTask.get(FutureTask.java:192) at org.apache.carbondata.core.scan.result.iterator.AbstractSearchModeResultIterator.hasNext(AbstractSearchModeResultIterator.java:71) ... 12 moreCaused by: org.apache.carbondata.core.scan.expression.exception.FilterUnsupportedException: TEXT_MATCH is not supported on table main at org.apache.carbondata.core.scan.filter.executer.RowLevelFilterExecuterImpl.applyFilter(RowLevelFilterExecuterImpl.java:199) at org.apache.carbondata.core.scan.scanner.impl.BlockletFilterScanner.executeFilter(BlockletFilterScanner.java:174) at org.apache.carbondata.core.scan.scanner.impl.BlockletFilterScanner.scanBlocklet(BlockletFilterScanner.java:101) at org.apache.carbondata.core.scan.processor.BlockScan.scan(BlockScan.java:70) at org.apache.carbondata.core.scan.result.iterator.AbstractSearchModeResultIterator$1.call(AbstractSearchModeResultIterator.java:59) at org.apache.carbondata.core.scan.result.iterator.AbstractSearchModeResultIterator$1.call(AbstractSearchModeResultIterator.java:53) at java.util.concurrent.FutureTask.run(FutureTask.java:266) ... 3 more18/06/15 03:30:45 INFO CarbonSparkSqlParser: Parsing command: DROP TABLE IF EXISTS main18/06/15 03:30:45 INFO CarbonSession: ScalaTest-run Search service started, but don't support: DROP TABLE IF EXISTS main, and will run it with SparkSQL18/06/15 03:30:45 INFO CarbonLateDecodeRule: ScalaTest-run skip CarbonOptimizer
issueID:CARBONDATA-2615
type:Sub-task
changed files:processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactDataHandlerColumnar.java
texts:Support page size less than 32000 in CarbondataV3
Since we support super long string, if it is long enough, a column page with 32000 rows will exceed 2GB, so we support a page less than 32000 rows.
issueID:CARBONDATA-2616
type:Bug
changed files:datamap/bloom/src/main/java/org/apache/carbondata/datamap/bloom/BloomCoarseGrainDataMap.java
texts:Incorrect explain and query result while using bloomfilter datamap
1. create a bloomfilter datamap with 2 index columns;2. (explain) query on this table with index columns as filters3. The explain result shows skipping negative number of blocklets4. The query result is duplicated
issueID:CARBONDATA-2617
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/scan/executor/impl/AbstractQueryExecutor.java
core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
core/src/main/java/org/apache/carbondata/core/mutate/CarbonUpdateUtil.java
texts:Invalid tuple and block id getting formed for non partition table
While creating a partition table a segment file was written in the Metadata folder under table structure. This was introduced during development of partition table feature. At that time segment file was written only for partition table and it was used to distinguish between parition and non partition table in the code.                                         But later the code was modified to write the segment file for both parititon and non partition table and the code to distinguish partition and non partition table was not modified which is causing this incorrect formation of block and tuple id.
issueID:CARBONDATA-2618
type:Sub-task
changed files:core/src/main/java/org/apache/carbondata/core/datastore/compression/SnappyCompressor.java
processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactDataHandlerColumnar.java
processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactDataHandlerModel.java
texts:Split to multiple pages if varchar column page exceeds 2GB/snappy limits
Currently a column page has 2GB limitation in size. This limitation can be achieved if a column is quite long.To avoid it, in Carbondata-2615, we decrease the page size to reduce the data size in one column page. But it needs the user the configure the page size.Here I raise this PR to do it automatically. CarbonData will split into multiple pages if one column page exceed 2GB limitation.
issueID:CARBONDATA-262
type:Bug
changed files:integration/spark-common/src/main/java/org/apache/carbondata/spark/merger/CarbonCompactionExecutor.java
core/src/main/java/org/apache/carbondata/core/scan/executor/QueryExecutorFactory.java
core/src/main/java/org/apache/carbondata/core/scan/result/iterator/DetailQueryResultIterator.java
core/src/main/java/org/apache/carbondata/core/scan/executor/impl/QueryExecutorProperties.java
core/src/main/java/org/apache/carbondata/core/scan/model/QueryModel.java
hadoop/src/main/java/org/apache/carbondata/hadoop/CarbonRecordReader.java
core/src/main/java/org/apache/carbondata/core/scan/executor/impl/AbstractQueryExecutor.java
core/src/main/java/org/apache/carbondata/core/scan/executor/impl/DetailQueryExecutor.java
core/src/main/java/org/apache/carbondata/core/scan/executor/QueryExecutor.java
core/src/main/java/org/apache/carbondata/core/scan/result/iterator/AbstractDetailQueryResultIterator.java
texts:limit query memory and thread leak issue
Problem: In case of limit query if limit value is 100 so after consuming 100 records executor service is not getting shutdown, and it may cause memory issue Solution: Add executor service in query model need to shutdown the executor after query execution in carbonscan rdd
issueID:CARBONDATA-2621
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/util/CarbonProperties.java
texts:Lock problem in index datamap
The locking for the index Datamap is not correct.The HDFS lock will not work properly, because the lock is getting created the the local filesystem instead of HDFS. 
issueID:CARBONDATA-2623
type:Bug
changed files:
texts:Add DataMap Pre and Pevent listener

issueID:CARBONDATA-2624
type:Sub-task
changed files:
texts:Add validations for Create table command for complex dataType columns for Local Dictionary Support

issueID:CARBONDATA-2626
type:Bug
changed files:processing/src/main/java/org/apache/carbondata/processing/store/TablePage.java
texts:Logic for dictionary/nodictionary column pages in TablePage is wrong
To reproduce:Create table and load data to it. The table schema is: ```sql( s"""CREATE TABLE $bloomDMSampleTable(id INT, name STRING, city STRING, age INT,s1 STRING, s2 STRING, s3 STRING, s4 STRING, s5 STRING, s6 STRING, s7 STRING, s8 STRING)STORED BY 'carbondata' TBLPROPERTIES('table_blocksize'='128', 'dictionary_include'='id', 'sort_columns'='name')""".stripMargin)``` In tablespec, the first dimension spec is 'name' which is sort_columns but not dictionary. But Carbondata now treat it as dictionary page. (See TablePage Line94)Then for 'id' which is dictionary, carbondata treat it as no-dictionary page (see TablePage Line101) 
issueID:CARBONDATA-2627
type:Bug
changed files:
texts:remove dependecy of tech.allegro.schema.json2avro
currently tech.allegro.schema.json2avro is used for json to avro converter but it is not formally supported by AVRO and may feature does not work in converter like byte data type. Below code can be used instead of  def jsonToAvro( json:String, schemaStr:String) :GenericRecord= { var input :InputStream= null; var writer :DataFileWriter&#91;GenericRecord&#93; = null; var encoder :Encoder= null; var output :ByteArrayOutputStream= null; try { val schema = new org.apache.avro.Schema.Parser().parse(schemaStr); val reader = new GenericDatumReader[GenericRecord](schema); input = new ByteArrayInputStream(json.getBytes()); output = new ByteArrayOutputStream(); val din = new DataInputStream(input); writer = new DataFileWriter[GenericRecord](new GenericDatumWriter[GenericRecord]()); writer.create(schema, output); val decoder = DecoderFactory.get().jsonDecoder(schema, din); var datum :GenericRecord=null; datum = reader.read(null, decoder); return datum; } finally { try { input.close(); writer.close(); } catch { case e:Exception => { e.printStackTrace() } } } }
issueID:CARBONDATA-2629
type:Bug
changed files:examples/spark2/src/main/java/org/apache/carbondata/examples/sdk/SDKS3Example.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/CarbonTable.java
texts:SDK carbon reader don&#39;t support filter in HDFS and S3
SDK carbon reader don't support filter in HDFS and S3Code:       EqualToExpression equalToExpression = new EqualToExpression(            new ColumnExpression("name", DataTypes.STRING),            new LiteralExpression("robot1", DataTypes.STRING));        CarbonReader reader = CarbonReader                .builder(path, "_temp")                .projection(new String[]{"name", "age"})                .setAccessKey(args[0])                .setSecretKey(args[1])            .filter(equalToExpression)                .setEndPoint(args[2])                .build();Error:log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.Exception in thread "main" java.lang.RuntimeException: Carbon index file not exists. at org.apache.carbondata.core.metadata.schema.table.CarbonTable.buildTable(CarbonTable.java:249) at org.apache.carbondata.sdk.file.CarbonReaderBuilder.build(CarbonReaderBuilder.java:184) at org.apache.carbondata.examples.sdk.SDKS3Example.main(SDKS3Example.java:77)
issueID:CARBONDATA-263
type:Improvement
changed files:integration/spark-common/src/main/java/org/apache/carbondata/spark/load/CarbonLoaderUtil.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
texts:Configurable blocklet distribution
Problem: Currently blocklet distribution is not configurable , in some case it may impact query performanceSolution: Expose parameter to enable and disable blocklet distribution
issueID:CARBONDATA-2630
type:Improvement
changed files:
texts:Alter table set Table comment is throwing exception in spark-2.2 cluster
PreCondition : ]started spark-2.2 cluster and launched beelineTest step： 1. Create Table sample_comment5 (id int,dim1 string,name string,tech string,measure int,amount int,dim2 string,M1 int,dim3 string,M2 int,dim4 string,dim5 string,M3 int,dim6 string,dim7 string,M4 int,dim8 string,dim9 string,M5 int,dim10 string,dim11 string,dim12 string,M6 int,dim13 string,dim14 string,dim15 string,M7 int,dim16 string,dim17 string,dim18 string,dim19 string) CoMMent "@" STORED BY 'org.apache.carbondata.format';2. alter table sample_comment5 SET TBLPROPERTIES(comment="malathi");*Expected Output：*comment should have been updated*Actual Output：*Error: java.lang.RuntimeException: Alter table properties operation failed: org.apache.spark.sql.SparkSession cannot be cast to org.apache.spark.sql.CarbonSession (state=,code=0)
issueID:CARBONDATA-2632
type:Bug
changed files:
texts:BloomFilter DataMap Bugs and Optimization
This is an umbrella Jira for bloomfilter bugs
issueID:CARBONDATA-2633
type:Sub-task
changed files:processing/src/main/java/org/apache/carbondata/processing/loading/converter/impl/FieldEncoderFactory.java
datamap/bloom/src/main/java/org/apache/carbondata/datamap/bloom/BloomDataMapWriter.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockletDataMapFactory.java
datamap/examples/src/minmaxdatamap/main/java/org/apache/carbondata/datamap/examples/MinMaxIndexDataMapFactory.java
processing/src/main/java/org/apache/carbondata/processing/loading/AbstractDataLoadProcessorStep.java
datamap/bloom/src/main/java/org/apache/carbondata/datamap/bloom/DataConvertUtil.java
core/src/main/java/org/apache/carbondata/core/datamap/Segment.java
datamap/bloom/src/main/java/org/apache/carbondata/datamap/bloom/BloomCoarseGrainDataMap.java
processing/src/main/java/org/apache/carbondata/processing/datamap/DataMapWriterListener.java
processing/src/main/java/org/apache/carbondata/processing/loading/converter/impl/DirectDictionaryFieldConverterImpl.java
processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactDataHandlerModel.java
datamap/bloom/src/main/java/org/apache/carbondata/datamap/bloom/BloomCoarseGrainDataMapFactory.java
core/src/main/java/org/apache/carbondata/core/keygenerator/directdictionary/timestamp/TimeStampDirectDictionaryGenerator.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
core/src/main/java/org/apache/carbondata/core/datamap/dev/DataMapWriter.java
processing/src/main/java/org/apache/carbondata/processing/loading/converter/FieldConverter.java
datamap/bloom/src/main/java/org/apache/carbondata/datamap/bloom/BloomDataMapModel.java
core/src/main/java/org/apache/carbondata/core/keygenerator/directdictionary/timestamp/DateDirectDictionaryGenerator.java
core/src/main/java/org/apache/carbondata/core/datamap/dev/DataMapFactory.java
datamap/bloom/src/main/java/org/apache/carbondata/datamap/bloom/BloomDataMapBuilder.java
datamap/lucene/src/main/java/org/apache/carbondata/datamap/lucene/LuceneDataMapFactoryBase.java
processing/src/main/java/org/apache/carbondata/processing/loading/converter/impl/MeasureFieldConverterImpl.java
processing/src/main/java/org/apache/carbondata/processing/loading/converter/impl/ComplexFieldConverterImpl.java
processing/src/main/java/org/apache/carbondata/processing/loading/converter/impl/NonDictionaryFieldConverterImpl.java
texts:Bugs are found when bloomindex column is dictionary/sort/date column

issueID:CARBONDATA-2634
type:Sub-task
changed files:core/src/main/java/org/apache/carbondata/core/metadata/schema/datamap/DataMapProperty.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/TableSchema.java
texts:Provide more information about the datamap when showing datamaps
`Show datamap` should show the index_columns and other datamap properties
issueID:CARBONDATA-2635
type:Sub-task
changed files:core/src/main/java/org/apache/carbondata/core/datamap/DataMapProvider.java
texts:Support different provider based index datamaps on same column
It will be wasted to build bloom index on one column more than once
issueID:CARBONDATA-2637
type:Sub-task
changed files:core/src/main/java/org/apache/carbondata/core/scan/collector/impl/RowIdRawBasedResultCollector.java
datamap/bloom/src/main/java/org/apache/carbondata/datamap/bloom/BloomDataMapWriter.java
core/src/main/java/org/apache/carbondata/core/scan/collector/ResultCollectorFactory.java
core/src/main/java/org/apache/carbondata/core/datamap/dev/DataMapBuilder.java
datamap/bloom/src/main/java/org/apache/carbondata/datamap/bloom/AbstractBloomDataMapWriter.java
datamap/lucene/src/main/java/org/apache/carbondata/datamap/lucene/LuceneDataMapBuilder.java
datamap/bloom/src/main/java/org/apache/carbondata/datamap/bloom/BloomDataMapBuilder.java
texts:Fix bugs for deferred rebuild for bloomfilter datamap

issueID:CARBONDATA-2638
type:New Feature
changed files:
texts:Implement driver min max caching for specified columns and segregate block and blocklet cache
BackgroundCurrent implementation of Blocklet dataMap caching in driver is that it caches the min and max values of all the columns in schema by default. Problem Problem with this implementation is that as the number of loads increases the memory required to hold min and max values also increases considerably. We know that in most of the scenarios there is a single driver and memory configured for driver is less as compared to executor. With continuous increase in memory requirement driver can even go out of memory which makes the situation further worse.Solution1. Cache only the required columns in Driver2. Segregation of block and Blocklet level cache**For more details please check the attached document
issueID:CARBONDATA-2640
type:Sub-task
changed files:
texts:Support In Memory Locking mechanism

issueID:CARBONDATA-2641
type:Sub-task
changed files:
texts:Refactor code to create S3CarbonFile

issueID:CARBONDATA-2642
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/datastore/impl/DefaultFileTypeProvider.java
core/src/main/java/org/apache/carbondata/core/locks/HdfsFileLock.java
core/src/main/java/org/apache/carbondata/core/datamap/DataMapStoreManager.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/CarbonTable.java
core/src/main/java/org/apache/carbondata/core/writer/CarbonIndexFileMergeWriter.java
core/src/main/java/org/apache/carbondata/core/datastore/impl/FileTypeInterface.java
core/src/main/java/org/apache/carbondata/core/datastore/filesystem/AbstractDFSCarbonFile.java
core/src/main/java/org/apache/carbondata/core/datastore/filesystem/CarbonFile.java
store/sdk/src/main/java/org/apache/carbondata/sdk/file/CarbonReaderBuilder.java
core/src/main/java/org/apache/carbondata/core/datastore/impl/FileFactory.java
core/src/main/java/org/apache/carbondata/core/datastore/filesystem/LocalCarbonFile.java
datamap/bloom/src/main/java/org/apache/carbondata/datamap/bloom/BloomCoarseGrainDataMapFactory.java
core/src/main/java/org/apache/carbondata/core/metadata/SegmentFileStore.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
core/src/main/java/org/apache/carbondata/core/datamap/status/DiskBasedDataMapStatusProvider.java
core/src/main/java/org/apache/carbondata/core/locks/CarbonLockFactory.java
core/src/main/java/org/apache/carbondata/core/datastore/filesystem/AlluxioCarbonFile.java
core/src/main/java/org/apache/carbondata/core/datastore/filesystem/HDFSCarbonFile.java
core/src/main/java/org/apache/carbondata/core/datastore/filesystem/ViewFSCarbonFile.java
core/src/main/java/org/apache/carbondata/core/datastore/filesystem/S3CarbonFile.java
core/src/main/java/org/apache/carbondata/core/locks/LocalFileLock.java
texts:Introduce configurable Lock Path
Currently table level lock files are prepared in table path and system level locks are created inside store.When supporting S3, locking cannot be done is using S3 files as S3 support only  eventual consistency.So user can chose different HDFS location to configure locks, so that data can be on S3 and locks can be on HDFS.User also chose Zookeeper Lock if does not have HDFS location.So require to support a configuration path "carbon.lock.path" 
issueID:CARBONDATA-2644
type:Bug
changed files:processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/merger/UnsafeIntermediateMerger.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonLoadOptionConstants.java
core/src/main/java/org/apache/carbondata/core/util/CarbonProperties.java
texts:Validation not present for carbon.load.sortMemory.spill.percentage parameter
For the carbon.load.sortMemory.spill.percentage parameter the user inputs value outside the range of 0-100.
issueID:CARBONDATA-2645
type:Sub-task
changed files:core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockDataMap.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockletDataMapRowIndexes.java
core/src/main/java/org/apache/carbondata/core/util/BlockletDataMapUtil.java
core/src/main/java/org/apache/carbondata/core/indexstore/BlockletDataMapIndexWrapper.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockletDataMapFactory.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockletDataMap.java
core/src/main/java/org/apache/carbondata/core/indexstore/schema/SchemaGenerator.java
core/src/main/java/org/apache/carbondata/core/indexstore/BlockletDataMapIndexStore.java
core/src/main/java/org/apache/carbondata/core/indexstore/row/UnsafeDataMapRow.java
core/src/main/java/org/apache/carbondata/core/scan/executor/impl/AbstractQueryExecutor.java
core/src/main/java/org/apache/carbondata/core/datastore/block/TableBlockInfo.java
core/src/main/java/org/apache/carbondata/core/indexstore/BlockletDetailInfo.java
texts:Segregate block and blocklet cache
Separate block and blocklet cache using the cache level configuration
issueID:CARBONDATA-2646
type:Bug
changed files:processing/src/main/java/org/apache/carbondata/processing/loading/sort/impl/UnsafeParallelReadMergeSorterWithColumnRangeImpl.java
texts:While loading data into into a table with &#39;SORT_COLUMN_BOUNDS&#39; property, &#39;ERROR&#39; flag is displayed instead of &#39;WARN&#39; flag.
【Test step】：  1. Create a table with 'sort_column' tblproperty. 2. Load data into that table with the 'SORT_COLUMN_BOUNDS' property.Test queries: drop table if exists sortcolumnboundtable1; CREATE TABLE sortcolumnboundtable1 (ID Int, date Timestamp, country String, name String, phonetype String, serialname String, salary Int) STORED BY 'carbondata' tblproperties('sort_columns'='ID,name'); LOAD DATA INPATH '/user/prasanna/source_for_sort_column_bounds.csv' INTO TABLE sortcolumnboundtable1 OPTIONS('fileheader'='ID,date,country,name,phonetype,serialname,salary','sort_column_bounds'='10,aab1;20,aab1;100,aab1;1000,aab1','timestampformat'='yyyy/MM/dd'); 【check the log】  8/06/25 16:14:32 INFO loading.DataLoadExecutor: &#91;Executor task launch worker for task 0&#93;&#91;partitionID:sortcolumnboundtable1;queryID:2916995336294991&#93; Data Loading is started for table sortcolumnboundtable1 18/06/25 16:14:32 ERROR impl.UnsafeParallelReadMergeSorterWithColumnRangeImpl: &#91;Executor task launch worker for task 0&#93;&#91;partitionID:sortcolumnboundtable1;queryID:2916995336294991&#93; set temp location: /tmp/carbon2916995416352957_0/Fact/Part0/Segment_0/0/sortrowtmp 18/06/25 16:14:32 INFO memory.UnsafeSortMemoryManager: &#91;Executor task launch worker for task 0&#93;&#91;partitionID:sortcolumnboundtable1;queryID:2916995336294991&#93; It is not recommended to keep unsafe memory size less than 1024MB, so setting default value to 1024 18/06/25 16:14:32 INFO memory.UnsafeSortMemoryManager: &#91;Executor task launch worker for task 0&#93;&#91;partitionID:sortcolumnboundtable1;queryID:2916995336294991&#93; Sort Memory manager is created with size 1073741824 with org.apache.carbondata.core.memory.UnsafeMemoryAllocator@cd24739 18/06/25 16:14:32 INFO memory.UnsafeMemoryManager: &#91;Executor task launch worker for task 0&#93;&#91;partitionID:sortcolumnboundtable1;queryID:2916995336294991&#93; Working Memory manager is created with size 536870912 with org.apache.carbondata.core.memory.UnsafeMemoryAllocator@cd24739 18/06/25 16:14:32 ERROR impl.UnsafeParallelReadMergeSorterWithColumnRangeImpl: &#91;Executor task launch worker for task 0&#93;&#91;partitionID:sortcolumnboundtable1;queryID:2916995336294991&#93; set temp location: /tmp/carbon2916995416352957_0/Fact/Part0/Segment_0/0/sortrowtmp 18/06/25 16:14:32 ERROR impl.UnsafeParallelReadMergeSorterWithColumnRangeImpl: &#91;Executor task launch worker for task 0&#93;&#91;partitionID:sortcolumnboundtable1;queryID:2916995336294991&#93; set temp location: /tmp/carbon2916995416352957_0/Fact/Part0/Segment_0/0/sortrowtmp 18/06/25 16:14:32 ERROR impl.UnsafeParallelReadMergeSorterWithColumnRangeImpl: &#91;Executor task launch worker for task 0&#93;&#91;partitionID:sortcolumnboundtable1;queryID:2916995336294991&#93; set temp location: /tmp/carbon2916995416352957_0/Fact/Part0/Segment_0/0/sortrowtmp 18/06/25 16:14:32 ERROR impl.UnsafeParallelReadMergeSorterWithColumnRangeImpl: &#91;Executor task launch worker for task 0&#93;&#91;partitionID:sortcolumnboundtable1;queryID:2916995336294991&#93; set temp location: /tmp/carbon2916995416352957_0/Fact/Part0/Segment_0/0/sortrowtmp  
issueID:CARBONDATA-2647
type:Sub-task
changed files:core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/CarbonTable.java
texts:Add support for CACHE_LEVEL in create table and alter table properties
Add support for CACHE_LEVEL in create table and alter table properties
issueID:CARBONDATA-2648
type:Sub-task
changed files:core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
core/src/main/java/org/apache/carbondata/core/indexstore/TableBlockIndexUniqueIdentifierWrapper.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockletDataMap.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/CarbonTable.java
core/src/main/java/org/apache/carbondata/core/indexstore/row/DataMapRow.java
core/src/main/java/org/apache/carbondata/core/indexstore/BlockletDataMapIndexStore.java
core/src/main/java/org/apache/carbondata/core/datamap/DistributableDataMapFormat.java
texts:Add support for COLUMN_META_CACHE in create table and alter table properties
Add support for COLUMN_META_CACHE in create table and alter table properties
issueID:CARBONDATA-2649
type:Sub-task
changed files:core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RangeValueFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockletDataMapModel.java
core/src/main/java/org/apache/carbondata/core/util/BlockletDataMapUtil.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockletDataMap.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/CarbonTable.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/resolverinfo/MeasureColumnResolvedFilterInfo.java
core/src/main/java/org/apache/carbondata/core/indexstore/BlockletDataMapIndexStore.java
core/src/main/java/org/apache/carbondata/core/scan/executor/impl/AbstractQueryExecutor.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/resolverinfo/DimColumnResolvedFilterInfo.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/scan/executor/util/QueryUtil.java
core/src/main/java/org/apache/carbondata/core/indexstore/BlockletDetailInfo.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelRangeLessThanEqualFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelRangeGrtrThanEquaToFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/resolverinfo/ColumnResolvedFilterInfo.java
core/src/main/java/org/apache/carbondata/core/indexstore/schema/SchemaGenerator.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelRangeGrtThanFiterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/IncludeFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/scan/filter/FilterUtil.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockDataMap.java
core/src/main/java/org/apache/carbondata/core/datastore/block/SegmentPropertiesAndSchemaHolder.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelRangeLessThanFilterExecuterImpl.java
texts:Add code for caching min/max only for specified columns
Add code for caching min/max only for specified columns
issueID:CARBONDATA-265
type:Improvement
changed files:
texts:Improve Dataframe write to CarbonData file from CSV file
When loading using Dataframe, delete the temporary SUCCESS file instead of renaming all files
issueID:CARBONDATA-2650
type:Sub-task
changed files:hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonInputFormat.java
texts:explain query shows negative skipped blocklets for bloomfilter datamap
Currently in carbondata, blocklets are pruned by default blocklet datamap first and then will be pruned by other index datamap later.In the second step, the pruning is segment scope, it will not use the pruning output of the default blocklet datamap, so the result number of blocklets may be bigger than before, thus causing negative number in skipped blocklets.
issueID:CARBONDATA-2651
type:Sub-task
changed files:
texts:Update IDG for COLUMN_META_CACHE and CACHE_LEVEL properties
Update document for caching properties
issueID:CARBONDATA-2653
type:Sub-task
changed files:datamap/bloom/src/main/java/org/apache/carbondata/datamap/bloom/BloomDataMapWriter.java
datamap/bloom/src/main/java/org/apache/carbondata/datamap/bloom/BloomDataMapBuilder.java
texts:Fix bugs in incorrect blocklet number in bloomfilter
Incorrect blocklet number can be found during bloomfilter pruning.This is because bloomfilterwriter write a extra blocklet before it finish.
issueID:CARBONDATA-2654
type:Sub-task
changed files:core/src/main/java/org/apache/carbondata/core/datamap/dev/expr/AndDataMapExprWrapper.java
core/src/main/java/org/apache/carbondata/core/datamap/dev/expr/DataMapExprWrapper.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonInputFormat.java
core/src/main/java/org/apache/carbondata/core/profiler/ExplainCollector.java
core/src/main/java/org/apache/carbondata/core/datamap/dev/expr/DataMapWrapperSimpleInfo.java
core/src/main/java/org/apache/carbondata/core/datamap/dev/expr/OrDataMapExprWrapper.java
core/src/main/java/org/apache/carbondata/core/profiler/TablePruningInfo.java
core/src/main/java/org/apache/carbondata/core/datamap/dev/expr/DataMapExprWrapperImpl.java
texts:Optimize output for explaining query with datamap
Currently, If we have multiple datamaps and query hits all the datamaps. Carbondata explain command will only print the first datamap and all the other datamaps are not shown. We need to show all the datamap information used in the query.
issueID:CARBONDATA-2655
type:Sub-task
changed files:datamap/bloom/src/main/java/org/apache/carbondata/datamap/bloom/BloomCoarseGrainDataMap.java
datamap/bloom/src/main/java/org/apache/carbondata/datamap/bloom/BloomCoarseGrainDataMapFactory.java
texts:Support `in` operator for bloomfilter datamap
Currently bloomfilter datamap only support `equal` operator, it can further support `in` operator
issueID:CARBONDATA-2656
type:Improvement
changed files:integration/presto/src/main/java/org/apache/carbondata/presto/readers/BooleanStreamReader.java
integration/presto/src/main/java/org/apache/carbondata/presto/CarbondataPageSourceProvider.java
integration/presto/src/main/java/org/apache/carbondata/presto/PrestoCarbonVectorizedRecordReader.java
integration/presto/src/main/java/org/apache/carbondata/presto/CarbonVectorBatch.java
integration/presto/src/main/java/org/apache/carbondata/presto/readers/IntegerStreamReader.java
integration/presto/src/main/java/org/apache/carbondata/presto/readers/DecimalSliceStreamReader.java
integration/presto/src/main/java/org/apache/carbondata/presto/readers/DoubleStreamReader.java
integration/presto/src/main/java/org/apache/carbondata/presto/readers/LongStreamReader.java
integration/presto/src/main/java/org/apache/carbondata/presto/readers/ObjectStreamReader.java
integration/presto/src/main/java/org/apache/carbondata/presto/readers/ShortStreamReader.java
integration/presto/src/main/java/org/apache/carbondata/presto/readers/PrestoVectorBlockBuilder.java
integration/presto/src/main/java/org/apache/carbondata/presto/readers/SliceStreamReader.java
integration/presto/src/main/java/org/apache/carbondata/presto/readers/TimestampStreamReader.java
integration/presto/src/main/java/org/apache/carbondata/presto/CarbondataPageSource.java
texts:Presto Stream Readers performance Enhancement
Background:In the present system, we create carbonColumnVectorImpl object in carbonVectorbatch where carbon core fill up vector data (one by one) in column matched data type array, later which at the time presto block builder call, read by stream readers (based on data type) and iterated to fill up in block and returned to presto.Solution:We can eliminate the extra iteration over the carbonColumnVectorImpl object -> vectorArray, by extending it to create a directStreamReaders which will fill up carbon-core vector data (one by one) directly to the block(presto), and on the call of block builder it will return the block to the Presto.
issueID:CARBONDATA-2657
type:Sub-task
changed files:datamap/bloom/src/main/java/org/apache/carbondata/datamap/bloom/BloomCoarseGrainDataMap.java
datamap/bloom/src/main/java/org/apache/carbondata/datamap/bloom/DataConvertUtil.java
texts:Loading/Filtering empty value fails on bloom index columns
Steps to reproduce：1. create table t1(c1 string, c2 int) stored by 'carbondata';2. create datamap t1_bdm1 on table t1 using 'bloomfilter' dmproperties('index_columns'='c1');3.   Insert empty value on indexed columnsinsert into t1 select '',1,'xxx'; &#8212; This will cause failure4. besides, querying with empty value will cause failure: "key length must be > 0"
issueID:CARBONDATA-2658
type:Bug
changed files:processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/merger/UnsafeIntermediateMerger.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/UnsafeSortDataRows.java
texts:Fix bug in spilling in-memory pages
User configures the carbon.load.sortMemory.spill.percentage value and performs data load for local sort and batch sort. The data loaded is 30 million records. User checks the memory spilled to disk for different values(20%,50%,100%) of carbon.load.sortMemory.spill.percentage in the logs.No difference in memory spilled to disk for any value of carbon.load.sortMemory.spill.percentage 
issueID:CARBONDATA-2659
type:Improvement
changed files:
texts:Support partitioned carbon table by DataFrame.write
Currently only partition table is only supported by SQL, it should be supported by Spark DataFrame API also.
issueID:CARBONDATA-266
type:New Feature
changed files:
texts:support delete all carbon tables under one database
support delete all carbon tables under one database, but nor delete the other tables.
issueID:CARBONDATA-2660
type:Sub-task
changed files:
texts:Support filtering on longstring bloom index columns

issueID:CARBONDATA-2666
type:Bug
changed files:processing/src/main/java/org/apache/carbondata/processing/loading/converter/impl/FieldEncoderFactory.java
processing/src/main/java/org/apache/carbondata/processing/util/CarbonBadRecordUtil.java
core/src/main/java/org/apache/carbondata/core/util/path/CarbonTablePath.java
datamap/bloom/src/main/java/org/apache/carbondata/datamap/bloom/BloomCoarseGrainDataMap.java
processing/src/main/java/org/apache/carbondata/processing/loading/converter/impl/RowConverterImpl.java
processing/src/main/java/org/apache/carbondata/processing/loading/model/CarbonLoadModelBuilder.java
processing/src/main/java/org/apache/carbondata/processing/loading/model/CarbonLoadModel.java
processing/src/main/java/org/apache/carbondata/processing/loading/DataLoadProcessBuilder.java
processing/src/main/java/org/apache/carbondata/processing/loading/BadRecordsLoggerProvider.java
processing/src/main/java/org/apache/carbondata/processing/loading/CarbonDataLoadConfiguration.java
core/src/main/java/org/apache/carbondata/core/scan/filter/FilterUtil.java
core/src/main/java/org/apache/carbondata/core/scan/executor/util/QueryUtil.java
processing/src/main/java/org/apache/carbondata/processing/loading/model/LoadOption.java
core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonTableOutputFormat.java
texts:Rename command should not rename the table directory
Currently rename command updates the table name in metadata as well as the table path due to which table directory has to be renamed as well.This will cause problem in S3 because rename internally copies the content which can take a lot of time if the table size is huge.Solution is to change only the table name in metadata and keep the table name as old.
issueID:CARBONDATA-2669
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/datastore/columnar/BlockIndexerStorageForNoInvertedIndexForShort.java
core/src/main/java/org/apache/carbondata/core/localdictionary/generator/ColumnLocalDictionaryGenerator.java
processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactDataHandlerModel.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/dimension/legacy/DirectDictDimensionIndexCodec.java
processing/src/main/java/org/apache/carbondata/processing/loading/steps/CarbonRowDataWriterProcessorStepImpl.java
processing/src/main/java/org/apache/carbondata/processing/loading/steps/DataWriterProcessorStepImpl.java
core/src/main/java/org/apache/carbondata/core/localdictionary/PageLevelDictionary.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/dimension/legacy/DictDimensionIndexCodec.java
core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
core/src/main/java/org/apache/carbondata/core/datastore/columnar/BlockIndexerStorageForShort.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/dimension/legacy/HighCardDictDimensionIndexCodec.java
texts:Local Dictionary Store Size optimisation and other function issues
Local Dictionary Store Size optimisation and other function issues
issueID:CARBONDATA-267
type:New Feature
changed files:processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactDataHandlerModel.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/CarbonTable.java
processing/src/main/java/org/apache/carbondata/processing/store/writer/AbstractFactDataWriter.java
processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactDataHandlerColumnar.java
core/src/main/java/org/apache/carbondata/core/util/CarbonProperties.java
core/src/main/java/org/apache/carbondata/core/metadata/converter/ThriftWrapperSchemaConverterImpl.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/TableSchema.java
texts:Set block_size for table on table level
Set block_size for table on table level
issueID:CARBONDATA-2674
type:Bug
changed files:hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonTableInputFormat.java
texts:Streaming with merge index enabled does not consider the merge index file while pruning.

issueID:CARBONDATA-2675
type:Sub-task
changed files:
texts:Support config long_string_columns when create datamap

issueID:CARBONDATA-2676
type:Bug
changed files:store/sdk/src/main/java/org/apache/carbondata/sdk/file/CarbonWriterBuilder.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/TableSchemaBuilder.java
texts:Support local Dictionary for SDK Writer
Currently local dictionary is supported for managed table which is created using sql .We it should be supported for SDK Writer also.
issueID:CARBONDATA-2677
type:Improvement
changed files:
texts:support ZSTD compression for carbondata file

issueID:CARBONDATA-268
type:Bug
changed files:
texts:CarbonOptimizer has performance problem
1. invoke relation lookup many times 2. lookup list many times
issueID:CARBONDATA-2681
type:Sub-task
changed files:processing/src/main/java/org/apache/carbondata/processing/loading/sort/SortStepRowHandler.java
texts:Fix loading problem using global/batch sort fails when table has long string columns

issueID:CARBONDATA-2682
type:Sub-task
changed files:
texts:create table with long_string_columns property
1. create table with partition and no_inverted_index no long_string_columns should be blocked. eg: create table local_no_inverted_index0(id int,name string,description string,address string,note string) stored by 'carbondata' tblproperties('no_inverted_index'='note','long_string_columns'='note') ;create table local_partion(id int,name string,description string,address string) partition by(note string) stored by 'carbondata' tblproperties('long_string_columns'='note')2. create table with duplicate long_string_column should be blocked.eg:create table local_longstringtbl(id int,name string,description string,address string,note string) stored by 'carbondata' tblproperties('long_string_columns'='note,note') ;
issueID:CARBONDATA-2683
type:Sub-task
changed files:core/src/main/java/org/apache/carbondata/core/util/DataTypeUtil.java
texts:Fix data convertion problem for Varchar
ClassCastException occurs as below when we run update statement or select statement with 100+ columns on table with 107 columns .  java.lang.ClassCastException: java.lang.String cannot be cast to org.apache.spark.unsafe.types.UTF8String at org.apache.spark.sql.catalyst.expressions.BaseGenericInternalRow$class.getUTF8String(rows.scala:46) at org.apache.spark.sql.catalyst.expressions.GenericInternalRow.getUTF8String(rows.scala:194) at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply_22$(Unknown Source) at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source) at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)
issueID:CARBONDATA-2684
type:Bug
changed files:
texts:Code Generator Error is thrown when Select filter contains more than one count of distinct of ComplexColumn with group by Clause

issueID:CARBONDATA-2685
type:Sub-task
changed files:core/src/main/java/org/apache/carbondata/core/datamap/DataMapStoreManager.java
hadoop/src/main/java/org/apache/carbondata/hadoop/CarbonRecordReader.java
texts:make datamap rebuild for all segments in parallel
Currently in carbondata, while rebuilding datamap, one spark job will bestarted for each segment and all the jobs are executed serially. If wehave many historical segments, the rebuild will take a lot of time.
issueID:CARBONDATA-2686
type:Improvement
changed files:
texts:Implement left outer join in mv
User should able to create left outer join queries in MV and inner join queries can also query from same outer join MV datamap.
issueID:CARBONDATA-2687
type:Sub-task
changed files:
texts:update document for bloomfilter
Cache behavior for bloomfilter datmap has been changed, need to update the document as well
issueID:CARBONDATA-2689
type:Sub-task
changed files:core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
texts:Add test cases for alter statement for Local Dictionary Support and add validations for complex data Type columns

issueID:CARBONDATA-269
type:Improvement
changed files:
texts:Change delete segment parser because its may confused with hive grammar
the delete segment grammar in carbon is too similar with that delete grammar in hive,  while they have different usage. so change carbon's grammar to make carbon easy use.
issueID:CARBONDATA-2690
type:Sub-task
changed files:
texts:basic framework: use java to rewrite master and work

issueID:CARBONDATA-2691
type:Sub-task
changed files:
texts:add RESTful API

issueID:CARBONDATA-2692
type:Sub-task
changed files:
texts:add filter expression parser

issueID:CARBONDATA-2693
type:Sub-task
changed files:core/src/main/java/org/apache/carbondata/core/datamap/DataMapStoreManager.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/CarbonTable.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/DiskBasedDMSchemaStorageProvider.java
datamap/bloom/src/main/java/org/apache/carbondata/datamap/bloom/BloomCoarseGrainDataMapFactory.java
texts:Fix bug for alter rename is renameing the existing table on which bloomfilter datamp exists
【Detailed description】：Alter rename is renaming the existing table on which bloomfilter datamap exists【Test step】： 1. Create a carbon table 2. create bloomfilter data map on the table 3. rename the table using alterCREATE TABLE datamap_test_join1_sortintStr (id int,name string,salary float,dob date,doj timestamp,bonus double,status boolean,marks decimal(10,3))STORED BY 'carbondata' tblproperties('sortcolumns'='id');create datamap dm_datamap_test_join1_196_1_intstr on table datamap_test_join1_sortintStr using 'bloomfilter' DMPROPERTIES('INDEX_COLUMNS' = 'id,name', 'BLOOM_SIZE'='640000', 'BLOOM_FPP'='0.00001', 'BLOOM_COMPRESS'='true'); alter table datamap_test_join1_sortintStr rename to str1; 
issueID:CARBONDATA-2694
type:Sub-task
changed files:
texts:show long_string_columns in desc table command

issueID:CARBONDATA-2695
type:Sub-task
changed files:
texts:Block alter name/datatype of the long_string_columns

issueID:CARBONDATA-2696
type:Sub-task
changed files:
texts:Block alter table property of long_string_columns

issueID:CARBONDATA-2698
type:Sub-task
changed files:core/src/main/java/org/apache/carbondata/core/datamap/dev/DataMapFactory.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/CarbonTable.java
datamap/bloom/src/main/java/org/apache/carbondata/datamap/bloom/BloomCoarseGrainDataMapFactory.java
texts:Block alter datatype of bloom index columns

issueID:CARBONDATA-2699
type:Sub-task
changed files:
texts:Whether to support altering name for bloom index columns

issueID:CARBONDATA-27
type:Bug
changed files:
texts:filter expression to_date( productdate ) = &#39;2012-12-12&#39; not working
Scenario:CREATE TABLE comb04 (imei string,name string,age int,productdate timestamp,enddate timestamp,country string,city string,sale int,num double,gamePointId double,level decimal(10,3),score decimal(18,3)) STORED BY 'org.apache.carbondata.format' tblproperties('DICTIONARY_INCLUDE'='age');LOAD DATA LOCAL INPATH '$testData' INTO TABLE comb04 options ('DELIMITER'=',', 'QUOTECHAR'='\"')data:imei,name,age,productdate,enddate,country,city,sale,num,gamePointId,level,scoreimei0,name0,1,2016-05-02 05:45:49,2016-05-02 05:45:59,zhejiang,hangzhou,15895877,145,15.692,1025412.235,89imei1,name1,2,2016-05-03 05:15:49,2016-05-03 05:25:59,zhejiang,wenzhou,15895237,152,15.529,1025412.854,81imei1,name1,2,2016-05-03 05:15:49,2016-05-03 05:25:59,zhejiang,wenzhou,15529,152,15.529,15.529,81
issueID:CARBONDATA-270
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/scan/filter/FilterUtil.java
core/src/main/java/org/apache/carbondata/core/cache/dictionary/ColumnDictionaryInfo.java
core/src/main/java/org/apache/carbondata/core/util/DataTypeUtil.java
texts:[Filter Optimization] double data type value comparison optimization
EqualsToExpression evaluation for double values first check for the equality of nan values and then the double value comparison happens, since nan comparison scenarios are rare we can push the comparison of nan after the double value comparison.
issueID:CARBONDATA-2700
type:Sub-task
changed files:core/src/main/java/org/apache/carbondata/core/datamap/dev/DataMapFactory.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/CarbonTable.java
datamap/bloom/src/main/java/org/apache/carbondata/datamap/bloom/BloomCoarseGrainDataMapFactory.java
texts:Block dropping index columns for index datamap

issueID:CARBONDATA-2701
type:Sub-task
changed files:core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockletDataMapModel.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockletDataMapRowIndexes.java
core/src/main/java/org/apache/carbondata/core/util/BlockletDataMapUtil.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockletDataMapFactory.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockletDataMap.java
core/src/main/java/org/apache/carbondata/core/indexstore/UnsafeMemoryDMStore.java
core/src/main/java/org/apache/carbondata/core/indexstore/BlockletDataMapIndexStore.java
core/src/main/java/org/apache/carbondata/core/scan/executor/impl/AbstractQueryExecutor.java
core/src/main/java/org/apache/carbondata/core/indexstore/BlockletDetailInfo.java
core/src/main/java/org/apache/carbondata/core/indexstore/SafeMemoryDMStore.java
core/src/main/java/org/apache/carbondata/core/indexstore/row/DataMapRow.java
core/src/main/java/org/apache/carbondata/core/indexstore/AbstractMemoryDMStore.java
core/src/main/java/org/apache/carbondata/core/indexstore/schema/SchemaGenerator.java
core/src/main/java/org/apache/carbondata/core/indexstore/Blocklet.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockDataMap.java
core/src/main/java/org/apache/carbondata/core/datastore/block/SegmentPropertiesAndSchemaHolder.java
core/src/main/java/org/apache/carbondata/core/indexstore/ExtendedBlocklet.java
texts:Refactor code to store minimal required info in Block and Blocklet Cache
Refactor code to store minimal required info in Block and Blocklet Cache
issueID:CARBONDATA-2702
type:Sub-task
changed files:datamap/bloom/src/main/java/org/apache/carbondata/datamap/bloom/BloomCoarseGrainDataMapFactory.java
texts:Fix bugs in clear bloom datamap
In test output, sometimes there are error logs like:```org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 805.0 failed 1 times, most recent failure: Lost task 3.0 in stage 805.0 (TID 5751, localhost, executor driver): java.lang.NegativeArraySizeExceptionat java.util.AbstractCollection.toArray(AbstractCollection.java:136)at java.util.ArrayList.<init>(ArrayList.java:177)at org.apache.carbondata.datamap.bloom.BloomCoarseGrainDataMapFactory.clear(BloomCoarseGrainDataMapFactory.java:340)at org.apache.carbondata.core.datamap.TableDataMap.clear(TableDataMap.java:206)at org.apache.carbondata.core.datamap.DataMapStoreManager.clearDataMaps(DataMapStoreManager.java:430)at org.apache.carbondata.core.datamap.DistributableDataMapFormat$1.initialize(DistributableDataMapFormat.java:125)at org.apache.carbondata.spark.rdd.DataMapPruneRDD.internalCompute(SparkDataMapJob.scala:73)```This error is caused by concurrent clear for datamaps, can refer to BlockletDataMapFactory at issue 2496 (PR2324)
issueID:CARBONDATA-2703
type:Bug
changed files:
texts:Fix bugs in tests

issueID:CARBONDATA-2704
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/metadata/SegmentFileStore.java
core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
core/src/main/java/org/apache/carbondata/core/writer/CarbonIndexFileMergeWriter.java
texts:Index file size in describe formatted command is not updated correctly with the segment file

issueID:CARBONDATA-2705
type:Sub-task
changed files:
texts:CarbonStore Java API and implementation
Support two implementations:1. LocalCarbonStore for usage in local mode2. DistributedCarbonStore leveraging multiple server (Master and Workers) via RPC
issueID:CARBONDATA-2706
type:Sub-task
changed files:core/src/main/java/org/apache/carbondata/core/util/DeleteLoadFolders.java
datamap/bloom/src/main/java/org/apache/carbondata/datamap/bloom/BloomCoarseGrainDataMapFactory.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockletDataMapFactory.java
datamap/examples/src/minmaxdatamap/main/java/org/apache/carbondata/datamap/examples/MinMaxIndexDataMapFactory.java
core/src/main/java/org/apache/carbondata/core/datamap/dev/DataMapFactory.java
core/src/main/java/org/apache/carbondata/core/datamap/TableDataMap.java
datamap/lucene/src/main/java/org/apache/carbondata/datamap/lucene/LuceneDataMapFactoryBase.java
texts:clear bloom index file after segment is deleted

issueID:CARBONDATA-2708
type:Sub-task
changed files:datamap/bloom/src/main/java/org/apache/carbondata/datamap/bloom/BloomCoarseGrainDataMapFactory.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockletDataMapFactory.java
datamap/examples/src/minmaxdatamap/main/java/org/apache/carbondata/datamap/examples/MinMaxIndexDataMapFactory.java
core/src/main/java/org/apache/carbondata/core/datamap/dev/DataMapFactory.java
core/src/main/java/org/apache/carbondata/core/datamap/TableDataMap.java
datamap/lucene/src/main/java/org/apache/carbondata/datamap/lucene/LuceneDataMapFactoryBase.java
texts:clear index file if dataloading is failed

issueID:CARBONDATA-271
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/scan/filter/FilterUtil.java
texts:Non Filter data mismatch issue
Problem: While generating the default end key we are taking LONG.MAX key and using segment key generator we are generating the end key if cardinality is less than it will give some value with in its cardinality and btree searching will failSolution: From segment property get the dimension cardinality as this is the max value for segment
issueID:CARBONDATA-2710
type:Improvement
changed files:
texts:Refactor CarbonSparkSqlParser for better code reuse.

issueID:CARBONDATA-2711
type:Bug
changed files:integration/presto/src/main/java/org/apache/carbondata/presto/impl/CarbonTableReader.java
texts:carbonFileList is not initalized when updatetablelist call
when I execute this sql SELECT table_name FROM information_schema.tables WHERE table_schema ='tmp_sbu_vadmdb'exception like thisjava.lang.NullPointerException: undefinedatorg.apache.carbondata.presto.impl.CarbonTableReader.updateTableList(CarbonTableReader.java:208)atorg.apache.carbondata.presto.impl.CarbonTableReader.getTableNames(CarbonTableReader.java:197)atorg.apache.carbondata.presto.CarbondataMetadata.listTables(CarbondataMetadata.java:73)atcom.facebook.presto.spi.connector.classloader.ClassLoaderSafeConnectorMetadata.listTables(ClassLoadpr: https://github.com/apache/carbondata/pull/2468
issueID:CARBONDATA-2712
type:Sub-task
changed files:core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
texts:Add fix for Local Dictionary Exclude for multi level complex columns

issueID:CARBONDATA-2714
type:Sub-task
changed files:core/src/main/java/org/apache/carbondata/core/util/BlockletDataMapUtil.java
core/src/main/java/org/apache/carbondata/core/metadata/SegmentFileStore.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
core/src/main/java/org/apache/carbondata/core/writer/CarbonIndexFileMergeWriter.java
core/src/main/java/org/apache/carbondata/core/indexstore/BlockletDataMapIndexStore.java
processing/src/main/java/org/apache/carbondata/processing/merger/CompactionType.java
texts:Support merge index files for the segment
We already have discussed the merge index advantages in the community.  Please find the link below.http://apache-carbondata-dev-mailing-list-archive.1130556.n5.nabble.com/Discussion-Merging-carbonindex-files-for-each-segments-and-across-segments-td24441.htmlBut the feature is not completed and have some gaps like this feature is not supported for some of the features like pre-aggregate table, streaming table.In this JIRA, Merge index feature will be completed by supporting all the existing impacted features.
issueID:CARBONDATA-2715
type:Bug
changed files:
texts:Failed to run tests for Search Mode With Lucene in Windows env
The test for search mode with lucene datamap runs failed in windows env. Corresponding testcase is 'LuceneFineGrainDataMapWIthSearchModeSuite'.The reason is that the file separator is mismatched while con compare two pathes, thus result in empty pruned blocklets and the query result will be empty.
issueID:CARBONDATA-2716
type:Sub-task
changed files:processing/src/main/java/org/apache/carbondata/processing/store/writer/v3/CarbonFactDataWriterImplV3.java
processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactDataHandlerModel.java
processing/src/main/java/org/apache/carbondata/processing/datamap/DataMapWriterListener.java
texts:Add validate for datamap writer while loading data

issueID:CARBONDATA-2717
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/locks/CarbonLockUtil.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonFileInputFormat.java
core/src/main/java/org/apache/carbondata/core/locks/CarbonLockFactory.java
texts:Table id is empty when taking drop lock which causes failure

issueID:CARBONDATA-2718
type:Sub-task
changed files:
texts:Support SQL interface in REST API

issueID:CARBONDATA-2719
type:Sub-task
changed files:
texts:Table update/delete is needed block on table having datamaps
Table update/delete is allowed on Table having bloom filter but select query fails 
issueID:CARBONDATA-272
type:Bug
changed files:
texts:Two test case are failing , on second time maven build without  &#39;clean&#39;
Two test case are failing , during second time build without mvn cleaneg: > 1) run : mvn  -Pspark-1.6 -Dspark.version=1.6.2  install2) After successful build, again run mvn  -Pspark-1.6 -Dspark.version=1.6.2  install [31m*** 2 SUITES ABORTED *** [0m&#91;INFO&#93; ------------------------------------------------------------------------&#91;INFO&#93; Reactor Summary:&#91;INFO&#93; &#91;INFO&#93; Apache CarbonData :: Parent ........................ SUCCESS [ 11.412 s]&#91;INFO&#93; Apache CarbonData :: Common ........................ SUCCESS [  5.585 s]&#91;INFO&#93; Apache CarbonData :: Format ........................ SUCCESS [  7.079 s]&#91;INFO&#93; Apache CarbonData :: Core .......................... SUCCESS [ 15.874 s]&#91;INFO&#93; Apache CarbonData :: Processing .................... SUCCESS [ 12.417 s]&#91;INFO&#93; Apache CarbonData :: Hadoop ........................ SUCCESS [ 17.330 s]&#91;INFO&#93; Apache CarbonData :: Spark ......................... FAILURE &#91;07:47 min&#93;&#91;INFO&#93; Apache CarbonData :: Assembly ...................... SKIPPED&#91;INFO&#93; Apache CarbonData :: Examples ...................... SKIPPED&#91;INFO&#93; ------------------------------------------------------------------------&#91;INFO&#93; BUILD FAILURE&#91;INFO&#93; ------------------------------------------------------------------------Reason for failure is that two tables created by test cases AllDataTypesTestCaseAggregate and NO_DICTIONARY_COL_TestCase are not dropping tables properly.Refer error below error log [32m- skip auto identify high cardinality column for column group [0m [32mAllDataTypesTestCaseAggregate: [0mERROR 24-09 08:31:29,368 - Table alldatatypescubeAGG not found: default.alldatatypescubeAGG table not foundAUDIT 24-09 08:31:29,383 - &#91;vinod&#93;&#91;vinod&#93;&#91;Thread-1&#93;Creating Table with Database name &#91;default&#93; and Table name &#91;alldatatypestableagg&#93;AUDIT 24-09 08:31:29,385 - &#91;vinod&#93;&#91;vinod&#93;&#91;Thread-1&#93;Table creation with Database name &#91;default&#93; and Table name &#91;alldatatypestableagg&#93; failed. Table &#91;alldatatypestableagg&#93; already exists under database &#91;default&#93;ERROR 24-09 08:31:29,401 - Table Desc1 not found: default.Desc1 table not foundERROR 24-09 08:31:29,414 - Table Desc2 not found: default.Desc2 table not foundAUDIT 24-09 08:31:29,422 - &#91;vinod&#93;&#91;vinod&#93;&#91;Thread-1&#93;Creating Table with Database name &#91;default&#93; and Table name &#91;desc1&#93; [31mException encountered when invoking run on a nested suite - Table &#91;alldatatypestableagg&#93; already exists under database &#91;default&#93; *** ABORTED *** [0m [31m  java.lang.RuntimeException: Table &#91;alldatatypestableagg&#93; already exists under database &#91;default&#93; [0m [31m  at scala.sys.package$.error(package.scala:27) [0m [31m  at org.apache.spark.sql.execution.command.CreateTable.run(carbonTableSchema.scala:853) [0m [31m  at org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult$lzycompute(commands.scala:58) [0m [31m  at org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult(commands.scala:56) [0m [31m  at org.apache.spark.sql.execution.ExecutedCommand.doExecute(commands.scala:70) [0m [31m  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$5.apply(SparkPlan.scala:132) [0m [31m  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$5.apply(SparkPlan.scala:130) [0m [31m  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150) [0m [31m  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:130) [0m [31m  at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:55) [0m [31m  ... [0m[32mNO_DICTIONARY_COL_TestCase: [0mERROR 24-09 08:31:29,954 - Table filtertestTables not found: default.filtertestTables table not foundAUDIT 24-09 08:31:30,041 - &#91;vinod&#93;&#91;vinod&#93;&#91;Thread-1&#93;Deleting table &#91;no_dictionary_carbon_6&#93; under database &#91;default&#93;AUDIT 24-09 08:31:30,115 - &#91;vinod&#93;&#91;vinod&#93;&#91;Thread-1&#93;Deleted table &#91;no_dictionary_carbon_6&#93; under database &#91;default&#93;AUDIT 24-09 08:31:30,122 - &#91;vinod&#93;&#91;vinod&#93;&#91;Thread-1&#93;Deleting table &#91;no_dictionary_carbon_7&#93; under database &#91;default&#93;AUDIT 24-09 08:31:30,191 - &#91;vinod&#93;&#91;vinod&#93;&#91;Thread-1&#93;Deleted table &#91;no_dictionary_carbon_7&#93; under database &#91;default&#93;AUDIT 24-09 08:31:30,454 - &#91;vinod&#93;&#91;vinod&#93;&#91;Thread-1&#93;Creating Table with Database name &#91;default&#93; and Table name &#91;no_dictionary_carbon_6&#93;AUDIT 24-09 08:31:30,480 - &#91;vinod&#93;&#91;vinod&#93;&#91;Thread-1&#93;Table created with Database name &#91;default&#93; and Table name &#91;no_dictionary_carbon_6&#93;AUDIT 24-09 08:31:30,583 - &#91;vinod&#93;&#91;vinod&#93;&#91;Thread-1&#93;Data load request has been received for table default.no_dictionary_carbon_6AUDIT 24-09 08:31:30,665 - &#91;vinod&#93;&#91;vinod&#93;&#91;Thread-1&#93;Data load is successful for default.no_dictionary_carbon_6AUDIT 24-09 08:31:30,684 - &#91;vinod&#93;&#91;vinod&#93;&#91;Thread-1&#93;Creating Table with Database name &#91;default&#93; and Table name &#91;no_dictionary_carbon_7&#93;AUDIT 24-09 08:31:30,727 - &#91;vinod&#93;&#91;vinod&#93;&#91;Thread-1&#93;Table created with Database name &#91;default&#93; and Table name &#91;no_dictionary_carbon_7&#93;AUDIT 24-09 08:31:30,822 - &#91;vinod&#93;&#91;vinod&#93;&#91;Thread-1&#93;Data load request has been received for table default.no_dictionary_carbon_7AUDIT 24-09 08:31:31,077 - &#91;vinod&#93;&#91;vinod&#93;&#91;Thread-1&#93;Data load is successful for default.no_dictionary_carbon_7AUDIT 24-09 08:31:31,090 - &#91;vinod&#93;&#91;vinod&#93;&#91;Thread-1&#93;Creating Table with Database name &#91;default&#93; and Table name &#91;filtertesttable&#93;AUDIT 24-09 08:31:31,092 - &#91;vinod&#93;&#91;vinod&#93;&#91;Thread-1&#93;Table creation with Database name &#91;default&#93; and Table name &#91;filtertesttable&#93; failed. Table &#91;filtertesttable&#93; already exists under database &#91;default&#93; [31mException encountered when invoking run on a nested suite - Table &#91;filtertesttable&#93; already exists under database &#91;default&#93; *** ABORTED *** [0m [31m  java.lang.RuntimeException: Table &#91;filtertesttable&#93; already exists under database &#91;default&#93; [0m [31m  at scala.sys.package$.error(package.scala:27) [0m [31m  at org.apache.spark.sql.execution.command.CreateTable.run(carbonTableSchema.scala:853) [0m [31m  at org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult$lzycompute(commands.scala:58) [0m [31m  at org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult(commands.scala:56) [0m [31m  at org.apache.spark.sql.execution.ExecutedCommand.doExecute(commands.scala:70) [0m [31m  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$5.apply(SparkPlan.scala:132) [0m [31m  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$5.apply(SparkPlan.scala:130) [0m [31m  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150) [0m [31m  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:130) [0m [31m  at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:55) [0m [31m  ... [0m
issueID:CARBONDATA-2720
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/memory/MemoryBlock.java
core/src/main/java/org/apache/carbondata/core/scan/executor/infos/BlockExecutionInfo.java
core/src/main/java/org/apache/carbondata/core/scan/collector/ResultCollectorFactory.java
processing/src/main/java/org/apache/carbondata/processing/merger/CarbonCompactionUtil.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/merger/UnsafeIntermediateMerger.java
processing/src/main/java/org/apache/carbondata/processing/exception/DataLoadingException.java
core/src/main/java/org/apache/carbondata/events/OperationListenerBus.java
processing/src/main/java/org/apache/carbondata/processing/loading/steps/DataWriterProcessorStepImpl.java
core/src/main/java/org/apache/carbondata/core/cache/CacheType.java
processing/src/main/java/org/apache/carbondata/processing/exception/MultipleMatchingException.java
core/src/main/java/org/apache/carbondata/core/datastore/columnar/ColumnWithRowId.java
core/src/main/java/org/apache/carbondata/core/util/CarbonProperties.java
core/src/main/java/org/apache/carbondata/core/writer/CarbonDeleteDeltaWriterImpl.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
core/src/main/java/org/apache/carbondata/core/util/path/CarbonTablePath.java
core/src/main/java/org/apache/carbondata/core/locks/ZookeeperInit.java
core/src/main/java/org/apache/carbondata/core/scan/expression/logical/RangeExpression.java
core/src/main/java/org/apache/carbondata/core/datastore/block/SegmentProperties.java
core/src/main/java/org/apache/carbondata/core/scan/expression/exception/FilterUnsupportedException.java
processing/src/main/java/org/apache/carbondata/processing/store/CarbonDataFileAttributes.java
processing/src/main/java/org/apache/carbondata/processing/merger/CompactionResultSortProcessor.java
core/src/main/java/org/apache/carbondata/core/util/AbstractDataFileFooterConverter.java
common/src/main/java/org/apache/carbondata/common/logging/LogServiceFactory.java
core/src/main/java/org/apache/carbondata/core/datamap/dev/expr/DataMapDistributableWrapper.java
core/src/main/java/org/apache/carbondata/core/datastore/TableSegmentUniqueIdentifier.java
core/src/main/java/org/apache/carbondata/core/stats/QueryStatisticsModel.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RangeValueFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/scan/wrappers/ByteArrayWrapper.java
core/src/main/java/org/apache/carbondata/core/stats/DriverQueryStatisticsRecorderImpl.java
core/src/main/java/org/apache/carbondata/core/scan/expression/logical/FalseExpression.java
core/src/main/java/org/apache/carbondata/core/scan/scanner/impl/BlockletFullScanner.java
core/src/main/java/org/apache/carbondata/core/cache/dictionary/ColumnDictionaryInfo.java
core/src/main/java/org/apache/carbondata/core/mutate/CarbonUpdateUtil.java
core/src/main/java/org/apache/carbondata/core/readcommitter/ReadCommittedScope.java
core/src/main/java/org/apache/carbondata/core/scan/executor/impl/AbstractQueryExecutor.java
core/src/main/java/org/apache/carbondata/core/datastore/row/CarbonRow.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/column/ColumnSchema.java
processing/src/main/java/org/apache/carbondata/processing/loading/steps/InputProcessorStepImpl.java
core/src/main/java/org/apache/carbondata/core/keygenerator/mdkey/AbstractKeyGenerator.java
core/src/main/java/org/apache/carbondata/core/writer/ThriftWriter.java
processing/src/main/java/org/apache/carbondata/processing/loading/exception/BadRecordFoundException.java
processing/src/main/java/org/apache/carbondata/processing/loading/steps/SortProcessorStepImpl.java
core/src/main/java/org/apache/carbondata/core/keygenerator/KeyGenerator.java
core/src/main/java/org/apache/carbondata/core/datamap/DistributableDataMapFormat.java
processing/src/main/java/org/apache/carbondata/processing/exception/SliceMergerException.java
processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactDataHandlerModel.java
processing/src/main/java/org/apache/carbondata/processing/loading/steps/DataConverterProcessorStepImpl.java
processing/src/main/java/org/apache/carbondata/processing/loading/steps/JsonInputProcessorStepImpl.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/TableSchemaBuilder.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/dimension/legacy/IndexStorageEncoder.java
processing/src/main/java/org/apache/carbondata/processing/merger/CarbonDataMergerUtil.java
processing/src/main/java/org/apache/carbondata/processing/util/CarbonDataProcessorUtil.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/store/ColumnPageWrapper.java
core/src/main/java/org/apache/carbondata/core/scan/expression/exception/FilterIllegalMemberException.java
core/src/main/java/org/apache/carbondata/core/util/CarbonMetadataUtil.java
processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactHandlerFactory.java
core/src/main/java/org/apache/carbondata/core/exception/InvalidConfigurationException.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/DefaultEncodingFactory.java
core/src/main/java/org/apache/carbondata/core/datamap/DataMapProvider.java
core/src/main/java/org/apache/carbondata/core/util/TaskMetricsMap.java
core/src/main/java/org/apache/carbondata/core/scan/result/impl/NonFilterQueryScannedResult.java
processing/src/main/java/org/apache/carbondata/processing/loading/steps/CarbonRowDataWriterProcessorStepImpl.java
common/src/main/java/org/apache/carbondata/common/exceptions/sql/MalformedCarbonCommandException.java
processing/src/main/java/org/apache/carbondata/processing/loading/steps/InputProcessorStepWithNoConverterImpl.java
processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactDataHandlerColumnar.java
core/src/main/java/org/apache/carbondata/core/scan/executor/util/QueryUtil.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/DimensionColumnPage.java
integration/spark2/src/main/java/org/apache/carbondata/datamap/IndexDataMapProvider.java
core/src/main/java/org/apache/carbondata/core/scan/filter/optimizer/RangeFilterOptmizer.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/impl/ParallelReadMergeSorterWithColumnRangeImpl.java
core/src/main/java/org/apache/carbondata/core/scan/result/BlockletScannedResult.java
core/src/main/java/org/apache/carbondata/core/indexstore/schema/CarbonRowSchema.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/adaptive/AdaptiveFloatingCodec.java
core/src/main/java/org/apache/carbondata/core/datastore/TableSpec.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/impl/ParallelReadMergeSorterImpl.java
core/src/main/java/org/apache/carbondata/core/devapi/DictionaryGenerationException.java
core/src/main/java/org/apache/carbondata/core/memory/HeapMemoryAllocator.java
core/src/main/java/org/apache/carbondata/core/metadata/datatype/DecimalConverterFactory.java
core/src/main/java/org/apache/carbondata/core/scan/filter/FilterUtil.java
core/src/main/java/org/apache/carbondata/core/locks/CarbonLockUtil.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/impl/UnsafeParallelReadMergeSorterWithColumnRangeImpl.java
core/src/main/java/org/apache/carbondata/core/datastore/page/EncodedTablePage.java
core/src/main/java/org/apache/carbondata/core/util/DataFileFooterConverter2.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/impl/VariableLengthDimensionColumnPage.java
core/src/main/java/org/apache/carbondata/core/datastore/page/key/TablePageKey.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/column/CarbonColumn.java
processing/src/main/java/org/apache/carbondata/processing/loading/exception/NoRetryException.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/CarbonTable.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/impl/FixedLengthDimensionColumnPage.java
core/src/main/java/org/apache/carbondata/core/datastore/block/AbstractIndex.java
processing/src/main/java/org/apache/carbondata/processing/loading/AbstractDataLoadProcessorStep.java
core/src/main/java/org/apache/carbondata/core/keygenerator/directdictionary/DirectDictionaryKeyGeneratorFactory.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/resolverinfo/TrueConditionalResolverImpl.java
core/src/main/java/org/apache/carbondata/core/cache/CacheProvider.java
core/src/main/java/org/apache/carbondata/core/cache/dictionary/AbstractColumnDictionaryInfo.java
processing/src/main/java/org/apache/carbondata/processing/loading/events/LoadEvents.java
core/src/main/java/org/apache/carbondata/core/scan/expression/logical/TrueExpression.java
core/src/main/java/org/apache/carbondata/core/scan/executor/exception/QueryExecutionException.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/column/CarbonImplicitDimension.java
core/src/main/java/org/apache/carbondata/core/scan/result/impl/FilterQueryScannedResult.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
core/src/main/java/org/apache/carbondata/core/memory/MemoryLocation.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/column/CarbonDimension.java
core/src/main/java/org/apache/carbondata/core/constants/SortScopeOptions.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/impl/UnsafeParallelReadMergeSorterImpl.java
core/src/main/java/org/apache/carbondata/core/writer/CarbonDeleteDeltaWriter.java
core/src/main/java/org/apache/carbondata/core/metadata/converter/ThriftWrapperSchemaConverterImpl.java
texts:Remove dead code from carbonData
Due to enhancements and functionality changes, many dead code are left in CarbonData leading to unnecessary maintenance effort.Remove such methods and code which are not required any more.
issueID:CARBONDATA-2721
type:Bug
changed files:processing/src/main/java/org/apache/carbondata/processing/loading/parser/impl/JsonRowParser.java
texts:[SDK] [JsonWriter] NPE when schema and data are not of same length or Data is null.

issueID:CARBONDATA-2722
type:Bug
changed files:processing/src/main/java/org/apache/carbondata/processing/loading/parser/impl/JsonRowParser.java
texts:[SDK] [JsonWriter] Json writer is writing only first element of an array and discarding the rest of the elements

issueID:CARBONDATA-2723
type:Sub-task
changed files:processing/src/main/java/org/apache/carbondata/processing/datamap/DataMapWriterListener.java
core/src/main/java/org/apache/carbondata/core/datamap/DataMapStoreManager.java
processing/src/main/java/org/apache/carbondata/processing/store/writer/AbstractFactDataWriter.java
datamap/bloom/src/main/java/org/apache/carbondata/datamap/bloom/AbstractBloomDataMapWriter.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/DiskBasedDMSchemaStorageProvider.java
datamap/bloom/src/main/java/org/apache/carbondata/datamap/bloom/BloomDataMapBuilder.java
core/src/main/java/org/apache/carbondata/core/datamap/DataMapMeta.java
texts:Failed to recreate the table which has bloomfilter on it with same table name but different bloom index
Steps to reproduce:```create table xu_t11(id int,name string,sal float) stored by 'carbondata' tblproperties('sort_columns'='id')create datamap xu_dm_t1_new1 on table xu_t11 using 'bloomfilter' dmproperties('index_columns'='id')insert into xu_t11 select 1,'m',122.33// id.bloomindex is generateddrop table if exists xu_t11// recreate table with same datamap but with different index columncreate table xu_t11(id int,name string,sal float) stored by 'carbondata' tblproperties('sort_columns'='id')create datamap xu_dm_t1_new1 on table xu_t11 using 'bloomfilter' dmproperties('index_columns'='sal')insert into xu_t11 select 1,'m',122.33// Expecte sal.bloomindex to be generated, but actually id.bloomindex is generated.```This will not be reproduced in testcase, can only be generated in cluster (with multiple executors).
issueID:CARBONDATA-2724
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
texts:Unsupported create datamap on table with V1 or V2 format data
Unsupported create datamap on table with V1 or V2 format data
issueID:CARBONDATA-2727
type:Sub-task
changed files:datamap/bloom/src/main/java/org/apache/carbondata/datamap/bloom/BloomDataMapWriter.java
core/src/main/java/org/apache/carbondata/core/scan/collector/ResultCollectorFactory.java
datamap/bloom/src/main/java/org/apache/carbondata/datamap/bloom/BloomCoarseGrainDataMapFactory.java
core/src/main/java/org/apache/carbondata/core/scan/collector/impl/RowIdRestructureBasedRawResultCollector.java
datamap/bloom/src/main/java/org/apache/carbondata/datamap/bloom/BloomDataMapBuilder.java
datamap/bloom/src/main/java/org/apache/carbondata/datamap/bloom/AbstractBloomDataMapWriter.java
texts:Support create bloom datamap on newly added column

issueID:CARBONDATA-2729
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/metadata/schema/table/DataMapSchema.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/TableSchema.java
texts:Schema Compatibility problem between version 1.3.0 and 1.4.0
The table schema created with 1.3 is not compatible with 1.4.0.Steps:Do the below operation  on hive meta store.1.  Create table and load data in 1.3 version.   spark.sql("create database carbonversion_1_1");spark.sql("use carbonversion_1_1");spark.sql("""create table testIntType (c1 string,c2 int) STORED BY 'org.apache.carbondata.format'""")spark.sql("insert into testIntType select 'a',200")2. Switch to 1.4 version and create data map on the table created in step 1.spark.sql("use carbonversion_1_3");spark.sql("create datamap dm8 on table testinttype3 using 'preaggregate' as select c1, sum(c2) from testinttype3 group by c1") Stack trace:Exception in thread "main" java.lang.NullPointerExceptionat org.apache.carbondata.core.metadata.converter.ThriftWrapperSchemaConverterImpl.fromWrapperToExternalSchemaEvolution(ThriftWrapperSchemaConverterImpl.java:87)at org.apache.carbondata.core.metadata.converter.ThriftWrapperSchemaConverterImpl.fromWrapperToExternalTableSchema(ThriftWrapperSchemaConverterImpl.java:277)at org.apache.carbondata.core.metadata.converter.ThriftWrapperSchemaConverterImpl.fromWrapperToExternalTableInfo(ThriftWrapperSchemaConverterImpl.java:310)at org.apache.spark.sql.hive.CarbonHiveMetaStore.getThriftTableInfo(CarbonHiveMetaStore.scala:101)
issueID:CARBONDATA-273
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/util/ObjectSerializationUtil.java
texts:Some constants should be written using carbon common constants instead of direct values
such as,"UTF-8" for charset, 'default" for database
issueID:CARBONDATA-2730
type:Sub-task
changed files:
texts:Block create bloomfilter datamap index on local_dictionary column
Block create bloomfilter datamap index on local_dictionary column;
issueID:CARBONDATA-2732
type:Sub-task
changed files:core/src/main/java/org/apache/carbondata/core/datamap/dev/DataMapFactory.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/CarbonTable.java
datamap/bloom/src/main/java/org/apache/carbondata/datamap/bloom/BloomCoarseGrainDataMapFactory.java
texts:Block create bloomfilter datamap index on column which its datatype is complex type

issueID:CARBONDATA-2734
type:Sub-task
changed files:core/src/main/java/org/apache/carbondata/core/scan/executor/impl/AbstractQueryExecutor.java
core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
core/src/main/java/org/apache/carbondata/core/mutate/CarbonUpdateUtil.java
texts:[BUG] support struct of date in create table
Currently due to code issue. StringOutOfBound exception will be thrown
issueID:CARBONDATA-2735
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/datastore/page/SafeVarLengthColumnPage.java
core/src/main/java/org/apache/carbondata/core/datastore/page/UnsafeDecimalColumnPage.java
core/src/main/java/org/apache/carbondata/core/datastore/page/UnsafeVarLengthColumnPage.java
core/src/main/java/org/apache/carbondata/core/datastore/page/VarLengthColumnPageBase.java
core/src/main/java/org/apache/carbondata/core/datastore/page/ColumnPage.java
core/src/main/java/org/apache/carbondata/core/datastore/page/SafeFixLengthColumnPage.java
core/src/main/java/org/apache/carbondata/core/datastore/page/SafeDecimalColumnPage.java
core/src/main/java/org/apache/carbondata/core/datastore/page/UnsafeFixLengthColumnPage.java
texts:Performance issue for complex array data type when number of elements in array is more
ProblemQuery is taking more time when number of elements in array type is more.Root CauseThis is because offset of array values are stored in list and when number of elements are high creating a big list to store offset is taking more time Solution:Instead of list use column page to store offset so when number of elements are high there will not be any gc.
issueID:CARBONDATA-2738
type:Bug
changed files:
texts:Block Preaggregate, Dictionary Exclude/Include for child columns for Complex datatype

issueID:CARBONDATA-274
type:Improvement
changed files:
texts:Use exist method in CarbonMetastoreCatalog to read/write thrift TableInfo
Use exist method in CarbonMetastoreCatalog to read/write thrift TableInfo
issueID:CARBONDATA-2740
type:Bug
changed files:processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactDataHandlerModel.java
texts:flat folder structure is not handled for implicit column and segment file is not getting deleted after load is failed
flat folder structure is not handled for implicit column and segment file is not getting deleted after load is failed
issueID:CARBONDATA-2741
type:Bug
changed files:
texts:Exception occurs after alter add few columns and selecting in random order
create table tb1 (imei string,AMSize string,channelsId string,ActiveCountry string, Activecity string,gamePointId double,deviceInformationId double,productionDate Timestamp,deliveryDate timestamp,deliverycharge double) STORED BY 'org.apache.carbondata.format' TBLPROPERTIES('table_blocksize'='1','COLUMN_META_CACHE'='AMSize'); LOAD DATA INPATH 'hdfs://hacluster/csv/vardhandaterestruct.csv' INTO TABLE tb1 OPTIONS('DELIMITER'=',', 'QUOTECHAR'= '"','BAD_RECORDS_ACTION'='FORCE','FILEHEADER'= 'imei,deviceInformationId,AMSize,channelsId,ActiveCountry,Activecity,gamePointId,productionDate,deliveryDate,deliverycharge');alter table tb1 add columns(age int, name string);select * from tb1 where name is NULL or channelsId =4;Exception occurs :Error: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 6508.0 failed 4 times, most recent failure: Lost task 0.3 in stage 6508.0 (TID 140476, linux-49, executor 3): java.lang.RuntimeException: internal error: org.apache.carbondata.core.datastore.page.encoding.adaptive.AdaptiveFloatingCodec&#91;src type: DOUBLE, target type: INT, stats(min: 1.0, max: 1000000.0, decimal: 1 )&#93; at org.apache.carbondata.core.datastore.page.encoding.adaptive.AdaptiveFloatingCodec$3.decodeLong(AdaptiveFloatingCodec.java:185) at org.apache.carbondata.core.datastore.page.LazyColumnPage.getLong(LazyColumnPage.java:64) at org.apache.carbondata.core.scan.result.vector.MeasureDataVectorProcessor$IntegralMeasureVectorFiller.fillMeasureVector(MeasureDataVectorProcessor.java:73) at org.apache.carbondata.core.scan.result.impl.FilterQueryScannedResult.fillColumnarMeasureBatch(FilterQueryScannedResult.java:129) at org.apache.carbondata.core.scan.collector.impl.DictionaryBasedVectorResultCollector.fillResultToColumnarBatch(DictionaryBasedVectorResultCollector.java:167) at org.apache.carbondata.core.scan.collector.impl.RestructureBasedVectorResultCollector.collectResultInColumnarBatch(RestructureBasedVectorResultCollector.java:127) at org.apache.carbondata.core.scan.processor.DataBlockIterator.processNextBatch(DataBlockIterator.java:245) at org.apache.carbondata.core.scan.result.iterator.VectorDetailQueryResultIterator.processNextBatch(VectorDetailQueryResultIterator.java:48) at org.apache.carbondata.spark.vectorreader.VectorizedCarbonRecordReader.nextBatch(VectorizedCarbonRecordReader.java:290) at org.apache.carbondata.spark.vectorreader.VectorizedCarbonRecordReader.nextKeyValue(VectorizedCarbonRecordReader.java:180) at org.apache.carbondata.spark.rdd.CarbonScanRDD$$anon$1.hasNext(CarbonScanRDD.scala:497) at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.scan_nextBatch$(Unknown Source) at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source) at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43) at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:381) at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source) at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43) at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:381) at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:231) at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:225) at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:828) at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:828) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87) at org.apache.spark.scheduler.Task.run(Task.scala:99) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:325) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748)Driver stacktrace: (state=,code=0) 
issueID:CARBONDATA-2742
type:Bug
changed files:
texts:[MV] Wrong data displayed  after MV creation.
0: jdbc:hive2://10.18.16.173:23040/default> create table mytest_48 (rownumber int,name string, m1 int) stored by 'carbondata';---------+ Result  ---------+---------+No rows selected (1.267 seconds)0: jdbc:hive2://10.18.16.173:23040/default> load data inpath 'hdfs://hacluster/tmp/babu/testdata_1.csv' into table mytest_48 ;---------+ Result  ---------+---------–+ 0: jdbc:hive2://10.18.16.173:23040/default> show datamap  on table mytest_48;------------------------------------------------------------ DataMapName   ClassName   Associated Table   DataMap Properties  ------------------------------------------------------------------------------------------------------------------------No rows selected (0.162 seconds)0: jdbc:hive2://10.18.16.173:23040/default>0: jdbc:hive2://10.18.16.173:23040/default> select * from mytest_48;-------------------------+ rownumber   name      m1    -------------------------+ 1           aaa    1000      2           aaa    65000     3           aaa    1000000   1           ddd    1000      2           ddd    65000     3           ddd    1000000  -------------------------+6 rows selected (1.266 seconds)0: jdbc:hive2://10.18.16.173:23040/default> create datamap map9 using 'mv' as select sum(m1),name from mytest_48 group by name;---------+ Result  ---------+---------+No rows selected (0.82 seconds)0: jdbc:hive2://10.18.16.173:23040/default> select sum(m1),name from mytest_48 group by name;--------------- sum(m1)   name  ------------------------------No rows selected (2.615 seconds)0: jdbc:hive2://10.18.16.173:23040/default> explain select sum(m1),name from mytest_48 group by name;---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+                                                                                                                                                                                                                                                  plan                                                                                                                                                                                                                                                   ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ == CarbonData Profiler ==                                                                                                                                                                                                                                                                                                                                                                                                                                                                               == Physical Plan ==*HashAggregate(keys=mytest_48_name#297, functions=sum(sum_m1#296L))+- Exchange hashpartitioning(mytest_48_name#297, 200)   +- *HashAggregate(keys=mytest_48_name#297, functions=partial_sum(sum_m1#296L))      +- *BatchedScan CarbonDatasourceHadoopRelation [ Database name :babu, Table name :map9_table, Schema :Some(StructType(StructField(sum_m1,LongType,true), StructField(mytest_48_name,StringType,true))) ] babu.map9_tablesum_m1#296L,mytest_48_name#297  ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------–+  Data in CSVrownumber,name,m11,aaa,10002,aaa,650003,aaa,10000001,ddd,10002,ddd,650003,ddd,1000000
issueID:CARBONDATA-2745
type:Sub-task
changed files:core/src/main/java/org/apache/carbondata/core/reader/CarbonDeleteDeltaFileReaderImpl.java
core/src/main/java/org/apache/carbondata/core/fileoperations/AtomicFileOperationS3Impl.java
core/src/main/java/org/apache/carbondata/core/metadata/SegmentFileStore.java
core/src/main/java/org/apache/carbondata/core/fileoperations/AtomicFileOperationsImpl.java
core/src/main/java/org/apache/carbondata/core/statusmanager/SegmentStatusManager.java
core/src/main/java/org/apache/carbondata/core/datamap/status/DiskBasedDataMapStatusProvider.java
processing/src/main/java/org/apache/carbondata/processing/util/CarbonLoaderUtil.java
datamap/examples/src/minmaxdatamap/main/java/org/apache/carbondata/datamap/examples/MinMaxIndexDataMap.java
core/src/main/java/org/apache/carbondata/core/writer/ThriftWriter.java
core/src/main/java/org/apache/carbondata/core/fileoperations/AtomicFileOperationFactory.java
texts:Add a seperate Impl for AtomicFileOperations for s3
AtomicFileOperationImpl creates a temporary file and then renames the file to actual file name. This is risky in S3 storage as the file has to be deleted and then recreated.  S3 supports atomic file overwrite. hdfs rename is atomic, while overwrite is not atomic and can result in empty file read temporarily.So separate implementations for both hdfs and S3 to ensure consistancy of overwrite and read
issueID:CARBONDATA-2746
type:Sub-task
changed files:datamap/bloom/src/main/java/org/apache/carbondata/datamap/bloom/BloomCoarseGrainDataMapFactory.java
texts:Fix bug for getting datamap file when table has multiple datamaps

issueID:CARBONDATA-2747
type:Bug
changed files:datamap/lucene/src/main/java/org/apache/carbondata/datamap/lucene/LuceneDataMapFactoryBase.java
core/src/main/java/org/apache/carbondata/core/datamap/DataMapChooser.java
datamap/bloom/src/main/java/org/apache/carbondata/datamap/bloom/BloomCoarseGrainDataMapFactory.java
texts:Fix Lucene datamap choosing  and DataMapDistributable building
similar problem in bloom datamap is in issue CARBONDATA-2746; but test result is wrong  if we apply same fix Analysis:In `DataMapChooser#extractColumnExpression`, it does not deal with `MatchExpression`. This makes no information to use the column name to filter datamap. In `DataMapChooser#contains`, all datamap are marked as useful if lucene datamap is hit ( `ExpressionType.TEXT_MATCH`). Then the first datamap is chosen after sort step(sort by number of index column) .  In `LuceneDataMapFactoryBase#toDistributable`, carbon getAllIndexDirs and build DataMapDistributable for each index in same segment. This means that one segment will be applied `prune` by different index datamap(lucene use `indexPath` in `LuceneDataMapDistributable` to init its datamap object and build the `indexSearcherMap`) In out test case, we build datamaps  on columns:name and city, one for each.Query uses column `name` as filter. Unfortunately, in the `DataMapChooser`, it chooses datamap of city column.Then in `toDistributable` method, it gets all datamaps and build `LuceneDataMapDistributable`. Here in out test, it will prune and get result from each datamap.On datamap of city, query "name:c10"  in lucene return no row. On datamap of name, query "name:c10"  in lucene return actual what we want. So, if we apply same fix in CARBONDATA-2746 for lucene,  we will get only one datamap ( which is for city column) and prune result will be nothing. To Fix: choose correct datamap in DataMapChooser for lucene apply same fix in CARBONDATA-2746 to build correct `LuceneDataMapDistributable`
issueID:CARBONDATA-2749
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/fileoperations/AtomicFileOperationS3Impl.java
core/src/main/java/org/apache/carbondata/core/metadata/SegmentFileStore.java
core/src/main/java/org/apache/carbondata/core/fileoperations/AtomicFileOperationsImpl.java
core/src/main/java/org/apache/carbondata/core/statusmanager/SegmentStatusManager.java
core/src/main/java/org/apache/carbondata/core/fileoperations/AtomicFileOperations.java
core/src/main/java/org/apache/carbondata/core/datamap/status/DiskBasedDataMapStatusProvider.java
processing/src/main/java/org/apache/carbondata/processing/util/CarbonLoaderUtil.java
texts:In HDFS Empty tablestatus file is written during datalaod, iud or compaction when disk is full.
Issue: When a failure happens due to disk full during load, IUD or Compaction, then while updating the tablestatus file, the tablestaus.tmp file during atomic file operation remains empty, and in the finally block the empty tablestaus.tmp file is getting renamed to the actual file. This leads to empty tablestaus file.Problem: Once such problem happens the tablestatus file can not be retrieved and the already loaded data can be used.  
issueID:CARBONDATA-2750
type:Sub-task
changed files:
texts:Add Documention for Local Dictionary Support

issueID:CARBONDATA-2751
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/datastore/chunk/impl/DimensionRawColumnChunk.java
processing/src/main/java/org/apache/carbondata/processing/loading/steps/DataWriterProcessorStepImpl.java
processing/src/main/java/org/apache/carbondata/processing/loading/steps/InputProcessorStepImpl.java
core/src/main/java/org/apache/carbondata/core/datastore/page/VarLengthColumnPageBase.java
texts:Thread leak issue in data loading and Compatibility issue
Problem:*             Thread leak when user is killing data loading process from UI NPE when user is querying old store.                Solution*             When carbondata file writing is in progress during data loading and user is killing it from UI Producer and consumer thread are not getting shutdown. Need to handle the same in close method Old store (V1/V2) does not have datachunk3 object so while filling the local dictionary it is checking whether local dictionary is present in datachunk3 or not but datachunk3 null check is missing
issueID:CARBONDATA-2752
type:New Feature
changed files:
texts:Carbon provide Zeppelin support
Apache Zeppelin is a popular open web-based notebook that enables interactive data analytics. This is one of the favored solutions for providing UI frontend as it can support solutions like Spark already. Carbon can leverage this to provide a UI for its operations. After CARBONDATA-2688 which provides a carbon REST server, we can add a UI support from zeppelin to provide a complete solution.Reference: https://zeppelin.apache.org/Proposed solution: This JIRA propose to add a carbon based interpreter for Zeppelin.
issueID:CARBONDATA-2753
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/metadata/SegmentFileStore.java
core/src/main/java/org/apache/carbondata/core/scan/scanner/impl/BlockletFullScanner.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/SegmentIndexFileStore.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/CarbonTable.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/DataMapSchemaFactory.java
core/src/main/java/org/apache/carbondata/core/scan/filter/FilterUtil.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockletDataRefNode.java
core/src/main/java/org/apache/carbondata/core/metadata/AbsoluteTableIdentifier.java
texts:Fix Compatibility issues

issueID:CARBONDATA-2754
type:Improvement
changed files:store/sdk/src/main/java/org/apache/carbondata/store/CarbonStore.java
store/sdk/src/main/java/org/apache/carbondata/store/LocalCarbonStore.java
texts:fix failing UT for HiveMetastore

issueID:CARBONDATA-2755
type:Sub-task
changed files:processing/src/main/java/org/apache/carbondata/processing/loading/converter/impl/FieldEncoderFactory.java
processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactDataHandlerModel.java
core/src/main/java/org/apache/carbondata/core/scan/wrappers/ByteArrayWrapper.java
processing/src/main/java/org/apache/carbondata/processing/merger/CarbonCompactionUtil.java
processing/src/main/java/org/apache/carbondata/processing/util/CarbonDataProcessorUtil.java
processing/src/main/java/org/apache/carbondata/processing/datatypes/PrimitiveDataType.java
texts:Compaction of Complex DataType (STRUCT AND ARRAY)
Complex Type Enhancements - Compaction of Complex DataType
issueID:CARBONDATA-2756
type:Task
changed files:
texts:Add BSD license for ZSTD external dendency
BSD license is missing for ZSTD external dendency
issueID:CARBONDATA-2757
type:Sub-task
changed files:core/src/main/java/org/apache/carbondata/core/datastore/page/DecimalColumnPage.java
core/src/main/java/org/apache/carbondata/core/datastore/page/UnsafeDecimalColumnPage.java
core/src/main/java/org/apache/carbondata/core/datastore/page/ColumnPage.java
datamap/bloom/src/main/java/org/apache/carbondata/datamap/bloom/DataConvertUtil.java
datamap/bloom/src/main/java/org/apache/carbondata/datamap/bloom/AbstractBloomDataMapWriter.java
datamap/bloom/src/main/java/org/apache/carbondata/datamap/bloom/BloomCoarseGrainDataMap.java
core/src/main/java/org/apache/carbondata/core/datastore/page/SafeDecimalColumnPage.java
texts:Fix bug when building bloomfilter on measure column

issueID:CARBONDATA-2758
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/datastore/chunk/store/impl/LocalDictDimensionDataChunkStore.java
texts:selection on local dictionary fails when column having all null values more than default batch size.
ArrayIndexOutOfBound throws on following command.1. create table t1(s1 int,s2 string,s3 string) stored by 'carbondata' TBLPROPERTIES('SORT_SCOPE'='BATCH_SORT')2. load from a csv having all null values alteast 4097 rowsor insert into t1 select cast(null as int),cast(null as string),cast(null as string) 5000 times3. select * from t1;Error: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 6.0 failed 4 times, most recent failure: Lost task 0.3 in stage 6.0 (TID 207, BLR1000014267, executor 1): java.lang.ArrayIndexOutOfBoundsException: 4096  at org.apache.carbondata.spark.vectorreader.ColumnarVectorWrapper.putNull(ColumnarVectorWrapper.java:181)  at org.apache.carbondata.core.datastore.chunk.store.impl.LocalDictDimensionDataChunkStore.fillRow(LocalDictDimensionDataChunkStore.java:63)  at org.apache.carbondata.core.datastore.chunk.impl.VariableLengthDimensionColumnPage.fillVector(VariableLengthDimensionColumnPage.java:117)  at org.apache.carbondata.core.scan.result.BlockletScannedResult.fillColumnarNoDictionaryBatch(BlockletScannedResult.java:260)  at org.apache.carbondata.core.scan.collector.impl.DictionaryBasedVectorResultCollector.fillResultToColumnarBatch(DictionaryBasedVectorResultCollector.java:166)  at org.apache.carbondata.core.scan.collector.impl.DictionaryBasedVectorResultCollector.collectResultInColumnarBatch(DictionaryBasedVectorResultCollector.java:157)  at org.apache.carbondata.core.scan.processor.DataBlockIterator.processNextBatch(DataBlockIterator.java:245)  at org.apache.carbondata.core.scan.result.iterator.VectorDetailQueryResultIterator.processNextBatch(VectorDetailQueryResultIterator.java:48)  at org.apache.carbondata.spark.vectorreader.VectorizedCarbonRecordReader.nextBatch(VectorizedCarbonRecordReader.java:307)  at org.apache.carbondata.spark.vectorreader.VectorizedCarbonRecordReader.nextKeyValue(VectorizedCarbonRecordReader.java:182)  at org.apache.carbondata.spark.rdd.CarbonScanRDD$$anon$1.hasNext(CarbonScanRDD.scala:497)  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.scan_nextBatch$(Unknown Source)  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.agg_doAggregateWithKeys$(Unknown Source)  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)  at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)  at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:381)  at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)  at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:126)
issueID:CARBONDATA-2759
type:Improvement
changed files:
texts:Add Bad_Records_Options to STMPROPERTIES for Streaming Table

issueID:CARBONDATA-2760
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/datastore/page/ColumnPage.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
core/src/main/java/org/apache/carbondata/core/datastore/page/SafeFixLengthColumnPage.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/ColumnPageDecoder.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/reader/dimension/v3/DimensionChunkReaderV3.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/adaptive/AdaptiveDeltaFloatingCodec.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/adaptive/AdaptiveFloatingCodec.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/rle/RLECodec.java
core/src/main/java/org/apache/carbondata/core/localdictionary/PageLevelDictionary.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/IncludeFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/adaptive/AdaptiveIntegralCodec.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/compress/DirectCompressCodec.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/adaptive/AdaptiveDeltaIntegralCodec.java
core/src/main/java/org/apache/carbondata/core/datastore/page/UnsafeFixLengthColumnPage.java
texts:Reduce Memory footprint and store size for local dictionary encoded columns
Problem Local dictionary encoded page is using unsafevarlenghtcolumn column page which internally maintains offset of each value in another column page because of this memory footprint is high. for complex primitive string data type column page while compressing, it is converting to LV even if it is encoded with dictionary values, because of this store size is high.Solution: Use UnsafeFixedLength Column page for local dictionary encoded columns No need to convert to LV during query if local dictionary is present so use UnsafeFixLength Column page
issueID:CARBONDATA-2762
type:Bug
changed files:
texts:Long string column displayed as string in describe formatted
Steps :User creates a table with long string column and executes the describe formatted table command.0: jdbc:hive2://10.18.98.101:22550/default> create table t2(c1 string, c2 string) stored by 'carbondata' tblproperties('long_string_columns' = 'c2');---------+ Result ---------+---------+No rows selected (3.034 seconds)0: jdbc:hive2://10.18.98.101:22550/default> desc formatted t2;Actual Output : The describe formatted displays the c2 column as string instead of long string.0: jdbc:hive2://10.18.98.101:22550/default> desc formatted t2;-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ col_name  data_type  comment -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ c1  string  KEY COLUMN,null | c2 | string | KEY COLUMN,null |    ##Detailed Table Information    Database Name  default   Table Name  t2   CARBON Store Path  hdfs://hacluster/user/hive/warehouse/carbon.store/default/t2   Comment    Table Block Size  1024 MB   Table Data Size  0   Table Index Size  0   Last Update Time  0   SORT_SCOPE  LOCAL_SORT  LOCAL_SORT  CACHE_LEVEL  BLOCK   Streaming  false   Local Dictionary Enabled  true   Local Dictionary Threshold  10000   Local Dictionary Include  c1,c2      ##Detailed Column property    ADAPTIVE    SORT_COLUMNS  c1  -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+22 rows selected (2.847 seconds) Expected Output : The describe formatted should display the c2 column as long string.
issueID:CARBONDATA-2763
type:Bug
changed files:
texts:Create table with partition and no_inverted_index on long_string column is not blocked
Steps : Create table with partition using long_string column  CREATE TABLE local_no_inverted_index(id int, name string, description string,address string, note string) STORED BY 'org.apache.carbondata.format' tblproperties('no_inverted_index'='note','long_string_columns'='note'); 2. Create table with no_inverted_index   CREATE TABLE local1_partition(id int,name string, description string,address string)  partitioned by (note string) STORED BY 'org.apache.carbondata.format' tblproperties('long_string_columns'='note'); Actual Output : The Create table with partition and no_inverted_index on long_string column is successful.0: jdbc:hive2://10.18.98.101:22550/default> CREATE TABLE local_no_inverted_index(id int, name string, description string,address string, note string) STORED BY 'org.apache.carbondata.format' tblproperties('no_inverted_index'='note','long_string_columns'='note');---------+ Result ---------+---------+No rows selected (2.604 seconds)0: jdbc:hive2://10.18.98.101:22550/default> CREATE TABLE local1_partition(id int,name string, description string,address string) partitioned by (note string) STORED BY 'org.apache.carbondata.format' tblproperties('long_string_columns'='note');---------+ Result ---------+---------+No rows selected (1.989 seconds)Expected Output - The Create table with partition and no_inverted_index on long_string column should be blocked. 
issueID:CARBONDATA-2767
type:Bug
changed files:
texts:Query take more than 5 seconds for RACK_LOCAL
If the Spark cluster and the Hadoop cluster are two different machine cluster, the Spark tasks will run in RACK_LOCAL mode. So no need to provide the preferred locations to the task.
issueID:CARBONDATA-2768
type:Sub-task
changed files:
texts:Fix error test for external format

issueID:CARBONDATA-2769
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/util/path/CarbonTablePath.java
texts:Fix bug when getting shard name from data before version 1.4
bug is found when we query on old data(version 1.3) after building bloom filter datamap on it
issueID:CARBONDATA-277
type:Bug
changed files:
texts:tblstatus file modification
When the tblstatus file is edited manually assuming a segment is loaded even though it is not, for that non-existing segment the "SHOW SEGMENTS FOR TABLE " and "DELETE SEGMENT FROM TABLE" commands are behaving normally.
issueID:CARBONDATA-2770
type:Sub-task
changed files:core/src/main/java/org/apache/carbondata/core/scan/collector/impl/RowIdRawBasedResultCollector.java
core/src/main/java/org/apache/carbondata/core/scan/collector/impl/RowIdRestructureBasedRawResultCollector.java
texts:Optimize code to get blocklet id when rebuilding datamap

issueID:CARBONDATA-2771
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/statusmanager/SegmentStatusManager.java
texts:Block update and delete if compaction is in progress
Block update and delete if compaction is in progress, as it may leads to data mismatch
issueID:CARBONDATA-2772
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/localdictionary/generator/ColumnLocalDictionaryGenerator.java
core/src/main/java/org/apache/carbondata/core/localdictionary/dictionaryholder/MapBasedDictionaryStore.java
texts:Size based dictionary fallback is failing even threshold is not reached.
create table and load data ~2 Billion Check fallback logs. for some column fallback happens with below message even  threshold is not reached."Unable to generate dictionary. Dictionary Size crossed 2GB limit"
issueID:CARBONDATA-2774
type:Sub-task
changed files:datamap/bloom/src/main/java/org/apache/carbondata/datamap/bloom/BloomCoarseGrainDataMap.java
texts:Exception should be thrown if expression do not satisfy bloomFilter&#39;s requirement

issueID:CARBONDATA-2775
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/datastore/page/UnsafeFixLengthColumnPage.java
texts:Adaptive encoding fails for Unsafe OnHeap if, target data type is SHORT_INT

issueID:CARBONDATA-2777
type:Bug
changed files:hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonTableInputFormat.java
texts:NonTransactional tables, Select count(*) is not giving latest results for incremental load with same segment ID (UUID)

issueID:CARBONDATA-2778
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/mutate/CarbonUpdateUtil.java
texts:Empty result in query after IUD delete operation
drop table if exists t1 create table t1 (c1 int,c2 string) STORED BY 'org.apache.carbondata.format' TBLPROPERTIES('table_blocksize'='1', 'dictionary_exclude'='c2') LOAD DATA LOCAL INPATH 'test.csv' INTO table t1 options('fileheader'='c1,c2') run delete command which should delete a whole block Run clean file operation. select from t1. NOTE: Disable mergeindex property
issueID:CARBONDATA-2779
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/IndexWrapper.java
core/src/main/java/org/apache/carbondata/core/scan/executor/impl/AbstractQueryExecutor.java
texts:Filter query is failing for store created with V1/V2 format
Filter query is failing for store created with V1/V2 format with Arrayindexoutofbound exception
issueID:CARBONDATA-278
type:New Feature
changed files:core/src/main/java/org/apache/carbondata/core/scan/filter/FilterExpressionProcessor.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/ConditionalFilterResolverImpl.java
core/src/main/java/org/apache/carbondata/core/scan/expression/conditional/BinaryConditionalExpression.java
core/src/main/java/org/apache/carbondata/core/scan/expression/conditional/EqualToExpression.java
texts:IS NULL and IS NOT NULL shall be push down to carbon
IS NULL and IS NOT NULL shall be push down to carbon layer since carbon layer can process these filters faster using block/block-let pruning , also while processing filters in executers  carbon is applying binary search for applying filter values.
issueID:CARBONDATA-2780
type:Bug
changed files:
texts:Load one file for multiple times in one load command cause wrong query result
SparkSQL now support load multiple files in one load command. The file path can be comma separated.But when I try to load one file for multiple times in one load command, the query result is wrong.The load command looks like below:```LOAD DATA LOCAL INPATH 'file1,file1,file1' INTO TABLE test_table;```The expected result should be the triple of the file content, but actually the result is exactly the file content, not tripled.I'm wondering if this is intended or a bug.
issueID:CARBONDATA-2781
type:Bug
changed files:
texts:Add fix for Null Pointer Exception when Pre-aggregate create command is killed from UI

issueID:CARBONDATA-2782
type:Improvement
changed files:
texts:dead code in class &#39;CarbonCleanFilesCommand&#39;
class:CarbonCleanFilesCommand  dead code:override def processMetadata(sparkSession: SparkSession): Seq&#91;Row&#93; = { carbonTable = CarbonEnv.getCarbonTable(databaseNameOp, tableName.get)(sparkSession)val dms = carbonTable.getTableInfo.getDataMapSchemaList.asScala.map(_.getDataMapName)  val indexDms = DataMapStoreManager.getInstance.getAllDataMap(carbonTable).asScala  .filter(_.getDataMapSchema.isIndexDataMap)...}the variables(dms、indexDms) are nerver used.
issueID:CARBONDATA-2783
type:Sub-task
changed files:
texts:Update document of bloom filter datamap

issueID:CARBONDATA-2784
type:Bug
changed files:processing/src/main/java/org/apache/carbondata/processing/loading/iterator/CarbonOutputIteratorWrapper.java
core/src/main/java/org/apache/carbondata/core/util/CarbonProperties.java
texts:[SDK writer] Forever blocking wait with more than 20 batch of data, when consumer is dead due to data loading exception
problem:&#91;SDK writer&#93; Forever blocking wait with more than 21 batch of data, when consumer is dead due to data loading exception (bad record / out of memory) root cause:When the consumer is dead due to data loading exception, writer will be forcefully closed. but queue.clear() cleared only snapshot of entries (10 batches) and close is set to true after that. In between clear() and close = true, If more than 10 batches of data is again put into queue. For 11th batch, queue.put() goes for forever block as consumer is dead. Solution:set close = true, before clearing the queue. This will avoid adding more batches to queue from write(). 
issueID:CARBONDATA-2788
type:Sub-task
changed files:core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockDataMap.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockletDataMapFactory.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonInputFormat.java
texts:Fix bugs in incorrect query result with bloom datamap
revert modification in PR2539
issueID:CARBONDATA-2789
type:Task
changed files:
texts:Support Hadoop 2.8.3 eco-system integration

issueID:CARBONDATA-279
type:New Feature
changed files:integration/spark-common/src/main/java/org/apache/carbondata/spark/load/CarbonLoaderUtil.java
processing/src/main/java/org/apache/carbondata/processing/model/CarbonLoadModel.java
texts:[DataLoading]Save a DataFrame to CarbonData file without writing CSV file
Directly save a DataFrame to CarbonData file without writing CSV file
issueID:CARBONDATA-2790
type:Sub-task
changed files:datamap/bloom/src/main/java/org/apache/carbondata/datamap/bloom/BloomCoarseGrainDataMapFactory.java
texts:Optimize default parameter for bloomfilter datamap
Optimize default parameter for bloomfilter datamap to provide better query performance by default
issueID:CARBONDATA-2791
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/DefaultEncodingFactory.java
texts:Fix Adaptive Encoding for Double if exceeds LONG.Max_value

issueID:CARBONDATA-2792
type:Bug
changed files:
texts:Create external table fails post schema restructure
Steps to reproduce.sql("create table rstest1 (c1 string,c2 int) STORED BY 'org.apache.carbondata.format'")sql("Alter table rstest1 drop columns(c2)")sql( "Alter table rstest1 add columns(c4 string) TBLPROPERTIES('DICTIONARY_EXCLUDE'='c4', " + "'DEFAULT.VALUE.c4'='def')")sql(s"""CREATE EXTERNAL TABLE rsext STORED BY 'carbondata' LOCATION '$storeLocation/rstest1'""")
issueID:CARBONDATA-2793
type:Sub-task
changed files:
texts:Add document for 32k feature

issueID:CARBONDATA-2795
type:Bug
changed files:
texts:Add documentation on the usage of S3 as carbon store

issueID:CARBONDATA-2796
type:Sub-task
changed files:processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/TableFieldStat.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/SortStepRowHandler.java
processing/src/main/java/org/apache/carbondata/processing/loading/row/IntermediateSortTempRow.java
processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactDataHandlerModel.java
texts:Fix data loading problem when table has  complex column and long string column
currently both varchar column and complex column believes itself is the last one member in noDictionary group when converting carbon row from raw format to 3-parted format. Since they need to be proceeded in different way, exception will occur if we deal the column in wrong way.To fix this, we marked the info of complex columns explicitly like varchar columns, and keep the order of noDictionary group as : normal Dim & varchar & complex
issueID:CARBONDATA-2798
type:Bug
changed files:processing/src/main/java/org/apache/carbondata/processing/datatypes/StructDataType.java
processing/src/main/java/org/apache/carbondata/processing/datatypes/ArrayDataType.java
processing/src/main/java/org/apache/carbondata/processing/loading/converter/impl/FieldEncoderFactory.java
processing/src/main/java/org/apache/carbondata/processing/loading/model/LoadOption.java
texts:Fix Dictionary_Include for ComplexDataType

issueID:CARBONDATA-2799
type:Bug
changed files:datamap/bloom/src/main/java/org/apache/carbondata/datamap/bloom/BloomCoarseGrainDataMap.java
texts:Query failed with bloom datamap on preagg table with dictionary column
Steps to reproduce:CREATE TABLE datamap_test (id int,name string,salary float,dob date)STORED BY 'carbondata' TBLPROPERTIES('dictionary_include'='id');LOAD DATA INPATH 'hdfs://hacluster/user/surbhi/datamap_test.csv' into table datamap_test OPTIONS('DELIMITER'=',', 'QUOTECHAR'='"','BAD_RECORDS_ACTION'='FORCE','FILEHEADER'='id,name,salary,dob');LOAD DATA INPATH 'hdfs://hacluster/user/surbhi/datamap_test.csv' into table datamap_test OPTIONS('DELIMITER'=',', 'QUOTECHAR'='"','BAD_RECORDS_ACTION'='FORCE','FILEHEADER'='id,name,salary,dob');LOAD DATA INPATH 'hdfs://hacluster/user/surbhi/datamap_test.csv' into table datamap_test OPTIONS('DELIMITER'=',', 'QUOTECHAR'='"','BAD_RECORDS_ACTION'='FORCE','FILEHEADER'='id,name,salary,dob');CREATE DATAMAP dm_datamap_test2 ON TABLE datamap_test USING 'bloomfilter' DMPROPERTIES ('INDEX_COLUMNS' = 'id', 'BLOOM_SIZE'='320000', 'BLOOM_FPP'='0.01', 'BLOOM_COMPRESS'='true');LOAD DATA INPATH 'hdfs://hacluster/user/surbhi/datamap_test.csv' into table datamap_test OPTIONS('DELIMITER'=',', 'QUOTECHAR'='"','BAD_RECORDS_ACTION'='FORCE','FILEHEADER'='id,name,salary,dob');create datamap datamap_preaggr ON TABLE datamap_test USING "preaggregate" as select id,count(id) from datamap_test group by id;create datamap dm_preag_bloom_cust_id on table datamap_test_datamap_preaggr using 'bloomfilter' dmproperties('index_columns'='datamap_test_id');select id,count(id) from datamap_test where id = 12 group by id;QUERY FAILED：Error: org.apache.spark.sql.catalyst.errors.package$TreeNodeException: execute, tree: drop datamap dm_preag_bloom_cust_id on table datamap_test_datamap_preaggr;select id,count(id) from datamap_test where id = 12 group by id; QUERY OKcreate datamap dm_preag_bloom_cust_id on table datamap_test_datamap_preaggr using 'bloomfilter' dmproperties('index_columns'='datamap_test_id');drop datamap dm_preag_bloom_cust_id on table datamap_test_datamap_preaggr;select id,count(id) from datamap_test where id = 12 group by id;QUERY FAILED：Error: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 117.0 failed 4 times
issueID:CARBONDATA-28
type:Bug
changed files:
texts:CarbonContext create table exception
Hi all, I am using Spark 1.6.1, Hadoop 2.2.0 and fork the latest code from  master, but when I run code according to Quick-Start, I met the following exception:scala> cc.sql("create table if not exists table1 (id string, name string, city string, age Int) STORED BY 'org.apache.carbondata.format'")INFO  01-07 10:39:16,043 - main Property file path: /home/iteblog/spark-1.6.1-bin-2.2.0/../../../conf/carbon.propertiesINFO  01-07 10:39:16,043 - main ------Using Carbon.properties --------INFO  01-07 10:39:16,043 - main {}INFO  01-07 10:39:16,043 - main Query [CREATE TABLE IF NOT EXISTS TABLE1 (ID STRING, NAME STRING, CITY STRING, AGE INT) STORED BY 'ORG.APACHE.CARBONDATA.FORMAT']INFO  01-07 10:39:16,327 - Parsing command: create table if not exists table1 (id string, name string, city string, age Int) STORED BY 'org.apache.carbondata.format'INFO  01-07 10:39:17,091 - Parse Completedjava.lang.AbstractMethodError: org.apache.spark.sql.CarbonContext$$anon$1.org$apache$spark$sql$catalyst$analysis$OverrideCatalog$_setter_$org$apache$spark$sql$catalyst$analysis$OverrideCatalog$$overrides_$eq(Ljava/util/concurrent/ConcurrentHashMap;)V at org.apache.spark.sql.catalyst.analysis.OverrideCatalog$class.$init$(Catalog.scala:132) at org.apache.spark.sql.CarbonContext$$anon$1.<init>(CarbonContext.scala:46) at org.apache.spark.sql.CarbonContext.catalog$lzycompute(CarbonContext.scala:46) at org.apache.spark.sql.CarbonContext.catalog(CarbonContext.scala:44) at org.apache.spark.sql.CarbonContext.analyzer$lzycompute(CarbonContext.scala:50) at org.apache.spark.sql.CarbonContext.analyzer(CarbonContext.scala:50) at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:34) at org.apache.spark.sql.DataFrame.<init>(DataFrame.scala:133) at org.carbondata.spark.rdd.CarbonDataFrameRDD.<init>(CarbonDataFrameRDD.scala:23) at org.apache.spark.sql.CarbonContext.sql(CarbonContext.scala:75) at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:33) at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:38) at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:40) at $iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:42) at $iwC$$iwC$$iwC$$iwC.<init>(<console>:44) at $iwC$$iwC$$iwC.<init>(<console>:46) at $iwC$$iwC.<init>(<console>:48) at $iwC.<init>(<console>:50) at <init>(<console>:52) at .<init>(<console>:56) at .<clinit>(<console>) at .<init>(<console>:7) at .<clinit>(<console>) at $print(<console>) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:606) at org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:1065) at org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1346) at org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:840) at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:871) at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:819) at org.apache.spark.repl.SparkILoop.reallyInterpret$1(SparkILoop.scala:857) at org.apache.spark.repl.SparkILoop.interpretStartingWith(SparkILoop.scala:902) at org.apache.spark.repl.SparkILoop.command(SparkILoop.scala:814) at org.apache.spark.repl.SparkILoop.processLine$1(SparkILoop.scala:657) at org.apache.spark.repl.SparkILoop.innerLoop$1(SparkILoop.scala:665) at org.apache.spark.repl.SparkILoop.org$apache$spark$repl$SparkILoop$$loop(SparkILoop.scala:670) at org.apache.spark.repl.SparkILoop$$anonfun$org$apache$spark$repl$SparkILoop$$process$1.apply$mcZ$sp(SparkILoop.scala:997) at org.apache.spark.repl.SparkILoop$$anonfun$org$apache$spark$repl$SparkILoop$$process$1.apply(SparkILoop.scala:945) at org.apache.spark.repl.SparkILoop$$anonfun$org$apache$spark$repl$SparkILoop$$process$1.apply(SparkILoop.scala:945) at scala.tools.nsc.util.ScalaClassLoader$.savingContextLoader(ScalaClassLoader.scala:135) at org.apache.spark.repl.SparkILoop.org$apache$spark$repl$SparkILoop$$process(SparkILoop.scala:945) at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:1059) at org.apache.spark.repl.Main$.main(Main.scala:31) at org.apache.spark.repl.Main.main(Main.scala) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:606) at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731) at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181) at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206) at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121) at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)What is the cause of the exception? Thank you
issueID:CARBONDATA-280
type:Bug
changed files:
texts:when table properties is repeated it only set the last one
when table properties is repeated it only set the last one:For example,CREATE TABLE IF NOT EXISTS carbontable(ID Int, date Timestamp, country String,name String, phonetype String, serialname String, salary Int)STORED BY 'carbondata'     TBLPROPERTIES('DICTIONARY_EXCLUDE'='country','DICTIONARY_INCLUDE'='ID', 'DICTIONARY_EXCLUDE'='phonetype', 'DICTIONARY_INCLUDE'='salary')only salary is set to DICTIONARY_INCLUDE and only phonetype is set to DICTIONARY_EXCLUDE.
issueID:CARBONDATA-2800
type:Sub-task
changed files:
texts:Add useful tips for bloomfilter datamap

issueID:CARBONDATA-2801
type:Improvement
changed files:
texts:Add documentation for flat folder

issueID:CARBONDATA-2802
type:Bug
changed files:
texts:Creation of Bloomfilter Datamap is failing after UID,compaction,pre-aggregate datamap creation
Steps :1.CREATE TABLE uniqdata(CUST_ID int,CUST_NAME String,ACTIVE_EMUI_VERSION string, DOB timestamp, DOJ timestamp, BIGINT_COLUMN1 bigint,BIGINT_COLUMN2 bigint,DECIMAL_COLUMN1 decimal(30,10), DECIMAL_COLUMN2 decimal(36,36),Double_COLUMN1 double, Double_COLUMN2 double,INTEGER_COLUMN1 int) STORED BY 'org.apache.carbondata.format';2.LOAD DATA INPATH 'hdfs://hacluster/user/rahul/2000_UniqData.csv' into table uniqdata OPTIONS('DELIMITER'=',' , 'QUOTECHAR'='"','BAD_RECORDS_ACTION'='FORCE','FILEHEADER'='CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1,Double_COLUMN2,INTEGER_COLUMN1');3.update uniqdata set (active_emui_version) = ('ACTIVE_EMUI_VERSION_00001') where cust_id = 9000;4.delete from uniqdata where cust_id = 9000;5.insert into uniqdata select 9000,'CUST_NAME_00000','ACTIVE_EMUI_VERSION_00000','1970-01-01 01:00:03.0','1970-01-01 02:00:03.0',123372036854,-223372036854,12345678901.1234000000,22345678901.1234000000,1.12345674897976E10, -1.12345674897976E10,1;6.alter table uniqdata compact 'major';7.create datamap uniqdata_agg on table uniqdata using 'preaggregate' as select cust_name, avg(cust_id) from uniqdata group by cust_id, cust_name;8.CREATE DATAMAP bloom_dob ON TABLE uniqdata USING 'bloomfilter' DMPROPERTIES ('INDEX_COLUMNS' = 'dob', 'BLOOM_SIZE'='640000', 'BLOOM_FPP'='0.00001');Actual output :0: jdbc:hive2://ha-cluster/default> CREATE DATAMAP bloom_dob ON TABLE uniqdata USING 'bloomfilter' DMPROPERTIES ('INDEX_COLUMNS' = 'dob', 'BLOOM_SIZE'='640000', 'BLOOM_FPP'='0.00001');Error: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 199.0 failed 4 times, most recent failure: Lost task 1.3 in stage 199.0 (TID 484, BLR1000025336, executor 182): java.io.InvalidClassException: scala.collection.convert.Wrappers$MutableSetWrapper; no valid constructor at java.io.ObjectStreamClass$ExceptionInfo.newInvalidClassException(ObjectStreamClass.java:157) at java.io.ObjectStreamClass.checkDeserialize(ObjectStreamClass.java:862) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2041) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1571) at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2285) at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2209) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2067) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1571) at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431) at java.util.ArrayList.readObject(ArrayList.java:797) at sun.reflect.GeneratedMethodAccessor10.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1158) at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2176) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2067) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1571) at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2285) at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2209) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2067) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1571) at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2285) at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2209) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2067) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1571) at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2285) at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2209) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2067) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1571) at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2285) at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2209) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2067) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1571) at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2285) at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2209) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2067) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1571) at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431) at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75) at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:114) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:80) at org.apache.spark.scheduler.Task.run(Task.scala:108) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748)Driver stacktrace: (state=,code=0) Expected output : Datamap Creation should be success
issueID:CARBONDATA-2803
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockDataMap.java
core/src/main/java/org/apache/carbondata/core/metadata/SegmentFileStore.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/CarbonTable.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonTableInputFormat.java
texts:data size is wrong in table status & handle local dictionary for older tables is not proper, null pointer exption is thrown
data size was calculation wrongly,when multiple bloklets are presnt in block and multiple datafiles are present in segment  handle local dictionary for older tables is not proper, null pointer exption is thrown, when the older table tableproperties do not have local dictioanry properties, select query fails null pinter exception
issueID:CARBONDATA-2804
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
texts:Incorrect error message when bloom filter or preaggregate datamap tried to be created on older V1-V2 version stores
Steps :User creates a table with V1 version store and loads data to the table.create table brinjal (imei string,AMSize string,channelsId string,ActiveCountry string, Activecity string,gamePointId double,deviceInformationId double,productionDate Timestamp,deliveryDate timestamp,deliverycharge double) STORED BY 'org.apache.carbondata.format' TBLPROPERTIES('table_blocksize'='1'); LOAD DATA INPATH 'hdfs://hacluster/chetan/vardhandaterestruct.csv' INTO TABLE brinjal OPTIONS('DELIMITER'=',', 'QUOTECHAR'= '"','BAD_RECORDS_ACTION'='FORCE','FILEHEADER'= 'imei,deviceInformationId,AMSize,channelsId,ActiveCountry,Activecity,gamePointId,productionDate,deliveryDate,deliverycharge');In 1.4.1 version user refreshes the table with V1 store and tries to create a bloom filter datamap.CREATE DATAMAP dm_brinjal ON TABLE brinjal2 USING 'bloomfilter' DMPROPERTIES ('INDEX_COLUMNS' = 'AMSize', 'BLOOM_SIZE'='640000', 'BLOOM_FPP'='0.00001');create datamap brinjal_agg on table brinjal2 using 'preaggregate' as select AMSize, avg(gamePointId) from brinjal group by gamePointId, AMSize;Issue : Bloom filter or preaggregate datamap fails with incorrect error message.0: jdbc:hive2://10.18.98.101:22550/default> CREATE DATAMAP dm_brinjal ON TABLE brinjal2 USING 'bloomfilter' DMPROPERTIES ('INDEX_COLUMNS' = 'AMSize', 'BLOOM_SIZE'='640000', 'BLOOM_FPP'='0.00001'); Error: java.io.IOException: org.apache.thrift.protocol.TProtocolException: Required field 'version' was not found in serialized data! Struct: org.apache.carbondata.format.FileHeader$FileHeaderStandardScheme@4d5aa8b2 (state=,code=0) 0: jdbc:hive2://10.18.98.101:22550/default> create datamap brinjal_agg on table brinjal2 using 'preaggregate' as select AMSize, avg(gamePointId) from brinjal group by gamePointId, AMSize; Error: java.io.IOException: org.apache.thrift.protocol.TProtocolException: Required field 'version' was not found in serialized data! Struct: org.apache.carbondata.format.FileHeader$FileHeaderStandardScheme@55d8323c (state=,code=0)Expected : Correct error message should be displayed when bloom filter or preaggregate datamap creation is blocked/fails.
issueID:CARBONDATA-2805
type:Bug
changed files:processing/src/main/java/org/apache/carbondata/processing/merger/CarbonDataMergerUtil.java
texts:Wrong order in custom compaction
when we have segments from 0 to 6 and i give 1, 2, 3 for custom compaction, then it should create 1.1 as compacted segment, but sometimes it will create 3.1 as compacted segment which is wrong.----------------------------------------------------------------------------+SegmentSequenceId Status Load Start Time Load End TimeMerged ToFile Format----------------------------------------------------------------------------+ 4 Success2018-07-27 07:25:...2018-07-27 07:25:... NACOLUMNAR_V3 3.1 Success2018-07-27 07:25:...2018-07-27 07:25:... NACOLUMNAR_V3 3Compacted2018-07-27 07:25:...2018-07-27 07:25:... 3.1COLUMNAR_V3 2Compacted2018-07-27 07:25:...2018-07-27 07:25:... 3.1COLUMNAR_V3 1Compacted2018-07-27 07:25:...2018-07-27 07:25:... 3.1COLUMNAR_V3 0 Success2018-07-27 07:25:...2018-07-27 07:25:... NACOLUMNAR_V3----------------------------------------------------------------------------+ 
issueID:CARBONDATA-2807
type:Improvement
changed files:processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/merger/UnsafeIntermediateMerger.java
texts:Fixed data load performance issue with more number of records
*Problem:*Data Loading is taking more time when number of records are high.*Root cause:* As number of records are high intermediate merger is taking more time.*Solution:* Checking the number of files present in file list is done is synchronized block because of this each intermediate request is taking sometime and when number of records are high it impacting overall data loading performance
issueID:CARBONDATA-2808
type:Bug
changed files:
texts:Insert into select is crashing as both are sharing the same task context
Insert into select is failing as both are running as the same task and both are sharing the same taskcontext and resources are cleared once any one of the RDD's task is completed.
issueID:CARBONDATA-2809
type:Bug
changed files:integration/spark2/src/main/java/org/apache/carbondata/datamap/IndexDataMapProvider.java
core/src/main/java/org/apache/carbondata/core/datamap/dev/DataMapFactory.java
core/src/main/java/org/apache/carbondata/core/datamap/DataMapProvider.java
texts:Manually rebuilding non-lazy datamap cause error
Steps to reproduce:1. create base table2. load data to base table3. create index datamap (such as bloomfilter datamap) on base table4. rebuild datamap ---- This will give errorIn step3, the data of datamap has already been generated, if we trigger rebuild, the procedure does not clean the files properly, thus causing the error.Actually, the rebuild is not required. We can fix this issue by skipping the rebuild procedure.
issueID:CARBONDATA-281
type:Improvement
changed files:
texts:improve the test cases in LCM module.
improving the test cases in the lcm. adding the test cases for the compaction with boundary test cases. added test cases to verify the minor compaction threshold check.
issueID:CARBONDATA-2810
type:Bug
changed files:
texts:datamap does not get cleaned if creating datamap fails
Problems:Currently when we create datamap such as bloomfilter datamap, it will:1. create datamap metadata2. generate datamap dataIf it fails in the 2nd step, the index data files are not cleaned
issueID:CARBONDATA-2811
type:Sub-task
changed files:
texts:Add query test case using search mode on table with bloom filter

issueID:CARBONDATA-2812
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/datastore/chunk/store/ColumnPageWrapper.java
texts:Implement freeMemory for complex pages

issueID:CARBONDATA-2813
type:Bug
changed files:processing/src/main/java/org/apache/carbondata/processing/merger/CarbonDataMergerUtil.java
texts:Major compaction on partition table created in 1.3.x store is throwing Unable to get file status error.
Steps to reproduce: Create a partitioned table in 1.3.x version. Load data into the table. move the table to current version cluster(1.4.x). Load data into table on 1.4.x version Run major compaction
issueID:CARBONDATA-2815
type:Improvement
changed files:
texts:Add documentation for memory spill and rebuild datamap

issueID:CARBONDATA-2817
type:Bug
changed files:processing/src/main/java/org/apache/carbondata/processing/store/writer/v3/CarbonFactDataWriterImplV3.java
processing/src/main/java/org/apache/carbondata/processing/loading/steps/CarbonRowDataWriterProcessorStepImpl.java
processing/src/main/java/org/apache/carbondata/processing/store/writer/AbstractFactDataWriter.java
core/src/main/java/org/apache/carbondata/core/util/BlockletDataMapUtil.java
texts:Thread Leak in Update and in No sort flow
After update  finished , loading threads (input process,convet,sort..etc) are still alive. "Thread-36" #172 daemon prio=5 os_prio=0 tid=0x00007fd021eba000 nid=0x17136 waiting on condition &#91;0x00007fd011111000&#93; java.lang.Thread.State: TIMED_WAITING (sleeping) at java.lang.Thread.sleep(Native Method) at org.apache.carbondata.processing.loading.AbstractDataLoadProcessorStep$1.run(AbstractDataLoadProcessorStep.java:81)"Thread-35" #171 daemon prio=5 os_prio=0 tid=0x0000000001a7f000 nid=0x17135 waiting on condition &#91;0x00007fd011010000&#93; java.lang.Thread.State: TIMED_WAITING (sleeping) at java.lang.Thread.sleep(Native Method) at org.apache.carbondata.processing.loading.AbstractDataLoadProcessorStep$1.run(AbstractDataLoadProcessorStep.java:81)"Thread-34" #170 daemon prio=5 os_prio=0 tid=0x0000000001e40000 nid=0x17134 waiting on condition &#91;0x00007fd019aa9000&#93; java.lang.Thread.State: TIMED_WAITING (sleeping) at java.lang.Thread.sleep(Native Method) at org.apache.carbondata.processing.loading.AbstractDataLoadProcessorStep$1.run(AbstractDataLoadProcessorStep.java:81) "NoSortDataWriterPool:tbl_data_event_410000_carbon_nosort" #96 prio=5 os_prio=0 tid=0x0000000000f2e800 nid=0x129a2 waiting on condition &#91;0x00007fd0197a6000&#93; java.lang.Thread.State: WAITING (parking) at sun.misc.Unsafe.park(Native Method) parking to wait for <0x00000006a35989a0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject) at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175) at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039) at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442) at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)   
issueID:CARBONDATA-282
type:Improvement
changed files:
texts:Add segment management example
Add segment delete using segment id and date example in CarbonExample
issueID:CARBONDATA-2820
type:Improvement
changed files:
texts:Block rebuilding for preagg, bloom and lucene datamap
currently we will block rebuilding these datamap
issueID:CARBONDATA-2822
type:Bug
changed files:
texts:Carbon Configuration - "carbon.invisible.segments.preserve.count"  configuration property is not working as expected.
For the carbon.invisible.segments.preserve.count configuration, it is not working as expected.Steps to reproduce:1) Setting up "carbon.invisible.segments.preserve.count=20" in carbon.properties and restarting the thrift server. 2) After performing Loading 40 times and Compaction 4 times.3) Perform clean files, so that the tablestatus.history file would be generated with invisible segments details. So Total 44 segments would be created including visible and invisible segments.(40 load segment (like segment ID from 0,1,2...39) + 4 compacted new segment(like 0.1,20.1,22.1,0.2))In that, 41 segments information are present in the "tablestatus.history" file(*which holds invisible(marked for delete and compacted) segments details) and 3 segments information are present in the "tablestatus" file(which holds visible segments(0 .2 -final compacted segment) along with (1^st^ segment - 0th segment) and (last segment-39th segment)). *But invisible segment preserve count is configured to 20, which is not followed for the tablestatus.history file.Expected result:tablestatus.history file should preserve only the latest 20 segments, as per the configuration.Actual result:tablestatus.history file is having 41 invisible segments details.(which is above the configured value: 20) This is tested with ANT cluster.
issueID:CARBONDATA-2823
type:Bug
changed files:datamap/lucene/src/main/java/org/apache/carbondata/datamap/lucene/LuceneFineGrainDataMapFactory.java
datamap/bloom/src/main/java/org/apache/carbondata/datamap/bloom/BloomCoarseGrainDataMapFactory.java
texts:Alter table set local dictionary include after bloom creation fails throwing incorrect error
Steps : create table create bloom/lucene datamap load data alter table set tblProperties0: jdbc:hive2://10.18.98.101:22550/default> CREATE TABLE uniqdata_load (CUST_ID int,CUST_NAME String,ACTIVE_EMUI_VERSION string, DOB timestamp, DOJ timestamp, BIGINT_COLUMN1 bigint,BIGINT_COLUMN2 bigint,DECIMAL_COLUMN1 decimal(30,10), DECIMAL_COLUMN2 decimal(36,36),Double_COLUMN1 double, Double_COLUMN2 double,INTEGER_COLUMN1 int) STORED BY 'org.apache.carbondata.format';---------+ Result ---------+---------+No rows selected (1.43 seconds)0: jdbc:hive2://10.18.98.101:22550/default> CREATE DATAMAP dm_uniqdata1_tmstmp6 ON TABLE uniqdata_load USING 'bloomfilter' DMPROPERTIES ('INDEX_COLUMNS' = 'DOJ', 'BLOOM_SIZE'='640000', 'BLOOM_FPP'='0.00001');---------+ Result ---------+---------+No rows selected (0.828 seconds)0: jdbc:hive2://10.18.98.101:22550/default> LOAD DATA INPATH 'hdfs://hacluster/chetan/2000_UniqData.csv' into table uniqdata_load OPTIONS('DELIMITER'=',' , 'QUOTECHAR'='"','BAD_RECORDS_ACTION'='FORCE','FILEHEADER'='CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1,Double_COLUMN2,INTEGER_COLUMN1');---------+ Result ---------+---------+No rows selected (4.903 seconds)0: jdbc:hive2://10.18.98.101:22550/default> alter table uniqdata_load set tblproperties('local_dictionary_include'='CUST_NAME');Error: org.apache.carbondata.common.exceptions.sql.MalformedCarbonCommandException: streaming is not supported for index datamap (state=,code=0) Issue : Alter table set local dictionary include fails with incorrect error.0: jdbc:hive2://10.18.98.101:22550/default> alter table uniqdata_load set tblproperties('local_dictionary_include'='CUST_NAME');Error: org.apache.carbondata.common.exceptions.sql.MalformedCarbonCommandException: streaming is not supported for index datamap (state=,code=0) Expected : Operation should be success. If the operation is unsupported it should throw correct error message. 
issueID:CARBONDATA-2829
type:Bug
changed files:
texts:Fix creating merge index on older V1 V2 store
Block creating merge index on older V1 V2 version
issueID:CARBONDATA-283
type:Bug
changed files:
texts:Fix status metadata access for concurrent scenarios
Improve test cases for data retention concurrent scenarios
issueID:CARBONDATA-2831
type:Bug
changed files:
texts:Support Merge index files read from non transactional table.
problem : Currently  SDK read/ nontransactional table read from external table gives null output when carbonMergeindex file is present instead of carobnindex files. cause : In LatestFileReadCommitted, while taking snapshot, merge index files were not considered.solution: consider the merge index files while taking snapshot 
issueID:CARBONDATA-2832
type:Bug
changed files:
texts:Block loading error for select query executed after merge index command executed on V1/V2 store table
Steps :Create and load data in V1/V2 carbon store:create table brinjal (imei string,AMSize string,channelsId string,ActiveCountry string, Activecity string,gamePointId double,deviceInformationId double,productionDate Timestamp,deliveryDate timestamp,deliverycharge double) STORED BY 'org.apache.carbondata.format' TBLPROPERTIES('table_blocksize'='1');LOAD DATA INPATH 'hdfs://hacluster/chetan/vardhandaterestruct.csv' INTO TABLE brinjal OPTIONS('DELIMITER'=',', 'QUOTECHAR'= '"','BAD_RECORDS_ACTION'='FORCE','FILEHEADER'= 'imei,deviceInformationId,AMSize,channelsId,ActiveCountry,Activecity,gamePointId,productionDate,deliveryDate,deliverycharge');In 1.4.1refresh table brinjal;alter table brinjal compact 'segment_index';select * from brinjal where AMSize='8RAM size'; Issue : Block loading error for select query executed after merge index command executed on V1/V2 store table.0: jdbc:hive2://10.18.98.101:22550/default> select * from brinjal where AMSize='8RAM size';Error: java.io.IOException: Problem in loading segment blocks. (state=,code=0)Expected : select query executed after merge index command executed on V1/V2 store table should return correct result set without error**
issueID:CARBONDATA-2834
type:Bug
changed files:hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonTableInputFormat.java
texts:Refactor code to remove nested for loop to extract invalidTimestampRange.
Reactor getInvalidTimestampRange method in SegmentUpdateStatusManager because it has an unnecessary nested loop to get timestamp from invalid segments.This will cause query performance degradation
issueID:CARBONDATA-2835
type:Sub-task
changed files:
texts:Block MV datamap on streaming table
We should block creating MV datamap on streaming table;Also we should block setting streaming property for table which has MV datamap.
issueID:CARBONDATA-2836
type:Improvement
changed files:processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/holder/UnsafeInmemoryHolder.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/merger/UnsafeIntermediateFileMerger.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/UnsafeCarbonRowPage.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/SingleThreadFinalSortFilesMerger.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/holder/UnsafeSortTempFileChunkHolder.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/SortTempFileChunkHolder.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/IntermediateFileMerger.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/SortStepRowHandler.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/holder/UnsafeFinalMergePageHolder.java
processing/src/main/java/org/apache/carbondata/processing/loading/row/IntermediateSortTempRow.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/merger/UnsafeSingleThreadFinalSortFilesMerger.java
texts:Fixed data loading performance issue
Problem: Data Loading is taking more time when number of records are high(3.5 billion) recordsRoot Cause: In case of Final merge sort temp row conversion is done in main thread because of this final step processing became slower.Solution: Mode conversion logic to pre-fetch thread for parallel processing
issueID:CARBONDATA-2837
type:Improvement
changed files:
texts:Add MV Example in examples module

issueID:CARBONDATA-2838
type:Sub-task
changed files:
texts:Add SDV test cases for Local Dictionary Support

issueID:CARBONDATA-2839
type:Task
changed files:
texts:Add custom compaction example
Add custom compaction example
issueID:CARBONDATA-284
type:Improvement
changed files:hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonTableInputFormat.java
core/src/main/java/org/apache/carbondata/hadoop/internal/index/Block.java
core/src/main/java/org/apache/carbondata/hadoop/CarbonInputSplit.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonTableOutputFormat.java
texts:Abstracting Index and Segment interface
This issue is intended to abstract developer API and user API to achieve following goals:Goal 1: User can choose the place to store Index data, it can be stored inprocessing framework's memory space (like in spark driver memory) or inanother service outside of the processing framework (like using aindependent database service, which can be shared across client)Goal 2: Developer can add more index of his choice to CarbonData files.Besides B+ tree on multi-dimensional key which current CarbonData supports,developers are free to add other indexing technology to make certainworkload faster. These new indices should be added in a pluggable way.This Jira has been discussed in maillist: http://apache-carbondata-mailing-list-archive.1130556.n5.nabble.com/Abstracting-CarbonData-s-Index-Interface-td1587.html
issueID:CARBONDATA-2844
type:Sub-task
changed files:core/src/main/java/org/apache/carbondata/core/datastore/impl/DefaultFileTypeProvider.java
core/src/main/java/org/apache/carbondata/core/scan/executor/QueryExecutorFactory.java
integration/presto/src/main/java/org/apache/carbondata/presto/CarbondataPageSourceProvider.java
processing/src/main/java/org/apache/carbondata/processing/merger/CarbonCompactionExecutor.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonInputFormat.java
core/src/main/java/org/apache/carbondata/core/datastore/impl/FileTypeInterface.java
core/src/main/java/org/apache/carbondata/core/scan/executor/impl/AbstractQueryExecutor.java
integration/hive/src/main/java/org/apache/carbondata/hive/CarbonHiveRecordReader.java
core/src/main/java/org/apache/carbondata/core/util/CarbonProperties.java
store/sdk/src/main/java/org/apache/carbondata/sdk/file/CarbonReaderBuilder.java
integration/presto/src/main/java/org/apache/carbondata/presto/impl/CarbonTableReader.java
core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonTableOutputFormat.java
core/src/main/java/org/apache/carbondata/core/datastore/impl/FileFactory.java
core/src/main/java/org/apache/carbondata/core/scan/executor/impl/VectorDetailQueryExecutor.java
core/src/main/java/org/apache/carbondata/core/datastore/impl/DFSFileReaderImpl.java
store/sdk/src/main/java/org/apache/carbondata/sdk/file/JsonCarbonWriter.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonTableInputFormat.java
hadoop/src/main/java/org/apache/carbondata/hadoop/util/CarbonInputFormatUtil.java
integration/presto/src/main/java/org/apache/carbondata/presto/PrestoCarbonVectorizedRecordReader.java
core/src/main/java/org/apache/carbondata/core/util/SessionParams.java
hadoop/src/main/java/org/apache/carbondata/hadoop/CarbonRecordReader.java
integration/spark-datasource/src/main/scala/org/apache/carbondata/spark/vectorreader/VectorizedCarbonRecordReader.java
core/src/main/java/org/apache/carbondata/core/scan/executor/impl/DetailQueryExecutor.java
core/src/main/java/org/apache/carbondata/core/util/ThreadLocalSessionInfo.java
store/sdk/src/main/java/org/apache/carbondata/sdk/file/CarbonWriterBuilder.java
datamap/lucene/src/main/java/org/apache/carbondata/datamap/lucene/LuceneDataMapWriter.java
texts:SK AK not getting passed to executors for global sort
Introduce CarbonConfiguration class to serialize only SK AK to executors.
issueID:CARBONDATA-2845
type:Sub-task
changed files:datamap/bloom/src/main/java/org/apache/carbondata/datamap/bloom/BloomIndexFileStore.java
datamap/bloom/src/main/java/org/apache/carbondata/datamap/bloom/BloomCoarseGrainDataMapFactory.java
datamap/bloom/src/main/java/org/apache/carbondata/datamap/bloom/BloomDataMapDistributable.java
datamap/bloom/src/main/java/org/apache/carbondata/datamap/bloom/BloomDataMapCache.java
datamap/bloom/src/main/java/org/apache/hadoop/util/bloom/CarbonBloomFilter.java
datamap/bloom/src/main/java/org/apache/carbondata/datamap/bloom/AbstractBloomDataMapWriter.java
datamap/bloom/src/main/java/org/apache/carbondata/datamap/bloom/BloomCoarseGrainDataMap.java
texts:Merge bloom index files of multi-shards for each index column

issueID:CARBONDATA-285
type:Improvement
changed files:
texts:Use path parameter in Spark datasource API
Currently, when using carbon with spark datasource API, it need to give database name and table name as parameter, it is not the normal way of datasource API usage. In this PR, database name and table name is not required to give, user need to specify the `path` parameter (indicating the path to table folder) only when using datasource API
issueID:CARBONDATA-2851
type:Sub-task
changed files:core/src/main/java/org/apache/carbondata/core/datastore/page/LazyColumnPage.java
core/src/main/java/org/apache/carbondata/core/datastore/page/VarLengthColumnPageBase.java
core/src/main/java/org/apache/carbondata/core/util/BlockletDataMapUtil.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/reader/dimension/v3/DimensionChunkReaderV3.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/adaptive/AdaptiveDeltaFloatingCodec.java
core/src/main/java/org/apache/carbondata/core/metadata/datatype/DataType.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/ColumnPageEncoderMeta.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/reader/measure/v3/MeasureChunkPageReaderV3.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/compress/DirectCompressCodec.java
core/src/main/java/org/apache/carbondata/core/scan/executor/util/QueryUtil.java
core/src/main/java/org/apache/carbondata/core/datastore/compression/ZstdCompressor.java
core/src/main/java/org/apache/carbondata/core/datastore/page/SafeDecimalColumnPage.java
core/src/main/java/org/apache/carbondata/core/datastore/page/UnsafeFixLengthColumnPage.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/impl/DimensionRawColumnChunk.java
core/src/main/java/org/apache/carbondata/core/datastore/page/DecimalColumnPage.java
core/src/main/java/org/apache/carbondata/core/datastore/page/SafeVarLengthColumnPage.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/dimension/legacy/IndexStorageCodec.java
core/src/main/java/org/apache/carbondata/core/datastore/page/UnsafeDecimalColumnPage.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/dimension/legacy/DirectDictDimensionIndexCodec.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/rle/RLEEncoderMeta.java
core/src/main/java/org/apache/carbondata/core/datastore/page/ColumnPage.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/adaptive/AdaptiveFloatingCodec.java
core/src/main/java/org/apache/carbondata/core/datastore/page/SafeFixLengthColumnPage.java
hadoop/src/main/java/org/apache/carbondata/hadoop/stream/StreamBlockletReader.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/rle/RLECodec.java
processing/src/main/java/org/apache/carbondata/processing/loading/DataLoadProcessBuilder.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/adaptive/AdaptiveIntegralCodec.java
processing/src/main/java/org/apache/carbondata/processing/store/TablePage.java
streaming/src/main/java/org/apache/carbondata/streaming/CarbonStreamRecordWriter.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/reader/measure/AbstractMeasureChunkReader.java
processing/src/main/java/org/apache/carbondata/processing/loading/CarbonDataLoadConfiguration.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/reader/measure/v3/MeasureChunkReaderV3.java
processing/src/main/java/org/apache/carbondata/processing/loading/model/CarbonLoadModel.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/dimension/legacy/HighCardDictDimensionIndexCodec.java
core/src/main/java/org/apache/carbondata/core/datastore/page/LocalDictColumnPage.java
core/src/main/java/org/apache/carbondata/core/datastore/page/UnsafeVarLengthColumnPage.java
streaming/src/main/java/org/apache/carbondata/streaming/StreamBlockletWriter.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/dimension/legacy/ComplexDimensionIndexCodec.java
core/src/main/java/org/apache/carbondata/core/util/ByteUtil.java
core/src/main/java/org/apache/carbondata/core/datastore/compression/CompressorFactory.java
core/src/main/java/org/apache/carbondata/core/localdictionary/PageLevelDictionary.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/ColumnPageEncoder.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/dimension/legacy/DictDimensionIndexCodec.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/adaptive/AdaptiveDeltaIntegralCodec.java
processing/src/main/java/org/apache/carbondata/processing/loading/model/CarbonLoadModelBuilder.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonTableOutputFormat.java
processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactDataHandlerModel.java
core/src/main/java/org/apache/carbondata/core/datastore/compression/Compressor.java
core/src/main/java/org/apache/carbondata/core/datastore/page/ComplexColumnPage.java
core/src/main/java/org/apache/carbondata/core/util/CarbonMetadataUtil.java
integration/spark2/src/main/scala/org/apache/carbondata/stream/CarbonStreamRecordReader.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/EncodingFactory.java
core/src/main/java/org/apache/carbondata/core/datastore/compression/SnappyCompressor.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/DefaultEncodingFactory.java
core/src/main/java/org/apache/carbondata/core/datastore/page/DecoderBasedFallbackEncoder.java
texts:support zstd as column compressor

issueID:CARBONDATA-2852
type:Sub-task
changed files:core/src/main/java/org/apache/carbondata/core/datastore/page/LazyColumnPage.java
core/src/main/java/org/apache/carbondata/core/datastore/page/VarLengthColumnPageBase.java
core/src/main/java/org/apache/carbondata/core/util/BlockletDataMapUtil.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/reader/dimension/v3/DimensionChunkReaderV3.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/adaptive/AdaptiveDeltaFloatingCodec.java
core/src/main/java/org/apache/carbondata/core/metadata/datatype/DataType.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/ColumnPageEncoderMeta.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/reader/measure/v3/MeasureChunkPageReaderV3.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/compress/DirectCompressCodec.java
core/src/main/java/org/apache/carbondata/core/scan/executor/util/QueryUtil.java
core/src/main/java/org/apache/carbondata/core/datastore/compression/ZstdCompressor.java
core/src/main/java/org/apache/carbondata/core/datastore/page/SafeDecimalColumnPage.java
core/src/main/java/org/apache/carbondata/core/datastore/page/UnsafeFixLengthColumnPage.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/impl/DimensionRawColumnChunk.java
core/src/main/java/org/apache/carbondata/core/datastore/page/DecimalColumnPage.java
core/src/main/java/org/apache/carbondata/core/datastore/page/SafeVarLengthColumnPage.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/dimension/legacy/IndexStorageCodec.java
core/src/main/java/org/apache/carbondata/core/datastore/page/UnsafeDecimalColumnPage.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/dimension/legacy/DirectDictDimensionIndexCodec.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/rle/RLEEncoderMeta.java
core/src/main/java/org/apache/carbondata/core/datastore/page/ColumnPage.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/adaptive/AdaptiveFloatingCodec.java
core/src/main/java/org/apache/carbondata/core/datastore/page/SafeFixLengthColumnPage.java
hadoop/src/main/java/org/apache/carbondata/hadoop/stream/StreamBlockletReader.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/rle/RLECodec.java
processing/src/main/java/org/apache/carbondata/processing/loading/DataLoadProcessBuilder.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/adaptive/AdaptiveIntegralCodec.java
processing/src/main/java/org/apache/carbondata/processing/store/TablePage.java
streaming/src/main/java/org/apache/carbondata/streaming/CarbonStreamRecordWriter.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/reader/measure/AbstractMeasureChunkReader.java
processing/src/main/java/org/apache/carbondata/processing/loading/CarbonDataLoadConfiguration.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/reader/measure/v3/MeasureChunkReaderV3.java
processing/src/main/java/org/apache/carbondata/processing/loading/model/CarbonLoadModel.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/dimension/legacy/HighCardDictDimensionIndexCodec.java
core/src/main/java/org/apache/carbondata/core/datastore/page/LocalDictColumnPage.java
core/src/main/java/org/apache/carbondata/core/datastore/page/UnsafeVarLengthColumnPage.java
streaming/src/main/java/org/apache/carbondata/streaming/StreamBlockletWriter.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/dimension/legacy/ComplexDimensionIndexCodec.java
core/src/main/java/org/apache/carbondata/core/util/ByteUtil.java
core/src/main/java/org/apache/carbondata/core/datastore/compression/CompressorFactory.java
core/src/main/java/org/apache/carbondata/core/localdictionary/PageLevelDictionary.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/ColumnPageEncoder.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/dimension/legacy/DictDimensionIndexCodec.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/adaptive/AdaptiveDeltaIntegralCodec.java
processing/src/main/java/org/apache/carbondata/processing/loading/model/CarbonLoadModelBuilder.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonTableOutputFormat.java
processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactDataHandlerModel.java
core/src/main/java/org/apache/carbondata/core/datastore/compression/Compressor.java
core/src/main/java/org/apache/carbondata/core/datastore/page/ComplexColumnPage.java
core/src/main/java/org/apache/carbondata/core/util/CarbonMetadataUtil.java
integration/spark2/src/main/scala/org/apache/carbondata/stream/CarbonStreamRecordReader.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/EncodingFactory.java
core/src/main/java/org/apache/carbondata/core/datastore/compression/SnappyCompressor.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/DefaultEncodingFactory.java
core/src/main/java/org/apache/carbondata/core/datastore/page/DecoderBasedFallbackEncoder.java
texts:support zstd on legacy store
Currently carbondata reads the column compressor from system property. This will cause problems on legacy store if we have changed the compressor.It should read that information from metadata in data files.
issueID:CARBONDATA-2853
type:Sub-task
changed files:streaming/src/main/java/org/apache/carbondata/streaming/StreamBlockletWriter.java
core/src/main/java/org/apache/carbondata/core/stream/StreamPruner.java
core/src/main/java/org/apache/carbondata/core/stream/StreamFile.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonTableInputFormat.java
integration/spark2/src/main/scala/org/apache/carbondata/stream/CarbonStreamRecordReader.java
core/src/main/java/org/apache/carbondata/core/util/CarbonMetadataUtil.java
streaming/src/main/java/org/apache/carbondata/streaming/index/StreamFileIndex.java
streaming/src/main/java/org/apache/carbondata/streaming/CarbonStreamRecordWriter.java
streaming/src/main/java/org/apache/carbondata/streaming/segment/StreamSegment.java
texts:Add min/max index for streaming segment
Streaming index file in stream segment adds min/max meta index for each streaming file during streaming ingestion. So the filter query can use the min/max index to prune the streaming files to reduce the number of the spark tasks in the driver side. Streaming file adds min/max into the blocklet header, so the filter query can skip data during scanning file.
issueID:CARBONDATA-2854
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/statusmanager/SegmentStatusManager.java
texts:Release table status file lock before delete physical files when execute &#39;clean files&#39; command
Release table status file lock before delete physical files when execute 'clean files' command, otherwise table status file will be locked during deleting physical files, it may take a long time, other operations will fail to get table status file lock.
issueID:CARBONDATA-2856
type:Sub-task
changed files:datamap/bloom/src/main/java/org/apache/carbondata/datamap/bloom/BloomDataMapWriter.java
core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
texts:Fix bug in bloom index on multiple dictionary columns
create bloom index on a table which has date and string columns and the string columns is global dictionary.The data loading procedure will fail
issueID:CARBONDATA-2857
type:Improvement
changed files:
texts:Improvement in "How to contribute to Apache CarbonData" page
Improvement  to "How to contribute to Apache CarbonData" page
issueID:CARBONDATA-2859
type:Sub-task
changed files:
texts:add sdv test case for bloomfilter datamap
add sdv test case for bloomfilter datamap
issueID:CARBONDATA-286
type:New Feature
changed files:
texts:Support Append mode when writing Dataframe to CarbonData
Support df.write using Append as save mode, it will become a new segment in CarbonData
issueID:CARBONDATA-2862
type:Bug
changed files:
texts:Fix exception message for datamap rebuild command

issueID:CARBONDATA-2866
type:Bug
changed files:
texts:Should block schema when creating external table
Currently, CarbonData does not support specifying schema when creating external table. This feature should be blocked until CARBONDATA-2858 is implemented
issueID:CARBONDATA-2869
type:Sub-task
changed files:processing/src/main/java/org/apache/carbondata/processing/loading/converter/impl/FieldEncoderFactory.java
store/sdk/src/main/java/org/apache/carbondata/sdk/file/AvroCarbonWriter.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonInputFormat.java
core/src/main/java/org/apache/carbondata/core/util/AbstractDataFileFooterConverter.java
core/src/main/java/org/apache/carbondata/core/scan/executor/util/QueryUtil.java
core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
core/src/main/java/org/apache/carbondata/core/util/DataTypeConverterImpl.java
processing/src/main/java/org/apache/carbondata/processing/loading/parser/CarbonParserFactory.java
store/sdk/src/main/java/org/apache/carbondata/sdk/file/Field.java
core/src/main/java/org/apache/carbondata/core/metadata/datatype/MapType.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/TableSchemaBuilder.java
processing/src/main/java/org/apache/carbondata/processing/util/CarbonDataProcessorUtil.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
core/src/main/java/org/apache/carbondata/core/datastore/page/ComplexColumnPage.java
core/src/main/java/org/apache/carbondata/core/util/DataTypeConverter.java
core/src/main/java/org/apache/carbondata/core/scan/complextypes/ArrayQueryType.java
core/src/main/java/org/apache/carbondata/core/scan/complextypes/MapQueryType.java
integration/spark-datasource/src/main/scala/org/apache/carbondata/converter/SparkDataTypeConverterImpl.java
store/sdk/src/main/java/org/apache/carbondata/sdk/file/CarbonWriterBuilder.java
core/src/main/java/org/apache/carbondata/core/metadata/converter/ThriftWrapperSchemaConverterImpl.java
texts:SDK support for Map DataType

issueID:CARBONDATA-287
type:Improvement
changed files:
texts:Save the sorted temp files to multi local dirs to improve dataloading perfomance
Now for each dataloading, we use only a different local dir to save  the sorted temp files. I think it is neccessary to use multi local dirs for each dataloading to improve dataloading performance.
issueID:CARBONDATA-2872
type:New Feature
changed files:integration/spark-datasource/src/main/scala/org/apache/carbondata/spark/vectorreader/ColumnarVectorWrapper.java
store/sdk/src/main/java/org/apache/carbondata/sdk/file/CarbonWriterBuilder.java
core/src/main/java/org/apache/carbondata/core/datamap/DataMapStoreManager.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonTableInputFormat.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonInputFormat.java
core/src/main/java/org/apache/carbondata/core/scan/executor/impl/AbstractQueryExecutor.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonFileInputFormat.java
integration/spark-datasource/src/main/scala/org/apache/carbondata/spark/vectorreader/VectorizedCarbonRecordReader.java
core/src/main/java/org/apache/carbondata/core/datamap/DataMapUtil.java
integration/spark-common/src/main/java/org/apache/carbondata/spark/util/Util.java
integration/spark-datasource/src/main/scala/org/apache/carbondata/converter/SparkDataTypeConverterImpl.java
core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
integration/spark-datasource/src/main/spark2.1andspark2.2/org/apache/spark/sql/CarbonDictionaryWrapper.java
core/src/main/java/org/apache/carbondata/core/metadata/AbsoluteTableIdentifier.java
texts:Support standard spark&#39;s FIleFormat interface in carbon
Current carbondata has deep integration with spark to provide optimizations in performance and also supports features like compaction, IUD, data maps and metadata management etc. This type of integration forces user to use CarbonSession instance to use carbon even for read and write operations.For the users who wants a same spark datasource integration to support read and write data carbon should support FIleFormat interface exposed by spark.
issueID:CARBONDATA-2874
type:Bug
changed files:processing/src/main/java/org/apache/carbondata/processing/loading/steps/JsonInputProcessorStepImpl.java
processing/src/main/java/org/apache/carbondata/processing/util/CarbonDataProcessorUtil.java
processing/src/main/java/org/apache/carbondata/processing/loading/model/CarbonLoadModel.java
processing/src/main/java/org/apache/carbondata/processing/loading/DataLoadProcessBuilder.java
processing/src/main/java/org/apache/carbondata/processing/loading/steps/InputProcessorStepImpl.java
store/sdk/src/main/java/org/apache/carbondata/sdk/file/CarbonWriterBuilder.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonTableOutputFormat.java
texts:Support SDK writer as thread safe api
Support SDK writer as thread safe api
issueID:CARBONDATA-2876
type:Improvement
changed files:store/sdk/src/main/java/org/apache/carbondata/sdk/file/AvroCarbonWriter.java
texts:Support Avro datatype conversion to Carbon Format
1.Support Avro Complex Types: Enum, Union, Fixed with Carbon.2.Support Avro Logical Types: TimeMillis, TimeMicros, Decimal with Carbon. Please find the design document in the below link:https://docs.google.com/document/d/1Jne8vNZ3OSYmJ_72hTIk_5I4EeIVtxGNE5mN_hBlnVE/edit?usp=sharing
issueID:CARBONDATA-2879
type:Improvement
changed files:processing/src/main/java/org/apache/carbondata/processing/loading/parser/impl/JsonRowParser.java
processing/src/main/java/org/apache/carbondata/processing/loading/model/CarbonLoadModelBuilder.java
store/sdk/src/main/java/org/apache/carbondata/sdk/file/CarbonWriterBuilder.java
store/sdk/src/main/java/org/apache/carbondata/sdk/file/Field.java
texts:Support Sort Scope for SDK

issueID:CARBONDATA-288
type:Bug
changed files:processing/src/main/java/org/apache/carbondata/processing/util/TableOptionConstant.java
processing/src/main/java/org/apache/carbondata/processing/surrogatekeysgenerator/csvbased/BadRecordsLogger.java
processing/src/main/java/org/apache/carbondata/processing/model/CarbonLoadModel.java
integration/spark-common/src/main/java/org/apache/carbondata/spark/load/CarbonLoaderUtil.java
common/src/main/java/org/apache/carbondata/common/constants/LoggerAction.java
texts:In hdfs bad record logger is failing in writting the bad records
For HDFS file system CarbonFile logFile = FileFactory.getCarbonFile(filePath, FileType.HDFS);if filePath does not exits thenCalling CarbonFile.getPath() throws NullPointerException.Solution:If file does not exist then before accessing the file must be created first
issueID:CARBONDATA-2884
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RangeValueFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/util/ByteUtil.java
processing/src/main/java/org/apache/carbondata/processing/datatypes/PrimitiveDataType.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/store/ColumnPageWrapper.java
core/src/main/java/org/apache/carbondata/core/util/DataTypeUtil.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/store/impl/safe/SafeVariableLengthDimensionDataChunkStore.java
core/src/main/java/org/apache/carbondata/core/scan/executor/util/RestructureUtil.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/store/impl/unsafe/UnsafeVariableLengthDimensionDataChunkStore.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelRangeLessThanFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelRangeLessThanEqualFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/scan/complextypes/PrimitiveQueryType.java
texts:Should rename the methods of ByteUtil class to avoid the misuse
the method toBytes will execute XOR operation on data.So the result is not the byte array of the real value.Better to rename the methods of ByteUtil class to avoid the misuse
issueID:CARBONDATA-2885
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/scan/result/iterator/AbstractDetailQueryResultIterator.java
texts:Broadcast Issue and Small file distribution Issue
Carbon Relation size is getting calculated wrongly ( always 0 ) for External Table.Root Cause:- Because Tablestatus file is not present for external table  
issueID:CARBONDATA-2886
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/scan/executor/impl/AbstractQueryExecutor.java
texts:select filter with int datatype is showing incorrect result in case of table created and loaded on old version and queried in new version
select filter with int datatype is showing incorrect result in case of table created and loaded on old version and queried in new version Steps:created table and loaded data in base carbon version 1.1refresh table in 1.4.1select filter query for integer column with valid existing value   expected result: the filtered row must be shownactual result: filtered row is not shown Note:1.  this happens for a particular table in the store2. If filter has integer column with any other column then it displays the resultselect cust_id,cust_name from uniqdata_load where cust_name= 'CUST_NAME_00075' or cust_id=9010;-------------------------- cust_id  cust_name -------------------------- 9010  CUST_NAME_00010  9075  CUST_NAME_00075 --------------------------2 rows selected (0.652 seconds) select cust_id,cust_name from uniqdata_load where cust_id=9010;-------------------- cust_id  cust_name ----------------------------------------No rows selected (0.351 seconds) 
issueID:CARBONDATA-2887
type:Bug
changed files:processing/src/main/java/org/apache/carbondata/processing/loading/steps/InputProcessorStepWithNoConverterImpl.java
texts:Filters in complex datatypes are not working in carbon using fileformat
Filters n complex datatypes are not working in carbon using fileformat
issueID:CARBONDATA-2888
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/SegmentIndexFileStore.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/CarbonTable.java
texts:Support multi level sdk read support for carbon tables
Current SDK reader cannot support  multi level folders. it would be better to read data under subfolders as well.
issueID:CARBONDATA-2889
type:Sub-task
changed files:processing/src/main/java/org/apache/carbondata/processing/store/writer/v3/CarbonFactDataWriterImplV3.java
core/src/main/java/org/apache/carbondata/core/datastore/page/LocalDictColumnPage.java
core/src/main/java/org/apache/carbondata/core/datastore/blocklet/BlockletEncodedColumnPage.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
core/src/main/java/org/apache/carbondata/core/datastore/page/ActualDataBasedFallbackEncoder.java
core/src/main/java/org/apache/carbondata/core/datastore/page/ColumnPage.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/reader/dimension/v3/DimensionChunkReaderV3.java
processing/src/main/java/org/apache/carbondata/processing/store/writer/v3/BlockletDataHolder.java
core/src/main/java/org/apache/carbondata/core/datastore/blocklet/EncodedBlocklet.java
integration/spark-datasource/src/main/scala/org/apache/carbondata/spark/vectorreader/VectorizedCarbonRecordReader.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/reader/dimension/AbstractDimensionChunkReader.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/reader/dimension/v3/DimensionChunkPageReaderV3.java
core/src/main/java/org/apache/carbondata/core/datastore/page/DecoderBasedFallbackEncoder.java
core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
texts:Support Decoder based fall back mechanism in Local Dictionary
Currently, when the fallback is initiated for a column page in case of local dictionary, we are keeping both encoded dataand actual data in memory and then we form the new column page without dictionary encoding and then at last we free the Encoded Column Page.Because of this offheap memory footprint increases. We can reduce the offheap memory footprint. This can be done using decoder based fallback mechanism.This means, no need to keep the actual data along with encoded data in encoded column page. We can keep only encoded data and to form a new column page, get the dictionary data from encoded column page by uncompressing and using dictionary data get the actual data using local dictionary generator and put it in new column page created and compress it again and give to consumer for writing blocklet.  The above process may slow down the loading, but it will reduces the memory footprint. So we can give a property which will decide whether to take current fallback procedure or decoder based fallback mechanism dring fallback
issueID:CARBONDATA-289
type:Bug
changed files:
texts:Support MB/M for table block size and update the doc about this new feature.
Support MB/M for table block size and update the doc about this new feature.
issueID:CARBONDATA-2894
type:Sub-task
changed files:store/sdk/src/main/java/org/apache/carbondata/sdk/file/AvroCarbonWriter.java
store/sdk/src/main/java/org/apache/carbondata/sdk/file/Field.java
store/sdk/src/main/java/org/apache/carbondata/sdk/file/CarbonWriterBuilder.java
texts:Add support for complex map type through spark carbon file format API

issueID:CARBONDATA-2895
type:Bug
changed files:
texts:[Batch-sort]Query result mismatch with Batch-sort in save to disk (sort temp files) scenario.
probelm: Query result mismatch with Batch-sort in save to disk (sort temp files) scenario.scenario:a) Configure batchsort but give batch size more than UnsafeMemoryManager.INSTANCE.getUsableMemory().b) Load data that is greater than batch size. Observe that unsafeMemoryManager save to disk happened as it cannot process one batch.  c) so load happens in 2 batch. d) When query the results. There result data rows is more than expected data rows.root cause:For each batch, createSortDataRows() will be called.Files saved to disk during sorting of previous batch was considered for this batch.solution:Files saved to disk during sorting of previous batch ,should not be considered for this batch.Hence use batchID as rangeID field of sorttempfiles.So getFilesToMergeSort() will select files of only this batch.
issueID:CARBONDATA-2896
type:New Feature
changed files:processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/SortParameters.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/comparator/UnsafeRowComparator.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RestructureEvaluatorImpl.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelFilterExecuterImpl.java
datamap/bloom/src/main/java/org/apache/carbondata/datamap/bloom/BloomCoarseGrainDataMap.java
core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelRangeGrtrThanEquaToFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/adaptive/AdaptiveCodec.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/dimension/legacy/DirectDictDimensionIndexCodec.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/holder/UnsafeSortTempFileChunkHolder.java
processing/src/main/java/org/apache/carbondata/processing/loading/CarbonDataLoadConfiguration.java
processing/src/main/java/org/apache/carbondata/processing/merger/CompactionResultSortProcessor.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/holder/UnsafeFinalMergePageHolder.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelRangeLessThanFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/dimension/legacy/HighCardDictDimensionIndexCodec.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RangeValueFilterExecuterImpl.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/holder/UnsafeInmemoryHolder.java
processing/src/main/java/org/apache/carbondata/processing/merger/RowResultMergerProcessor.java
core/src/main/java/org/apache/carbondata/core/datastore/columnar/BlockIndexerStorageForNoDictionary.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/dimension/legacy/DictDimensionIndexCodec.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/TableFieldStat.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/ExcludeFilterExecuterImpl.java
processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactDataHandlerModel.java
processing/src/main/java/org/apache/carbondata/processing/loading/steps/DataConverterProcessorStepImpl.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/dimension/legacy/IndexStorageEncoder.java
processing/src/main/java/org/apache/carbondata/processing/util/CarbonDataProcessorUtil.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/store/ColumnPageWrapper.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/EncodingFactory.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/IncludeFilterExecuterImpl.java
datamap/bloom/src/main/java/org/apache/carbondata/datamap/bloom/BloomDataMapBuilder.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/adaptive/AdaptiveIntegralCodec.java
processing/src/main/java/org/apache/carbondata/processing/loading/converter/impl/MeasureFieldConverterImpl.java
core/src/main/java/org/apache/carbondata/core/datastore/columnar/ColumnWithRowIdForNoDictionary.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/DefaultEncodingFactory.java
processing/src/main/java/org/apache/carbondata/processing/loading/converter/impl/FieldEncoderFactory.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/SortDataRows.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/NewRowComparator.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/IntermediateSortTempRowComparator.java
processing/src/main/java/org/apache/carbondata/processing/loading/steps/CarbonRowDataWriterProcessorStepImpl.java
core/src/main/java/org/apache/carbondata/core/util/DataTypeUtil.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/adaptive/AdaptiveDeltaFloatingCodec.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/reader/dimension/v3/DimensionChunkReaderV3.java
processing/src/main/java/org/apache/carbondata/processing/loading/steps/InputProcessorStepWithNoConverterImpl.java
processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactDataHandlerColumnar.java
core/src/main/java/org/apache/carbondata/core/datastore/columnar/BlockIndexerStorage.java
datamap/bloom/src/main/java/org/apache/carbondata/datamap/bloom/AbstractBloomDataMapWriter.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/SortStepRowHandler.java
core/src/main/java/org/apache/carbondata/core/scan/executor/util/QueryUtil.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/store/impl/unsafe/UnsafeVariableLengthDimensionDataChunkStore.java
core/src/main/java/org/apache/carbondata/core/util/NonDictionaryUtil.java
core/src/main/java/org/apache/carbondata/core/scan/result/BlockletScannedResult.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/adaptive/AdaptiveFloatingCodec.java
core/src/main/java/org/apache/carbondata/core/datastore/TableSpec.java
core/src/main/java/org/apache/carbondata/core/datastore/page/statistics/TablePageStatistics.java
processing/src/main/java/org/apache/carbondata/processing/store/TablePage.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/SortTempFileChunkHolder.java
core/src/main/java/org/apache/carbondata/core/scan/filter/FilterUtil.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/RowLevelRangeFilterResolverImpl.java
core/src/main/java/org/apache/carbondata/core/datastore/columnar/BlockIndexerStorageForShort.java
core/src/main/java/org/apache/carbondata/core/datastore/columnar/BlockIndexerStorageForNoInvertedIndexForShort.java
datamap/bloom/src/main/java/org/apache/carbondata/datamap/bloom/BloomDataMapWriter.java
processing/src/main/java/org/apache/carbondata/processing/loading/partition/impl/RawRowComparator.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/dimension/legacy/ComplexDimensionIndexCodec.java
core/src/main/java/org/apache/carbondata/core/datastore/page/key/TablePageKey.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/ColumnPageEncoder.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/adaptive/AdaptiveDeltaIntegralCodec.java
core/src/main/java/org/apache/carbondata/core/util/CarbonUnsafeUtil.java
processing/src/main/java/org/apache/carbondata/processing/loading/row/IntermediateSortTempRow.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelRangeLessThanEqualFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/scan/result/impl/FilterQueryScannedResult.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelRangeGrtThanFiterExecuterImpl.java
texts:Adaptive encoding for primitive data types
Currently Encoding and Decoding is present only for Dictionary, Measure Columns, but for no dictionary Primitive types encoding is absent.Encoding is a technique used to reduce the storage size and  after all these encoding, result will be compressed with snappy compression to further reduce the storage size.With this feature, we support encoding on the no dictionary primitive data types also.
issueID:CARBONDATA-2897
type:Bug
changed files:
texts:Assign to datamap only for supported expression in datamap chooser
Related discussion in community: http://apache-carbondata-dev-mailing-list-archive.1130556.n5.nabble.com/Issue-Bloomfilter-datamap-td63254.html 
issueID:CARBONDATA-2898
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/datamap/DataMapStoreManager.java
core/src/main/java/org/apache/carbondata/core/datastore/page/statistics/PrimitivePageStatsCollector.java
processing/src/main/java/org/apache/carbondata/processing/loading/model/CarbonLoadModelBuilder.java
core/src/main/java/org/apache/carbondata/core/datastore/impl/FileFactory.java
texts:Double boundary condition and clear datamaps are not working properly
1.DataMaps are not clearing properly as it creates temp table for each request.  In double value bounadry cases loading fails as carbon does not handle infinite properly. Added validations for sort columns cannot be used while inferring the schema
issueID:CARBONDATA-2899
type:Improvement
changed files:
texts:Add MV modules to assembly JAR
When compile the project with profile -Pmv, MV module class should be added into assembly JAR
issueID:CARBONDATA-29
type:New Feature
changed files:core/src/main/java/org/apache/carbondata/core/scan/filter/executer/IncludeFilterExecuterImpl.java
processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactDataHandlerColumnar.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/ExcludeFilterExecuterImpl.java
texts:Make inverted index can be configurable
We should make inverted index can be configurable, the user can use a parameter to open/close this function during loading data.
issueID:CARBONDATA-290
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/metadata/AbsoluteTableIdentifier.java
texts:When part of table name has database name, then query will show  segment path not found
When part of table name has database name, ex: in default database,  CREATE TABLE IF NOT EXISTS t3default then load and then query, we will get the exception that segment not found
issueID:CARBONDATA-2900
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/util/SessionParams.java
texts:Add dynamic configuration support for some system properties
Following system level properties are added for dynamic configuration. User can se SET command to set it.carbon.number.of.cores.while.loadingcarbon.number.of.cores.while.compactingcarbon.blockletgroup.size.in.mbcarbon.major.compaction.sizecarbon.enable.vector.readerenable.unsafe.in.query.processing
issueID:CARBONDATA-2901
type:Bug
changed files:processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/UnsafeSortDataRows.java
texts:Problem: Jvm crash in Load scenario when unsafe memory allocation is failed.
Problem: Jvm crash in Load scenario when unsafe memory allocation is failed.scenario: a) Have many cores while loading. suggested more than 10. &#91;carbon.number.of.cores.while.loading&#93;b) Load huge data with local sort, more than 5GB (keeping default unsafe memory manager as 512 MB)c) when task failes due to not enough unsafae memory, JVM crashes with SIGSEGV.root casue:while sorting, all iterator threads are waiting at UnsafeSortDataRows.addRowBatch as all iterator works on one row page.Only one iterator thread will try to allocate memory. Before that it has freed current page in handlePreviousPage().When allocate memory failed, row page will still have that old reference. next thread will again use same reference and call handlePreviousPage().So, Jvm crashes as freed memory is accessed.solution:When allocation failed, set row page reference to null.So, that next thread will not do any operation.
issueID:CARBONDATA-2902
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/util/BlockletDataMapUtil.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockletDataMapFactory.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockletDataMap.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonInputFormat.java
core/src/main/java/org/apache/carbondata/core/profiler/ExplainCollector.java
core/src/main/java/org/apache/carbondata/core/profiler/TablePruningInfo.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockDataMap.java
texts:Fix showing negative pruning result for explain command
 when CACHE_LEVEL is BLOCK or carbon data file is LegacyStore,   pruned result of default datamap are assigned with blocklet_id = -1, which means all blocklets in block need to be scanned. But explain command takes pruned result size as blocklets hit.If only 2 of 3 blocklet in one block is hit, negative pruning result  `1 (block) - 2(blocklet）= -1 ` occur
issueID:CARBONDATA-2903
type:Improvement
changed files:
texts:Fix compiler warnings
When build using mvn, there are some compiler warnings. They should be fixed.
issueID:CARBONDATA-2905
type:Improvement
changed files:
texts:Should allow set stream property on streaming table
For streaming table with table property "streaming"="true", we should allow set the streaming table property to false
issueID:CARBONDATA-2906
type:Improvement
changed files:common/src/main/java/org/apache/carbondata/common/Strings.java
texts:Show segment data size in SHOW SEGMENT command
In SHOW SEGMENT command, output the segment data size and index size so that user can check the size of each segment easier.
issueID:CARBONDATA-2907
type:Improvement
changed files:processing/src/main/java/org/apache/carbondata/processing/store/writer/v3/CarbonFactDataWriterImplV3.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/TableSchemaBuilder.java
texts:Support setting blocklet size in table property
When creating table, should support setting blocklet size in MB.
issueID:CARBONDATA-2908
type:Bug
changed files:
texts:the option of sort_scope don&#39;t effects while creating table by data frame

issueID:CARBONDATA-2909
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/readcommitter/TableStatusReadCommittedScope.java
core/src/main/java/org/apache/carbondata/core/util/BlockletDataMapUtil.java
examples/spark2/src/main/java/org/apache/carbondata/examples/sdk/CarbonReaderExample.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockletDataMapFactory.java
core/src/main/java/org/apache/carbondata/core/reader/CarbonHeaderReader.java
core/src/main/java/org/apache/carbondata/core/writer/CarbonIndexFileMergeWriter.java
core/src/main/java/org/apache/carbondata/core/indexstore/BlockletDataMapIndexStore.java
core/src/main/java/org/apache/carbondata/core/util/DataFileFooterConverter.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonFileInputFormat.java
core/src/main/java/org/apache/carbondata/core/datastore/filesystem/AbstractDFSCarbonFile.java
store/sdk/src/main/java/org/apache/carbondata/sdk/file/CSVCarbonWriter.java
core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
examples/spark2/src/main/java/org/apache/carbondata/examples/sdk/SDKS3Example.java
core/src/main/java/org/apache/carbondata/core/util/DataFileFooterConverterV3.java
core/src/main/java/org/apache/carbondata/core/datamap/dev/DataMapModel.java
datamap/bloom/src/main/java/org/apache/carbondata/datamap/bloom/BloomCoarseGrainDataMapFactory.java
core/src/main/java/org/apache/carbondata/core/metadata/SegmentFileStore.java
core/src/main/java/org/apache/carbondata/core/indexstore/TableBlockIndexUniqueIdentifierWrapper.java
core/src/main/java/org/apache/carbondata/core/reader/CarbonIndexFileReader.java
core/src/main/java/org/apache/carbondata/core/statusmanager/SegmentStatusManager.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockDataMap.java
core/src/main/java/org/apache/carbondata/core/util/AbstractDataFileFooterConverter.java
core/src/main/java/org/apache/carbondata/core/reader/ThriftReader.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockletDataMapModel.java
core/src/main/java/org/apache/carbondata/core/util/DataFileFooterConverter2.java
store/sdk/src/main/java/org/apache/carbondata/sdk/file/AvroCarbonWriter.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/SegmentIndexFileStore.java
core/src/main/java/org/apache/carbondata/core/mutate/CarbonUpdateUtil.java
core/src/main/java/org/apache/carbondata/core/readcommitter/ReadCommittedScope.java
processing/src/main/java/org/apache/carbondata/processing/util/CarbonLoaderUtil.java
core/src/main/java/org/apache/carbondata/core/datamap/DataMapUtil.java
store/sdk/src/main/java/org/apache/carbondata/sdk/file/CarbonReaderBuilder.java
core/src/main/java/org/apache/carbondata/core/datamap/Segment.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonTableOutputFormat.java
core/src/main/java/org/apache/carbondata/core/datastore/impl/FileFactory.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/SchemaReader.java
processing/src/main/java/org/apache/carbondata/processing/merger/CarbonDataMergerUtil.java
core/src/main/java/org/apache/carbondata/core/indexstore/BlockletDataMapIndexWrapper.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonTableInputFormat.java
store/sdk/src/main/java/org/apache/carbondata/sdk/file/JsonCarbonWriter.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonOutputCommitter.java
datamap/lucene/src/main/java/org/apache/carbondata/datamap/lucene/LuceneFineGrainDataMapFactory.java
core/src/main/java/org/apache/carbondata/core/datamap/dev/DataMap.java
datamap/bloom/src/main/java/org/apache/carbondata/datamap/bloom/BloomDataMapModel.java
store/sdk/src/main/java/org/apache/carbondata/sdk/file/CarbonWriterBuilder.java
texts:Support Multiple User reading and writing through SDK.

issueID:CARBONDATA-291
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/stats/QueryStatistic.java
texts:Some STATISTIC log still present even though disable STATISTIC
The following STATISTIC log still present even though disable STATISTIC," STATISTIC Time taken for Carbon Optimizer to optimize:  26 "
issueID:CARBONDATA-2910
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/scan/expression/ColumnExpression.java
core/src/main/java/org/apache/carbondata/core/util/BlockletDataMapUtil.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockletDataMapFactory.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/CarbonTable.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonInputFormat.java
core/src/main/java/org/apache/carbondata/core/scan/executor/impl/AbstractQueryExecutor.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonFileInputFormat.java
core/src/main/java/org/apache/carbondata/core/datamap/dev/fgdatamap/FineGrainDataMap.java
core/src/main/java/org/apache/carbondata/core/scan/executor/util/QueryUtil.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockDataMap.java
core/src/main/java/org/apache/carbondata/core/datamap/dev/cgdatamap/CoarseGrainDataMap.java
core/src/main/java/org/apache/carbondata/core/scan/model/QueryModelBuilder.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonTableInputFormat.java
core/src/main/java/org/apache/carbondata/core/datamap/dev/DataMap.java
core/src/main/java/org/apache/carbondata/core/scan/model/QueryModel.java
core/src/main/java/org/apache/carbondata/core/datamap/TableDataMap.java
core/src/main/java/org/apache/carbondata/core/util/AbstractDataFileFooterConverter.java
texts:Support backward compatability in fileformat and support different sort colums per load
Currently if the data is loaded by old version with all dictionary exclude carbon fileformat cannot read. And also if the sort columns are given different per load while loading through SDK does not work,
issueID:CARBONDATA-2911
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/scan/result/iterator/AbstractDetailQueryResultIterator.java
texts:Remove unused BTree related code

issueID:CARBONDATA-2915
type:Improvement
changed files:processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/SortParameters.java
processing/src/main/java/org/apache/carbondata/processing/util/CarbonDataProcessorUtil.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/SingleThreadFinalSortFilesMerger.java
processing/src/main/java/org/apache/carbondata/processing/loading/DataLoadProcessBuilder.java
core/src/main/java/org/apache/carbondata/core/util/CarbonProperties.java
texts:Updates to CarbonData documentation and structure
 1.Split Our carbondata command into DDL and DML2.Add Presto integration along with Spark into quick start3.Add a master reference manual which lists all the commands supported in carbondata.This manual shall have links to DDL and DML supported4.Add a introduction to carbondata covering architecture,design and features supported5.Merge FAQ and troubleshooting documents into single document6.Add a separate md file to explain user how to navigate across our documentation7.Add the TOC (Table of Contents) to all the md files which has multiple sections8.Add list of supported properties at the beginning of each DDL or DML so that user knows all the properties that are supported9.Rewrite the configuration properties description to explain the property in bit more detail and also highlight when to use the command and any caveats10.ReOrder our configuration properties table to group features wise11.Update our webpage(carbondata.apache.org) to have a better navigation for documentation section12.Change the grammar and sentences
issueID:CARBONDATA-2916
type:New Feature
changed files:core/src/main/java/org/apache/carbondata/core/datastore/chunk/impl/DimensionRawColumnChunk.java
core/src/main/java/org/apache/carbondata/core/util/path/CarbonTablePath.java
store/sdk/src/main/java/org/apache/carbondata/sdk/file/CarbonWriterBuilder.java
core/src/main/java/org/apache/carbondata/core/util/ByteUtil.java
tools/cli/src/main/java/org/apache/carbondata/tool/DataSummary.java
tools/cli/src/main/java/org/apache/carbondata/tool/CarbonCli.java
tools/cli/src/main/java/org/apache/carbondata/tool/DataFile.java
tools/cli/src/main/java/org/apache/carbondata/tool/TableFormatter.java
processing/src/main/java/org/apache/carbondata/processing/loading/model/CarbonLoadModelBuilder.java
core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
tools/cli/src/main/java/org/apache/carbondata/tool/ShardPrinter.java
texts:Support CarbonCli tool for data summary
When I am tuning carbon performance, very often that I want to check the metadata in carbon files without launching spark shell or sql. In order to do that, I am writing a tool to print metadata information of a given data folder. Currently, I am planning to do like this:usage: CarbonCli a,-all                    print all information b,-tblProperties          print table properties c,-column <column name>   column to print statistics -cmd <command name>         command to execute, supported commands are:                             summary d,-detailSize             print each blocklet size h,-help                   print this message m,-showSegment            print segment information p,-path <path>            the path which contains carbondata files,                             nested folder is supported s,-schema                 print the schemaIn first phase, I think “summary” command is high priority, and developers can add more command in the future.
issueID:CARBONDATA-2919
type:New Feature
changed files:core/src/main/java/org/apache/carbondata/core/locks/HdfsFileLock.java
core/src/main/java/org/apache/carbondata/core/locks/S3FileLock.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/CarbonTable.java
core/src/main/java/org/apache/carbondata/core/locks/ICarbonLock.java
core/src/main/java/org/apache/carbondata/core/util/SessionParams.java
processing/src/main/java/org/apache/carbondata/processing/loading/parser/impl/RowParserImpl.java
streaming/src/main/java/org/apache/carbondata/streaming/segment/StreamSegment.java
integration/spark-common/src/main/java/org/apache/carbondata/spark/util/Util.java
core/src/main/java/org/apache/carbondata/core/locks/LocalFileLock.java
core/src/main/java/org/apache/carbondata/core/locks/AbstractCarbonLock.java
texts:StreamSQL support ingest from Kafka

issueID:CARBONDATA-292
type:Improvement
changed files:
texts:add COLUMNDICT operation info in DML operation guide
there is no COLUMNDICT operation guide in DML-Operations-on-Carbon.md, so need to add.
issueID:CARBONDATA-2921
type:Improvement
changed files:
texts:support long string columns with spark FileFormat and SDK with "long_string_columns" TableProperties

issueID:CARBONDATA-2922
type:Sub-task
changed files:store/sdk/src/main/java/org/apache/carbondata/sdk/file/Field.java
store/sdk/src/main/java/org/apache/carbondata/sdk/file/CarbonWriterBuilder.java
texts:support long string columns with spark FileFormat and SDK with "long_string_columns" TableProperties
CSV and Json SDk writer takes carbonschema, hence directly varchar can be given.but AVRO writer needs this table property.Also spark file format needs this table property to convert string columns to varchar columns. 
issueID:CARBONDATA-2924
type:Bug
changed files:store/sdk/src/main/java/org/apache/carbondata/sdk/file/AvroCarbonWriter.java
core/src/main/java/org/apache/carbondata/core/indexstore/TableBlockIndexUniqueIdentifierWrapper.java
datamap/examples/src/minmaxdatamap/main/java/org/apache/carbondata/datamap/examples/MinMaxIndexDataMapFactory.java
core/src/main/java/org/apache/carbondata/core/indexstore/BlockletDataMapIndexStore.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/SortStepRowHandler.java
store/sdk/src/main/java/org/apache/carbondata/sdk/file/CarbonWriterBuilder.java
texts:Fix parsing issue for map as a nested array child and change the error message in sort column validation for SDK
Issue1:Parsing exception thrown while parsing map as child of nested array type like array<array<map>> and struct<array<map>> (Image attached)Issue2:Wrong error message is displayed when map type is specified as sort column while writing through SDK (Image attached)Issue3:When complex type data  length is more than short data type length for one row during loading then NegativeArraySize exception is thrownjava.lang.NegativeArraySizeException at org.apache.carbondata.processing.loading.sort.SortStepRowHandler.unpackNoSortFromBytes(SortStepRowHandler.java:271) at org.apache.carbondata.processing.loading.sort.SortStepRowHandler.readRowFromMemoryWithNoSortFieldConvert(SortStepRowHandler.java:461) at org.apache.carbondata.processing.loading.sort.unsafe.UnsafeCarbonRowPage.getRow(UnsafeCarbonRowPage.java:93) at org.apache.carbondata.processing.loading.sort.unsafe.holder.UnsafeInmemoryHolder.readRow(UnsafeInmemoryHolder.java:61)
issueID:CARBONDATA-2925
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/indexstore/BlockletDetailInfo.java
texts:Wrong data displayed for spark file format if carbon file has mtuiple blocklet
// LoadDatadef loadData(spark: SparkSession): Unit ={ spark.experimental.extraOptimizations=Seq(new CarbonFileIndexReplaceRule()) val fields=new Array&#91;Field&#93;(8) fields(0)=new Field("myid",DataTypes.INT); fields(1)=new Field("event_id",DataTypes.STRING); fields(2)=new Field("eve_time",DataTypes.DATE); fields(3)=new Field("ingestion_time",DataTypes.TIMESTAMP); fields(4)=new Field("alldate",DataTypes.createArrayType(DataTypes.DATE)); fields(5)=new Field("subject",DataTypes.STRING); fields(6)=new Field("from_email",DataTypes.STRING); fields(7)=new Field("sal",DataTypes.DOUBLE); import scala.collection.JavaConverters._ val options=Map("bad_records_action">"FORCE","complex_delimiter_level_1">"$").asJava val writer=CarbonWriter.builder().outputPath("D:/data/sdkpath/1").sortBy(Array("myid","ingestion_time","event_id")).withLoadOptions(options).buildWriterForCSVInput(new Schema(fields)) val timeF=new SimpleDateFormat("yyyy-MM-dd HH:mm:ss") val date_F=new SimpleDateFormat("yyyy-MM-dd") for(i<-0 to 5000000){ val time=new Date(System.currentTimeMillis()) writer.write(Array(""+i,"event_"+i,""+date_F.format(time),""+timeF.format(time),""+date_F.format(time)+"$"+date_F.format(time),"Subject_0","FromEmail",""+new Random().nextDouble())) } writer.close()}  // Query def queryUsingFormat(spark: SparkSession): Unit ={ spark.experimental.extraOptimizations=Seq(new CarbonFileIndexReplaceRule()) val df= spark.read.format("carbon").load("D:/data/sdkpath/1") println("==============================="+df.count())}   it is giving  2496000Expected :-5000001  
issueID:CARBONDATA-2926
type:Bug
changed files:processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactDataHandlerModel.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/TableSchemaBuilder.java
core/src/main/java/org/apache/carbondata/core/datastore/TableSpec.java
processing/src/main/java/org/apache/carbondata/processing/store/TablePage.java
store/sdk/src/main/java/org/apache/carbondata/sdk/file/CarbonWriterBuilder.java
texts:ArrayIndexOutOfBoundException if varchar column is present before dictionary columns along with empty sort_columns.
ArrayIndexOutOfBoundException if varchar column is present before dictionary columns along with empty sort_columns.cause:CarbonFactDataHandlerColumnar.isVarcharColumnFull() method uses model.getVarcharDimIdxInNoDict()and index of varchar column in no dictonary array became negative.currently index was calculated based on ordinal-number of dictionary columns. This can go negative in no_sort column case,solution:take the varchar dimension index from no dictionary array from at runtime based on schema.
issueID:CARBONDATA-2927
type:Bug
changed files:processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/SortDataRows.java
core/src/main/java/org/apache/carbondata/core/indexstore/UnsafeMemoryDMStore.java
core/src/main/java/org/apache/carbondata/core/memory/UnsafeMemoryManager.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/UnsafeCarbonRowPage.java
processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactDataHandlerColumnar.java
core/src/main/java/org/apache/carbondata/core/util/ReUsableByteArrayDataOutputStream.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/SortStepRowHandler.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/UnsafeSortDataRows.java
texts:Multiple issue fixes for varchar column and complex columns that grows more than 2MB
Fixed: 1. varchar data length is more than 2MB, buffer overflow exception (thread local row buffer)root casue*: thread* loaclbuffer was hardcoded with 2MB. solution: grow dynamically based on the row size. 2. read data from carbon file having one row of varchar data with 150 MB length is very slow.root casue:  At UnsafeDMStore, ensure memory is just incresing by 8KB each time and lot of time malloc and free happens before reaching 150MB. hence very slow performance.solution: directly check and allocate the required size. 3. Jvm crash when data size is more than 128 MB in unsafe sort step.root cause: unsafeCarbonRowPage is of 128MB, so if data is more than 128MB  for one row, we access block beyond allocated, leading to JVM crash. solution: validate the size before access and prompt user to increase unsafe memory. (by carbon property)
issueID:CARBONDATA-2929
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockletDataMap.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonInputFormat.java
core/src/main/java/org/apache/carbondata/core/profiler/ExplainCollector.java
core/src/main/java/org/apache/carbondata/core/profiler/TablePruningInfo.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockDataMap.java
texts:Add block skipped info for explain command

issueID:CARBONDATA-293
type:Improvement
changed files:
texts:Add scan_blocklet_num for query statistics
Add scan_blocklet_num for query statistics
issueID:CARBONDATA-2930
type:Sub-task
changed files:core/src/main/java/org/apache/carbondata/core/util/BlockletDataMapUtil.java
integration/spark2/src/main/scala/org/apache/carbondata/stream/CarbonStreamRecordReader.java
core/src/main/java/org/apache/carbondata/core/util/CarbonMetadataUtil.java
core/src/main/java/org/apache/carbondata/core/datastore/compression/CompressorFactory.java
streaming/src/main/java/org/apache/carbondata/streaming/CarbonStreamRecordWriter.java
texts:Support customize column compressor
Support customize compressor to compress the final store.User can create their own compressor and specify it during creating table or loading data.
issueID:CARBONDATA-2932
type:Bug
changed files:examples/spark2/src/main/java/org/apache/carbondata/examples/sdk/CarbonReaderExample.java
texts:CarbonReaderExample throw some exception: Projection can&#39;t be empty
run org.apache.carbondata.examples.sdk.CarbonReaderExample and then some exception:Finishedjava.lang.RuntimeException: Projection can't be empty at org.apache.carbondata.hadoop.api.CarbonInputFormat.setColumnProjection(CarbonInputFormat.java:219) at org.apache.carbondata.hadoop.api.CarbonFileInputFormat.getSplits(CarbonFileInputFormat.java:155) at org.apache.carbondata.sdk.file.CarbonReaderBuilder.build(CarbonReaderBuilder.java:213) at org.apache.carbondata.examples.sdk.CarbonReaderExample.main(CarbonReaderExample.java:121)Projection can't be empty
issueID:CARBONDATA-2933
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/util/AbstractDataFileFooterConverter.java
core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
texts:Fix errors in spelling

issueID:CARBONDATA-2935
type:Sub-task
changed files:processing/src/main/java/org/apache/carbondata/processing/store/writer/v3/CarbonFactDataWriterImplV3.java
processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactDataHandlerColumnar.java
processing/src/main/java/org/apache/carbondata/processing/store/writer/AbstractFactDataWriter.java
processing/src/main/java/org/apache/carbondata/processing/store/writer/CarbonFactDataWriter.java
texts:Write is_sorted field in file footer
write a new field (is_sorted) in file footer to indicate whether the file data is sorted, it is used for compaction to decide whether to use merge sort or not
issueID:CARBONDATA-2938
type:Improvement
changed files:
texts:Update comment of blockletId in IndexDataMapRebuildRDD

issueID:CARBONDATA-294
type:Bug
changed files:
texts:Timestamp Data Error
In CarbonExample, When Loading 2015/7/23 as a Timestamp, when querying, it will return 2015-01-23 xx:xx:xx:xx. Six months have been stolen.
issueID:CARBONDATA-2940
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/datastore/blocklet/BlockletEncodedColumnPage.java
core/src/main/java/org/apache/carbondata/core/scan/collector/impl/DictionaryBasedResultCollector.java
texts:Fix BufferUnderFlowException for ComplexPushDown

issueID:CARBONDATA-2942
type:Sub-task
changed files:core/src/main/java/org/apache/carbondata/core/scan/filter/executer/ImplicitIncludeFilterExecutorImpl.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RestructureExcludeFilterExecutorImpl.java
core/src/main/java/org/apache/carbondata/core/datastore/page/statistics/SimpleStatsResult.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockletDataMapRowIndexes.java
core/src/main/java/org/apache/carbondata/core/stream/StreamPruner.java
core/src/main/java/org/apache/carbondata/core/util/BlockletDataMapUtil.java
core/src/main/java/org/apache/carbondata/core/datastore/page/statistics/DummyStatsCollector.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/AndFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/reader/dimension/v3/DimensionChunkReaderV3.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/AbstractRawColumnChunk.java
core/src/main/java/org/apache/carbondata/core/scan/scanner/impl/BlockletFilterScanner.java
core/src/main/java/org/apache/carbondata/core/util/CarbonProperties.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/TrueFilterExecutor.java
core/src/main/java/org/apache/carbondata/core/datastore/page/statistics/LVStringStatsCollector.java
processing/src/main/java/org/apache/carbondata/processing/store/writer/v3/CarbonFactDataWriterImplV3.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelRangeGrtrThanEquaToFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/datastore/DataRefNode.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/ImplicitColumnFilterExecutor.java
core/src/main/java/org/apache/carbondata/core/indexstore/row/DataMapRowImpl.java
core/src/main/java/org/apache/carbondata/core/metadata/blocklet/index/BlockletMinMaxIndex.java
core/src/main/java/org/apache/carbondata/core/indexstore/row/DataMapRow.java
core/src/main/java/org/apache/carbondata/core/datastore/page/statistics/TablePageStatistics.java
core/src/main/java/org/apache/carbondata/core/scan/filter/FilterExpressionProcessor.java
streaming/src/main/java/org/apache/carbondata/streaming/segment/StreamSegment.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockDataMap.java
core/src/main/java/org/apache/carbondata/core/util/AbstractDataFileFooterConverter.java
datamap/examples/src/minmaxdatamap/main/java/org/apache/carbondata/datamap/examples/MinMaxIndexDataMap.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelRangeLessThanFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RangeValueFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/datastore/page/statistics/KeyPageStatsCollector.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockletDataMap.java
core/src/main/java/org/apache/carbondata/core/indexstore/UnsafeMemoryDMStore.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/ColumnPageEncoder.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockletDataRefNode.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/ExcludeFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/FalseFilterExecutor.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelRangeLessThanEqualFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/FilterExecuter.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
core/src/main/java/org/apache/carbondata/core/util/CarbonMetadataUtil.java
integration/spark2/src/main/scala/org/apache/carbondata/stream/CarbonStreamRecordReader.java
core/src/main/java/org/apache/carbondata/core/indexstore/schema/SchemaGenerator.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/OrFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RestructureIncludeFilterExecutorImpl.java
core/src/main/java/org/apache/carbondata/core/datastore/page/statistics/PrimitivePageStatsCollector.java
core/src/main/java/org/apache/carbondata/core/indexstore/row/UnsafeDataMapRow.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/IncludeFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelRangeGrtThanFiterExecuterImpl.java
texts:Add read and write support for writing min max based on configurable bytes count
Add read and write support for writing min max based on configurable bytes count for transactional and non transactional table which covers standard carbon table, File format and SDK
issueID:CARBONDATA-2945
type:New Feature
changed files:
texts:Support JSON record in StreamSQL
Support ingest from JSON record in Kafka/socket stream source in StreamSQL
issueID:CARBONDATA-2947
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/datastore/chunk/impl/AbstractDimensionColumnPage.java
processing/src/main/java/org/apache/carbondata/processing/loading/converter/impl/FieldEncoderFactory.java
core/src/main/java/org/apache/carbondata/core/util/DataTypeUtil.java
datamap/bloom/src/main/java/org/apache/carbondata/datamap/bloom/DataConvertUtil.java
processing/src/main/java/org/apache/carbondata/processing/loading/converter/impl/RowConverterImpl.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/SortStepRowHandler.java
core/src/main/java/org/apache/carbondata/core/scan/executor/util/QueryUtil.java
core/src/main/java/org/apache/carbondata/core/util/CarbonUnsafeUtil.java
datamap/bloom/src/main/java/org/apache/carbondata/datamap/bloom/BloomCoarseGrainDataMap.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/DimensionColumnPage.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/store/ColumnPageWrapper.java
processing/src/main/java/org/apache/carbondata/processing/store/TablePage.java
streaming/src/main/java/org/apache/carbondata/streaming/CarbonStreamRecordWriter.java
processing/src/main/java/org/apache/carbondata/processing/merger/CompactionResultSortProcessor.java
core/src/main/java/org/apache/carbondata/core/scan/filter/FilterUtil.java
processing/src/main/java/org/apache/carbondata/processing/loading/converter/impl/MeasureFieldConverterImpl.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/DefaultEncodingFactory.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelRangeLessThanFilterExecuterImpl.java
texts:Adaptive encoding support for timestamp no dictionary and Refactor ColumnPageWrapper

issueID:CARBONDATA-2948
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/datastore/page/LazyColumnPage.java
core/src/main/java/org/apache/carbondata/core/datastore/page/VarLengthColumnPageBase.java
core/src/main/java/org/apache/carbondata/core/datastore/page/statistics/DummyStatsCollector.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/adaptive/AdaptiveDeltaFloatingCodec.java
core/src/main/java/org/apache/carbondata/core/metadata/datatype/DataType.java
core/src/main/java/org/apache/carbondata/core/util/DataTypeUtil.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/ColumnPageEncoderMeta.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/SortStepRowHandler.java
core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
core/src/main/java/org/apache/carbondata/core/datastore/page/statistics/LVStringStatsCollector.java
core/src/main/java/org/apache/carbondata/core/datastore/page/UnsafeFixLengthColumnPage.java
core/src/main/java/org/apache/carbondata/core/datastore/page/DecimalColumnPage.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/adaptive/AdaptiveCodec.java
core/src/main/java/org/apache/carbondata/core/datastore/page/ColumnPage.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/adaptive/AdaptiveFloatingCodec.java
core/src/main/java/org/apache/carbondata/core/datastore/page/SafeFixLengthColumnPage.java
core/src/main/java/org/apache/carbondata/core/datastore/page/statistics/ColumnPageStatsCollector.java
core/src/main/java/org/apache/carbondata/core/util/comparator/Comparator.java
integration/spark-datasource/src/main/scala/org/apache/carbondata/spark/vectorreader/VectorizedCarbonRecordReader.java
core/src/main/java/org/apache/carbondata/core/scan/filter/FilterUtil.java
integration/spark-datasource/src/main/scala/org/apache/carbondata/spark/vectorreader/ColumnarVectorWrapper.java
core/src/main/java/org/apache/carbondata/core/datastore/page/LocalDictColumnPage.java
core/src/main/java/org/apache/carbondata/core/scan/result/vector/impl/CarbonColumnVectorImpl.java
core/src/main/java/org/apache/carbondata/core/util/ByteUtil.java
store/sdk/src/main/java/org/apache/carbondata/sdk/file/AvroCarbonWriter.java
core/src/main/java/org/apache/carbondata/core/datastore/page/statistics/KeyPageStatsCollector.java
core/src/main/java/org/apache/carbondata/core/scan/collector/impl/AbstractScannedResultCollector.java
core/src/main/java/org/apache/carbondata/core/util/CarbonUnsafeUtil.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/ExcludeFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/scan/result/vector/CarbonColumnVector.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/store/ColumnPageWrapper.java
core/src/main/java/org/apache/carbondata/core/datastore/page/statistics/PrimitivePageStatsCollector.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/IncludeFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/DefaultEncodingFactory.java
core/src/main/java/org/apache/carbondata/core/scan/result/vector/MeasureDataVectorProcessor.java
core/src/main/java/org/apache/carbondata/core/metadata/converter/ThriftWrapperSchemaConverterImpl.java
integration/presto/src/main/java/org/apache/carbondata/presto/CarbonColumnVectorWrapper.java
texts:Support Float and Byte Datatypes for SDK and DataSource
Currently float is supported by internally storing the data as double and changing the data type to Double. This poses some problems while using SparkCarbonFileFormat for reading the float type data.Internally as the data type is changed from Float to Double therefore the data is retrieved as a Double page instead of float. If the user tried to create a table using file format by specifying the datatype as float for any column then the query will fail. User is restricted to use double to retrieve the data.
issueID:CARBONDATA-295
type:Improvement
changed files:
texts:Abstract Snappy interface and seperate it from Compressor interface
Currently, we only have snappy compressor who extends form Compressor interface, for future expansion, we need to abstract Snappy interface and seperate it from Compressor interface, it means Compressor interface is the parent of all compressors, and SnappyCompressor interface and the other compressor's interface(or abstract class) should extends Compressor interface, as to different data type for different compressor, it would extend its own interface/abstract class.for example: Compressor -> SnappyCompressor -> SnappyDoubleCompression.
issueID:CARBONDATA-2950
type:Bug
changed files:integration/spark-common/src/main/java/org/apache/carbondata/spark/util/Util.java
texts:Alter table add columns fails for hive table in carbon session for spark version above 2.1
spark does not support add columns in spark-2.1, but it is supported in 2.2 and abovewhen add column is fired for hive table in carbon session, for spark -version above 2.1, it throws error as unsupported operation on hive table
issueID:CARBONDATA-2952
type:Sub-task
changed files:store/sdk/src/main/java/org/apache/carbondata/sdk/file/CarbonReaderBuilder.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
store/sdk/src/main/java/org/apache/carbondata/sdk/file/CarbonReader.java
texts:Provide CarbonReader C++ interface for SDK
Provide CarbonReader C++ interface for SDK1.init carbonreader2.config data path and tablename3.configure projection4.build carbon reader5.hasNext6.readNextRow7.close
issueID:CARBONDATA-2953
type:Bug
changed files:processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/IntermediateSortTempRowComparator.java
processing/src/main/java/org/apache/carbondata/processing/loading/partition/impl/RawRowComparator.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/NewRowComparator.java
texts:Dataload fails when sort column is given, and query returns null value from another session
when dataload is done with sort columns, it fails with following exeptionsjava.lang.ClassCastException: java.lang.Integer cannot be cast to [B at org.apache.carbondata.processing.sort.sortdata.IntermediateSortTempRowComparator.compare(IntermediateSortTempRowComparator.java:71) at org.apache.carbondata.processing.loading.sort.unsafe.holder.UnsafeInmemoryHolder.compareTo(UnsafeInmemoryHolder.java:71) at org.apache.carbondata.processing.loading.sort.unsafe.holder.UnsafeInmemoryHolder.compareTo(UnsafeInmemoryHolder.java:26) at java.util.PriorityQueue.siftUpComparable(PriorityQueue.java:656) at java.util.PriorityQueue.siftUp(PriorityQueue.java:647) at java.util.PriorityQueue.offer(PriorityQueue.java:344) at java.util.PriorityQueue.add(PriorityQueue.java:321) at org.apache.carbondata.processing.loading.sort.unsafe.merger.UnsafeSingleThreadFinalSortFilesMerger.startSorting(UnsafeSingleThreadFinalSortFilesMerger.java:129) at org.apache.carbondata.processing.loading.sort.unsafe.merger.UnsafeSingleThreadFinalSortFilesMerger.startFinalMerge(UnsafeSingleThreadFinalSortFilesMerger.java:94) at org.apache.carbondata.processing.loading.sort.impl.UnsafeParallelReadMergeSorterImpl.sort(UnsafeParallelReadMergeSorterImpl.java:110) at org.apache.carbondata.processing.loading.steps.SortProcessorStepImpl.execute(SortProcessorStepImpl.java:55) at org.apache.carbondata.processing.loading.steps.DataWriterProcessorStepImpl.execute(DataWriterProcessorStepImpl.java:112) at org.apache.carbondata.processing.loading.DataLoadExecutor.execute(DataLoadExecutor.java:51) at org.apache.carbondata.spark.rdd.NewCarbonDataLoadRDD$$anon$1.<init>(NewCarbonDataLoadRDD.scala:212) at org.apache.carbondata.spark.rdd.NewCarbonDataLoadRDD.internalCompute(NewCarbonDataLoadRDD.scala:188) at org.apache.carbondata.spark.rdd.CarbonRDD.compute(CarbonRDD.scala:78) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) when two sessions are running in parallel, the follow below steps in session1   drop table  create table  load data to table   follow below step in session2   query on table(select * from table limit 1), then the query returns null result instead for proper result
issueID:CARBONDATA-2954
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/metadata/schema/table/CarbonTable.java
core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
texts:Fix error when create external table command fired when path already exists

issueID:CARBONDATA-2955
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/adaptive/AdaptiveDeltaIntegralCodec.java
texts:bug for legacy store and compaction with zstd compressor and adaptiveDeltaIntegralCodec
if table is configured with zstd compressor, compaction will fail if we use adaptiveDeltaIntegralCodec;
issueID:CARBONDATA-2956
type:Bug
changed files:examples/spark2/src/main/java/org/apache/carbondata/examples/sdk/SDKS3ReadExample.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/CarbonTable.java
store/sdk/src/main/java/org/apache/carbondata/sdk/file/CarbonReaderBuilder.java
texts:CarbonReader can&#39;t support use configuration to read S3 data
Test code:package org.apache.carbondata.examples.sdk;import org.apache.carbondata.common.logging.LogService;import org.apache.carbondata.common.logging.LogServiceFactory;import org.apache.carbondata.core.metadata.datatype.DataTypes;import org.apache.carbondata.core.scan.expression.ColumnExpression;import org.apache.carbondata.core.scan.expression.LiteralExpression;import org.apache.carbondata.core.scan.expression.conditional.EqualToExpression;import org.apache.carbondata.sdk.file.*;import org.apache.hadoop.conf.Configuration;import static org.apache.hadoop.fs.s3a.Constants.ACCESS_KEY;import static org.apache.hadoop.fs.s3a.Constants.ENDPOINT;import static org.apache.hadoop.fs.s3a.Constants.SECRET_KEY;/** * Example for testing CarbonWriter on S3 */public class SDKS3Example {    public static void main(String[] args) throws Exception {        LogService logger = LogServiceFactory.getLogService(SDKS3Example.class.getName());        if (args == null || args.length < 3) {            logger.error("Usage: java CarbonS3Example: <access-key> <secret-key>"                + "<s3-endpoint> [table-path-on-s3] [rows]");            System.exit(0);        }        String path = "s3a://sdk/WriterOutput";        if (args.length > 3) {            path=args[3];        }        int num = 3;        if (args.length > 4) {            num = Integer.parseInt(args[4]);        }        Configuration conf = new Configuration();        conf.set(ACCESS_KEY,args[0]);        conf.set(SECRET_KEY,args[1]);        conf.set(ENDPOINT,args[2]);//        Field[] fields = new Field[2];//        fields[0] = new Field("name", DataTypes.STRING);//        fields[1] = new Field("age", DataTypes.INT);//        CarbonWriterBuilder builder = CarbonWriter.builder()//                .setAccessKey(args[0])//                .setSecretKey(args[1])//                .setEndPoint(args[2])//                .outputPath(path);////        CarbonWriter writer = builder.buildWriterForCSVInput(new Schema(fields), conf);////        for (int i = 0; i < num; i++) {//            writer.write(new String[]{"robot" + (i % 10), String.valueOf(i)});//        }//        writer.close();        // Read data        EqualToExpression equalToExpression = new EqualToExpression(            new ColumnExpression("name", DataTypes.STRING),            new LiteralExpression("robot1", DataTypes.STRING));        CarbonReader reader = CarbonReader            .builder(path, "_temp")            .projection(new String[]{"name", "age"})            .filter(equalToExpression)            .build(conf);        System.out.println("\nData:");        int i = 0;        while (i < 20 && reader.hasNext()) {            Object[] row = (Object[]) reader.readNextRow();            System.out.println(row[0] + " " + row[1]);            i++;        }        System.out.println("\nFinished");        reader.close();    }}Exception:log4j:WARN No appenders could be found for logger (org.apache.hadoop.util.Shell).log4j:WARN Please initialize the log4j system properly.log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.Exception in thread "main" com.amazonaws.AmazonClientException: Unable to load AWS credentials from any provider in the chain at com.amazonaws.auth.AWSCredentialsProviderChain.getCredentials(AWSCredentialsProviderChain.java:117) at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:3521) at com.amazonaws.services.s3.AmazonS3Client.headBucket(AmazonS3Client.java:1031) at com.amazonaws.services.s3.AmazonS3Client.doesBucketExist(AmazonS3Client.java:994) at org.apache.hadoop.fs.s3a.S3AFileSystem.initialize(S3AFileSystem.java:297) at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2653) at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:92) at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2687) at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2669) at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:371) at org.apache.hadoop.fs.Path.getFileSystem(Path.java:295) at org.apache.carbondata.core.datastore.filesystem.AbstractDFSCarbonFile.<init>(AbstractDFSCarbonFile.java:74) at org.apache.carbondata.core.datastore.filesystem.AbstractDFSCarbonFile.<init>(AbstractDFSCarbonFile.java:66) at org.apache.carbondata.core.datastore.filesystem.HDFSCarbonFile.<init>(HDFSCarbonFile.java:41) at org.apache.carbondata.core.datastore.filesystem.S3CarbonFile.<init>(S3CarbonFile.java:41) at org.apache.carbondata.core.datastore.impl.DefaultFileTypeProvider.getCarbonFile(DefaultFileTypeProvider.java:53) at org.apache.carbondata.core.datastore.impl.FileFactory.getCarbonFile(FileFactory.java:99) at org.apache.carbondata.core.metadata.schema.table.CarbonTable.buildTable(CarbonTable.java:241) at org.apache.carbondata.sdk.file.CarbonReaderBuilder.build(CarbonReaderBuilder.java:191) at org.apache.carbondata.examples.sdk.SDKS3Example.main(SDKS3Example.java:91)Process finished with exit code 1if configure like: CarbonReader reader = CarbonReader            .builder(path, "_temp")            .projection(new String[]{"name", "age"})            .filter(equalToExpression)            .setAccessKey(args[0])            .setSecretKey(args[1])            .setEndPoint(args[2])            .build(new Configuration(false));Exception2:log4j:WARN No appenders could be found for logger (org.apache.hadoop.util.Shell).log4j:WARN Please initialize the log4j system properly.log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.Exception in thread "main" com.amazonaws.AmazonClientException: Unable to execute HTTP request: sdk.obs.cn-north-1.myhwclouds.com: nodename nor servname provided, or not known at com.amazonaws.http.AmazonHttpClient.executeHelper(AmazonHttpClient.java:454) at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:232) at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:3528) at com.amazonaws.services.s3.AmazonS3Client.headBucket(AmazonS3Client.java:1031) at com.amazonaws.services.s3.AmazonS3Client.doesBucketExist(AmazonS3Client.java:994) at org.apache.hadoop.fs.s3a.S3AFileSystem.initialize(S3AFileSystem.java:297) at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2653) at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:92) at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2687) at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2669) at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:371) at org.apache.hadoop.fs.Path.getFileSystem(Path.java:295) at org.apache.carbondata.core.datastore.filesystem.AbstractDFSCarbonFile.<init>(AbstractDFSCarbonFile.java:74) at org.apache.carbondata.core.datastore.filesystem.AbstractDFSCarbonFile.<init>(AbstractDFSCarbonFile.java:66) at org.apache.carbondata.core.datastore.filesystem.HDFSCarbonFile.<init>(HDFSCarbonFile.java:41) at org.apache.carbondata.core.datastore.filesystem.S3CarbonFile.<init>(S3CarbonFile.java:41) at org.apache.carbondata.core.datastore.impl.DefaultFileTypeProvider.getCarbonFile(DefaultFileTypeProvider.java:53) at org.apache.carbondata.core.datastore.impl.FileFactory.getCarbonFile(FileFactory.java:99) at org.apache.carbondata.core.metadata.schema.table.CarbonTable.buildTable(CarbonTable.java:241) at org.apache.carbondata.sdk.file.CarbonReaderBuilder.build(CarbonReaderBuilder.java:191) at org.apache.carbondata.examples.sdk.SDKS3Example.main(SDKS3Example.java:91)Caused by: java.net.UnknownHostException: sdk.obs.cn-north-1.myhwclouds.com: nodename nor servname provided, or not known at java.net.Inet6AddressImpl.lookupAllHostAddr(Native Method) at java.net.InetAddress$2.lookupAllHostAddr(InetAddress.java:928) at java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1323) at java.net.InetAddress.getAllByName0(InetAddress.java:1276) at java.net.InetAddress.getAllByName(InetAddress.java:1192) at java.net.InetAddress.getAllByName(InetAddress.java:1126) at org.apache.http.impl.conn.SystemDefaultDnsResolver.resolve(SystemDefaultDnsResolver.java:45) at org.apache.http.impl.conn.DefaultClientConnectionOperator.resolveHostname(DefaultClientConnectionOperator.java:278) at org.apache.http.impl.conn.DefaultClientConnectionOperator.openConnection(DefaultClientConnectionOperator.java:162) at org.apache.http.impl.conn.ManagedClientConnectionImpl.open(ManagedClientConnectionImpl.java:294) at org.apache.http.impl.client.DefaultRequestDirector.tryConnect(DefaultRequestDirector.java:641) at org.apache.http.impl.client.DefaultRequestDirector.execute(DefaultRequestDirector.java:480) at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:906) at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:805) at com.amazonaws.http.AmazonHttpClient.executeHelper(AmazonHttpClient.java:384) ... 20 moreProcess finished with exit code 1
issueID:CARBONDATA-2957
type:Sub-task
changed files:
texts:update document about zstd support in carbondata

issueID:CARBONDATA-2958
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/datastore/chunk/reader/dimension/v3/DimensionChunkPageReaderV3.java
texts:Compaction with CarbonProperty &#39;carbon.enable.page.level.reader.in.compaction&#39; enabled fails as Compressor is null

issueID:CARBONDATA-296
type:Sub-task
changed files:processing/src/main/java/org/apache/carbondata/processing/csvload/StringArrayWritable.java
processing/src/main/java/org/apache/carbondata/processing/csvload/BoundedInputStream.java
processing/src/main/java/org/apache/carbondata/processing/csvload/CSVInputFormat.java
texts:1.Add CSVInputFormat to read csv files.
Add CSVInputFormat to read csv files, it should use Univocity parser to read csv files to get optimal performance.
issueID:CARBONDATA-2960
type:Improvement
changed files:store/sdk/src/main/java/org/apache/carbondata/sdk/file/CarbonReaderBuilder.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonInputFormat.java
texts:SDK reader not working without projection columns

issueID:CARBONDATA-2961
type:Improvement
changed files:examples/spark2/src/main/java/org/apache/carbondata/examples/sdk/SDKS3Example.java
examples/spark2/src/main/java/org/apache/carbondata/examples/sdk/CarbonReaderExample.java
store/sdk/src/main/java/org/apache/carbondata/store/MetaCachedCarbonStore.java
examples/spark2/src/main/java/org/apache/carbondata/examples/sdk/SDKS3ReadExample.java
store/sdk/src/main/java/org/apache/carbondata/sdk/file/CarbonWriterBuilder.java
store/sdk/src/main/java/org/apache/carbondata/sdk/file/CarbonReaderBuilder.java
texts:Simplify SDK API interfaces
CARBONDATA-2961 Simplify SDK API interfacesproblem: current SDK API interfaces are not simpler and don't follow builder pattern.If new features are added, it will become more complex.Solution: Simplify the SDK interfaces as per builder pattern.Refer the latest sdk-guide.Added:changes in Carbon Writer:public CarbonWriterBuilder withThreadSafe(short numOfThreads)public CarbonWriterBuilder withHadoopConf(Configuration conf)public CarbonWriterBuilder withCsvInput(Schema schema)public CarbonWriterBuilder withAvroInput(org.apache.avro.Schema avroSchema)public CarbonWriterBuilder withJsonInput(Schema carbonSchema)public CarbonWriter build() throws IOException, InvalidLoadOptionExceptionChanges in carbon Readerpublic CarbonReaderBuilder withHadoopConf(Configuration conf)public CarbonWriter build() throws IOException, InvalidLoadOptionExceptionRemoved:changes in Carbon Writer:public CarbonWriterBuilder isTransactionalTable(boolean isTransactionalTable)public CarbonWriterBuilder persistSchemaFile(boolean persist);setAccessKeysetAccessKeysetSecretKeysetSecretKeysetEndPointsetEndPointpublic CarbonWriter buildWriterForCSVInput(Schema schema, Configuration configuration)public CarbonWriter buildThreadSafeWriterForCSVInput(Schema schema, short numOfThreads,Configuration configuration)public CarbonWriter buildWriterForAvroInput(org.apache.avro.Schema avroSchema,Configuration configuration)public CarbonWriter buildThreadSafeWriterForAvroInput(org.apache.avro.Schema avroSchema,short numOfThreads, Configuration configuration)public JsonCarbonWriter buildWriterForJsonInput(Schema carbonSchema, Configuration configuration)public JsonCarbonWriter buildThreadSafeWriterForJsonInput(Schema carbonSchema, short numOfThreads,Configuration configuration)Changes in carbon Readerpublic CarbonReaderBuilder isTransactionalTable(boolean isTransactionalTable)public CarbonWriter build(Configuration conf) throws IOException, InvalidLoadOptionException
issueID:CARBONDATA-2962
type:Improvement
changed files:processing/src/main/java/org/apache/carbondata/processing/store/writer/AbstractFactDataWriter.java
texts:Even after carbon file is copied to targetfolder(local/hdfs), carbon files is not deleted from temp directory

issueID:CARBONDATA-2963
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/scan/executor/util/QueryUtil.java
core/src/main/java/org/apache/carbondata/core/util/comparator/Comparator.java
texts:Add support to add byte column as a sort column

issueID:CARBONDATA-2964
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/util/CarbonMetadataUtil.java
core/src/main/java/org/apache/carbondata/core/scan/collector/impl/AbstractScannedResultCollector.java
texts:Unsupported Float datatype exception for query with more than 1 page

issueID:CARBONDATA-2965
type:New Feature
changed files:core/src/main/java/org/apache/carbondata/core/util/DataFileFooterConverterV3.java
tools/cli/src/main/java/org/apache/carbondata/tool/Command.java
tools/cli/src/main/java/org/apache/carbondata/tool/ScanBenchmark.java
tools/cli/src/main/java/org/apache/carbondata/tool/DataSummary.java
tools/cli/src/main/java/org/apache/carbondata/tool/CarbonCli.java
tools/cli/src/main/java/org/apache/carbondata/tool/DataFile.java
tools/cli/src/main/java/org/apache/carbondata/tool/FileCollector.java
texts:Support scan performance benchmark tool

issueID:CARBONDATA-2966
type:Improvement
changed files:
texts:Update Documentation For Avro DataType conversion

issueID:CARBONDATA-2967
type:Bug
changed files:
texts:Select is failing on pre-aggregate datamap when thrift server is restarted.
Problem: NullPointerException is thrown when select query is fired on a datamap. This is because to access dictionary files of the parent table the child table tries to get tablePath from CarbonTable object of the parent. Because the metadata is not populated therefore NullpointerException is thrown. 1, 10.2.3.19, executor 1): java.lang.RuntimeException: Error while resolving filter expression         at org.apache.carbondata.core.metadata.schema.table.CarbonTable.resolveFilter(CarbonTable.java:1043)         at org.apache.carbondata.core.scan.model.QueryModelBuilder.build(QueryModelBuilder.java:322)         at org.apache.carbondata.hadoop.api.CarbonInputFormat.createQueryModel(CarbonInputFormat.java:632)         at org.apache.carbondata.spark.rdd.CarbonScanRDD.internalCompute(CarbonScanRDD.scala:419)         at org.apache.carbondata.spark.rdd.CarbonRDD.compute(CarbonRDD.scala:78)         at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)         at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)         at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)         at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)         at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)         at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)         at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)         at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)         at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)         at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)         at org.apache.spark.scheduler.Task.run(Task.scala:109)         at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)         at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)         at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)         at java.lang.Thread.run(Thread.java:748) Caused by: java.lang.NullPointerException         at org.apache.carbondata.core.scan.executor.util.QueryUtil.getTableIdentifierForColumn(QueryUtil.java:401)         at org.apache.carbondata.core.scan.filter.FilterUtil.getForwardDictionaryCache(FilterUtil.java:1416)         at org.apache.carbondata.core.scan.filter.FilterUtil.getFilterValues(FilterUtil.java:712)         at org.apache.carbondata.core.scan.filter.resolver.resolverinfo.visitor.DictionaryColumnVisitor.populateFilterResolvedInfo(DictionaryColumnVisitor.java:60)         at org.apache.carbondata.core.scan.filter.resolver.resolverinfo.DimColumnResolvedFilterInfo.populateFilterInfoBasedOnColumnType(DimColumnResolvedFilterInfo.java:119)         at org.apache.carbondata.core.scan.filter.resolver.ConditionalFilterResolverImpl.resolve(ConditionalFilterResolverImpl.java:107)         at 
issueID:CARBONDATA-2968
type:Bug
changed files:
texts:Single pass load fails 2nd time in Spark submit execution due to port binding error

issueID:CARBONDATA-2969
type:Bug
changed files:integration/spark-datasource/src/main/spark2.1andspark2.2/org/apache/spark/sql/CarbonVectorProxy.java
integration/spark-datasource/src/main/spark2.3plus/org/apache/spark/sql/CarbonVectorProxy.java
integration/spark-datasource/src/main/scala/org/apache/carbondata/spark/vectorreader/ColumnarVectorWrapper.java
integration/spark-datasource/src/main/spark2.1andspark2.2/org/apache/spark/sql/CarbonDictionaryWrapper.java
integration/spark-datasource/src/main/spark2.3plus/org/apache/spark/sql/CarbonDictionaryWrapper.java
texts:Query on local dictionary column is giving empty data
Spark-2.3 when local dictionary is enabled for a column in spark-2.3, query on that column always gives empty data.
issueID:CARBONDATA-297
type:Sub-task
changed files:processing/src/main/java/org/apache/carbondata/processing/loading/constants/DataLoadProcessorConstants.java
processing/src/main/java/org/apache/carbondata/processing/loading/exception/CarbonDataLoadingException.java
processing/src/main/java/org/apache/carbondata/processing/loading/AbstractDataLoadProcessorStep.java
core/src/main/java/org/apache/carbondata/core/datastore/row/CarbonRow.java
processing/src/main/java/org/apache/carbondata/processing/loading/row/CarbonRowBatch.java
processing/src/main/java/org/apache/carbondata/processing/loading/CarbonDataLoadConfiguration.java
processing/src/main/java/org/apache/carbondata/processing/loading/DataField.java
texts:2. Add interfaces for data loading.
Add the major interface classes for data loading so that the following jiras can use this interfaces to implement it.
issueID:CARBONDATA-2970
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/locks/CarbonLockFactory.java
texts:Basic queries like drop table and load are not working in ViewFS
when default fs is set to ViewFS then the drop table and load fails with follwing exceptionorg.apache.carbondata.spark.exception.ProcessMetaDataException: operation failed for default.tb: Dropping table default.tb failed: Acquire table lock failed after retry, please try after some time at org.apache.spark.sql.execution.command.MetadataProcessOpeation$class.throwMetadataException(package.scala:52) at org.apache.spark.sql.execution.command.AtomicRunnableCommand.throwMetadataException(package.scala:86) at org.apache.spark.sql.execution.command.table.CarbonDropTableCommand.processMetadata(CarbonDropTableCommand.scala:157) at org.apache.spark.sql.execution.command.AtomicRunnableCommand.run(package.scala:90) at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:71) at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:69) at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:80) at org.apache.spark.sql.Dataset$$anonfun$6.apply(Dataset.scala:190) at org.apache.spark.sql.Dataset$$anonfun$6.apply(Dataset.scala:190) at org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3259) at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77) at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3258) at org.apache.spark.sql.Dataset.<init>(Dataset.scala:190) at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:75) at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:642) at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:694) at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:245) at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1$$anon$2.run(SparkExecuteStatementOperation.scala:177) 
issueID:CARBONDATA-2971
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockletDataMapFactory.java
core/src/main/java/org/apache/carbondata/core/indexstore/Blocklet.java
texts:Add shard info of blocklet for debugging

issueID:CARBONDATA-2972
type:Improvement
changed files:processing/src/main/java/org/apache/carbondata/processing/store/TablePage.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/ColumnPageEncoder.java
texts:Debug Logs and a function for type of Adaptive Encoding

issueID:CARBONDATA-2973
type:Improvement
changed files:
texts:Add Documentation for complex Columns for Local Dictionary Support

issueID:CARBONDATA-2974
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/datamap/DataMapChooser.java
datamap/bloom/src/main/java/org/apache/carbondata/datamap/bloom/BloomCoarseGrainDataMap.java
texts:Bloomfilter not working when created bloom on multiple columns and queried
Please check the link for more informationhttp://apache-carbondata-dev-mailing-list-archive.1130556.n5.nabble.com/Issue-Bloomfilter-datamap-td63254.html
issueID:CARBONDATA-2975
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelRangeGrtrThanEquaToFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RangeValueFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/scan/filter/FilterUtil.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelRangeGrtThanFiterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelRangeLessThanFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelRangeLessThanEqualFilterExecuterImpl.java
texts:DefaultValue choosing and removeNullValues on range filters is incorrect

issueID:CARBONDATA-2976
type:New Feature
changed files:tools/cli/src/main/java/org/apache/carbondata/tool/DataSummary.java
tools/cli/src/main/java/org/apache/carbondata/tool/CarbonCli.java
tools/cli/src/main/java/org/apache/carbondata/tool/DataFile.java
texts:Support dumping column chunk meta in CarbonCli

issueID:CARBONDATA-2977
type:New Feature
changed files:core/src/main/java/org/apache/carbondata/core/datastore/page/LocalDictColumnPage.java
core/src/main/java/org/apache/carbondata/core/datastore/page/VarLengthColumnPageBase.java
core/src/main/java/org/apache/carbondata/core/datastore/page/ColumnPage.java
core/src/main/java/org/apache/carbondata/core/util/CarbonMetadataUtil.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/ColumnPageEncoder.java
core/src/main/java/org/apache/carbondata/core/datastore/page/UnsafeFixLengthColumnPage.java
texts:Write uncompress_size to ChunkCompressMeta in the file

issueID:CARBONDATA-2978
type:Bug
changed files:hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonTableOutputFormat.java
texts:JVM crashes when data inserted from one table to other table with unsafe true
JVM crashes when data inserted from one table to other table with unsafe true
issueID:CARBONDATA-2979
type:Bug
changed files:store/sdk/src/main/java/org/apache/carbondata/sdk/file/AvroCarbonWriter.java
texts:select count fails when carbondata file is written through SDK and read through sparkfileformat for complex datatype map(struct->array->map)
Steps:create carabondata and carbonindex file using SDKplace the files in a hdfs locationRead files using spark file formatcreate table schema15_int using carbon location 'hdfs://hacluster/user/rahul/map/mapschema15_int';Select count from  schema15_int;Actual Result:Error: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 24.0 failed 4 times, most recent failure: Lost task 0.3 in stage 24.0 (TID 34, BLR1000014238, executor 3): java.io.IOException: All the files doesn't have same schema. Unsupported operation on nonTransactional table. Check logs. at org.apache.carbondata.core.scan.executor.impl.AbstractQueryExecutor.updateColumns(AbstractQueryExecutor.java:276) at org.apache.carbondata.core.scan.executor.impl.AbstractQueryExecutor.getDataBlocks(AbstractQueryExecutor.java:234) at org.apache.carbondata.core.scan.executor.impl.AbstractQueryExecutor.initQuery(AbstractQueryExecutor.java:141) at org.apache.carbondata.core.scan.executor.impl.AbstractQueryExecutor.getBlockExecutionInfos(AbstractQueryExecutor.java:401) at org.apache.carbondata.core.scan.executor.impl.VectorDetailQueryExecutor.execute(VectorDetailQueryExecutor.java:44) at org.apache.carbondata.spark.vectorreader.VectorizedCarbonRecordReader.initialize(VectorizedCarbonRecordReader.java:143) at org.apache.spark.sql.carbondata.execution.datasources.SparkCarbonFileFormat$$anonfun$buildReaderWithPartitionValues$2.apply(SparkCarbonFileFormat.scala:395) at org.apache.spark.sql.carbondata.execution.datasources.SparkCarbonFileFormat$$anonfun$buildReaderWithPartitionValues$2.apply(SparkCarbonFileFormat.scala:361) at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:124) at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:174) at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:105) at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.scan_nextBatch$(Unknown Source) at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.agg_doAggregateWithoutKey$(Unknown Source) at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source) at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43) at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:395) at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408) at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125) at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96) at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53) at org.apache.spark.scheduler.Task.run(Task.scala:108) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745)
issueID:CARBONDATA-298
type:Sub-task
changed files:processing/src/main/java/org/apache/carbondata/processing/loading/parser/CarbonParserFactory.java
processing/src/main/java/org/apache/carbondata/processing/loading/parser/ComplexParser.java
processing/src/main/java/org/apache/carbondata/processing/loading/complexobjects/StructObject.java
processing/src/main/java/org/apache/carbondata/processing/loading/constants/DataLoadProcessorConstants.java
processing/src/main/java/org/apache/carbondata/processing/loading/parser/impl/StructParserImpl.java
processing/src/main/java/org/apache/carbondata/processing/loading/parser/impl/ArrayParserImpl.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
processing/src/main/java/org/apache/carbondata/processing/loading/parser/impl/PrimitiveParserImpl.java
processing/src/main/java/org/apache/carbondata/processing/loading/parser/RowParser.java
processing/src/main/java/org/apache/carbondata/processing/loading/AbstractDataLoadProcessorStep.java
processing/src/main/java/org/apache/carbondata/processing/loading/complexobjects/ArrayObject.java
processing/src/main/java/org/apache/carbondata/processing/loading/parser/impl/RowParserImpl.java
core/src/main/java/org/apache/carbondata/core/util/CarbonProperties.java
processing/src/main/java/org/apache/carbondata/processing/newflow/steps/InputProcessorStepImpl.java
processing/src/main/java/org/apache/carbondata/processing/loading/parser/GenericParser.java
texts:3. Add InputProcessorStep which should iterate recordreader and parse the data as per the data type.
Add InputProcessorStep which should iterate recordreader/RecordBufferedWriter and parse the data as per the data types.
issueID:CARBONDATA-2980
type:Bug
changed files:datamap/bloom/src/main/java/org/apache/carbondata/datamap/bloom/BloomCoarseGrainDataMapFactory.java
texts:clear bloomindex cache when dropping datamap
should clear the bloomindex cache when we drop datamap, otherwise query will fail if we drop and recreate a brand new table and datamap and the stale cache still exists.
issueID:CARBONDATA-2981
type:Sub-task
changed files:store/sdk/src/main/java/org/apache/carbondata/sdk/file/CarbonReader.java
core/src/main/java/org/apache/carbondata/core/datastore/row/CarbonRow.java
store/sdk/src/main/java/org/apache/carbondata/sdk/file/Field.java
store/sdk/src/main/java/org/apache/carbondata/sdk/file/RowUtil.java
texts:Support read primitive data type in CSDK
Now PR2738 only support read string array in CSDK, we should support read primitive data type in CSDK
issueID:CARBONDATA-2982
type:Bug
changed files:examples/spark2/src/main/java/org/apache/carbondata/examples/sdk/CarbonReaderExample.java
store/sdk/src/main/java/org/apache/carbondata/sdk/file/CarbonSchemaReader.java
texts:CarbonSchemaReader don&#39;t support Array<string>
CarbonSchemaReader don't support Array<string>When we read schema from index file and the data include array<string> data typerun org.apache.carbondata.examples.sdk.CarbonReaderExample :    Schema schema = CarbonSchemaReader                .readSchemaInIndexFile(dataFiles[0].getAbsolutePath())                .asOriginOrder();            // Transform the schema            String[] strings = new String[schema.getFields().length];            for (int i = 0; i < schema.getFields().length; i++) {                strings[i] = (schema.getFields())[i].getFieldName();                System.out.println(strings[i] + "\t" + schema.getFields()[i].getSchemaOrdinal());            }and throw some exception:log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.arrayfield.val0 -1stringfield 0shortfield 1intfield 2longfield 3doublefield 4boolfield 5datefield 6timefield 7decimalfield 8varcharfield 9arrayfield 10Complex child columns projection NOT supported through CarbonReaderjava.lang.UnsupportedOperationException: Complex child columns projection NOT supported through CarbonReader at org.apache.carbondata.sdk.file.CarbonReaderBuilder.build(CarbonReaderBuilder.java:155) at org.apache.carbondata.examples.sdk.CarbonReaderExample.main(CarbonReaderExample.java:110)It print arrayfield.val0 -1, child schema
issueID:CARBONDATA-2983
type:Improvement
changed files:datamap/bloom/src/main/java/org/apache/carbondata/datamap/bloom/BloomCoarseGrainDataMap.java
texts:Change bloom query model to proceed multiple filter values

issueID:CARBONDATA-2984
type:Bug
changed files:streaming/src/main/java/org/apache/carbondata/streaming/segment/StreamSegment.java
streaming/src/main/java/org/apache/carbondata/streaming/CarbonStreamRecordWriter.java
texts:streaming throw NPE when there is no data in the task of a batch

issueID:CARBONDATA-2985
type:Improvement
changed files:processing/src/main/java/org/apache/carbondata/processing/merger/CarbonDataMergerUtil.java
texts:Fix issues in Table level compaction and TableProperties

issueID:CARBONDATA-2986
type:Bug
changed files:
texts:Table Properties are lost when multiple driver concurrently creating table
Create 2 Sets of create table (each with 100 commands)run set 1 with JDBCserver (beeline)  and Run set2 using spark-submit .Some tables table properties like block_size,sort columns are lost and defect is taken 
issueID:CARBONDATA-2987
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/datastore/chunk/store/ColumnPageWrapper.java
texts:Data mismatch after compaction with measure sort columns
problem: Data mismatch after compaction with measure sort columnsroot cause : In compaction flow (DictionaryBasedResultCollector), in ColumnPageWrapper inverted index mapping is not handled. Because of this, row of no dictionary dimension columns gets data form other rows.Hence the data mismatch solution: Handle inverted index mapping for  DictionaryBasedResultCollector flow in ColumnPageWrapper 
issueID:CARBONDATA-2988
type:Bug
changed files:
texts:use unsafe for query model based on system property

issueID:CARBONDATA-2989
type:Improvement
changed files:
texts:Upgrade spark integration version to 2.3.2
Upgrade spark integration version to 2.3.2
issueID:CARBONDATA-299
type:Sub-task
changed files:core/src/main/java/org/apache/carbondata/core/devapi/BiDictionary.java
core/src/main/java/org/apache/carbondata/core/devapi/DictionaryGenerator.java
texts:4. Add dictionary generator interfaces and give implementation for pre created dictionary.
Add dictionary generator interfaces and give implementation for pre-created dictionary(which is generated separetly).
issueID:CARBONDATA-2990
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/memory/MemoryBlock.java
core/src/main/java/org/apache/carbondata/core/memory/MemoryType.java
core/src/main/java/org/apache/carbondata/core/indexstore/UnsafeMemoryDMStore.java
core/src/main/java/org/apache/carbondata/core/memory/UnsafeMemoryManager.java
core/src/main/java/org/apache/carbondata/core/memory/HeapMemoryAllocator.java
core/src/main/java/org/apache/carbondata/core/memory/UnsafeMemoryAllocator.java
texts:JVM crashes when rebuilding the datamap.
create table brinjal (imei string,AMSize string,channelsId string,ActiveCountry string, Activecity string,gamePointId double,deviceInformationId double,productionDate Timestamp,deliveryDate timestamp,deliverycharge double) STORED BY 'org.apache.carbondata.format' TBLPROPERTIES('table_blocksize'='1');  LOAD DATA INPATH '/home/root1/Downloads/vardhandaterestruct.csv' INTO TABLE brinjal OPTIONS('DELIMITER'=',', 'QUOTECHAR'= '"','BAD_RECORDS_ACTION'='FORCE','FILEHEADER'= 'imei,deviceInformationId,AMSize,channelsId,ActiveCountry,Activecity,gamePointId,productionDate,deliveryDate,deliverycharge');CREATE DATAMAP dm_brinjal ON TABLE brinjal USING 'bloomfilter' DMPROPERTIES ('INDEX_COLUMNS' = 'AMSize', 'BLOOM_SIZE'='640000', 'BLOOM_FPP'='0.00001'); Error: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 4.0 failed 4 times, most recent failure: Lost task 0.3 in stage 4.0 (TID 13, 192.168.0.12, executor 11): ExecutorLostFailure (executor 11 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.Driver stacktrace: (state=,code=0)
issueID:CARBONDATA-2991
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/util/ThreadLocalTaskInfo.java
core/src/main/java/org/apache/carbondata/core/datastore/page/VarLengthColumnPageBase.java
core/src/main/java/org/apache/carbondata/core/memory/IntPointerBuffer.java
store/sdk/src/main/java/org/apache/carbondata/sdk/file/CarbonReader.java
core/src/main/java/org/apache/carbondata/core/indexstore/AbstractMemoryDMStore.java
core/src/main/java/org/apache/carbondata/core/memory/UnsafeMemoryManager.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/store/impl/unsafe/UnsafeAbstractDimensionDataChunkStore.java
core/src/main/java/org/apache/carbondata/core/util/CarbonTaskInfo.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/UnsafeCarbonRowPage.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/UnsafeSortDataRows.java
core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
core/src/main/java/org/apache/carbondata/core/memory/UnsafeSortMemoryManager.java
core/src/main/java/org/apache/carbondata/core/datastore/page/UnsafeFixLengthColumnPage.java
texts:NegativeArraySizeException during query execution
During Query Execution sometime NegativeArraySizeException  Exception in Some Tasks . And sometime Executor is lost (JVM crash) ava.lang.NegativeArraySizeException at org.apache.carbondata.core.datastore.chunk.store.impl.unsafe.UnsafeVariableLengthDimesionDataChunkStore.getRow(UnsafeVariableLengthDimesionDataChunkStore.java:157) at org.apache.carbondata.core.datastore.chunk.impl.AbstractDimensionDataChunk.getChunkData(AbstractDimensionDataChunk.java:46) at org.apache.carbondata.core.scan.result.AbstractScannedResult.getNoDictionaryKeyArray(AbstractScannedResult.java:470) at org.apache.carbondata.core.scan.result.impl.NonFilterQueryScannedResult.getNoDictionaryKeyArray(NonFilterQueryScannedResult.java:102) at org.apache.carbondata.core.scan.collector.impl.DictionaryBasedResultCollector.collectData(DictionaryBasedResultCollector.java:101) at org.apache.carbondata.core.scan.processor.impl.DataBlockIteratorImpl.next(DataBlockIteratorImpl.java:51) at org.apache.carbondata.core.scan.processor.impl.DataBlockIteratorImpl.next(DataBlockIteratorImpl.java:32) at org.apache.carbondata.core.scan.result.iterator.DetailQueryResultIterator.getBatchResult(DetailQueryResultIterator.java:49)   Issue Analysis :- Possible Root Cause :- It is because existing memoryblock is removed while it was in-use. This happened  because duplicate taskid generated. Sometime freed same memory addresses are assigned to another task which will initialize memory block to0 and this cause NegativeSizeArrayException whereas sometime freed memory will not be used any task of executor process but running task will try to access it and as that address is not part of process so JVM crash will happen. Steps to find cause Add the code to create tasklist ids and add taskid to list on setCarbonTaskInfo()  , and if it is duplicate then Log a Warn message.Please check attachment.   Run the Query multiple time and found warn message in executor logs2018-09-29 14:48:41,840 | INFO | [&#91;Executor task launch worker for task 435242&#93;&#91;partitionID:1;queryID:29971946625611231&#93;] | &#91;Executor task launch worker for task 435242&#93;&#91;partitionID:1;queryID:29971946625611231&#93; Total memory used after task 29971946381679677 is 0 Current tasks running now are : [] | org.apache.carbondata.common.logging.impl.StandardLogService.logInfoMessage(StandardLogService.java:150)2018-09-29 14:48:41,840 | INFO | [&#91;Executor task launch worker for task 435242&#93;&#91;partitionID:1;queryID:29971946625611231&#93;] | Finished task 17091.0 in stage 22.0 (TID 435242). 1412 bytes result sent to driver | org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)2018-09-29 14:48:41,842 | WARN | &#91;Executor task launch worker for task 435393&#93; | Executor task launch worker for task 435393 Already This Task is is Present29971946637094373 | org.apache.carbondata.common.logging.impl.StandardLogService.logWarnMessage(StandardLogService.java:168)2018-09-29 14:48:41,842 | INFO | &#91;dispatcher-event-loop-13&#93; | Got assigned task 435395 | org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
issueID:CARBONDATA-2992
type:Bug
changed files:
texts:Fixed Between Query Data Mismatch issue for timestamp data type
Problem:Between query is giving wrong result.Root cause:For timestamp time when filter is given in yyyy-mm-dd format instead of yyyy-mm-dd HH:MM:SS format it will add cast, In CastExpressionOptimization it is using SimpleDateFormat object to parse the filter value which is failing as filter values is not same.Solution:Use SPARK:DateTimeUtils.stringToTime method as spark is handling for above scenario.
issueID:CARBONDATA-2993
type:Bug
changed files:store/sdk/src/main/java/org/apache/carbondata/sdk/file/AvroCarbonWriter.java
texts:Concurrent data load throwing NPE randomly.

issueID:CARBONDATA-2994
type:Bug
changed files:processing/src/main/java/org/apache/carbondata/processing/util/CarbonBadRecordUtil.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonTableOutputFormat.java
texts:Unify property name for badrecords path in create and load.

issueID:CARBONDATA-2995
type:Bug
changed files:
texts:Queries slow down after some time due to broadcast issue
Problem DescriptionIt is observed that during consecutive run of queries after some time queries are slowing down. This is causing the degrade in query performance.No exception is thrown in driver and executor logs but as observed from the logs the time to broadcast hadoop conf is increasing after every query run.
issueID:CARBONDATA-2996
type:Bug
changed files:examples/spark2/src/main/java/org/apache/carbondata/examples/sdk/CarbonReaderExample.java
store/sdk/src/main/java/org/apache/carbondata/sdk/file/Field.java
store/sdk/src/main/java/org/apache/carbondata/sdk/file/CarbonSchemaReader.java
store/sdk/src/main/java/org/apache/carbondata/sdk/file/Schema.java
texts:readSchemaInIndexFile can&#39;t read schema by folder path
Test code: Schema schema = CarbonSchemaReader                .readSchemaInIndexFile(path)                .asOriginOrder();            // Transform the schema            String[] strings = new String[schema.getFields().length];            for (int i = 0; i < schema.getFields().length; i++) {                strings[i] = (schema.getFields())[i].getFieldName();            }Error:java.io.IOException: Not an index file nameNot an index file name at org.apache.carbondata.sdk.file.CarbonSchemaReader.readSchemaInIndexFile(CarbonSchemaReader.java:90) at org.apache.carbondata.examples.sdk.CarbonReaderExample.main(CarbonReaderExample.java:97)
issueID:CARBONDATA-2997
type:Sub-task
changed files:store/sdk/src/main/java/org/apache/carbondata/sdk/file/Schema.java
texts:Support read schema from index file and data file in CSDK
Support read schema from index file and data file in CSDK
issueID:CARBONDATA-2998
type:Bug
changed files:processing/src/main/java/org/apache/carbondata/processing/merger/CompactionResultSortProcessor.java
texts:Refresh column schema for old store(before V3) for SORT_COLUMNS option

issueID:CARBONDATA-2999
type:Sub-task
changed files:core/src/main/java/org/apache/carbondata/core/datastore/impl/FileFactory.java
examples/spark2/src/main/java/org/apache/carbondata/examples/sdk/SDKS3Example.java
store/sdk/src/main/java/org/apache/carbondata/sdk/file/CarbonSchemaReader.java
examples/spark2/src/main/java/org/apache/carbondata/examples/sdk/SDKS3SchemaReadExample.java
texts:support read schema from S3
support read schema from S3Code:      String path = "s3a://sdk/WriterOutput/carbondata5";        if (args.length > 3) {            path=args[3];        }        Schema schema = CarbonSchemaReader.readSchema(path);        System.out.println(schema.getFieldsLength());Exception:WARN  NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicableException in thread "main" com.amazonaws.AmazonClientException: Unable to load AWS credentials from any provider in the chain at com.amazonaws.auth.AWSCredentialsProviderChain.getCredentials(AWSCredentialsProviderChain.java:117) at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:3521) at com.amazonaws.services.s3.AmazonS3Client.headBucket(AmazonS3Client.java:1031) at com.amazonaws.services.s3.AmazonS3Client.doesBucketExist(AmazonS3Client.java:994) at org.apache.hadoop.fs.s3a.S3AFileSystem.initialize(S3AFileSystem.java:297) at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2653) at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:92) at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2687) at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2669) at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:371) at org.apache.hadoop.fs.Path.getFileSystem(Path.java:295) at org.apache.carbondata.core.datastore.filesystem.AbstractDFSCarbonFile.<init>(AbstractDFSCarbonFile.java:74) at org.apache.carbondata.core.datastore.filesystem.AbstractDFSCarbonFile.<init>(AbstractDFSCarbonFile.java:66) at org.apache.carbondata.core.datastore.filesystem.HDFSCarbonFile.<init>(HDFSCarbonFile.java:41) at org.apache.carbondata.core.datastore.filesystem.S3CarbonFile.<init>(S3CarbonFile.java:41) at org.apache.carbondata.core.datastore.impl.DefaultFileTypeProvider.getCarbonFile(DefaultFileTypeProvider.java:53) at org.apache.carbondata.core.datastore.impl.FileFactory.getCarbonFile(FileFactory.java:99) at org.apache.carbondata.sdk.file.CarbonSchemaReader.getCarbonFile(CarbonSchemaReader.java:79) at org.apache.carbondata.sdk.file.CarbonSchemaReader.readSchema(CarbonSchemaReader.java:150) at org.apache.carbondata.sdk.file.CarbonSchemaReader.readSchema(CarbonSchemaReader.java:109) at org.apache.carbondata.examples.sdk.SDKS3SchemaReadExample.main(SDKS3SchemaReadExample.java:51)
issueID:CARBONDATA-30
type:New Feature
changed files:
texts:Record load performance statistics
We should use a parameter which can be configured by user to determine whether the statistics will be recorded and calculated during data loading.
issueID:CARBONDATA-300
type:Sub-task
changed files:processing/src/main/java/org/apache/carbondata/processing/loading/converter/impl/FieldEncoderFactory.java
core/src/main/java/org/apache/carbondata/core/scan/result/RowBatch.java
processing/src/main/java/org/apache/carbondata/processing/loading/converter/impl/AbstractDictionaryFieldConverterImpl.java
store/sdk/src/main/java/org/apache/carbondata/sdk/file/RowUtil.java
core/src/main/java/org/apache/carbondata/core/devapi/DictionaryGenerator.java
core/src/main/java/org/apache/carbondata/core/devapi/BiDictionary.java
processing/src/main/java/org/apache/carbondata/processing/loading/converter/RowConverter.java
core/src/main/java/org/apache/carbondata/core/scan/result/iterator/ChunkRowIterator.java
processing/src/main/java/org/apache/carbondata/processing/loading/converter/impl/RowConverterImpl.java
store/sdk/src/main/java/org/apache/carbondata/sdk/file/CarbonReaderBuilder.java
store/sdk/src/main/java/org/apache/carbondata/sdk/file/CarbonReader.java
processing/src/main/java/org/apache/carbondata/processing/loading/converter/impl/DirectDictionaryFieldConverterImpl.java
processing/src/main/java/org/apache/carbondata/processing/loading/steps/DataConverterProcessorStepImpl.java
processing/src/main/java/org/apache/carbondata/processing/loading/exception/CarbonDataLoadingException.java
core/src/main/java/org/apache/carbondata/core/devapi/DictionaryGenerationException.java
hadoop/src/main/java/org/apache/carbondata/hadoop/CarbonRecordReader.java
processing/src/main/java/org/apache/carbondata/processing/loading/converter/FieldConverter.java
processing/src/main/java/org/apache/carbondata/processing/loading/converter/impl/ComplexFieldConverterImpl.java
processing/src/main/java/org/apache/carbondata/processing/loading/converter/impl/NonDictionaryFieldConverterImpl.java
texts:5. Add EncodeProcessorStep which encodes the data with dictionary.
Add EncodeProcessorStep which encodes the data with dictionary.This dictionary can be obtained from dictionary interface.
issueID:CARBONDATA-3000
type:Sub-task
changed files:examples/spark2/src/main/java/org/apache/carbondata/examples/sdk/SDKS3Example.java
store/sdk/src/main/java/org/apache/carbondata/sdk/file/CarbonWriterBuilder.java
texts:Provide C++ interface for writing carbon data
Provide C++ interface for writing carbon data
issueID:CARBONDATA-3001
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/metadata/blocklet/BlockletInfo.java
processing/src/main/java/org/apache/carbondata/processing/datatypes/PrimitiveDataType.java
processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactDataHandlerColumnar.java
processing/src/main/java/org/apache/carbondata/processing/datatypes/StructDataType.java
processing/src/main/java/org/apache/carbondata/processing/datatypes/GenericDataType.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockletDataRefNode.java
processing/src/main/java/org/apache/carbondata/processing/store/writer/v3/CarbonFactDataWriterImplV3.java
processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactDataHandlerModel.java
core/src/main/java/org/apache/carbondata/core/util/DataFileFooterConverterV3.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/TableSchemaBuilder.java
processing/src/main/java/org/apache/carbondata/processing/datatypes/ArrayDataType.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
core/src/main/java/org/apache/carbondata/core/util/CarbonMetadataUtil.java
core/src/main/java/org/apache/carbondata/core/datastore/blocklet/EncodedBlocklet.java
processing/src/main/java/org/apache/carbondata/processing/store/TablePage.java
store/sdk/src/main/java/org/apache/carbondata/sdk/file/CarbonWriterBuilder.java
texts:Propose configurable page size in MB (via carbon property)
For better in-memory processing of carbondata pages, I am proposing configurable page size in MB (via carbon property).please find the attachment for more details.
issueID:CARBONDATA-3002
type:Bug
changed files:processing/src/main/java/org/apache/carbondata/processing/merger/CarbonCompactionUtil.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/SegmentIndexFileStore.java
core/src/main/java/org/apache/carbondata/core/scan/executor/impl/AbstractQueryExecutor.java
processing/src/main/java/org/apache/carbondata/processing/util/CarbonLoaderUtil.java
core/src/main/java/org/apache/carbondata/core/datastore/filesystem/AbstractDFSCarbonFile.java
core/src/main/java/org/apache/carbondata/core/scan/result/iterator/AbstractDetailQueryResultIterator.java
core/src/main/java/org/apache/carbondata/core/util/CarbonProperties.java
core/src/main/java/org/apache/carbondata/core/datastore/filesystem/CarbonFile.java
core/src/main/java/org/apache/carbondata/core/util/annotations/CarbonProperty.java
store/sdk/src/main/java/org/apache/carbondata/sdk/file/CarbonReader.java
core/src/main/java/org/apache/carbondata/core/datastore/impl/FileReaderImpl.java
core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
core/src/main/java/org/apache/carbondata/core/datastore/filesystem/LocalCarbonFile.java
core/src/main/java/org/apache/carbondata/core/datastore/impl/DFSFileReaderImpl.java
integration/presto/src/main/java/org/apache/carbondata/presto/PrestoCarbonVectorizedRecordReader.java
core/src/main/java/org/apache/carbondata/core/datastore/filesystem/HDFSCarbonFile.java
core/src/main/java/org/apache/carbondata/core/datastore/FileReader.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/UnsafeSortDataRows.java
hadoop/src/main/java/org/apache/carbondata/hadoop/CarbonRecordReader.java
processing/src/main/java/org/apache/carbondata/processing/loading/model/CarbonLoadModel.java
integration/spark-datasource/src/main/scala/org/apache/carbondata/spark/vectorreader/VectorizedCarbonRecordReader.java
core/src/main/java/org/apache/carbondata/core/datastore/filesystem/AlluxioCarbonFile.java
processing/src/main/java/org/apache/carbondata/processing/loading/CarbonDataLoadConfiguration.java
core/src/main/java/org/apache/carbondata/core/locks/CarbonLockUtil.java
core/src/main/java/org/apache/carbondata/core/datastore/filesystem/ViewFSCarbonFile.java
core/src/main/java/org/apache/carbondata/core/datastore/filesystem/S3CarbonFile.java
texts:Fix some spell error and remove the data after test case finished running
Fix some spell error and remove the data after test case finished runningretrive
issueID:CARBONDATA-3003
type:Sub-task
changed files:
texts:Suppor read batch row in CSDK
Suppor read batch row in CSDK
issueID:CARBONDATA-3004
type:Sub-task
changed files:
texts:Fix bug in writing dataframe to carbon table while the field order is different
More information about this issue can be found in this link: http://apache-carbondata-dev-mailing-list-archive.1130556.n5.nabble.com/Issue-Long-string-columns-config-for-big-strings-not-work-td64876.html
issueID:CARBONDATA-3005
type:New Feature
changed files:core/src/main/java/org/apache/carbondata/core/datastore/compression/GzipCompressor.java
core/src/main/java/org/apache/carbondata/core/datastore/compression/ZstdCompressor.java
core/src/main/java/org/apache/carbondata/core/datastore/compression/AbstractCompressor.java
core/src/main/java/org/apache/carbondata/core/datastore/compression/CompressorFactory.java
texts:Supporting Gzip as Column Compressor
Currently CarbonData uses Snappy as default codec to compress its columnar file, Other than SNAPPY carbondata supports zstd. This Issue is targeted to support:1. Gzip compression codec.Benefits of Gzip are : Gzip offers reduced file size compared to other codec like snappy but at the cost of processing speed. Gzip is suitable for users who have cold data i.e. data which which are stored permanently and will be queried rarely.
issueID:CARBONDATA-3007
type:Bug
changed files:
texts:Fix error in document

issueID:CARBONDATA-3008
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
core/src/main/java/org/apache/carbondata/core/util/CarbonProperties.java
texts:make yarn-local and multiple dir for temp data enable by default
About a year ago, we introduced 'multiple dirs for temp data during data loading' to solve disk hotspot problem. For about one years' usage in productive environment, this feature turns to be effective and correct. So here I propose to enable the related parameters by default. The related parameters contains:`carbon.use.local.dir` : Currently it is `false` by default, we will turn it to `true` by default;`carbon.user.multiple.dir` : Currently it is `false` by default, we will turn it to `true` by default.
issueID:CARBONDATA-3009
type:Improvement
changed files:
texts:Optimize the entry point of code for MergeIndex

issueID:CARBONDATA-301
type:Sub-task
changed files:processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/SortDataRows.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/SortParameters.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/Sorter.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/IntermediateFileMerger.java
processing/src/main/java/org/apache/carbondata/processing/util/CarbonDataProcessorUtil.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/impl/ParallelReadMergeSorterImpl.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/SingleThreadFinalSortFilesMerger.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/SortIntermediateFileMerger.java
core/src/main/java/org/apache/carbondata/core/datastore/row/CarbonRow.java
processing/src/main/java/org/apache/carbondata/processing/loading/steps/InputProcessorStepImpl.java
processing/src/main/java/org/apache/carbondata/processing/loading/steps/SortProcessorStepImpl.java
processing/src/main/java/org/apache/carbondata/processing/util/NonDictionaryUtil.java
core/src/main/java/org/apache/carbondata/core/datastore/exception/CarbonDataWriterException.java
texts:6. Add SortProcessorStep which sorts the data as per dimension order and write the sorted files to temp location.
Add SortProcessorStep which sorts the data as per dimension order and write the sorted files to temp location.
issueID:CARBONDATA-3011
type:Sub-task
changed files:core/src/main/java/org/apache/carbondata/core/scan/model/QueryModel.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
core/src/main/java/org/apache/carbondata/core/util/CarbonProperties.java
texts:Add carbon property to configure vector based row pruning push down
Add configuration in carbon to enable or disable row filter push down for vector
issueID:CARBONDATA-3012
type:Sub-task
changed files:core/src/main/java/org/apache/carbondata/core/scan/executor/infos/BlockExecutionInfo.java
core/src/main/java/org/apache/carbondata/core/datastore/page/VarLengthColumnPageBase.java
core/src/main/java/org/apache/carbondata/core/datastore/columnar/UnBlockIndexer.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/ColumnPageEncoderMeta.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/store/DimensionChunkStoreFactory.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/store/impl/safe/SafeFixedLengthDimensionDataChunkStore.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/compress/DirectCompressCodec.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/impl/DimensionRawColumnChunk.java
core/src/main/java/org/apache/carbondata/core/scan/result/vector/impl/CarbonDictionaryImpl.java
integration/spark-datasource/src/main/scala/org/apache/carbondata/spark/vectorreader/ColumnarVectorWrapperDirect.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/store/impl/safe/AbstractNonDictionaryVectorFiller.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/rle/RLECodec.java
core/src/main/java/org/apache/carbondata/core/scan/model/QueryModel.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/store/DimensionDataChunkStore.java
integration/spark-datasource/src/main/scala/org/apache/carbondata/spark/vectorreader/VectorizedCarbonRecordReader.java
core/src/main/java/org/apache/carbondata/core/stats/QueryStatisticsModel.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/impl/MeasureRawColumnChunk.java
core/src/main/java/org/apache/carbondata/core/util/ByteUtil.java
core/src/main/java/org/apache/carbondata/core/scan/scanner/impl/BlockletFullScanner.java
core/src/main/java/org/apache/carbondata/core/datastore/columnar/BlockIndexerStorageForNoDictionary.java
core/src/main/java/org/apache/carbondata/core/scan/executor/impl/AbstractQueryExecutor.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/reader/DimensionColumnChunkReader.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/EncodingFactory.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/store/impl/unsafe/UnsafeAbstractDimensionDataChunkStore.java
core/src/main/java/org/apache/carbondata/core/keygenerator/directdictionary/timestamp/DateDirectDictionaryGenerator.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/reader/dimension/v3/DimensionChunkPageReaderV3.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/adaptive/AdaptiveIntegralCodec.java
integration/presto/src/main/java/org/apache/carbondata/presto/CarbonColumnVectorWrapper.java
core/src/main/java/org/apache/carbondata/core/datastore/page/ColumnPageValueConverter.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/ColumnPageDecoder.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/reader/dimension/v3/DimensionChunkReaderV3.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/adaptive/AdaptiveDeltaFloatingCodec.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/store/impl/safe/SafeVariableLengthDimensionDataChunkStore.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/store/impl/LocalDictDimensionDataChunkStore.java
core/src/main/java/org/apache/carbondata/core/scan/result/vector/ColumnVectorInfo.java
core/src/main/java/org/apache/carbondata/core/datastore/columnar/BlockIndexerStorage.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/reader/measure/v3/MeasureChunkPageReaderV3.java
core/src/main/java/org/apache/carbondata/core/scan/executor/util/QueryUtil.java
core/src/main/java/org/apache/carbondata/core/datastore/impl/FileReaderImpl.java
core/src/main/java/org/apache/carbondata/core/mutate/DeleteDeltaVo.java
core/src/main/java/org/apache/carbondata/core/datastore/page/SafeDecimalColumnPage.java
integration/spark-datasource/src/main/spark2.3plus/org/apache/spark/sql/CarbonDictionaryWrapper.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/reader/MeasureColumnChunkReader.java
core/src/main/java/org/apache/carbondata/core/scan/result/BlockletScannedResult.java
core/src/main/java/org/apache/carbondata/core/scan/result/vector/CarbonDictionary.java
core/src/main/java/org/apache/carbondata/core/datastore/page/ColumnPage.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/adaptive/AdaptiveFloatingCodec.java
integration/spark-datasource/src/main/spark2.1andspark2.2/org/apache/spark/sql/CarbonVectorProxy.java
core/src/main/java/org/apache/carbondata/core/metadata/datatype/DecimalConverterFactory.java
integration/spark-datasource/src/main/spark2.3plus/org/apache/spark/sql/CarbonVectorProxy.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/reader/measure/AbstractMeasureChunkReader.java
integration/presto/src/main/java/org/apache/carbondata/presto/readers/SliceStreamReader.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/reader/measure/v3/MeasureChunkReaderV3.java
integration/spark-datasource/src/main/scala/org/apache/carbondata/spark/vectorreader/ColumnarVectorWrapper.java
core/src/main/java/org/apache/carbondata/core/datastore/columnar/BlockIndexerStorageForShort.java
core/src/main/java/org/apache/carbondata/core/scan/result/vector/CarbonColumnarBatch.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/impl/VariableLengthDimensionColumnPage.java
core/src/main/java/org/apache/carbondata/core/scan/result/vector/impl/CarbonColumnVectorImpl.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/impl/FixedLengthDimensionColumnPage.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/adaptive/AdaptiveDeltaIntegralCodec.java
integration/spark2/src/main/scala/org/apache/carbondata/stream/CarbonStreamRecordReader.java
core/src/main/java/org/apache/carbondata/core/scan/result/vector/CarbonColumnVector.java
core/src/main/java/org/apache/carbondata/core/scan/collector/impl/DictionaryBasedVectorResultCollector.java
texts:Support full scan queries for vector direct fill.
Add support for full scan queries which it fills the vector after decoding the column page.
issueID:CARBONDATA-3013
type:Sub-task
changed files:core/src/main/java/org/apache/carbondata/core/scan/filter/executer/ImplicitIncludeFilterExecutorImpl.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RangeValueFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/AndFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/scan/scanner/impl/BlockletFilterScanner.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RestructureEvaluatorImpl.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/TrueFilterExecutor.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/ExcludeFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelRangeLessThanEqualFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/FalseFilterExecutor.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelRangeGrtrThanEquaToFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/FilterExecuter.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/OrFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/IncludeFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelRangeGrtThanFiterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelRangeLessThanFilterExecuterImpl.java
texts:Support filter interface to allow prune the pages and fill the vector.
Support filter interface to allow prune the pages and fill the vector. After pages are pruned through min/max meta the column pages will be decoded and fill the data directly to vector.
issueID:CARBONDATA-3014
type:Sub-task
changed files:core/src/main/java/org/apache/carbondata/core/scan/result/vector/impl/directread/AbstractCarbonColumnarVector.java
core/src/main/java/org/apache/carbondata/core/scan/collector/ResultCollectorFactory.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/adaptive/AdaptiveDeltaFloatingCodec.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/reader/dimension/v3/DimensionChunkReaderV3.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/store/impl/safe/SafeVariableLengthDimensionDataChunkStore.java
core/src/main/java/org/apache/carbondata/core/scan/scanner/impl/BlockletFilterScanner.java
core/src/main/java/org/apache/carbondata/core/scan/result/vector/ColumnVectorInfo.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/store/impl/safe/SafeFixedLengthDimensionDataChunkStore.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RestructureEvaluatorImpl.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/compress/DirectCompressCodec.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/adaptive/AdaptiveDeltaIntegralCodec.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelRangeGrtrThanEquaToFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/adaptive/AdaptiveFloatingCodec.java
core/src/main/java/org/apache/carbondata/core/metadata/datatype/DecimalConverterFactory.java
core/src/main/java/org/apache/carbondata/core/scan/result/vector/impl/directread/ColumnarVectorWrapperDirectWithInvertedIndex.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/store/impl/safe/AbstractNonDictionaryVectorFiller.java
core/src/main/java/org/apache/carbondata/core/scan/result/vector/impl/directread/ColumnarVectorWrapperDirectWithDeleteDeltaAndInvertedIndex.java
core/src/main/java/org/apache/carbondata/core/scan/result/vector/impl/directread/ColumnarVectorWrapperDirectWithDeleteDelta.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/adaptive/AdaptiveIntegralCodec.java
core/src/main/java/org/apache/carbondata/core/scan/result/vector/impl/directread/ColumnarVectorWrapperDirectFactory.java
core/src/main/java/org/apache/carbondata/core/scan/result/vector/impl/directread/ConvertableVector.java
texts:Support inverted index and delete delta fillings to vector for direct fill vector
Support inverted index and delete delta fillings to vector for direct fill vector
issueID:CARBONDATA-3015
type:Sub-task
changed files:core/src/main/java/org/apache/carbondata/core/scan/result/vector/impl/directread/AbstractCarbonColumnarVector.java
core/src/main/java/org/apache/carbondata/core/scan/result/vector/impl/CarbonColumnVectorImpl.java
core/src/main/java/org/apache/carbondata/core/scan/scanner/impl/BlockletFullScanner.java
core/src/main/java/org/apache/carbondata/core/scan/scanner/impl/BlockletFilterScanner.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/compress/DirectCompressCodec.java
core/src/main/java/org/apache/carbondata/core/scan/scanner/LazyPageLoader.java
core/src/main/java/org/apache/carbondata/core/scan/scanner/LazyBlockletLoader.java
core/src/main/java/org/apache/carbondata/core/scan/result/BlockletScannedResult.java
integration/spark2/src/main/scala/org/apache/carbondata/stream/CarbonStreamRecordReader.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
core/src/main/java/org/apache/carbondata/core/scan/result/vector/CarbonColumnVector.java
core/src/main/java/org/apache/carbondata/core/datastore/page/SafeFixLengthColumnPage.java
integration/spark-datasource/src/main/scala/org/apache/carbondata/spark/vectorreader/ColumnarVectorWrapperDirect.java
integration/spark-datasource/src/main/spark2.1andspark2.2/org/apache/spark/sql/CarbonVectorProxy.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/store/impl/safe/AbstractNonDictionaryVectorFiller.java
integration/spark-datasource/src/main/spark2.3plus/org/apache/spark/sql/CarbonVectorProxy.java
integration/spark-datasource/src/main/scala/org/apache/carbondata/spark/vectorreader/ColumnarVectorWrapper.java
integration/presto/src/main/java/org/apache/carbondata/presto/CarbonColumnVectorWrapper.java
texts:Support lazy loading in carbon.
Support lazy loading in carbon. 
issueID:CARBONDATA-3017
type:Sub-task
changed files:processing/src/main/java/org/apache/carbondata/processing/loading/parser/CarbonParserFactory.java
processing/src/main/java/org/apache/carbondata/processing/loading/parser/impl/MapParserImpl.java
store/sdk/src/main/java/org/apache/carbondata/sdk/file/CarbonWriterBuilder.java
processing/src/main/java/org/apache/carbondata/processing/loading/ComplexDelimitersEnum.java
processing/src/main/java/org/apache/carbondata/processing/loading/parser/impl/ArrayParserImpl.java
processing/src/main/java/org/apache/carbondata/processing/loading/DataLoadProcessBuilder.java
processing/src/main/java/org/apache/carbondata/processing/loading/parser/impl/RowParserImpl.java
processing/src/main/java/org/apache/carbondata/processing/loading/model/CarbonLoadModelBuilder.java
processing/src/main/java/org/apache/carbondata/processing/loading/model/LoadOption.java
processing/src/main/java/org/apache/carbondata/processing/loading/model/CarbonLoadModel.java
core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonTableOutputFormat.java
texts:Create DDL Support for Map Type

issueID:CARBONDATA-3019
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/util/comparator/Comparator.java
processing/src/main/java/org/apache/carbondata/processing/merger/CompactionResultSortProcessor.java
processing/src/main/java/org/apache/carbondata/processing/merger/RowResultMergerProcessor.java
texts:Add error log in catch block to avoid to abort the exception which is thrown from catch block when there is an exception thrown in finally block
Add error log in catch block to avoid to abort the exception which is thrown from catch block when there is an exception thrown in finally block. enhance log output.
issueID:CARBONDATA-302
type:Sub-task
changed files:processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactDataHandlerModel.java
processing/src/main/java/org/apache/carbondata/processing/util/CarbonDataProcessorUtil.java
processing/src/main/java/org/apache/carbondata/processing/newflow/steps/DataWriterProcessorStepImpl.java
processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactHandlerFactory.java
core/src/main/java/org/apache/carbondata/core/datastore/row/CarbonRow.java
texts:7. Add DataWriterProcessorStep which reads the data from sort temp files and creates carbondata files.
Add DataWriterProcessorStep which reads the data from sort temp files and merge sort it, and apply mdk generator on key and creates carbondata files.
issueID:CARBONDATA-3022
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/scan/executor/util/QueryUtil.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/store/ColumnPageWrapper.java
core/src/main/java/org/apache/carbondata/core/util/DataTypeUtil.java
texts:Refactor ColumnPageWrapper

issueID:CARBONDATA-3023
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RangeValueFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/store/ColumnPageWrapper.java
texts:Alter add column issue with SORT_COLUMNS

issueID:CARBONDATA-3024
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/metadata/schema/table/TableInfo.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/SortParameters.java
processing/src/main/java/org/apache/carbondata/processing/merger/CarbonCompactionUtil.java
core/src/main/java/org/apache/carbondata/core/datamap/DataMapStoreManager.java
processing/src/main/java/org/apache/carbondata/processing/loading/steps/DataWriterProcessorStepImpl.java
core/src/main/java/org/apache/carbondata/core/util/CarbonProperties.java
core/src/main/java/org/apache/carbondata/core/writer/CarbonDeleteDeltaWriterImpl.java
processing/src/main/java/org/apache/carbondata/processing/loading/DataLoadExecutor.java
core/src/main/java/org/apache/carbondata/core/util/path/HDFSLeaseUtils.java
core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
processing/src/main/java/org/apache/carbondata/processing/loading/csvinput/CSVRecordReaderIterator.java
datamap/bloom/src/main/java/org/apache/carbondata/datamap/bloom/BloomIndexFileStore.java
core/src/main/java/org/apache/carbondata/core/reader/CarbonDeleteFilesDataReader.java
datamap/lucene/src/main/java/org/apache/carbondata/datamap/lucene/LuceneFineGrainDataMap.java
core/src/main/java/org/apache/carbondata/core/mutate/SegmentUpdateDetails.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/holder/UnsafeSortTempFileChunkHolder.java
integration/spark-datasource/src/main/scala/org/apache/carbondata/spark/vectorreader/VectorizedCarbonRecordReader.java
processing/src/main/java/org/apache/carbondata/processing/loading/DataLoadProcessBuilder.java
streaming/src/main/java/org/apache/carbondata/streaming/segment/StreamSegment.java
processing/src/main/java/org/apache/carbondata/processing/merger/CompactionResultSortProcessor.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/holder/UnsafeFinalMergePageHolder.java
core/src/main/java/org/apache/carbondata/core/datastore/filesystem/S3CarbonFile.java
datamap/lucene/src/main/java/org/apache/carbondata/datamap/lucene/LuceneDataMapWriter.java
core/src/main/java/org/apache/carbondata/core/reader/CarbonDeleteDeltaFileReaderImpl.java
core/src/main/java/org/apache/carbondata/core/locks/HdfsFileLock.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/holder/UnsafeInmemoryHolder.java
core/src/main/java/org/apache/carbondata/core/memory/IntPointerBuffer.java
core/src/main/java/org/apache/carbondata/core/stats/DriverQueryStatisticsRecorderImpl.java
processing/src/main/java/org/apache/carbondata/processing/merger/RowResultMergerProcessor.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/SegmentIndexFileStore.java
core/src/main/java/org/apache/carbondata/core/scan/executor/impl/AbstractQueryExecutor.java
processing/src/main/java/org/apache/carbondata/processing/util/CarbonLoaderUtil.java
processing/src/main/java/org/apache/carbondata/processing/loading/model/LoadOption.java
core/src/main/java/org/apache/carbondata/core/locks/ZooKeeperLocking.java
core/src/main/java/org/apache/carbondata/core/memory/UnsafeSortMemoryManager.java
processing/src/main/java/org/apache/carbondata/processing/datamap/DataMapWriterListener.java
processing/src/main/java/org/apache/carbondata/processing/util/CarbonDataProcessorUtil.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/SortIntermediateFileMerger.java
core/src/main/java/org/apache/carbondata/core/util/SessionParams.java
integration/presto/src/main/java/org/apache/carbondata/presto/impl/CarbonTableReader.java
core/src/main/java/org/apache/carbondata/core/util/TaskMetricsMap.java
processing/src/main/java/org/apache/carbondata/processing/util/CarbonBadRecordUtil.java
processing/src/main/java/org/apache/carbondata/processing/loading/steps/CarbonRowDataWriterProcessorStepImpl.java
datamap/examples/src/minmaxdatamap/main/java/org/apache/carbondata/datamap/examples/MinMaxIndexDataMapFactory.java
core/src/main/java/org/apache/carbondata/core/util/DataTypeUtil.java
core/src/main/java/org/apache/carbondata/core/fileoperations/AtomicFileOperationsImpl.java
core/src/main/java/org/apache/carbondata/core/datastore/filesystem/AbstractDFSCarbonFile.java
datamap/bloom/src/main/java/org/apache/carbondata/datamap/bloom/AbstractBloomDataMapWriter.java
core/src/main/java/org/apache/carbondata/core/datastore/blocklet/BlockletEncodedColumnPage.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/impl/ParallelReadMergeSorterImpl.java
core/src/main/java/org/apache/carbondata/core/cache/CarbonLRUCache.java
core/src/main/java/org/apache/carbondata/core/scan/filter/FilterExpressionProcessor.java
processing/src/main/java/org/apache/carbondata/processing/store/TablePage.java
streaming/src/main/java/org/apache/carbondata/streaming/CarbonStreamRecordWriter.java
core/src/main/java/org/apache/carbondata/core/datastore/filesystem/AlluxioCarbonFile.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/SorterFactory.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockDataMap.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/SortTempFileChunkHolder.java
core/src/main/java/org/apache/carbondata/core/datastore/filesystem/ViewFSCarbonFile.java
integration/spark-datasource/src/main/scala/org/apache/carbondata/spark/vectorreader/ColumnarVectorWrapper.java
integration/hive/src/main/java/org/apache/carbondata/hive/MapredCarbonInputFormat.java
core/src/main/java/org/apache/carbondata/core/datastore/page/LocalDictColumnPage.java
core/src/main/java/org/apache/carbondata/core/mutate/DeleteDeltaBlockDetails.java
store/sdk/src/main/java/org/apache/carbondata/sdk/file/AvroCarbonWriter.java
processing/src/main/java/org/apache/carbondata/processing/datatypes/PrimitiveDataType.java
processing/src/main/java/org/apache/carbondata/processing/loading/AbstractDataLoadProcessorStep.java
core/src/main/java/org/apache/carbondata/core/scan/collector/impl/RestructureBasedRawResultCollector.java
processing/src/main/java/org/apache/carbondata/processing/loading/converter/impl/RowConverterImpl.java
examples/spark2/src/main/java/org/apache/carbondata/examples/sdk/SDKS3ReadExample.java
core/src/main/java/org/apache/carbondata/core/datastore/impl/FileFactory.java
core/src/main/java/org/apache/carbondata/core/datastore/filesystem/LocalCarbonFile.java
hadoop/src/main/java/org/apache/carbondata/hadoop/util/CarbonInputFormatUtil.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonOutputCommitter.java
processing/src/main/java/org/apache/carbondata/processing/loading/partition/impl/RangePartitionerImpl.java
datamap/examples/src/minmaxdatamap/main/java/org/apache/carbondata/datamap/examples/MinMaxDataWriter.java
core/src/main/java/org/apache/carbondata/core/datastore/block/SegmentPropertiesAndSchemaHolder.java
core/src/main/java/org/apache/carbondata/core/scan/collector/ResultCollectorFactory.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/merger/UnsafeIntermediateFileMerger.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/merger/UnsafeIntermediateMerger.java
datamap/bloom/src/main/java/org/apache/carbondata/datamap/bloom/BloomDataMapCache.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelFilterExecuterImpl.java
integration/presto/src/main/java/org/apache/carbondata/presto/CarbondataPageSource.java
datamap/bloom/src/main/java/org/apache/carbondata/datamap/bloom/BloomCoarseGrainDataMap.java
core/src/main/java/org/apache/carbondata/core/scan/result/iterator/RawResultIterator.java
core/src/main/java/org/apache/carbondata/core/locks/ZookeeperInit.java
processing/src/main/java/org/apache/carbondata/processing/loading/BadRecordsLogger.java
processing/src/main/java/org/apache/carbondata/processing/loading/csvinput/CSVInputFormat.java
store/sdk/src/main/java/org/apache/carbondata/store/LocalCarbonStore.java
core/src/main/java/org/apache/carbondata/core/datamap/status/DiskBasedDataMapStatusProvider.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/merger/UnsafeInMemoryIntermediateDataMerger.java
datamap/lucene/src/main/java/org/apache/carbondata/datamap/lucene/LuceneDataMapFactoryBase.java
core/src/main/java/org/apache/carbondata/core/datastore/filesystem/HDFSCarbonFile.java
common/src/main/java/org/apache/carbondata/common/logging/LogServiceFactory.java
processing/src/main/java/org/apache/carbondata/processing/merger/CarbonCompactionExecutor.java
core/src/main/java/org/apache/carbondata/core/stats/QueryStatisticsRecorderImpl.java
common/src/main/java/org/apache/carbondata/common/logging/LogService.java
core/src/main/java/org/apache/carbondata/core/mutate/CarbonUpdateUtil.java
processing/src/main/java/org/apache/carbondata/processing/store/writer/AbstractFactDataWriter.java
core/src/main/java/org/apache/carbondata/core/indexstore/BlockletDetailInfo.java
processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactDataHandlerModel.java
processing/src/main/java/org/apache/carbondata/processing/merger/CarbonDataMergerUtil.java
core/src/main/java/org/apache/carbondata/core/keygenerator/directdictionary/timestamp/TimeStampDirectDictionaryGenerator.java
core/src/main/java/org/apache/carbondata/core/util/CarbonMetadataUtil.java
core/src/main/java/org/apache/carbondata/core/memory/UnsafeMemoryManager.java
core/src/main/java/org/apache/carbondata/core/keygenerator/directdictionary/timestamp/DateDirectDictionaryGenerator.java
processing/src/main/java/org/apache/carbondata/processing/loading/converter/impl/MeasureFieldConverterImpl.java
core/src/main/java/org/apache/carbondata/core/locks/LocalFileLock.java
processing/src/main/java/org/apache/carbondata/processing/loading/TableProcessingOperations.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/merger/UnsafeSingleThreadFinalSortFilesMerger.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/SortDataRows.java
core/src/main/java/org/apache/carbondata/core/scan/expression/RangeExpressionEvaluator.java
core/src/main/java/org/apache/carbondata/core/locks/S3FileLock.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/SingleThreadFinalSortFilesMerger.java
core/src/main/java/org/apache/carbondata/core/indexstore/BlockletDataMapIndexStore.java
processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactDataHandlerColumnar.java
core/src/main/java/org/apache/carbondata/core/scan/result/iterator/AbstractDetailQueryResultIterator.java
processing/src/main/java/org/apache/carbondata/processing/store/writer/v3/CarbonFactDataWriterImplV3.java
examples/spark2/src/main/java/org/apache/carbondata/examples/sdk/SDKS3Example.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonVersionConstants.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/impl/ParallelReadMergeSorterWithColumnRangeImpl.java
core/src/main/java/org/apache/carbondata/core/scan/result/BlockletScannedResult.java
core/src/main/java/org/apache/carbondata/core/scan/model/QueryModelBuilder.java
datamap/bloom/src/main/java/org/apache/carbondata/datamap/bloom/BloomCoarseGrainDataMapFactory.java
core/src/main/java/org/apache/carbondata/core/metadata/SegmentFileStore.java
core/src/main/java/org/apache/carbondata/core/statusmanager/SegmentStatusManager.java
core/src/main/java/org/apache/carbondata/core/locks/CarbonLockFactory.java
core/src/main/java/org/apache/carbondata/core/scan/filter/FilterUtil.java
datamap/examples/src/minmaxdatamap/main/java/org/apache/carbondata/datamap/examples/MinMaxIndexDataMap.java
core/src/main/java/org/apache/carbondata/core/locks/CarbonLockUtil.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/UnsafeSortDataRows.java
core/src/main/java/org/apache/carbondata/core/statusmanager/LoadMetadataDetails.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/impl/UnsafeParallelReadMergeSorterWithColumnRangeImpl.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/CarbonTable.java
core/src/main/java/org/apache/carbondata/core/cache/CacheProvider.java
core/src/main/java/org/apache/carbondata/core/util/CarbonLoadStatisticsImpl.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/ColumnPageEncoder.java
core/src/main/java/org/apache/carbondata/core/datamap/DataMapUtil.java
processing/src/main/java/org/apache/carbondata/processing/loading/model/CarbonLoadModelBuilder.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/holder/UnsafeInmemoryMergeHolder.java
core/src/main/java/org/apache/carbondata/core/util/DeleteLoadFolders.java
core/src/main/java/org/apache/carbondata/core/util/ObjectSizeCalculator.java
core/src/main/java/org/apache/carbondata/core/datastore/compression/SnappyCompressor.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/impl/UnsafeParallelReadMergeSorterImpl.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/IntermediateFileMerger.java
datamap/lucene/src/main/java/org/apache/carbondata/datamap/lucene/LuceneDataMapBuilder.java
texts:Use Log4j directly
Currently CarbonData's log is printing the line number in StandardLogService, it is not good for maintainability, a better way is to use log4j Logger directly so that it will print line number of where we are logging.
issueID:CARBONDATA-3025
type:Bug
changed files:processing/src/main/java/org/apache/carbondata/processing/store/writer/v3/CarbonFactDataWriterImplV3.java
store/sdk/src/main/java/org/apache/carbondata/sdk/file/CarbonWriterBuilder.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
tools/cli/src/main/java/org/apache/carbondata/tool/ScanBenchmark.java
tools/cli/src/main/java/org/apache/carbondata/tool/DataSummary.java
tools/cli/src/main/java/org/apache/carbondata/tool/CarbonCli.java
tools/cli/src/main/java/org/apache/carbondata/tool/DataFile.java
store/sdk/src/main/java/org/apache/carbondata/sdk/file/CarbonSchemaReader.java
tools/cli/src/main/java/org/apache/carbondata/tool/FileCollector.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonTableOutputFormat.java
tools/cli/src/main/java/org/apache/carbondata/tool/TableFormatter.java
tools/cli/src/main/java/org/apache/carbondata/tool/ShardPrinter.java
integration/presto/src/main/java/org/apache/carbondata/presto/impl/CarbonTableReader.java
texts:Add SQL support for cli, and enhance CLI , add more metadata to carbon file
support SQL integration for CLI enhancle cli with more info add more metadata to carbon file footer for better maintainability
issueID:CARBONDATA-3026
type:Bug
changed files:hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonTableOutputFormat.java
processing/src/main/java/org/apache/carbondata/processing/loading/TableProcessingOperations.java
texts:clear expired property that may cause GC problem
During data loading, we will write some temp files (sort tempfiles and temp fact data files) in some locations. In currentlyimplementation, we will add the locations to the CarbonProperties andassociated it with a special key that refers to the data loading.After data loading, the temp locations are cleared, but the addedproperty is still remain in the CarbonProperties and never to be cleared.This will cause the CarbonProperties object growing bigger and biggerand lead to OOM problems if the thrift-server is a long time runningservice. A local test shows that after adding different properties for11 Billion times, the OOM happens.
issueID:CARBONDATA-3029
type:Bug
changed files:
texts:Failed to run spark data source test cases in windows env

issueID:CARBONDATA-3030
type:Improvement
changed files:
texts:Remove no use parameter in test case
Remove no use parameter in test case1. remove persistSchema parameter in SDK test case2. remove isTransactional parameter in SDK test case because https://github.com/apache/carbondata/pull/2749 remove the parameter in SDK carbonWriter
issueID:CARBONDATA-3031
type:Improvement
changed files:processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactDataHandlerModel.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/SortParameters.java
core/src/main/java/org/apache/carbondata/core/util/CarbonProperties.java
processing/src/main/java/org/apache/carbondata/processing/merger/CompactionResultSortProcessor.java
processing/src/main/java/org/apache/carbondata/processing/merger/CarbonDataMergerUtil.java
processing/src/main/java/org/apache/carbondata/processing/util/CarbonDataProcessorUtil.java
core/src/main/java/org/apache/carbondata/core/reader/CarbonDeleteFilesDataReader.java
processing/src/main/java/org/apache/carbondata/processing/loading/DataLoadProcessBuilder.java
processing/src/main/java/org/apache/carbondata/processing/loading/CarbonDataLoadConfiguration.java
texts:Find wrong description in the document for &#39;carbon.number.of.cores.while.loading&#39;
The document says that the default value of ‘carbon.number.of.cores.while.loading’ is 2. But actually during data loading, carbondata use the the value of 'spark.executor.cores', which means that the description in document is incorrect.But this doesn't mean that the default value of 'carbon.number.of.cores.while.loading' is useless &#8211; in compaction and sdk, carbondata still use this default value.In a word, we need to fix the implementation as well as the document, maybe some refactoring is needed.
issueID:CARBONDATA-3032
type:Improvement
changed files:processing/src/main/java/org/apache/carbondata/processing/store/CarbonDataWriterFactory.java
texts:Remove carbon.blocklet.size from properties template
Remove carbon.blocklet.size from properties templatecarbon.blocklet.size  is in file format V2
issueID:CARBONDATA-3034
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
texts:Combing CarbonCommonConstants
Carding parameters，Organized by parameter category.
issueID:CARBONDATA-3035
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
core/src/main/java/org/apache/carbondata/core/memory/UnsafeSortMemoryManager.java
core/src/main/java/org/apache/carbondata/core/memory/UnsafeMemoryManager.java
core/src/main/java/org/apache/carbondata/core/util/CarbonProperties.java
texts:Optimize parameters for unsafe working and sort memory

issueID:CARBONDATA-3036
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockDataMap.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockletDataMap.java
core/src/main/java/org/apache/carbondata/core/indexstore/schema/SchemaGenerator.java
core/src/main/java/org/apache/carbondata/core/util/BlockletDataMapUtil.java
texts:Carbon 1.5.0 B010 - Select query fails when min/max exceeds and index tree cached
If configured cache columns and min, max present flag columns is different*, it is possible to get wrong data* due to wrong index is mapped.results inconsistent when cache is set but min/max exceeds. Column is dictionary excluded.set carbon.minmax.allowed.byte.count=50; 0: jdbc:hive2://192.168.137.14:22550/default> use db01;---------+ Result ---------+---------+No rows selected (0.06 seconds)0: jdbc:hive2://192.168.137.14:22550/default> show tables;-------------------------------------------+ database  tableName  isTemporary -------------------------------------------+ db01  jinling_localsort_3  false -------------------------------------------+1 row selected (0.068 seconds)0: jdbc:hive2://192.168.137.14:22550/default> alter table jinling_localsort_3 set TBLPROPERTIES('COLUMN_META_CACHE'='user_imsi,user_num,dim24');---------+ Result ---------+---------+No rows selected (3.495 seconds)0: jdbc:hive2://192.168.137.14:22550/default> select count from jinling_localsort_3;-----------+ count(1) -----------+ 9000000 -----------+1 row selected (2.606 seconds)0: jdbc:hive2://192.168.137.14:22550/default> select dim24 from jinling_localsort_3 limit 2;------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ dim24 ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ E6Phek Hobbit from the Shire and eight companions set out on a journey to destroy the powerful One Ring and save Middle-earth from the Dark Lord Sauron.AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA  Q4Plek Hobbit from the Shire and eight companions set out on a journey to destroy the powerful One Ring and save Middle-earth from the Dark Lord Sauron.AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+2 rows selected (457.462 seconds)0: jdbc:hive2://192.168.137.14:22550/default> select count from jinling_localsort_3 where dim24 = 'E6Phek Hobbit from the Shire and eight companions set out on a journey to destroy the powerful One Ring and save Middle-earth from the Dark Lord Sauron.AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA';-----------+ count(1) -----------+ 0 -----------+1 row selected (0.871 seconds)0: jdbc:hive2://192.168.137.14:22550/default> alter table jinling_localsort_3 set TBLPROPERTIES('COLUMN_META_CACHE'='user_imsi,user_num');---------+ Result ---------+---------+No rows selected (3.798 seconds)0: jdbc:hive2://192.168.137.14:22550/default> select count from jinling_localsort_3;-----------+ count(1) -----------+ 9000000 -----------+1 row selected (4.162 seconds)0: jdbc:hive2://192.168.137.14:22550/default> select count from jinling_localsort_3 where dim24 = 'E6Phek Hobbit from the Shire and eight companions set out on a journey to destroy the powerful One Ring and save Middle-earth from the Dark Lord Sauron.AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA';-----------+ count(1) -----------+ 90 -----------+1 row selected (4.515 seconds) 
issueID:CARBONDATA-3038
type:Sub-task
changed files:core/src/main/java/org/apache/carbondata/core/util/path/HDFSLeaseUtils.java
processing/src/main/java/org/apache/carbondata/processing/merger/CarbonDataMergerUtil.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
core/src/main/java/org/apache/carbondata/core/util/annotations/CarbonProperty.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonLoadOptionConstants.java
core/src/main/java/org/apache/carbondata/core/memory/HeapMemoryAllocator.java
core/src/main/java/org/apache/carbondata/core/cache/CacheProvider.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonV3DataFormatConstants.java
core/src/main/java/org/apache/carbondata/core/memory/UnsafeMemoryManager.java
core/src/main/java/org/apache/carbondata/core/locks/CarbonLockFactory.java
core/src/main/java/org/apache/carbondata/core/util/SessionParams.java
processing/src/main/java/org/apache/carbondata/processing/util/CarbonLoaderUtil.java
core/src/main/java/org/apache/carbondata/core/util/CarbonProperties.java
core/src/main/java/org/apache/carbondata/core/locks/CarbonLockUtil.java
texts:Add annotation for carbon properties and mark whether is dynamic configuration
Refactor dynamic configuration for carbon:1. Decide and collect all dynamic configurations which can be SET in carbondata application like in beeline.2. For every dynamic configuration, use an annotation to tag them. (re-use the CarbonProperty annotation and add dynamicConfigurable for it). This annotation should be used for validation when user invoking SET command. the dynamicConfigurable default value is false.
issueID:CARBONDATA-3039
type:Improvement
changed files:
texts:Fix Custom Deterministic Expression for rand() UDF

issueID:CARBONDATA-304
type:Bug
changed files:processing/src/main/java/org/apache/carbondata/processing/store/writer/AbstractFactDataWriter.java
texts:Load data failure when set table_blocksize=2048
First ,create a table with table_blocksize=2048CREATE TABLE IF NOT EXISTS t3 (ID Int, date Timestamp, country String, name String, phonetype String, serialname String, salary Int) STORED BY 'carbondata' TBLPROPERTIES('table_blocksize'='2048');Then load data, failure and catch exception:org.apache.carbondata.processing.store.writer.exception.CarbonDataWriterException: Problem while copying file from local store to carbon store
issueID:CARBONDATA-3040
type:Bug
changed files:datamap/bloom/src/main/java/org/apache/carbondata/datamap/bloom/BloomIndexFileStore.java
texts:Fix bug for merging bloom index

issueID:CARBONDATA-3041
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonLoadOptionConstants.java
processing/src/main/java/org/apache/carbondata/processing/util/CarbonLoaderUtil.java
core/src/main/java/org/apache/carbondata/core/util/CarbonProperties.java
processing/src/main/java/org/apache/carbondata/processing/loading/model/CarbonLoadModelBuilder.java
processing/src/main/java/org/apache/carbondata/processing/loading/model/LoadOption.java
texts:Optimize load minimum size strategy for data loading
1、Delete system property carbon.load.min.size.enabled，modified this property load_min_size_inmb to table property，and This property can also be specified in the load option.2、Support to alter table xxx set TBLPROPERTIES('load_min_size_inmb '='256') 3、If creating a table has this property  load_min_size_inmb，Display this property via the desc formatted command.
issueID:CARBONDATA-3042
type:Improvement
changed files:processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/SortParameters.java
processing/src/main/java/org/apache/carbondata/processing/merger/RowResultMergerProcessor.java
processing/src/main/java/org/apache/carbondata/processing/loading/steps/CarbonRowDataWriterProcessorStepImpl.java
processing/src/main/java/org/apache/carbondata/processing/loading/steps/DataWriterProcessorStepImpl.java
core/src/main/java/org/apache/carbondata/core/metadata/CarbonMetadata.java
core/src/main/java/org/apache/carbondata/core/util/DeleteLoadFolders.java
processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactDataHandlerModel.java
processing/src/main/java/org/apache/carbondata/processing/loading/steps/DataConverterProcessorStepImpl.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/impl/ParallelReadMergeSorterWithColumnRangeImpl.java
processing/src/main/java/org/apache/carbondata/processing/util/CarbonDataProcessorUtil.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/impl/ParallelReadMergeSorterImpl.java
core/src/main/java/org/apache/carbondata/core/statusmanager/SegmentStatusManager.java
processing/src/main/java/org/apache/carbondata/processing/loading/DataLoadProcessBuilder.java
processing/src/main/java/org/apache/carbondata/processing/merger/CompactionResultSortProcessor.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/impl/UnsafeParallelReadMergeSorterWithColumnRangeImpl.java
texts:Column Schema objects are present in Driver and Executor even after dropping table

issueID:CARBONDATA-3044
type:Sub-task
changed files:store/sdk/src/main/java/org/apache/carbondata/sdk/file/RowUtil.java
texts:Handle exception in CSDK
Handle exception in CSDK1.handle java/scala exception2.handle C++ exception
issueID:CARBONDATA-3045
type:Bug
changed files:
texts:query failed: Unable to get the Query Model
When i restart presto,the first query was normal. If i query another table, it will report this error. It is normal to excute under spark-shell.Create Table SQL Script just like :   carbon.sql("create table call_center( cc_call_center_sk   int, cc_call_center_id   string, cc_rec_start_date   string, cc_rec_end_date string, cc_closed_date_sk   int, cc_open_date_sk int, cc_name  string, cc_class string, cc_employees int, cc_sq_ft int, cc_hours string, cc_manager   string, cc_mkt_id int, cc_mkt_class string, cc_mkt_desc  string, cc_market_manager   string, cc_division  int, cc_division_name    string, cc_company   int, cc_company_name string, cc_street_number    string, cc_street_name   string, cc_street_type   string, cc_suite_number string, cc_city  string, cc_county string, cc_state string, cc_zip   string, cc_country   string, cc_gmt_offset    double, cc_tax_percentage   double) STORED BY 'org.apache.carbondata.format' TBLPROPERTIES ('table_blocksize'='300','SORT_COLUMNS'='')")
issueID:CARBONDATA-3046
type:Improvement
changed files:
texts:remove outdated configurations in template properties

issueID:CARBONDATA-3047
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/memory/UnsafeMemoryManager.java
texts:UnsafeMemoryManager fallback mechanism in case of memory not available
Currently when unsafe working memory is not available UnsafeMemoryManager is throwing MemoryException and killing the running task.To make system more easier for the user now added fallback to heap when offheap memory is not available
issueID:CARBONDATA-3048
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/util/ByteUtil.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/adaptive/AdaptiveFloatingCodec.java
integration/spark-datasource/src/main/spark2.1andspark2.2/org/apache/spark/sql/CarbonVectorProxy.java
integration/spark-datasource/src/main/scala/org/apache/carbondata/spark/vectorreader/ColumnarVectorWrapperDirect.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/adaptive/AdaptiveDeltaFloatingCodec.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/store/impl/safe/AbstractNonDictionaryVectorFiller.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/store/impl/safe/SafeVariableLengthDimensionDataChunkStore.java
core/src/main/java/org/apache/carbondata/core/scan/result/vector/impl/directread/ColumnarVectorWrapperDirectWithDeleteDeltaAndInvertedIndex.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/store/impl/LocalDictDimensionDataChunkStore.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/store/impl/safe/SafeFixedLengthDimensionDataChunkStore.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/adaptive/AdaptiveIntegralCodec.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/compress/DirectCompressCodec.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/adaptive/AdaptiveDeltaIntegralCodec.java
integration/spark-datasource/src/main/scala/org/apache/carbondata/spark/vectorreader/ColumnarVectorWrapper.java
core/src/main/java/org/apache/carbondata/core/scan/result/vector/impl/directread/ColumnarVectorWrapperDirectFactory.java
texts:Added Lazy Loading For 2.2/2.1
Problem:Currently in 2.2/2.1 For Direct fill Lazy loading is not added because of this when data is huge and number of columns are high query is taking more time Lazy to execute.Solution:Add Lazy loading for 2.2 and 2.1 
issueID:CARBONDATA-305
type:Sub-task
changed files:
texts:Switching between kettle flow and new data loading flow make configurable
Switching between kettle flow and new data loading flow make configurable. This configuration should switch it dynamically while loading the data.
issueID:CARBONDATA-3050
type:Improvement
changed files:
texts:Remove unused parameter doc

issueID:CARBONDATA-3051
type:Improvement
changed files:store/sdk/src/main/java/org/apache/carbondata/sdk/file/CarbonReaderBuilder.java
store/sdk/src/main/java/org/apache/carbondata/sdk/file/CarbonSchemaReader.java
texts:unclosed streams cause tests failure in windows env

issueID:CARBONDATA-3052
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/datastore/filesystem/LocalCarbonFile.java
core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
core/src/main/java/org/apache/carbondata/core/datastore/filesystem/CarbonFile.java
texts:Improve drop table performance by reducing the namenode RPC calls during physical deletion of files
Current drop table command takes more than 1 minute to delete 3000 files during drop table operation from HDFS. This Jira is raised to improve the drop table operation performance
issueID:CARBONDATA-3053
type:Improvement
changed files:tools/cli/src/main/java/org/apache/carbondata/tool/ScanBenchmark.java
tools/cli/src/main/java/org/apache/carbondata/tool/DataSummary.java
tools/cli/src/main/java/org/apache/carbondata/tool/CarbonCli.java
tools/cli/src/main/java/org/apache/carbondata/tool/DataFile.java
tools/cli/src/main/java/org/apache/carbondata/tool/FileCollector.java
texts:Un-closed file stream found in cli

issueID:CARBONDATA-3054
type:Improvement
changed files:
texts:Dictionary file cannot be read in S3a with CarbonDictionaryDecoder.doConsume() codeGen
problem: In S3a environment, when quiried the data which has dictionary files,Dictionary file cannot be read in S3a with CarbonDictionaryDecoder.doConsume() codeGen even though file is present. cause: CarbonDictionaryDecoder.doConsume() codeGen doesn't set hadoop conf in thread local variable, only doExecute() sets it.Hence, when getDictionaryWrapper() called from doConsume() codeGen,AbstractDictionaryCache.getDictionaryMetaCarbonFile() returns false for fileExists() operation. solution:In CarbonDictionaryDecoder.doConsume() codeGen, set hadoop conf in thread local variable
issueID:CARBONDATA-3056
type:Sub-task
changed files:store/sdk/src/main/java/org/apache/carbondata/sdk/file/CarbonReader.java
texts:Implement concurrent reading through CarbonReader
The current reading through SDK is slow as in CarbonReader, we are reading the carbondata files sequentially, even though we have individual CarbonRecordReader for each file. We can parallelize this by adding an API in CarbonReader classList<CarbonReader> readers = CarbonReader.split(numSplits)which returns a list of CarbonReaders, which can be used to read parallelly, as reading each file is independent of other files. This enables the SDK user to read the files as it is, or in a multithreaded environment.
issueID:CARBONDATA-3057
type:Sub-task
changed files:core/src/main/java/org/apache/carbondata/core/datastore/filesystem/LocalCarbonFile.java
hadoop/src/main/java/org/apache/carbondata/hadoop/util/CarbonVectorizedRecordReader.java
core/src/main/java/org/apache/carbondata/core/scan/result/vector/impl/CarbonColumnVectorImpl.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/store/impl/safe/SafeVariableLengthDimensionDataChunkStore.java
hadoop/src/main/java/org/apache/carbondata/hadoop/CarbonRecordReader.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonFileInputFormat.java
core/src/main/java/org/apache/carbondata/core/metadata/datatype/DecimalType.java
core/src/main/java/org/apache/carbondata/core/datastore/filesystem/AbstractDFSCarbonFile.java
core/src/main/java/org/apache/carbondata/core/datastore/filesystem/CarbonFile.java
core/src/main/java/org/apache/carbondata/core/metadata/datatype/StructType.java
store/sdk/src/main/java/org/apache/carbondata/sdk/file/CarbonReaderBuilder.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/compress/DirectCompressCodec.java
texts:Implement Vectorized CarbonReader for SDK
Implement Vectorized Reader and expose a API for the user to switchbetween CarbonReader/Vectorized reader. Additionally an API would beprovided for the user to extract the columnar batch instead of rows. Thiswould allow the user to have a deeper integration with carbon.Additionally the reduction in method calls for vector reader would improvethe read time.
issueID:CARBONDATA-3058
type:Bug
changed files:processing/src/main/java/org/apache/carbondata/processing/loading/steps/CarbonRowDataWriterProcessorStepImpl.java
processing/src/main/java/org/apache/carbondata/processing/loading/steps/DataWriterProcessorStepImpl.java
texts:Fix some exception coding in data loading

issueID:CARBONDATA-306
type:Improvement
changed files:processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactDataHandlerModel.java
core/src/main/java/org/apache/carbondata/core/util/ByteUtil.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/CarbonTable.java
processing/src/main/java/org/apache/carbondata/processing/store/writer/AbstractFactDataWriter.java
processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactDataHandlerColumnar.java
texts:block size info should be show in Desc Formatted and executor log
when run desc formatted command, the table block size should be show, as well as in executor log when run load command
issueID:CARBONDATA-3060
type:Bug
changed files:examples/spark2/src/main/java/org/apache/carbondata/examples/sdk/CarbonReaderExample.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/column/ColumnSchema.java
tools/cli/src/main/java/org/apache/carbondata/tool/CarbonCli.java
tools/cli/src/main/java/org/apache/carbondata/tool/DataFile.java
tools/cli/src/main/java/org/apache/carbondata/tool/DataSummary.java
texts:Improve CLI and fix other bugs in CLI tool
1. improve the syntax for CLI DDL, now the command can be given as** `CarbonCli for table <table_name> options('-cmd summary/benchmark -a -s -v -c <column_name> -m')`the options will take one string, which is basically a command, which user can directly paste into command promt and run as java commandNow user no nned to give -P also, internally when above commad is run we take table path into consideration in command line argumentsother issues:1. when numeric columns are included in dictionary, min max are wrong2. timestamp column's min and max details are wrong, showing in long value rather than actual timestanp format3. help command is not working in beeline4. complex types column min max are wrong, sometimes junk values
issueID:CARBONDATA-3061
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/metadata/ColumnarFormatVersion.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/reader/CarbonDataReaderFactory.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/reader/dimension/v3/DimensionChunkReaderV3.java
core/src/main/java/org/apache/carbondata/core/indexstore/BlockletDataMapIndexStore.java
core/src/main/java/org/apache/carbondata/core/metadata/encoder/Encoding.java
core/src/main/java/org/apache/carbondata/core/util/CarbonProperties.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/reader/measure/v3/MeasureChunkReaderV3.java
texts:Add validation for supported format version and Encoding type to throw proper exception to the user while reading a file
This jira is raised to handle forward compatibility. Through this PR if any data file is read using a lower version (>=1.5.1), a proper exception will be thrown if columnar format version or any encoding type is not supported for read in that version
issueID:CARBONDATA-3062
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockDataMap.java
core/src/main/java/org/apache/carbondata/core/datastore/block/SegmentPropertiesAndSchemaHolder.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockletDataMap.java
processing/src/main/java/org/apache/carbondata/processing/merger/RowResultMergerProcessor.java
texts:Fix Compatibility issue with cache_level as blocklet
Please find below steps to reproduce the issue: Create table and load data in legacy store In new store, load data and alter table set table properties 'CACHE_LEVEL'='BLOCKLET' Perform Filter operation on that table and find below Exception Error: java.io.IOException: Problem in loading segment blocks. (state=,code=0)  
issueID:CARBONDATA-3063
type:Sub-task
changed files:core/src/main/java/org/apache/carbondata/core/util/CarbonProperties.java
texts:Support set carbon property in CSDK
when user write CarbonData or read CarbonData in CSDK,  user maybe need to change or add carbon property to avoid some problem. such as OOM.So we should support set carbon property in CSDK
issueID:CARBONDATA-3064
type:Improvement
changed files:examples/spark2/src/main/java/org/apache/carbondata/examples/sql/JavaCarbonSessionExample.java
core/src/main/java/org/apache/carbondata/core/statusmanager/SegmentStatusManager.java
core/src/main/java/org/apache/carbondata/core/util/SessionParams.java
core/src/main/java/org/apache/carbondata/core/datamap/status/DiskBasedDataMapStatusProvider.java
processing/src/main/java/org/apache/carbondata/processing/util/Auditor.java
texts:Support separate audit log
Currently, CarbonData outputs audit log with other level log together in one log file, it is not easy for user to check the audit. And sometimes the audit information is not complete since it depends on each Command to invoke Logger in its run function. To improve it, I propose a new audit log implementation by following:1. Separate the audit log from normal log, user can configure log4j to output the audit log in a separate file2. The audit log should have a common format that includes at least: time, username, operation name, operation id that identify this operation, status (success or failure), other extra information like data loading size, time spent3. The audit log should be in JSON format to enable analytic tool support in the future.For example, the audit log will be look like following{"time":"2018-10-31 15:02:12","username":"anonymous","opName":"CREATE TABLE","opId":"115794874155743","opStatus":"START"}{"time":"2018-10-31 15:02:12","username":"anonymous","opName":"CREATE TABLE","opId":"115794874155743","opStatus":"SUCCESS","opTime":"542 ms","tableId":"default.t1","extraInfo":{"external":"false"}}{"time":"2018-10-31 15:02:15","username":"anonymous","opName":"INSERT INTO","opId":"115797876187366","opStatus":"START"}{"time":"2018-10-31 15:02:19","username":"anonymous","opName":"INSERT INTO","opId":"115797876187366","opStatus":"SUCCESS","opTime":”4043 ms","tableId":"default.t1","extraInfo":{"SegmentId":"0","DataSize":"403.0B","IndexSize":"246.0B"}}{"time":"2018-10-31 15:02:33","username":"anonymous","opName":"DROP TABLE","opId":"115816322828613","opStatus":"START"}{"time":"2018-10-31 15:02:34","username":"anonymous","opName":"DROP TABLE","opId":"115816322828613","opStatus":"SUCCESS","opTime":"131 ms","tableId":"default.t1","extraInfo":{}}{"time":"2018-10-31 15:02:49","username":"anonymous","opName":"SHOW SEGMENTS","opId":"115831939703565","opStatus":"START"}{"time":"2018-10-31 15:02:49","username":"anonymous","opName":"SHOW SEGMENTS","opId":"115831939703565","opStatus":"SUCCESS","opTime":"30 ms","tableId":"default.t2","extraInfo":{}}{"time":"2018-10-31 15:03:54","username":"anonymous","opName":"INSERT OVERWRITE","opId":"115896869484042","opStatus":"START"}{"time":"2018-10-31 15:03:56","username":"anonymous","opName":"INSERT OVERWRITE","opId":"115896869484042","opStatus":"SUCCESS","opTime":"2039 ms","tableId":"default.t2","extraInfo":{"SegmentId":"0","DataSize":"403.0B","IndexSize":"246.0B”}}
issueID:CARBONDATA-3065
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
store/sdk/src/main/java/org/apache/carbondata/sdk/file/CarbonWriterBuilder.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/TableSchemaBuilder.java
texts:by default disable inverted index for all the dimension column
Bottleneck with invertedIndex: As for each page first we will sort the data and generate inverted index, data loading performance will get impacted.because of this Store size is more because of stroing inverted index for each dimension column which results in more IO and it impacts query performance One extra lookup happenes during query due to presence of inverted index which is causing many cachline miss and it impacts the query performance
issueID:CARBONDATA-3066
type:Bug
changed files:
texts:ADD documentation for new APIs in SDK
ADD documentation for new APIs in SDK
issueID:CARBONDATA-3067
type:Improvement
changed files:datamap/bloom/src/main/java/org/apache/carbondata/datamap/bloom/BloomCoarseGrainDataMap.java
texts:Add check for debug to avoid string concat
for debug log, we should check before call log method to avoid unnecessary string concatenation
issueID:CARBONDATA-3068
type:Bug
changed files:
texts:cannot load data from hdfs files without hdfs prefix
sql:LOAD DATA INPATH '/tmp/test.csv' INTO TABLE test OPTIONS('QUOTECHAR'='"','TIMESTAMPFORMAT'='yyyy/MM/dd HH:mm:ss');error:org.apache.carbondata.processing.exception.DataLoadingException: The input file does not exist: /tmp/test.csv (state=,code=0)but the file "test.csv" is in hdfs path, and hadoop conf "core-site.xml" has the property:fs.defaultFShdfs://master:9000
issueID:CARBONDATA-3069
type:Bug
changed files:processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactDataHandlerModel.java
texts:fix bugs in setting cores for compaction

issueID:CARBONDATA-3071
type:Improvement
changed files:
texts:Add CarbonSession Java Example
Add CarbonSession example written in Java, including sql operations.
issueID:CARBONDATA-3073
type:Sub-task
changed files:examples/spark2/src/main/java/org/apache/carbondata/examples/sdk/SDKS3ReadExample.java
examples/spark2/src/main/java/org/apache/carbondata/examples/sdk/CarbonReaderExample.java
store/sdk/src/main/java/org/apache/carbondata/sdk/file/CarbonWriterBuilder.java
texts:Support  other interface in carbon writer of C++ SDK
when user create table and write data in C++ SDK, user sometimes need configure withTableProperties, so we should Support configure TableProperties in carbon writer of C++ SDK
issueID:CARBONDATA-3074
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
texts:Change default sort temp  compressor to SNAPPY

issueID:CARBONDATA-3075
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/scan/executor/impl/AbstractQueryExecutor.java
texts:Select Filter fails for Legacy store if DirectVectorFill is enabled
Please find below steps to reproduce the issue: Create table and load data in legacy store In new store, with Direct Vector filling Enabled, execute filter query and find below ExceptionThis operation is not supported in this reader org.apache.carbondata.core.datastore.chunk.reader.dimension.v2.CompressedDimensionChunkFileBasedReaderV2
issueID:CARBONDATA-3077
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/datamap/DataMapStoreManager.java
core/src/main/java/org/apache/carbondata/core/datamap/dev/DataMapFactory.java
texts:Fixed query failure in fileformat due stale cache issue
ProblemWhile using FileFormat API, if a table created, dropped and then recreated with the same name the query fails because of schema mismatch issue
issueID:CARBONDATA-3078
type:Bug
changed files:hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonTableInputFormat.java
texts:Exception caused by explain command for count star query without filter
Procedure to reproduce the problem: create table test_tbl; load some data into table; run query as "explain select count from test_tbl" ```Exception in thread "main" java.lang.IllegalStateException at org.apache.carbondata.core.profiler.ExplainCollector.getCurrentTablePruningInfo(ExplainCollector.java:162) at org.apache.carbondata.core.profiler.ExplainCollector.setShowPruningInfo(ExplainCollector.java:106) at org.apache.carbondata.core.indexstore.blockletindex.BlockDataMap.prune(BlockDataMap.java:696) at org.apache.carbondata.core.indexstore.blockletindex.BlockDataMap.prune(BlockDataMap.java:743) at org.apache.carbondata.core.indexstore.blockletindex.BlockletDataMapFactory.getAllBlocklets(BlockletDataMapFactory.java:391) at org.apache.carbondata.core.datamap.TableDataMap.prune(TableDataMap.java:132) at org.apache.carbondata.hadoop.api.CarbonTableInputFormat.getBlockRowCount(CarbonTableInputFormat.java:618) at org.apache.spark.sql.CarbonCountStar.doExecute(CarbonCountStar.scala:59) at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117) at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117) at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138) at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151) at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135) at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116) at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:92) at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:92) at org.apache.spark.sql.execution.command.table.CarbonExplainCommand.collectProfiler(CarbonExplainCommand.scala:54) at org.apache.spark.sql.execution.command.table.CarbonExplainCommand.processMetadata(CarbonExplainCommand.scala:45) at org.apache.spark.sql.execution.command.MetadataCommand.run(package.scala:68) at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58) at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56) at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:67) at org.apache.spark.sql.Dataset.<init>(Dataset.scala:183) at org.apache.spark.sql.CarbonSession$$anonfun$sql$1.apply(CarbonSession.scala:106) at org.apache.spark.sql.CarbonSession$$anonfun$sql$1.apply(CarbonSession.scala:95) at org.apache.spark.sql.CarbonSession.withProfiler(CarbonSession.scala:154) at org.apache.spark.sql.CarbonSession.sql(CarbonSession.scala:93) at org.apache.carbondata.examples.SQL_Prune$.main(Test.scala:101) at org.apache.carbondata.examples.SQL_Prune.main(Test.scala)Process finished with exit code 1```
issueID:CARBONDATA-308
type:Sub-task
changed files:core/src/main/java/org/apache/carbondata/core/datastore/block/Distributable.java
hadoop/src/main/java/org/apache/carbondata/hadoop/CarbonMultiBlockSplit.java
hadoop/src/main/java/org/apache/carbondata/hadoop/readsupport/CarbonReadSupport.java
hadoop/src/main/java/org/apache/carbondata/hadoop/util/CarbonInputFormatUtil.java
core/src/main/java/org/apache/carbondata/core/datastore/block/TableTaskInfo.java
hadoop/src/main/java/org/apache/carbondata/hadoop/readsupport/impl/DictionaryDecodeReadSupport.java
integration/spark-common/src/main/java/org/apache/carbondata/spark/merger/CarbonDataMergerUtil.java
hadoop/src/main/java/org/apache/carbondata/hadoop/CarbonRecordReader.java
core/src/main/java/org/apache/carbondata/hadoop/CarbonInputSplit.java
core/src/main/java/org/apache/carbondata/core/datastore/block/TableBlockInfo.java
core/src/main/java/org/apache/carbondata/core/scan/filter/FilterExpressionProcessor.java
integration/spark-common/src/main/java/org/apache/carbondata/spark/load/CarbonLoaderUtil.java
texts:Use CarbonInputFormat in CarbonScanRDD compute
Take CarbonScanRDD as the target RDD, modify as following:1. In driver side, only getSplit is required, so only filter condition is required, no need to create full QueryModel object, so we can move creation of QueryModel from driver side to executor side.2. use CarbonInputFormat.createRecordReader in CarbonScanRDD.compute instead of use QueryExecutor directly
issueID:CARBONDATA-3080
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/scan/executor/impl/AbstractQueryExecutor.java
hadoop/src/main/java/org/apache/carbondata/hadoop/util/CarbonVectorizedRecordReader.java
store/sdk/src/main/java/org/apache/carbondata/sdk/file/CarbonWriterBuilder.java
core/src/main/java/org/apache/carbondata/core/util/ThreadLocalTaskInfo.java
texts:Supporting local dictionary enable by default for SDK
Supporting local dictionary enable by default for SDK
issueID:CARBONDATA-3081
type:Bug
changed files:hadoop/src/main/java/org/apache/carbondata/hadoop/util/CarbonVectorizedRecordReader.java
core/src/main/java/org/apache/carbondata/core/metadata/datatype/DecimalType.java
texts:NPE when boolean column has null values with Vectorized SDK reader

issueID:CARBONDATA-3083
type:Bug
changed files:
texts:Null values are getting replaced by 0 after update operation.
create table negativeTable(intCol int, stringCol string, shortCol short) stored by 'carbondata'load data inpath 'hdfs://hacluster/user/dataWithNegativeValues.csv' into table negativeTable options('delimiter'=',','fileheader'='intCol,stringCol,shortCol','bad_records_action'='force')select * from negativeTableinsert into negativeTable select 0,null,-10insert into negativeTable select null,'inserted',20select * from negativeTableupdate negativeTable set (intCol) = (5) where intCol=0select * from negativeTable
issueID:CARBONDATA-3084
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/DefaultEncodingFactory.java
texts:data load with  float datatype falis with internal error
when data load is triggered for float datatype and data is exceeding the float max range, data load fails with following errorjava.lang.RuntimeException: internal error: FLOAT at org.apache.carbondata.core.datastore.page.encoding.DefaultEncodingFactory.fitMinMax(DefaultEncodingFactory.java:179) at org.apache.carbondata.core.datastore.page.encoding.DefaultEncodingFactory.selectCodecByAlgorithmForIntegral(DefaultEncodingFactory.java:259) at org.apache.carbondata.core.datastore.page.encoding.DefaultEncodingFactory.selectCodecByAlgorithmForFloating(DefaultEncodingFactory.java:337) at org.apache.carbondata.core.datastore.page.encoding.DefaultEncodingFactory.createEncoderForMeasureOrNoDictionaryPrimitive(DefaultEncodingFactory.java:130) at org.apache.carbondata.core.datastore.page.encoding.DefaultEncodingFactory.createEncoder(DefaultEncodingFactory.java:66) at org.apache.carbondata.processing.store.TablePage.encodeAndCompressMeasures(TablePage.java:385) at org.apache.carbondata.processing.store.TablePage.encode(TablePage.java:372) at org.apache.carbondata.processing.store.CarbonFactDataHandlerColumnar.processDataRows(CarbonFactDataHandlerColumnar.java:285) at org.apache.carbondata.processing.store.CarbonFactDataHandlerColumnar.access$500(CarbonFactDataHandlerColumnar.java:59) at org.apache.carbondata.processing.store.CarbonFactDataHandlerColumnar$Producer.call(CarbonFactDataHandlerColumnar.java:583) at org.apache.carbondata.processing.store.CarbonFactDataHandlerColumnar$Producer.call(CarbonFactDataHandlerColumnar.java:560)    Steps to reproduce arecreate table datatype_floa_byte(f float, b byte) using carbon;insert into datatype_floa_byte select 123.123,127;insert into datatype_floa_byte select "1.7976931348623157E308",-127;
issueID:CARBONDATA-3087
type:Improvement
changed files:processing/src/main/java/org/apache/carbondata/processing/store/writer/v3/CarbonFactDataWriterImplV3.java
processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactDataHandlerModel.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/TableInfo.java
processing/src/main/java/org/apache/carbondata/processing/util/CarbonDataProcessorUtil.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/CarbonTable.java
processing/src/main/java/org/apache/carbondata/processing/loading/DataLoadProcessBuilder.java
core/src/main/java/org/apache/carbondata/core/constants/SortScopeOptions.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/SorterFactory.java
processing/src/main/java/org/apache/carbondata/processing/loading/model/CarbonLoadModelBuilder.java
core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
texts:Prettify DESC FORMATTED output
Change output of DESC FORMATTED to:+----------------------------+-------------------------------------------------------------------------------------+-------+|col_name                    |data_type                                                                            |comment|+----------------------------+-------------------------------------------------------------------------------------+-------+|shortfield                  |smallint                                                                             |null   ||intfield                    |int                                                                                  |null   ||bigintfield                 |bigint                                                                               |null   ||doublefield                 |double                                                                               |null   ||stringfield                 |string                                                                               |null   ||timestampfield              |timestamp                                                                            |null   ||decimalfield                |decimal(18,2)                                                                        |null   ||datefield                   |date                                                                                 |null   ||charfield                   |string                                                                               |null   ||floatfield                  |double                                                                               |null   ||                            |                                                                                     |       ||## Table Basic Information  |                                                                                     |       ||Comment                     |                                                                                     |       ||Path                        |/Users/jacky/code/carbondata/examples/spark2/target/store/default/carbonsession_table|       ||Table Block Size            |1024 MB                                                                              |       ||Table Blocklet Size         |64 MB                                                                                |       ||Streaming                   |false                                                                                |       ||Flat Folder                 |false                                                                                |       ||Bad Record Path             |                                                                                     |       ||Min Input Per Node          |0.0B                                                                                 |       ||                            |                                                                                     |       ||## Index Information        |                                                                                     |       ||Sort Scope                  |LOCAL_SORT                                                                           |       ||Sort Columns                |stringfield,timestampfield,datefield,charfield                                       |       ||Index Cache Level           |BLOCK                                                                                |       ||Cached Index Columns        |All columns                                                                          |       ||                            |                                                                                     |       ||## Encoding Information     |                                                                                     |       ||Local Dictionary Enabled    |true                                                                                 |       ||Local Dictionary Threshold  |10000                                                                                |       ||Local Dictionary Include    |stringfield                                                                          |       ||                            |                                                                                     |       ||## Compaction Information   |                                                                                     |       ||MAJOR_COMPACTION_SIZE       |1024                                                                                 |       ||AUTO_LOAD_MERGE             |false                                                                                |       ||COMPACTION_LEVEL_THRESHOLD  |4,3                                                                                  |       ||COMPACTION_PRESERVE_SEGMENTS|0                                                                                    |       ||ALLOWED_COMPACTION_DAYS     |0                                                                                    |       ||                            |                                                                                     |       ||## Dynamic Information      |                                                                                     |       ||Table Data Size             |2.93KB                                                                               |       ||Table Index Size            |1.53KB                                                                               |       ||Last Update Time            |Wed Nov 07 17:16:01 CST 2018                                                         |       |+----------------------------+-------------------------------------------------------------------------------------+-------+
issueID:CARBONDATA-3088
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/scan/result/iterator/RawResultIterator.java
processing/src/main/java/org/apache/carbondata/processing/merger/CarbonCompactionExecutor.java
texts:enhance compaction performance by using prefetch

issueID:CARBONDATA-3095
type:Sub-task
changed files:store/sdk/src/main/java/org/apache/carbondata/sdk/file/CarbonReaderBuilder.java
store/sdk/src/main/java/org/apache/carbondata/sdk/file/CarbonWriterBuilder.java
texts:Optimize the documentation of SDK/CSDK

issueID:CARBONDATA-3096
type:Bug
changed files:integration/spark-datasource/src/main/scala/org/apache/carbondata/spark/vectorreader/VectorizedCarbonRecordReader.java
texts:Wrong records size on the input metrics
(1) Scanned record result size is taking from the default batch size. It should be taken from the records scanned.Steps to reproduce:spark.sql("DROP TABLE IF EXISTS person") spark.sql("create table person (id int, name string) stored by 'carbondata'") spark.sql("insert into person select 1,'a'") spark.sql("select * from person").show(false) (2) The intermediate page used to sort in adaptive encoding should be freed.
issueID:CARBONDATA-3098
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/datastore/page/statistics/PrimitivePageStatsCollector.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/adaptive/AdaptiveFloatingCodec.java
texts:Negative value exponents giving wrong results
Problem: When the value of exponent is a negative number then the data is incorrect due to loss of precision of Floating point values and wrong calculation of the count of decimal points. Steps to reproduce: -> "create table float_c(f float) using carbon"-> "insert into float_c select '1.4E-38' "
issueID:CARBONDATA-31
type:Improvement
changed files:
texts:Unify definition for SparkContext,CarbonContext,HiveContext,SQLContext
For SparkContext,CarbonContext,HiveContext,SQLContext definition, different places having different definition, for example:1)val sc = new CaronContext, val carboncontext = new CaronContext2)var oc: HiveContext , var context: HiveContext....To unify definition for SparkContext,CarbonContext,HiveContext,SQLContext as below:CarbonContext : ccSQLContext : sqlContextSparkContext : scHiveContext : hiveContext
issueID:CARBONDATA-310
type:Bug
changed files:
texts:Compilation failed when using spark 1.6.2
Compilation failed when using spark 1.6.2,caused by class not found: AggregateExpression
issueID:CARBONDATA-3100
type:Bug
changed files:
texts:Can not create mv：No package org.apache.carbondata.mv.datamap
scala> carbon.sql("create datamap web_site_mv USING 'mv' WITH DEFERRED REBUILD AS select * from web_site").showjava.lang.ClassNotFoundException: org.apache.carbondata.mv.datamap.MVDataMapProvider at scala.reflect.internal.util.AbstractFileClassLoader.findClass(AbstractFileClassLoader.scala:62) at java.lang.ClassLoader.loadClass(ClassLoader.java:424) at java.lang.ClassLoader.loadClass(ClassLoader.java:357) at java.lang.Class.forName0(Native Method) at java.lang.Class.forName(Class.java:348) at org.apache.spark.util.Utils$.classForName(Utils.scala:239) at org.apache.spark.util.CarbonReflectionUtils$.createObject(CarbonReflectionUtils.scala:322) at org.apache.carbondata.spark.util.CarbonScalaUtil$.createDataMapProvider(CarbonScalaUtil.scala:480) at org.apache.carbondata.spark.util.CarbonScalaUtil.createDataMapProvider(CarbonScalaUtil.scala) at org.apache.carbondata.datamap.DataMapManager.getDataMapProvider(DataMapManager.java:57) at org.apache.spark.sql.execution.command.datamap.CarbonCreateDataMapCommand.processMetadata(CarbonCreateDataMapCommand.scala:99) at org.apache.spark.sql.execution.command.AtomicRunnableCommand.run(package.scala:90) at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70) at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68) at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:79) at org.apache.spark.sql.Dataset$$anonfun$6.apply(Dataset.scala:190) at org.apache.spark.sql.Dataset$$anonfun$6.apply(Dataset.scala:190) at org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3259) at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77) at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3258) at org.apache.spark.sql.Dataset.<init>(Dataset.scala:190) at org.apache.spark.sql.CarbonSession$$anonfun$sql$1.apply(CarbonSession.scala:106) at org.apache.spark.sql.CarbonSession$$anonfun$sql$1.apply(CarbonSession.scala:95) at org.apache.spark.sql.CarbonSession.withProfiler(CarbonSession.scala:154) at org.apache.spark.sql.CarbonSession.sql(CarbonSession.scala:93) ... 53 elided
issueID:CARBONDATA-3102
type:Bug
changed files:
texts:There are some error when use thriftServer and beeline
When we run the thriftServer and beeline to connect, there are some exception:0: jdbc:hive2://127.0.0.1:10000> create  table xubo2 stored by 'carbondata' location 's3a://k8s/xubo3';Error: java.lang.NoClassDefFoundError: org/apache/http/impl/conn/PoolingClientConnectionManager (state=,code=0)
issueID:CARBONDATA-3104
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/indexstore/BlockletDataMapIndexWrapper.java
core/src/main/java/org/apache/carbondata/core/indexstore/BlockletDataMapIndexStore.java
texts:Extra Unnecessary Hadoop Conf is getting stored in LRU (~100K) for each LRU entry

issueID:CARBONDATA-3106
type:Bug
changed files:processing/src/main/java/org/apache/carbondata/processing/store/writer/v3/CarbonFactDataWriterImplV3.java
texts:Written_BY_APPNAME is not serialized in executor with GlobalSort
Problem:Written_By_APPNAME when added in carbonproperty is not serialized in executor with global sortSteps to Reproduce: Create table and set sort_scope='global_sort' Load data into table and find the exceptionException: There is an unexpected error: nullNOTE: This issue is reproducible only if driver and executor are running in a different JVM Process 
issueID:CARBONDATA-3107
type:Improvement
changed files:processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/SortDataRows.java
processing/src/main/java/org/apache/carbondata/processing/merger/CarbonCompactionUtil.java
core/src/main/java/org/apache/carbondata/core/datamap/DataMapStoreManager.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/merger/UnsafeIntermediateMerger.java
processing/src/main/java/org/apache/carbondata/processing/loading/steps/CarbonRowDataWriterProcessorStepImpl.java
core/src/main/java/org/apache/carbondata/core/util/DataTypeUtil.java
core/src/main/java/org/apache/carbondata/core/indexstore/BlockletDataMapIndexStore.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/SingleThreadFinalSortFilesMerger.java
processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactDataHandlerColumnar.java
tools/cli/src/main/java/org/apache/carbondata/tool/CarbonCli.java
core/src/main/java/org/apache/carbondata/core/datastore/filesystem/AbstractDFSCarbonFile.java
core/src/main/java/org/apache/carbondata/core/scan/result/iterator/AbstractDetailQueryResultIterator.java
core/src/main/java/org/apache/carbondata/core/util/CarbonProperties.java
integration/hive/src/main/java/org/apache/carbondata/hive/CarbonDictionaryDecodeReadSupport.java
core/src/main/java/org/apache/carbondata/core/util/path/HDFSLeaseUtils.java
core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/impl/ParallelReadMergeSorterWithColumnRangeImpl.java
core/src/main/java/org/apache/carbondata/core/locks/ZookeeperInit.java
core/src/main/java/org/apache/carbondata/core/scan/result/BlockletScannedResult.java
datamap/bloom/src/main/java/org/apache/carbondata/datamap/bloom/BloomCoarseGrainDataMapFactory.java
core/src/main/java/org/apache/carbondata/core/reader/CarbonDeleteFilesDataReader.java
core/src/main/java/org/apache/carbondata/core/util/ObjectSerializationUtil.java
datamap/lucene/src/main/java/org/apache/carbondata/datamap/lucene/LuceneFineGrainDataMap.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/impl/ParallelReadMergeSorterImpl.java
store/sdk/src/main/java/org/apache/carbondata/store/LocalCarbonStore.java
core/src/main/java/org/apache/carbondata/core/statusmanager/SegmentStatusManager.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/holder/UnsafeSortTempFileChunkHolder.java
integration/spark-datasource/src/main/scala/org/apache/carbondata/spark/vectorreader/VectorizedCarbonRecordReader.java
core/src/main/java/org/apache/carbondata/core/datastore/filesystem/AlluxioCarbonFile.java
processing/src/main/java/org/apache/carbondata/processing/merger/CompactionResultSortProcessor.java
core/src/main/java/org/apache/carbondata/core/scan/filter/FilterUtil.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/SortTempFileChunkHolder.java
core/src/main/java/org/apache/carbondata/core/datastore/filesystem/HDFSCarbonFile.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/UnsafeSortDataRows.java
core/src/main/java/org/apache/carbondata/core/statusmanager/LoadMetadataDetails.java
core/src/main/java/org/apache/carbondata/core/datastore/filesystem/ViewFSCarbonFile.java
core/src/main/java/org/apache/carbondata/core/datastore/filesystem/S3CarbonFile.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/impl/UnsafeParallelReadMergeSorterWithColumnRangeImpl.java
integration/hive/src/main/java/org/apache/carbondata/hive/MapredCarbonInputFormat.java
processing/src/main/java/org/apache/carbondata/processing/merger/RowResultMergerProcessor.java
processing/src/main/java/org/apache/carbondata/processing/loading/parser/impl/JsonRowParser.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/CarbonTable.java
core/src/main/java/org/apache/carbondata/core/mutate/CarbonUpdateUtil.java
processing/src/main/java/org/apache/carbondata/processing/loading/AbstractDataLoadProcessorStep.java
processing/src/main/java/org/apache/carbondata/processing/util/CarbonLoaderUtil.java
core/src/main/java/org/apache/carbondata/core/datamap/DataMapUtil.java
processing/src/main/java/org/apache/carbondata/processing/loading/converter/impl/RowConverterImpl.java
core/src/main/java/org/apache/carbondata/core/indexstore/BlockletDetailInfo.java
processing/src/main/java/org/apache/carbondata/processing/loading/model/CarbonLoadModelBuilder.java
core/src/main/java/org/apache/carbondata/core/datastore/impl/FileFactory.java
processing/src/main/java/org/apache/carbondata/processing/datamap/DataMapWriterListener.java
core/src/main/java/org/apache/carbondata/core/datastore/filesystem/LocalCarbonFile.java
processing/src/main/java/org/apache/carbondata/processing/merger/CarbonDataMergerUtil.java
datamap/lucene/src/main/java/org/apache/carbondata/datamap/lucene/LuceneFineGrainDataMapFactory.java
core/src/main/java/org/apache/carbondata/core/keygenerator/directdictionary/timestamp/DateDirectDictionaryGenerator.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/impl/UnsafeParallelReadMergeSorterImpl.java
integration/presto/src/main/java/org/apache/carbondata/presto/impl/CarbonTableReader.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/merger/UnsafeSingleThreadFinalSortFilesMerger.java
texts:Optimize error/exception coding for better debugging

issueID:CARBONDATA-3108
type:Bug
changed files:store/sdk/src/main/java/org/apache/carbondata/sdk/file/RowUtil.java
texts:Jvm will crash when CarbonRow use wrong index number in CSDK
Jvm will crash when CarbonRow use wrong index number in CSDKlike use : jobject row = reader.readNextRow();            carbonRow.setCarbonRow(row);            printf("%s\t", carbonRow.getString(1));crash:[Dynamic-linking native method java.lang.Float.intBitsToFloat ... JNI]## A fatal error has been detected by the Java Runtime Environment:##  SIGSEGV (0xb) at pc=0x000000010a24a94c, pid=25600, tid=0x0000000000000307## JRE version: Java(TM) SE Runtime Environment (8.0_171-b11) (build 1.8.0_171-b11)# Java VM: Java HotSpot(TM) 64-Bit Server VM (25.171-b11 mixed mode bsd-amd64 compressed oops)# Problematic frame:# V  [libjvm.dylib+0x32094c]## Failed to write core dump. Core dumps have been disabled. To enable core dumping, try "ulimit -c unlimited" before starting Java again## An error report file with more information is saved as:# /Users/xubo/Desktop/xubo/git/carbondata1/store/CSDK/cmake-build-debug/hs_err_pid25600.log## If you would like to submit a bug report, please visit:#   http://bugreport.java.com/bugreport/crash.jsp#List:getString,getVarchar,getArray,getDecimal
issueID:CARBONDATA-311
type:Improvement
changed files:
texts:Log the data size of blocklet during data load.
Log the data size of blocklet during data load.
issueID:CARBONDATA-3112
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/datastore/page/ColumnPageValueConverter.java
core/src/main/java/org/apache/carbondata/core/datastore/page/VarLengthColumnPageBase.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/ColumnPageDecoder.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/reader/dimension/v3/DimensionChunkReaderV3.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/adaptive/AdaptiveDeltaFloatingCodec.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/store/impl/safe/SafeVariableLengthDimensionDataChunkStore.java
core/src/main/java/org/apache/carbondata/core/datastore/compression/AbstractCompressor.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/compress/DirectCompressCodec.java
core/src/main/java/org/apache/carbondata/core/datastore/compression/ZstdCompressor.java
core/src/main/java/org/apache/carbondata/core/scan/result/vector/CarbonDictionary.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/adaptive/AdaptiveFloatingCodec.java
core/src/main/java/org/apache/carbondata/core/scan/result/vector/impl/CarbonDictionaryImpl.java
integration/spark-datasource/src/main/spark2.1andspark2.2/org/apache/spark/sql/CarbonVectorProxy.java
core/src/main/java/org/apache/carbondata/core/metadata/datatype/DecimalConverterFactory.java
core/src/main/java/org/apache/carbondata/core/scan/result/vector/impl/directread/ColumnarVectorWrapperDirectWithInvertedIndex.java
integration/spark-datasource/src/main/scala/org/apache/carbondata/spark/vectorreader/ColumnarVectorWrapperDirect.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/store/impl/safe/AbstractNonDictionaryVectorFiller.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/rle/RLECodec.java
core/src/main/java/org/apache/carbondata/core/scan/result/vector/impl/directread/ColumnarVectorWrapperDirectWithDeleteDeltaAndInvertedIndex.java
integration/spark-datasource/src/main/scala/org/apache/carbondata/spark/vectorreader/VectorizedCarbonRecordReader.java
integration/spark-datasource/src/main/spark2.3plus/org/apache/spark/sql/CarbonVectorProxy.java
core/src/main/java/org/apache/carbondata/core/scan/result/vector/impl/directread/ColumnarVectorWrapperDirectWithDeleteDelta.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/reader/measure/v3/MeasureChunkReaderV3.java
integration/spark-datasource/src/main/scala/org/apache/carbondata/spark/vectorreader/ColumnarVectorWrapper.java
core/src/main/java/org/apache/carbondata/core/scan/result/vector/impl/directread/AbstractCarbonColumnarVector.java
core/src/main/java/org/apache/carbondata/core/scan/result/vector/impl/CarbonColumnVectorImpl.java
core/src/main/java/org/apache/carbondata/core/util/ByteUtil.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/adaptive/AdaptiveDeltaIntegralCodec.java
integration/spark2/src/main/scala/org/apache/carbondata/stream/CarbonStreamRecordReader.java
core/src/main/java/org/apache/carbondata/core/scan/result/vector/CarbonColumnVector.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
core/src/main/java/org/apache/carbondata/core/datastore/compression/SnappyCompressor.java
core/src/main/java/org/apache/carbondata/core/datastore/page/statistics/PrimitivePageStatsCollector.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/adaptive/AdaptiveIntegralCodec.java
integration/presto/src/main/java/org/apache/carbondata/presto/CarbonColumnVectorWrapper.java
texts:Optimise decompressing while filling the vector during conversion of primitive types
We can possibly avoid one copy by filling the vector during the conversion of primitive types in codecs. 
issueID:CARBONDATA-3113
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/scan/executor/infos/BlockExecutionInfo.java
core/src/main/java/org/apache/carbondata/core/datastore/page/VarLengthColumnPageBase.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/ColumnPageDecoder.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/store/impl/safe/SafeVariableShortLengthDimensionDataChunkStore.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/reader/dimension/v3/DimensionChunkReaderV3.java
core/src/main/java/org/apache/carbondata/core/datastore/columnar/UnBlockIndexer.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/store/impl/safe/SafeVariableLengthDimensionDataChunkStore.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/adaptive/AdaptiveDeltaFloatingCodec.java
tools/cli/src/main/java/org/apache/carbondata/tool/ScanBenchmark.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/store/impl/LocalDictDimensionDataChunkStore.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/store/DimensionChunkStoreFactory.java
core/src/main/java/org/apache/carbondata/core/datastore/compression/AbstractCompressor.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/reader/measure/v3/MeasureChunkPageReaderV3.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/compress/DirectCompressCodec.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/store/impl/unsafe/UnsafeVariableLengthDimensionDataChunkStore.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/store/impl/unsafe/UnsafeFixedLengthDimensionDataChunkStore.java
core/src/main/java/org/apache/carbondata/core/datastore/compression/ZstdCompressor.java
core/src/main/java/org/apache/carbondata/core/scan/scanner/LazyPageLoader.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/impl/DimensionRawColumnChunk.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/reader/MeasureColumnChunkReader.java
core/src/main/java/org/apache/carbondata/core/scan/result/BlockletScannedResult.java
core/src/main/java/org/apache/carbondata/core/scan/result/vector/CarbonDictionary.java
core/src/main/java/org/apache/carbondata/core/datastore/page/ColumnPage.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/adaptive/AdaptiveFloatingCodec.java
core/src/main/java/org/apache/carbondata/core/scan/result/vector/impl/CarbonDictionaryImpl.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/store/impl/safe/AbstractNonDictionaryVectorFiller.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/rle/RLECodec.java
integration/spark-datasource/src/main/scala/org/apache/carbondata/spark/vectorreader/VectorizedCarbonRecordReader.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/reader/measure/AbstractMeasureChunkReader.java
core/src/main/java/org/apache/carbondata/core/datastore/ReusableDataBuffer.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/reader/measure/v3/MeasureChunkReaderV3.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/impl/MeasureRawColumnChunk.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/impl/VariableLengthDimensionColumnPage.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/store/impl/unsafe/UnsafeVariableIntLengthDimensionDataChunkStore.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/impl/FixedLengthDimensionColumnPage.java
core/src/main/java/org/apache/carbondata/core/scan/executor/impl/AbstractQueryExecutor.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/adaptive/AdaptiveDeltaIntegralCodec.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/store/impl/unsafe/UnsafeVariableShortLengthDimensionDataChunkStore.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/reader/DimensionColumnChunkReader.java
core/src/main/java/org/apache/carbondata/core/datastore/compression/Compressor.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/store/impl/unsafe/UnsafeAbstractDimensionDataChunkStore.java
core/src/main/java/org/apache/carbondata/core/datastore/compression/SnappyCompressor.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/store/impl/safe/SafeVariableIntLengthDimensionDataChunkStore.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/reader/dimension/v3/DimensionChunkPageReaderV3.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/adaptive/AdaptiveIntegralCodec.java
core/src/main/java/org/apache/carbondata/core/datastore/page/DecoderBasedFallbackEncoder.java
texts:Fixed Local Dictionary Query Performance  and Added reusable buffer for direct flow
1. Added reusable buffer for direct flowIn query for each page each column it is creating a byte array, when number of columns are high it is causing lots of minor gc and degrading query performance, as each page is getting uncompressed one by one we can use same buffer for all the columns and based on requested size it will resize.2. Fixed Local Dictionary performance issue.Reverted back #2895 and fixed NPE issue by setting null for local dictionary to vector In safe and Unsafe VariableLengthDataChunkStore
issueID:CARBONDATA-3114
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelRangeGrtrThanEquaToFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RangeValueFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/scan/filter/FilterUtil.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelRangeGrtThanFiterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelRangeLessThanFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelRangeLessThanEqualFilterExecuterImpl.java
texts:Remove Null Values for a Dictionary_Include Timestamp column for Range Filters
Issue:Null Values are not removed in case of RangeFilters, if column is a dictionary and no_inverted_index column
issueID:CARBONDATA-3115
type:Bug
changed files:
texts:Fix CodeGen error in preaggregate table and codegen display issue in oldstores
Issues: While querying a preaggregate table, codegen error is displayed In old stores, code is getting displayed while executing queries.
issueID:CARBONDATA-3116
type:Bug
changed files:
texts:set carbon.query.directQueryOnDataMap.enabled=true not working
When I run:    spark.sql("drop table if exists mainTable")    spark.sql(      """CREATE TABLE mainTable    (id Int,      name String,      city String,      age Int)    STORED BY 'org.apache.carbondata.format'""".stripMargin);    spark.sql("LOAD DATA LOCAL INPATH '/Users/xubo/Desktop/xubo/git/carbondata2/integration/spark-common-test/src/test/resources/sample.csv' into table mainTable");    spark.sql("create datamap preagg_sum on table mainTable using 'preaggregate' as select id,sum(age) from mainTable group by id");    spark.sql("show datamap on table mainTable");    spark.sql("set carbon.query.directQueryOnDataMap.enabled=true");    spark.sql("set carbon.query.directQueryOnDataMap.enabled");    spark.sql("select count(*) from maintable_preagg_sum").show();    spark.sql("select count(*) from maintable_preagg_sum").show();it will throw  Exception2018-11-22 00:06:01 AUDIT audit:93 - {"time":"November 22, 2018 12:06:01 AM CST","username":"xubo","opName":"SET","opId":"344656521959523","opStatus":"SUCCESS","opTime":"1 ms","table":"NA","extraInfo":{}}Exception in thread "main" org.apache.spark.sql.AnalysisException: Query On DataMap not supported; at org.apache.spark.sql.optimizer.CarbonLateDecodeRule.validateQueryDirectlyOnDataMap(CarbonLateDecodeRule.scala:131) at org.apache.spark.sql.optimizer.CarbonLateDecodeRule.checkIfRuleNeedToBeApplied(CarbonLateDecodeRule.scala:79) at org.apache.spark.sql.optimizer.CarbonLateDecodeRule.apply(CarbonLateDecodeRule.scala:53) at org.apache.spark.sql.optimizer.CarbonLateDecodeRule.apply(CarbonLateDecodeRule.scala:47) at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:85) at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:82) at scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:124) at scala.collection.immutable.List.foldLeft(List.scala:84) at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:82) at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:74) at scala.collection.immutable.List.foreach(List.scala:381) at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:74) at org.apache.spark.sql.hive.CarbonOptimizer.execute(CarbonOptimizer.scala:35) at org.apache.spark.sql.hive.CarbonOptimizer.execute(CarbonOptimizer.scala:27) at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:78) at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:78) at org.apache.spark.sql.execution.QueryExecution.sparkPlan$lzycompute(QueryExecution.scala:84) at org.apache.spark.sql.execution.QueryExecution.sparkPlan(QueryExecution.scala:80) at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:89) at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:89) at org.apache.spark.sql.Dataset.withAction(Dataset.scala:2837) at org.apache.spark.sql.Dataset.head(Dataset.scala:2150) at org.apache.spark.sql.Dataset.take(Dataset.scala:2363) at org.apache.spark.sql.Dataset.showString(Dataset.scala:241) at org.apache.spark.sql.Dataset.show(Dataset.scala:637) at org.apache.spark.sql.Dataset.show(Dataset.scala:596) at org.apache.spark.sql.Dataset.show(Dataset.scala:605) at org.apache.carbondata.examples.PreAggregateDataMapExample$.exampleBody(PreAggregateDataMapExample.scala:63) at org.apache.carbondata.examples.PreAggregateDataMapExample$.main(PreAggregateDataMapExample.scala:34) at org.apache.carbondata.examples.PreAggregateDataMapExample.main(PreAggregateDataMapExample.scala)
issueID:CARBONDATA-3117
type:Bug
changed files:
texts:Rearrange the projection list in the Scan

issueID:CARBONDATA-3118
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockDataMap.java
core/src/main/java/org/apache/carbondata/core/datamap/dev/cgdatamap/CoarseGrainDataMap.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockletDataMap.java
core/src/main/java/org/apache/carbondata/core/datamap/SegmentDataMapGroup.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonInputFormat.java
core/src/main/java/org/apache/carbondata/core/datamap/dev/DataMap.java
core/src/main/java/org/apache/carbondata/core/datamap/TableDataMap.java
core/src/main/java/org/apache/carbondata/core/profiler/TablePruningInfo.java
datamap/examples/src/minmaxdatamap/main/java/org/apache/carbondata/datamap/examples/MinMaxIndexDataMap.java
core/src/main/java/org/apache/carbondata/core/datamap/dev/fgdatamap/FineGrainDataMap.java
texts:Parallelize block pruning of default datamap in driver  for filter query processing
*"Parallelize block pruning of default datamap in driver for filter query processing"* Background: We do block pruning for the filter queries at the driver side. In real time big data scenario, we can have millions of carbon files for one carbon table. It is currently observed that for 1 million carbon files it takes around 5 seconds for block pruning. As each carbon file takes around 0.005ms for pruning (with only one filter columns set in 'column_meta_cache' tblproperty). If the files are more, we might take more time for block pruning. Also, spark Job will not be launched until block pruning is completed. so, the user will not know what is happening at that time and why spark job is not launching. currently, block pruning is taking time as each segment processing is happening sequentially. we can reduce the time by parallelizing it. *solution:*Keep default number of threads for block pruning as 4. User can reduce this number by a carbon property "carbon.max.driver.threads.for.pruning" to set between -> 1 to 4. In TableDataMap.prune(), group the segments as per the threads by distributing equal carbon files to each thread. Launch the threads for a group of segments to handle block pruning. 
issueID:CARBONDATA-3119
type:Improvement
changed files:processing/src/main/java/org/apache/carbondata/processing/loading/parser/impl/ArrayParserImpl.java
texts:The getOrCreateCarbonSession method &#39;storePath&#39; parameter only checks for null,Causes the data store path to be an empty string and no exception is reported.Results in no results for the final select data
val carbon = SparkSession.builder().config(sc.getConf).getOrCreateCarbonSession("");if (storePath != null) { carbonProperties.addProperty(CarbonCommonConstants.STORE_LOCATION, storePath) // In case if it is in carbon.properties for backward compatible} else if (carbonProperties.getProperty(CarbonCommonConstants.STORE_LOCATION) == null) { carbonProperties.addProperty(CarbonCommonConstants.STORE_LOCATION, session.sessionState.conf.warehousePath)}explain:The getOrCreateCarbonSession method storePath parameter only checks for null,Causes the data store path to be an empty string and no exception is reported.Results in no results for the final select data.Please see the attachment for examples
issueID:CARBONDATA-3120
type:Bug
changed files:
texts:apache-carbondata-1.5.1-rc1.tar.gz Datamap&#39;s core and plan project, pom.xml, is version 1.5.0, which results in an inability to compile properly
Hi,guy!       I download the apache-carbondata-1.5.1-rc1.tar.gz。       After decompression, the datamap mv/core mv/plan project was added to the main pom for compilation。       But the But the compilation failed。 LOG:[ERROR] [ERROR] Some problems were encountered while processing the POMs:[FATAL] Non-resolvable parent POM for org.apache.carbondata:carbondata-mv-core:[unknown-version]: Could not find artifact org.apache.carbondata:carbondata-parent:pom:1.5.0-SNAPSHOT and 'parent.relativePath' points at wrong local POM @ line 22, column 11[FATAL] Non-resolvable parent POM for org.apache.carbondata:carbondata-mv-plan:[unknown-version]: Could not find artifact org.apache.carbondata:carbondata-parent:pom:1.5.0-SNAPSHOT and 'parent.relativePath' points at wrong local POM @ line 22, column 11[WARNING] 'build.plugins.plugin.version' for com.ning.maven.plugins:maven-duplicate-finder-plugin is missing. @ org.apache.carbondata:carbondata-presto:[unknown-version], /Users/jonathanwei/summary/carbondata/carbondata-apache-carbondata-1.5.1-rc1/integration/presto/pom.xml, line 620, column 15[WARNING] 'build.plugins.plugin.version' for pl.project13.maven:git-commit-id-plugin is missing. @ org.apache.carbondata:carbondata-presto:[unknown-version], /Users/jonathanwei/summary/carbondata/carbondata-apache-carbondata-1.5.1-rc1/integration/presto/pom.xml, line 633, column 15[WARNING] 'build.plugins.plugin.version' for com.ning.maven.plugins:maven-duplicate-finder-plugin is missing. @ org.apache.carbondata:carbondata-examples-spark2:[unknown-version], /Users/jonathanwei/summary/carbondata/carbondata-apache-carbondata-1.5.1-rc1/examples/spark2/pom.xml, line 184, column 15 @[ERROR] The build could not read 2 projects -> [Help 1][ERROR][ERROR]   The project org.apache.carbondata:carbondata-mv-core:[unknown-version] (/Users/jonathanwei/summary/carbondata/carbondata-apache-carbondata-1.5.1-rc1/datamap/mv/core/pom.xml) has 1 error[ERROR]     Non-resolvable parent POM for org.apache.carbondata:carbondata-mv-core:[unknown-version]: Could not find artifact org.apache.carbondata:carbondata-parent:pom:1.5.0-SNAPSHOT and 'parent.relativePath' points at wrong local POM @ line 22, column 11 -> [Help 2][ERROR][ERROR]   The project org.apache.carbondata:carbondata-mv-plan:[unknown-version] (/Users/jonathanwei/summary/carbondata/carbondata-apache-carbondata-1.5.1-rc1/datamap/mv/plan/pom.xml) has 1 error[ERROR]     Non-resolvable parent POM for org.apache.carbondata:carbondata-mv-plan:[unknown-version]: Could not find artifact org.apache.carbondata:carbondata-parent:pom:1.5.0-SNAPSHOT and 'parent.relativePath' points at wrong local POM @ line 22, column 11 -> [Help 2][ERROR][ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.[ERROR] Re-run Maven using the -X switch to enable full debug logging.[ERROR][ERROR] For more information about the errors and possible solutions, please read the following articles:[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/ProjectBuildingException[ERROR] [Help 2] http://cwiki.apache.org/confluence/display/MAVEN/UnresolvableModelExceptionI check the pom file, parent.version is 1.5.0-snapshot. But apache-carbondata-1.5.1-rc1.tar.gz is 1.5.1.mv/core pom.xml<parent><groupId>org.apache.carbondata</groupId><artifactId>carbondata-parent</artifactId><version>1.5.0-SNAPSHOT</version><relativePath>../../../pom.xml</relativePath></parent><artifactId>carbondata-mv-core</artifactId><name>Apache CarbonData :: Materialized View Core</name>mv/plan pom.xml<parent><groupId>org.apache.carbondata</groupId><artifactId>carbondata-parent</artifactId><version>1.5.0-SNAPSHOT</version><relativePath>../../../pom.xml</relativePath></parent><artifactId>carbondata-mv-plan</artifactId><name>Apache CarbonData :: Materialized View Plan</name>
issueID:CARBONDATA-3121
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/scan/result/iterator/ChunkRowIterator.java
texts:CarbonReader build time is huge
CarbonReader build is fetching data and triggering I/O operation instead of only initializing the iterator, thus large build time.
issueID:CARBONDATA-3122
type:Bug
changed files:hadoop/src/main/java/org/apache/carbondata/hadoop/CarbonRecordReader.java
core/src/main/java/org/apache/carbondata/core/scan/processor/DataBlockIterator.java
texts:CarbonReader memory leak
CarbonReader For All split one last page will be always in memory until  CarbonReader object is not out of scope. 
issueID:CARBONDATA-3123
type:Bug
changed files:store/sdk/src/main/java/org/apache/carbondata/sdk/file/CarbonReader.java
texts:JVM crash when reading through CarbonReader
How to Reproduce: // ExecutorService executorService = Executors.newFixedThreadPool(8);try {  CarbonReader reader2 = CarbonReader.builder(dataDir).withRowRecordReader().build();  List<CarbonReader> multipleReaders = reader2.split(8);  try {    List<ReadLogic> tasks = new ArrayList<>();    List<Future> results = new ArrayList<>();    count = 0;    long start = System.currentTimeMillis();    for (CarbonReader reader_i : multipleReaders) {      results.add(executorService.submit(new ReadLogic(reader_i)));    }    for (Future result_i : results) {      count += (long) result_i.get();    }    long end = System.currentTimeMillis();    System.out.println("[Parallel read] Time: " + (end - start) + " ms");    Assert.assertEquals(numFiles * numRowsPerFile, count);  } catch (Exception e) {    e.printStackTrace();    Assert.fail(e.getMessage());  }} catch (Exception e) {  e.printStackTrace();  Assert.fail(e.getMessage());} finally {  executorService.shutdown();  executorService.awaitTermination(10, TimeUnit.MINUTES);  CarbonProperties.getInstance()      .addProperty(CarbonCommonConstants.ENABLE_UNSAFE_IN_QUERY_EXECUTION, "false");}When the above code is executed the JVM crashes with a SIGSEGV fault error.ERROR:## A fatal error has been detected by the Java Runtime Environment:## SIGSEGV (0xb) at pc=0x00007f28b31aeb4f, pid=78614, tid=139809329497856## JRE version: Java(TM) SE Runtime Environment (8.0_40-b26) (build 1.8.0_40-b26)# Java VM: Java HotSpot(TM) 64-Bit Server VM (25.40-b25 mixed mode linux-amd64 compressed oops)# Problematic frame:# C [libc.so.6+0x7bb4f] _int_malloc+0x2bf## Core dump written. Default location: /opt/SDKReaderConcurrent/core or core.78614 (max size 1 kB). To ensure a full core dump, try "ulimit -c unlimited" before starting Java again## If you would like to submit a bug report, please visit:# http://bugreport.java.com/bugreport/crash.jsp#--------------- T H R E A D ---------------Current thread (0x000000000289b000): JavaThread "pool-1-thread-3" [_thread_in_vm, id=78852, stack(0x00007f27e55bc000,0x00007f27e56bd000)]siginfo: si_signo: 11 (SIGSEGV), si_code: 128 (SI_KERNEL), si_addr: 0x0000000000000000Registers:RAX=0x000000000001f411, RBX=0x145cc4c0145cc4c0, RCX=0x00007f2888000078, RDX=0x000000000001f410RSP=0x00007f27e56bae10, RBP=0x00007f27e56baf20, RSI=0x000000000001f400, RDI=0x00007f2888000020R8 =0x000000000001f400, R9 =0x00007f287b038ab8, R10=0x00007f28a2d1a738, R11=0x0000000000000000R12=0x00007f2888fb4f30, R13=0x000000000001f400, R14=0x0000000000000000, R15=0x0000000000000000RIP=0x00007f28b31aeb4f, EFLAGS=0x0000000000010216, CSGSFS=0x0000000000000033, ERR=0x0000000000000000TRAPNO=0x000000000000000dTop of Stack: (sp=0x00007f27e56bae10)0x00007f27e56bae10: 00007f28a2a86e80 000000000001f4300x00007f27e56bae20: 00007f2888000088 000000000001f4000x00007f27e56bae30: 00007f2888000020 000000000001f4100x00007f27e56bae40: 0000007a00000000 000000000289b0000x00007f27e56bae50: 00007f27e56bae70 00007f28b23db2760x00007f27e56bae60: 00007f28a2a86aed 00007f28a2a86aed0x00007f27e56bae70: 00007f2888000078 00001f410289b0000x00007f27e56bae80: 000000000289b000 00007f28880000200x00007f27e56bae90: 00007f27e56baf20 000000000001f4000x00007f27e56baea0: 000000000001f400 00000000000000000x00007f27e56baeb0: 00007f27e56baf30 00007f28b31b11b70x00007f27e56baec0: 000000000289b000 00007f28b2ec99980x00007f27e56baed0: 00007f27e56baf20 000000000001f4000x00007f27e56baee0: 000000000001f400 00007f28b28694450x00007f27e56baef0: 000000070289b000 00007f27e56baf300x00007f27e56baf00: 0000000000000007 000000000001f4000x00007f27e56baf10: 00007f28b2efb6a0 000000000289b0000x00007f27e56baf20: 00007f27e56baf80 00007f28b28696a30x00007f27e56baf30: 0000000000000000 00000000000000000x00007f27e56baf40: 0000000000000000 00000000000000000x00007f27e56baf50: 0000000000000000 000000000001f4000x00007f27e56baf60: 00007f27e56bafc0 000000000289b0000x00007f27e56baf70: 000000000001f400 00007f28b2effc000x00007f27e56baf80: 00007f27e56bafc0 00007f28b29e1e2b0x00007f27e56baf90: 000000000289b000 000000074c19e5a00x00007f27e56bafa0: 00000005dc0ae7f0 00000000000000000x00007f27e56bafb0: 0000000000000006 00000007540c1fa80x00007f27e56bafc0: 00007f27e56bb010 00007f28a2d1a7a80x00007f27e56bafd0: 00000005dc0ae7f0 00007f28f803345b0x00007f27e56bafe0: 00000005d5dfd558 00000006546e0b500x00007f27e56baff0: 0000000000000002 00007f28a2b875680x00007f27e56bb000: 0000000000000001 00007f2888085740 Instructions: (pc=0x00007f28b31aeb4f)0x00007f28b31aeb2f: f8 48 81 7c 24 28 ff 03 00 00 77 0b 48 39 5c 240x00007f28b31aeb3f: 60 0f 84 aa 01 00 00 48 8b 4c 24 60 48 89 59 180x00007f28b31aeb4f: 48 89 4b 10 48 39 54 24 28 0f 84 8a 04 00 00 480x00007f28b31aeb5f: 81 fa ff 03 00 00 0f 86 25 ff ff ff 48 89 d0 48 Register to memory mapping:RAX=0x000000000001f411 is an unknown valueRBX=0x145cc4c0145cc4c0 is an unknown valueRCX=0x00007f2888000078 is an unknown valueRDX=0x000000000001f410 is an unknown valueRSP=0x00007f27e56bae10 is pointing into the stack for thread: 0x000000000289b000RBP=0x00007f27e56baf20 is pointing into the stack for thread: 0x000000000289b000RSI=0x000000000001f400 is an unknown valueRDI=0x00007f2888000020 is an unknown valueR8 =0x000000000001f400 is an unknown valueR9 =0x00007f287b038ab8 is pointing into metadataR10=0x00007f28a2d1a738 is at entry_point+56 in (nmethod*)0x00007f28a2d1a590R11=0x0000000000000000 is an unknown valueR12=0x00007f2888fb4f30 is an unknown valueR13=0x000000000001f400 is an unknown valueR14=0x0000000000000000 is an unknown valueR15=0x0000000000000000 is an unknown valueStack: [0x00007f27e55bc000,0x00007f27e56bd000], sp=0x00007f27e56bae10, free space=1019kNative frames: (J=compiled Java code, j=interpreted, Vv=VM code, C=native code)C [libc.so.6+0x7bb4f] _int_malloc+0x2bfV [libjvm.so+0x9076a3] os::malloc(unsigned long, MemoryType)+0x73V [libjvm.so+0xa7fe2b] Unsafe_AllocateMemory+0x1dbJ 2094 sun.misc.Unsafe.allocateMemory(J)J (0 bytes) @ 0x00007f28a2d1a7a8 [0x00007f28a2d1a700+0xa8]J 2269 C1 org.apache.carbondata.core.memory.UnsafeMemoryManager.allocateMemory(Lorg/apache/carbondata/core/memory/MemoryType;Ljava/lang/String;J)Lorg/apache/carbondata/core/memory/MemoryBlock; (211 bytes) @ 0x00007f28a29747a4 [0x00007f28a2973680+0x1124]
issueID:CARBONDATA-3124
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/memory/UnsafeMemoryManager.java
texts:Updated log message in Unsafe Memory Manager and changed faq.md accordingly.
>Log message in UnsafeMemoryManager was not displaying the onheap working memory block when debug mode enabled.>changed faq.md accordingly.
issueID:CARBONDATA-3126
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/metadata/schema/table/CarbonTable.java
core/src/main/java/org/apache/carbondata/core/scan/collector/impl/DictionaryBasedResultCollector.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonFileInputFormat.java
core/src/main/java/org/apache/carbondata/core/datastore/filesystem/CarbonFile.java
core/src/main/java/org/apache/carbondata/core/util/CarbonProperties.java
processing/src/main/java/org/apache/carbondata/processing/datatypes/StructDataType.java
processing/src/main/java/org/apache/carbondata/processing/loading/model/CarbonLoadModelBuilder.java
core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
core/src/main/java/org/apache/carbondata/core/datastore/impl/FileFactory.java
core/src/main/java/org/apache/carbondata/core/datastore/filesystem/LocalCarbonFile.java
common/src/main/java/org/apache/carbondata/common/Strings.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonTableInputFormat.java
core/src/main/java/org/apache/carbondata/core/statusmanager/SegmentStatusManager.java
core/src/main/java/org/apache/carbondata/core/scan/collector/impl/DictionaryBasedVectorResultCollector.java
core/src/main/java/org/apache/carbondata/core/statusmanager/LoadMetadataDetails.java
texts:There are some spell error in CarbonData
For example: rowbatch. => rowBatch metastoredb => metaStoreDB? porpeties. => propertiesothers: testvalidateLockType stram recurssive dont implictsExisted delimeter
issueID:CARBONDATA-3127
type:Bug
changed files:integration/hive/src/main/java/org/apache/carbondata/hive/MapredCarbonInputFormat.java
integration/hive/src/main/java/org/apache/carbondata/hive/CarbonHiveInputSplit.java
texts:Hive module test case has been commented off,can&#39; t run.
Hive module test case has been commented off，can' t run:/Users/xubo/Desktop/xubo/git/carbondata3/integration/hive/src/test/java/org/apache/carbondata/hive/TestCarbonSerde.java//package org.apache.carbondata.hive;////import junit.framework.TestCase;//import org.apache.hadoop.conf.Configuration;//import org.apache.hadoop.hive.common.type.HiveDecimal;//import org.apache.hadoop.hive.serde2.SerDeException;//import org.apache.hadoop.hive.serde2.SerDeUtils;//import org.apache.hadoop.hive.serde2.io.DoubleWritable;//import org.apache.hadoop.hive.serde2.io.HiveDecimalWritable;//import org.apache.hadoop.hive.serde2.io.ShortWritable;//import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;//import org.apache.hadoop.io.*;//import org.junit.Test;////import java.util.Properties;////public class TestCarbonSerde extends TestCase {//  @Test//  public void testCarbonHiveSerDe() throws Throwable {//    try {//      // Create the SerDe//      System.out.println("test: testCarbonHiveSerDe");////      final CarbonHiveSerDe serDe = new CarbonHiveSerDe();//      final Configuration conf = new Configuration();//      final Properties tbl = createProperties();//      SerDeUtils.initializeSerDe(serDe, conf, tbl, null);////      // Data//      final Writable[] arr = new Writable[7];We should find the reason and try to fix it, let it work
issueID:CARBONDATA-3128
type:Bug
changed files:
texts:HiveExample has some exception
HiveExample has some exception:org.apache.carbondata.hiveexample.HiveExampleVersion:clean -DskipTests -Pspark-2.1  -Pbuild-with-format  -Pspark.version=2.1.1 install2018-11-26 15:06:01 INFO  CarbonProperties:1512 - Considered value for min max byte limit for string is: 200Exception in thread "main" java.lang.ClassNotFoundException: org.apache.spark.sql.hive.CarbonSessionStateBuilder at java.net.URLClassLoader.findClass(URLClassLoader.java:381) at java.lang.ClassLoader.loadClass(ClassLoader.java:424) at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349) at java.lang.ClassLoader.loadClass(ClassLoader.java:357) at java.lang.Class.forName0(Native Method) at java.lang.Class.forName(Class.java:348) at org.apache.spark.util.Utils$.classForName(Utils.scala:230) at org.apache.spark.util.CarbonReflectionUtils$.createObject(CarbonReflectionUtils.scala:322) at org.apache.spark.util.CarbonReflectionUtils$.getSessionState(CarbonReflectionUtils.scala:218) at org.apache.spark.sql.CarbonSession.sessionState$lzycompute(CarbonSession.scala:56) at org.apache.spark.sql.CarbonSession.sessionState(CarbonSession.scala:55) at org.apache.spark.sql.CarbonSession$CarbonBuilder$$anonfun$getOrCreateCarbonSession$2.apply(CarbonSession.scala:258) at org.apache.spark.sql.CarbonSession$CarbonBuilder$$anonfun$getOrCreateCarbonSession$2.apply(CarbonSession.scala:258) at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99) at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99) at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230) at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40) at scala.collection.mutable.HashMap.foreach(HashMap.scala:99) at org.apache.spark.sql.CarbonSession$CarbonBuilder.getOrCreateCarbonSession(CarbonSession.scala:258) at org.apache.carbondata.hiveexample.HiveExample$.main(HiveExample.scala:51) at org.apache.carbondata.hiveexample.HiveExample.main(HiveExample.scala)Spark2.3:clean -DskipTests   -Pspark-2.3 -Pbuild-with-format  install2018-11-26 15:07:58 WARN  Utils:66 - Set SPARK_LOCAL_IP if you need to bind to another addressException in thread "main" java.lang.NoSuchMethodError: org.apache.spark.internal.config.package$.SHUFFLE_SPILL_NUM_ELEMENTS_FORCE_SPILL_THRESHOLD()Lorg/apache/spark/internal/config/ConfigEntry; at org.apache.spark.sql.internal.SQLConf$.<init>(SQLConf.scala:1011) at org.apache.spark.sql.internal.SQLConf$.<clinit>(SQLConf.scala) at org.apache.spark.sql.internal.StaticSQLConf$.<init>(StaticSQLConf.scala:31) at org.apache.spark.sql.internal.StaticSQLConf$.<clinit>(StaticSQLConf.scala) at org.apache.spark.sql.SparkSession$Builder.enableHiveSupport(SparkSession.scala:867) at org.apache.carbondata.hiveexample.HiveExample$.main(HiveExample.scala:50) at org.apache.carbondata.hiveexample.HiveExample.main(HiveExample.scala)Please test spark 2.2 tooWe should fix it.
issueID:CARBONDATA-3131
type:Sub-task
changed files:
texts:Update the requested columns to the Scan

issueID:CARBONDATA-3132
type:Bug
changed files:processing/src/main/java/org/apache/carbondata/processing/util/CarbonLoaderUtil.java
texts:Unequal distribution of tasks in case of compaction
When the load is done using replication factor 2 and all nodes are active and during compaction one node is down, basically it is not active executors, so the task distribution should take care to distribute the tasks equally among all the active executors instead of giving more tasks to single executor and less to other executor
issueID:CARBONDATA-3133
type:Improvement
changed files:
texts:Update carbondata build document
Update the document to add spark 2.3.2 and add datamap mv compiling method.
issueID:CARBONDATA-3134
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/metadata/schema/table/column/ColumnSchema.java
core/src/main/java/org/apache/carbondata/core/datastore/block/SegmentPropertiesAndSchemaHolder.java
texts:Wrong result when a column is dropped and added using alter with blocklet cache.
Steps to reproduce:spark.sql("drop table if exists tile")spark.sql("create table tile(b int, s int,bi bigint, t timestamp) partitioned by (i int) stored by 'carbondata' TBLPROPERTIES ('DICTIONARY_EXCLUDE'='b,s,i,bi,t','SORT_COLUMS'='b,s,i,bi,t', 'cache_level'='blocklet')")spark.sql("load data inpath 'C:/Users/k00475610/Documents/en_all.csv' into table tile options('fileheader'='b,s,i,bi,t','DELIMITER'=',')")spark.sql("select * from tile") spark.sql("alter table tile drop columns(t)") spark.sql("alter table tile add columns(t timestamp)") spark.sql("load data inpath 'C:/Users/k00475610/Documents/en_all.csv' into table tile options('fileheader'='b,s,i,bi,t','DELIMITER'=',')")spark.sql("select * from tile").show() Result:-----------------------| b| s| bi| t| i|-----------------------1002000093405673097null164555541002000093405673097null16455554-----------------------
issueID:CARBONDATA-3136
type:Improvement
changed files:
texts:JVM crash with preaggregate datamap
JVM crash with preaggregate datamap.callstack:Stack: &#91;0x00007efebd49a000,0x00007efebd59b000&#93;,  sp=0x00007efebd598dc8,  free space=1019k Native frames: (J=compiled Java code, j=interpreted, Vv=VM code, C=native code) V  &#91;libjvm.so+0x7b2b50&#93; J 7620  sun.misc.Unsafe.copyMemory(Ljava/lang/Object;JLjava/lang/Object;JJ)V (0 bytes) @ 0x00007eff4a3479e1 &#91;0x00007eff4a347900+0xe1&#93; j  org.apache.spark.unsafe.Platform.copyMemory(Ljava/lang/Object;JLjava/lang/Object;JJ)V+34 j  org.apache.spark.sql.catalyst.expressions.UnsafeRow.getBinary(I)[B+54 j  org.apache.spark.sql.catalyst.expressions.UnsafeRow.getDecimal(III)Lorg/apache/spark/sql/types/Decimal;+30 j  org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Lorg/apache/spark/sql/catalyst/InternalRow;)Lorg/apache/spark/sql/catalyst/expressions/UnsafeRow;+36 j  org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Ljava/lang/Object;)Ljava/lang/Object;+5 J 3104 C1 scala.collection.Iterator$$anon$11.next()Ljava/lang/Object; (19 bytes) @ 0x00007eff49154724 &#91;0x00007eff49154560+0x1c4&#93; j  org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(Lscala/collection/Iterator;)Lscala/collection/Iterator;+78 j  org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(Ljava/lang/Object;)Ljava/lang/Object;+5 J 14007 C1 org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(Ljava/lang/Object;Ljava/lang/Object;Ljava/lang/Object;)Ljava/lang/Object; (17 bytes) @ 0x00007eff4a6ed204 &#91;0x00007eff4a6ecc40+0x5c4&#93; J 11684 C1 org.apache.spark.rdd.MapPartitionsRDD.compute(Lorg/apache/spark/Partition;Lorg/apache/spark/TaskContext;)Lscala/collection/Iterator; (36 bytes) @ 0x00007eff4ad11274 &#91;0x00007eff4ad10f60+0x314&#93; J 13771 C1 org.apache.spark.rdd.RDD.iterator(Lorg/apache/spark/Partition;Lorg/apache/spark/TaskContext;)Lscala/collection/Iterator; (46 bytes) @ 0x00007eff4b39dd3c  &#91;0x00007eff4b39d160+0xbdc&#93;  test("Test Pre_aggregate with decimal column with order by") { sql("drop table if exists maintable") sql("create table maintable(name string, decimal_col decimal(30,16)) stored by 'carbondata'") sql("insert into table maintable select 'abc',452.564") sql( "create datamap ag1 on table maintable using 'preaggregate' as select name,avg(decimal_col)" + " from maintable group by name") checkAnswer(sql("select avg(decimal_col) from maintable group by name order by name"), Seq(Row(452.56400000000000000000)))}
issueID:CARBONDATA-3137
type:Sub-task
changed files:
texts:Update the Project List

issueID:CARBONDATA-3138
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/scan/filter/ColumnFilterInfo.java
core/src/main/java/org/apache/carbondata/core/datamap/TableDataMap.java
texts:Random count mismatch in query in multi-thread block-pruning scenario
problem: Random count mismatch in query in multi-thread block-pruning scenario.cause:Existing prune method not meant for multi-threading as synchronization was missiing. only in implicit filter scenario, while preparing the block ID list, synchronization was missing. Hence pruning was giving wrong result.solution: syncronize the imlicit filter prepartion, as prune now called in multi-thread  
issueID:CARBONDATA-3140
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/memory/UnsafeMemoryManager.java
texts:Block cretae table like command for carbon table
after create table like command on carbon table as source table, and dropping the new table, source table is geting dropped in spark 2.1,
issueID:CARBONDATA-3141
type:Test
changed files:
texts:Remove Carbon Table Detail Test Case

issueID:CARBONDATA-3142
type:Improvement
changed files:processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactDataHandlerColumnar.java
core/src/main/java/org/apache/carbondata/core/util/CarbonThreadFactory.java
texts:The names of threads created by CarbonThreadFactory  are all the same
The names of threads created by CarbonThreadFactory  are all the same, such as "ProducerPool_",  this situation is confused to look
issueID:CARBONDATA-3143
type:Bug
changed files:integration/presto/src/main/java/org/apache/carbondata/presto/CarbonColumnVectorWrapper.java
integration/presto/src/main/java/org/apache/carbondata/presto/readers/SliceStreamReader.java
texts:Fix local dictionary issue for presto
Fix local dictionary issue for presto
issueID:CARBONDATA-3145
type:Sub-task
changed files:core/src/main/java/org/apache/carbondata/core/scan/result/BlockletScannedResult.java
core/src/main/java/org/apache/carbondata/core/scan/complextypes/StructQueryType.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/scan/complextypes/ArrayQueryType.java
core/src/main/java/org/apache/carbondata/core/scan/complextypes/ComplexQueryType.java
core/src/main/java/org/apache/carbondata/core/scan/filter/GenericQueryType.java
core/src/main/java/org/apache/carbondata/core/scan/complextypes/PrimitiveQueryType.java
texts:Avoid duplicate decoding for complex column pages while querying

issueID:CARBONDATA-3147
type:Bug
changed files:processing/src/main/java/org/apache/carbondata/processing/util/CarbonLoaderUtil.java
texts:Preaggregate dataload fails in case of concurrent load in some cases
java.io.IOException: Entry not found to update in the table status fileat org.apache.carbondata.processing.util.CarbonLoaderUtil.recordNewLoadMetadata(CarbonLoaderUtil.java:320)at org.apache.carbondata.processing.util.CarbonLoaderUtil.recordNewLoadMetadata(CarbonLoaderUtil.java:207)at org.apache.carbondata.processing.util.CarbonLoaderUtil.updateTableStatusForFailure(CarbonLoaderUtil.java:467)at org.apache.spark.sql.execution.command.management.CarbonLoadDataCommand.processData(CarbonLoadDataCommand.scala:358)at org.apache.spark.sql.execution.command.preaaggregate.PreAggregateUtil$.startDataLoadForDataMap(PreAggregateUtil.scala:603)at org.apache.spark.sql.execution.command.preaaggregate.LoadPostAggregateListener$$anonfun$onEvent$10.apply(PreAggregateListeners.scala:488)at org.apache.spark.sql.execution.command.preaaggregate.LoadPostAggregateListener$$anonfun$onEvent$10.apply(PreAggregateListeners.scala:463)at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)at org.apache.spark.sql.execution.command.preaaggregate.LoadPostAggregateListener$.onEvent(PreAggregateListeners.scala:463)at org.apache.carbondata.events.OperationListenerBus.fireEvent(OperationListenerBus.java:83)at org.apache.carbondata.spark.rdd.CarbonDataRDDFactory$.loadCarbonData(CarbonDataRDDFactory.scala:524)at org.apache.spark.sql.execution.command.management.CarbonLoadDataCommand.loadData(CarbonLoadDataCommand.scala:594)at org.apache.spark.sql.execution.command.management.CarbonLoadDataCommand.processData(CarbonLoadDataCommand.scala:322)at org.apache.spark.sql.execution.command.AtomicRunnableCommand$$anonfun$run$3.apply(package.scala:147)at org.apache.spark.sql.execution.command.AtomicRunnableCommand$$anonfun$run$3.apply(package.scala:144)at org.apache.spark.sql.execution.command.Auditable$class.runWithAudit(package.scala:104)at org.apache.spark.sql.execution.command.AtomicRunnableCommand.runWithAudit(package.scala:140)at org.apache.spark.sql.execution.command.AtomicRunnableCommand.run(package.scala:144)at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:59)at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:57)at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:75)at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114)at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114)at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:135)at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:132)at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:113)at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:125)at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:125)at org.apache.spark.sql.Dataset.(Dataset.scala:185)at org.apache.spark.sql.CarbonSession$$anonfun$sql$1.apply(CarbonSession.scala:90)at org.apache.spark.sql.CarbonSession$$anonfun$sql$1.apply(CarbonSession.scala:89)at org.apache.spark.sql.CarbonSession.withProfiler(CarbonSession.scala:135)at org.apache.spark.sql.CarbonSession.sql(CarbonSession.scala:87)at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:699)at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:252)at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1$$anon$2.run(SparkExecuteStatementOperation.scala:183)at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1$$anon$2.run(SparkExecuteStatementOperation.scala:180)at java.security.AccessController.doPrivileged(Native Method)at javax.security.auth.Subject.doAs(Subject.java:422)at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1778)at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1.run(SparkExecuteStatementOperation.scala:193)at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)at java.util.concurrent.FutureTask.run(FutureTask.java:266)at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)at java.lang.Thread.run(Thread.java:748)
issueID:CARBONDATA-3149
type:New Feature
changed files:core/src/main/java/org/apache/carbondata/core/features/TableOperation.java
datamap/lucene/src/main/java/org/apache/carbondata/datamap/lucene/LuceneFineGrainDataMapFactory.java
datamap/bloom/src/main/java/org/apache/carbondata/datamap/bloom/BloomCoarseGrainDataMapFactory.java
texts:Support alter table column rename
Please find the mailing list link for the same http://apache-carbondata-dev-mailing-list-archive.1130556.n5.nabble.com/Discussion-Alter-table-column-rename-feature-tt69814.html
issueID:CARBONDATA-315
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/util/DataTypeUtil.java
texts:Data loading fails if parsing a double value returns infinity
During data load, if a value specified is too big for a double DataType column then while parsing that value as double result is returned as "Infinity". Due to this while we calculate min and max value for measures in carbon data writer step it throws an exception.ERROR 13-10 15:27:56,968 - &#91;t3: Graph - MDKeyGent3&#93;&#91;partitionID:0&#93; org.apache.carbondata.processing.store.writer.exception.CarbonDataWriterExceptionjava.util.concurrent.ExecutionException: org.apache.carbondata.processing.store.writer.exception.CarbonDataWriterExceptionat java.util.concurrent.FutureTask.report(FutureTask.java:122)at java.util.concurrent.FutureTask.get(FutureTask.java:188)at org.apache.carbondata.processing.store.CarbonFactDataHandlerColumnar.processWriteTaskSubmitList(CarbonFactDataHandlerColumnar.java:812)at org.apache.carbondata.processing.store.CarbonFactDataHandlerColumnar.finish(CarbonFactDataHandlerColumnar.java:779)at org.apache.carbondata.processing.mdkeygen.MDKeyGenStep.processRow(MDKeyGenStep.java:222)at org.pentaho.di.trans.step.RunThread.run(RunThread.java:50)at java.lang.Thread.run(Thread.java:745)Caused by: org.apache.carbondata.processing.store.writer.exception.CarbonDataWriterExceptionat org.apache.carbondata.processing.store.CarbonFactDataHandlerColumnar$Producer.call(CarbonFactDataHandlerColumnar.java:1244)at org.apache.carbondata.processing.store.CarbonFactDataHandlerColumnar$Producer.call(CarbonFactDataHandlerColumnar.java:1215)at java.util.concurrent.FutureTask.run(FutureTask.java:262)at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)... 1 more
issueID:CARBONDATA-3153
type:Bug
changed files:processing/src/main/java/org/apache/carbondata/processing/loading/model/LoadOption.java
store/sdk/src/main/java/org/apache/carbondata/sdk/file/CarbonWriterBuilder.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonTableOutputFormat.java
texts:Change of Complex Delimiters

issueID:CARBONDATA-3154
type:Bug
changed files:
texts:Fix spark-2.1 test error
Now the CI didn't run Spark2.1 UT, only compile, so when test spark2.1, there are some errors.for example:command:-Pspark-2.1  clean installerror1:2018-12-07 21:47:20 INFO  HiveMetaStore:746 - 0: get_database: global_temp2018-12-07 21:47:20 INFO  audit:371 - ugi=xubo ip=unknown-ip-addr cmd=get_database: global_temp 2018-12-07 21:47:20 WARN  ObjectStore:568 - Failed to get database global_temp, returning NoSuchObjectException*** RUN ABORTED ***  org.apache.spark.sql.catalyst.parser.ParseException: mismatched input 'location' expecting {<EOF>, '(', '.', 'SELECT', 'FROM', 'AS', 'WITH', 'VALUES', 'TABLE', 'INSERT', 'MAP', 'REDUCE', 'OPTIONS', 'CLUSTERED', 'PARTITIONED'}(line 1, pos 150)== SQL ==create table par_table(male boolean, age int, height double, name string, address string,salary long, floatField float, bytefield byte) using parquet location '/Users/xubo/Desktop/xubo/git/carbondata1/integration/spark-datasource/target/warehouse2'------------------------------------------------------------------------------------------------------------------------------------------------------^^^  at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:197)  at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:99)  at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:45)  at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:53)  at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:592)  at org.apache.spark.sql.carbondata.datasource.SparkCarbonDataSourceTest.createParquetTable(SparkCarbonDataSourceTest.scala:1126)  at org.apache.spark.sql.carbondata.datasource.SparkCarbonDataSourceTest.beforeAll(SparkCarbonDataSourceTest.scala:1359)  at org.scalatest.BeforeAndAfterAll$class.beforeAll(BeforeAndAfterAll.scala:187)  at org.apache.spark.sql.carbondata.datasource.SparkCarbonDataSourceTest.beforeAll(SparkCarbonDataSourceTest.scala:38)  at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:253)  ...[INFO] ------------------------------------------------------------------------[INFO] Reactor Summary:there are another 5 errors in org.apache.spark.sql.carbondata.datasource.SparkCarbonDataSourceTest
issueID:CARBONDATA-3157
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/scan/result/vector/impl/directread/AbstractCarbonColumnarVector.java
integration/presto/src/main/java/org/apache/carbondata/presto/CarbondataPageSourceProvider.java
core/src/main/java/org/apache/carbondata/core/scan/result/vector/impl/CarbonColumnVectorImpl.java
integration/presto/src/main/java/org/apache/carbondata/presto/ColumnarVectorWrapperDirect.java
integration/presto/src/main/java/org/apache/carbondata/presto/impl/CarbonTableConfig.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/adaptive/AdaptiveDeltaFloatingCodec.java
integration/presto/src/main/java/org/apache/carbondata/presto/CarbondataPageSource.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/compress/DirectCompressCodec.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/adaptive/AdaptiveDeltaIntegralCodec.java
integration/presto/src/main/java/org/apache/carbondata/presto/readers/BooleanStreamReader.java
core/src/main/java/org/apache/carbondata/core/scan/collector/impl/RestructureBasedVectorResultCollector.java
integration/presto/src/main/java/org/apache/carbondata/presto/CarbonColumnVectorWrapper.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/adaptive/AdaptiveFloatingCodec.java
core/src/main/java/org/apache/carbondata/core/scan/result/vector/CarbonColumnVector.java
integration/presto/src/main/java/org/apache/carbondata/presto/CarbonVectorBatch.java
integration/spark-datasource/src/main/scala/org/apache/carbondata/spark/vectorreader/ColumnarVectorWrapperDirect.java
integration/presto/src/main/java/org/apache/carbondata/presto/PrestoCarbonVectorizedRecordReader.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/store/impl/safe/AbstractNonDictionaryVectorFiller.java
core/src/main/java/org/apache/carbondata/core/scan/result/vector/impl/directread/SequentialFill.java
integration/presto/src/main/java/org/apache/carbondata/presto/readers/SliceStreamReader.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/adaptive/AdaptiveIntegralCodec.java
core/src/main/java/org/apache/carbondata/core/scan/result/vector/impl/directread/ColumnarVectorWrapperDirectFactory.java
integration/spark-datasource/src/main/scala/org/apache/carbondata/spark/vectorreader/ColumnarVectorWrapper.java
integration/presto/src/main/java/org/apache/carbondata/presto/impl/CarbonTableReader.java
texts:Integrate carbon lazy loading to presto carbon integration
Integrate carbon lazy loading to presto carbon integration 
issueID:CARBONDATA-3158
type:Improvement
changed files:integration/presto/src/main/java/org/apache/carbondata/presto/CarbondataPageSourceProvider.java
integration/presto/src/main/java/org/apache/carbondata/presto/impl/CarbonTableReader.java
texts:support presto-carbon to read sdk cabron files
Currently, carbon SDK files output (files without metadata folder and its contents) are read by spark using an external table with carbon session. But presto carbon integration doesn't support that. It can currently read only the transactional table output files. Hence we can enhance presto to read SDK output files. This will increase the use cases for presto-carbon integration. The above scenario can be achieved by inferring schema if metadata folder not exists and setting read committed scope to LatestFilesReadCommittedScope, if non-transctional table output files are present.
issueID:CARBONDATA-3159
type:Bug
changed files:
texts:Issue with SDK Write when empty array is given

issueID:CARBONDATA-316
type:Bug
changed files:
texts:Change BAD_RECORDS_LOGGER_ACTION to BAD_RECORDS_ACTION

issueID:CARBONDATA-3160
type:Sub-task
changed files:processing/src/main/java/org/apache/carbondata/processing/util/CarbonDataProcessorUtil.java
processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactDataHandlerModel.java
texts:Compaction support with MAP data type
Support compaction with MAP type
issueID:CARBONDATA-3162
type:Bug
changed files:processing/src/main/java/org/apache/carbondata/processing/loading/steps/CarbonRowDataWriterProcessorStepImpl.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
core/src/main/java/org/apache/carbondata/core/util/DataTypeUtil.java
core/src/main/java/org/apache/carbondata/core/scan/executor/impl/AbstractQueryExecutor.java
core/src/main/java/org/apache/carbondata/core/scan/filter/FilterUtil.java
processing/src/main/java/org/apache/carbondata/processing/loading/model/LoadOption.java
store/sdk/src/main/java/org/apache/carbondata/sdk/file/CarbonWriterBuilder.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonTableOutputFormat.java
texts:Range filters doesn&#39;t remove null values for no_sort direct dictionary dimension columns.
Range filters doesn't remove null values for no_sort direct dictionary dimension columns. TimestampDataTypeDirectDictionaryTest.test("test timestamp with dictionary include and no_inverted index")
issueID:CARBONDATA-3165
type:Bug
changed files:datamap/bloom/src/main/java/org/apache/carbondata/datamap/bloom/BloomCoarseGrainDataMap.java
texts:Query of BloomFilter  java.lang.NullPointerException
carbon.enable.distributed.datamap is true，run long time, use bloomfilter to query, the exception is:24274.0 (TID 664711) | org.apache.spark.internal.Logging$class.logError(Logging.scala:91)java.lang.NullPointerException        at java.util.ArrayList.<init>(ArrayList.java:177)        at org.apache.carbondata.datamap.bloom.BloomCoarseGrainDataMap.prune(BloomCoarseGrainDataMap.java:230)        at org.apache.carbondata.core.datamap.TableDataMap.prune(TableDataMap.java:379)        at org.apache.carbondata.core.datamap.DistributableDataMapFormat$1.initialize(DistributableDataMapFormat.java:108)        at org.apache.carbondata.spark.rdd.DataMapPruneRDD.internalCompute(SparkDataMapJob.scala:77)        at org.apache.carbondata.spark.rdd.CarbonRDD.compute(CarbonRDD.scala:82)        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)        at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)        at org.apache.spark.scheduler.Task.run(Task.scala:99)        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:325)        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)        at java.lang.Thread.run(Thread.java:748)
issueID:CARBONDATA-3166
type:Improvement
changed files:
texts:Changes in Document and Displaying Carbon Column Compressor used in Describe Formatted Command
Changes in Document and Displaying Carbon Column Compressor used in Describe Formatted Command
issueID:CARBONDATA-317
type:Bug
changed files:
texts:CSV having only space char is throwing NullPointerException

issueID:CARBONDATA-3173
type:Improvement
changed files:
texts:Add hive-guide and other guides to the root of the file ReadMe
In order to facilitate everyone's reading, we unified the integrated document index into the ReadMe file in the root directory.
issueID:CARBONDATA-3174
type:Bug
changed files:store/sdk/src/main/java/org/apache/carbondata/sdk/file/Field.java
store/sdk/src/main/java/org/apache/carbondata/sdk/file/CarbonWriterBuilder.java
texts:Fix trailing space issue with varchar column for SDK

issueID:CARBONDATA-3175
type:Test
changed files:
texts:Fix Testcase failures in complex delimiters

issueID:CARBONDATA-3176
type:Improvement
changed files:
texts:Optimize quick-start-guide documentation
1，the example isn't reproducible，met some problems    1) spark-sql with multi lines shoud be surrounded by """    2) shell sript with multi lines shoud be end with '/' 2，unified the doc style in doc 
issueID:CARBONDATA-3178
type:Bug
changed files:
texts:select query with in clause on timestamp column inconsistent with filter on same column
Steps : Create table :CREATE TABLE uniqdata (CUST_ID int,CUST_NAME String,ACTIVE_EMUI_VERSION string, DOB timestamp, DOJ timestamp, BIGINT_COLUMN1 bigint,BIGINT_COLUMN2 bigint,DECIMAL_COLUMN1 decimal(30,10), DECIMAL_COLUMN2 decimal(36,36),Double_COLUMN1 double, Double_COLUMN2 double,INTEGER_COLUMN1 int) STORED BY 'org.apache.carbondata.format' ;Load Data :LOAD DATA INPATH 'hdfs://hacluster/chetan/2000_UniqData.csv' into table uniqdata OPTIONS('DELIMITER'=',' , 'QUOTECHAR'='"','BAD_RECORDS_ACTION'='FORCE','FILEHEADER'='CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1,Double_COLUMN2,INTEGER_COLUMN1');Select Queries:select * from uniqdata where dob in ('1970-01-01 01:00:03.0');------------------------------------------------------------------------------------------------------------------------------------------------------------ cust_id  cust_name  active_emui_version  dob  doj  bigint_column1  bigint_column2  decimal_column1  decimal_column2  double_column1  double_column2  integer_column1 ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------No rows selected (0.702 seconds)select * from uniqdata where dob ='1970-01-01 01:00:03.0';------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ cust_id  cust_name  active_emui_version  dob  doj  bigint_column1  bigint_column2  decimal_column1  decimal_column2  double_column1  double_column2  integer_column1 ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ 9000  CUST_NAME_00000  ACTIVE_EMUI_VERSION_00000  1970-01-01 01:00:03.0  1970-01-01 02:00:03.0  123372036854  -223372036854  12345678901.1234000000  NULL  1.12345674897976E10  -1.12345674897976E10  1 ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------1 row selected (0.57 seconds) Actual Issue :Correct data is projected in case of filter query with '='. For same column with in clause, no data is projected.Expected :Both the select queries should show correct result. (As projected in second select query).
issueID:CARBONDATA-3179
type:Bug
changed files:
texts:DataLoad Failure in Map Data Type
Data Load failing for insert into table select * from table containing Map datatype
issueID:CARBONDATA-318
type:Sub-task
changed files:
texts:Implement an InMemory Sorter that makes maximum usage of memory while sorting
Change SortDataRows into an External Sorter, it should sort in memory until it reach configured size, then spill to disk. It should provide following interface:1. addRow:  insert rows into the sorter.2. getIterator: will return an iterator that iterates on sorted rows, the sorted row could come from memory or files
issueID:CARBONDATA-3181
type:Bug
changed files:datamap/bloom/src/main/java/org/apache/hadoop/util/bloom/CarbonBloomFilter.java
texts:Fix access field error for BitSet in bloom filter
```18/12/19 11:16:07 ERROR thriftserver.SparkExecuteStatementOperation: Error executing query, currentState RUNNING,java.lang.IllegalAccessError: tried to access field org.apache.hadoop.util.bloom.BloomFilter.bits from class org.apache.hadoop.util.bloom.CarbonBloomFilter at org.apache.hadoop.util.bloom.CarbonBloomFilter.membershipTest(CarbonBloomFilter.java:70) at org.apache.carbondata.datamap.bloom.BloomCoarseGrainDataMap.prune(BloomCoarseGrainDataMap.java:202) at org.apache.carbondata.core.datamap.TableDataMap.pruneWithFilter(TableDataMap.java:185) at org.apache.carbondata.core.datamap.TableDataMap.prune(TableDataMap.java:160) at org.apache.carbondata.core.datamap.dev.expr.DataMapExprWrapperImpl.prune(DataMapExprWrapperImpl.java:53) at org.apache.carbondata.hadoop.api.CarbonInputFormat.getPrunedBlocklets(CarbonInputFormat.java:517) at org.apache.carbondata.hadoop.api.CarbonInputFormat.getDataBlocksOfSegment(CarbonInputFormat.java:412) at org.apache.carbondata.hadoop.api.CarbonTableInputFormat.getSplits(CarbonTableInputFormat.java:529) at org.apache.carbondata.hadoop.api.CarbonTableInputFormat.getSplits(CarbonTableInputFormat.java:220) at org.apache.carbondata.spark.rdd.CarbonScanRDD.internalGetPartitions(CarbonScanRDD.scala:127) at org.apache.carbondata.spark.rdd.CarbonRDD.getPartitions(CarbonRDD.scala:66) at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:252) at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:250)```
issueID:CARBONDATA-3182
type:Sub-task
changed files:
texts:Fix SDV TestCase Failures in Delimiters

issueID:CARBONDATA-3184
type:Bug
changed files:
texts:Fix DataLoad failure with "using carbondata"
Steps to reproduce the issue: Create a table "using carbondata" in spark 2.1 and spark 2.3 Perform dataload using Load DDLFind the exception below:
issueID:CARBONDATA-3185
type:Bug
changed files:
texts:Rename schema file on alluxio failed when creating table

issueID:CARBONDATA-3186
type:Bug
changed files:integration/presto/src/main/java/org/apache/carbondata/presto/impl/CarbonTableReader.java
processing/src/main/java/org/apache/carbondata/processing/store/writer/AbstractFactDataWriter.java
texts:NPE when all the records in a file is badrecord with action redirect/ignore
problem: In the no_sort flow, writer will be open as there is no blocking sort step.So, when all the record goes as bad record with redirect in converted step.writer is closing the empty .carbondata file.when this empty carbondata file is queried , we get multiple issues including NPE.solution: When the file size is 0 bytes. do the followinga) If one data and one index file &#8211; delete carbondata file and avoid index file creationb) If multiple data and one index file (with few data file is full of bad recod) &#8211; delete carbondata files, remove them from blockIndexInfoList, so index file not will not have that info of empty carbon filesc) In case direct write to store path is enable. need to delete data file from there and avoid writing index file with that carbondata in info.
issueID:CARBONDATA-3187
type:Task
changed files:
texts:Global Dictionary Support for Complex Map

issueID:CARBONDATA-3188
type:Bug
changed files:
texts:Create carbon table as hive understandable metastore table needed by Presto and Hive
Current carbon table created in spark creates the hive table internally but it does not have much information like schema, input/output format and location details. So other execution engines like Presto and Hive cannot read the table.
issueID:CARBONDATA-319
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/util/DataTypeUtil.java
core/src/main/java/org/apache/carbondata/core/metadata/datatype/DataType.java
texts:Bad Records logging for column LONG data type is not proper
Bad Records logging for column data type is not proper in case of long, carbon system while creating the table metadata uses BIGINT instead of LONG , internally it converts the bigint to long type and processes , while processing the data if any long type data is having issue it will be logged into the bad record with data type Long which is not proper since as per the metadata the datatype of column is BIGINT.
issueID:CARBONDATA-3192
type:Bug
changed files:processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactDataHandlerModel.java
texts:Compaction Compatibilty Failure
Table Created, Loaded and Altered(Column added) in 1.5.1 version and Refreshed, Altered(Added Column dropped) , Loaded and Compacted with Varchar Columns in new version giving error.
issueID:CARBONDATA-3194
type:New Feature
changed files:core/src/main/java/org/apache/carbondata/core/datastore/impl/FileFactory.java
integration/presto/src/main/java/org/apache/carbondata/presto/CarbondataModule.java
integration/presto/src/main/java/org/apache/carbondata/presto/PrestoFilterUtil.java
integration/presto/src/main/java/org/apache/carbondata/presto/CarbondataPageSourceProvider.java
integration/presto/src/main/java/org/apache/carbondata/presto/CarbondataConnectorFactory.java
integration/presto/src/main/java/org/apache/carbondata/presto/impl/CarbonTableConfig.java
integration/presto/src/main/java/org/apache/carbondata/presto/impl/CarbonLocalMultiBlockSplit.java
integration/presto/src/main/java/org/apache/carbondata/presto/impl/CarbonTableReader.java
integration/presto/src/main/java/org/apache/carbondata/presto/CarbondataSplitManager.java
texts:Support Hive Metastore in Presto CarbonData.
Current Carbon Presto integration added a new presto connector that takes the carbon store folder and lists the databases and tables from the folders. In this implementation, we have many issues like. 1. DB and table always need to be in specific order and name of the folders should always match the DB name and table name. 2. The table which is created in presto cannot be reflected directly in other execution engines like Spark. 3. DB with location and table with location cannot work. 4. There will not be any access control on tables. 5. There is no interoperability between hive tables like ORC or Parquet with carbon. Like if we want to join some hive table with Carbon Table then it won't be possible. To overcome the above limitations we can support HiveMetastore in Presto Carbon. Basically, instead of creating a new Presto Connector for Carbon, we can extend the HiveConnector and override and add new CarbonPageSourceFactory for reading the data and FileWriterFactory for writing the data. So Carbon Table becomes one of the hive supported format for Presto.  So whatever the tables added in spark can be reflected immediately in Carbon and also the limitations mentioned above will be solved with this type of implementation. 
issueID:CARBONDATA-3195
type:Improvement
changed files:
texts:Added validation for inverted index

issueID:CARBONDATA-3196
type:Bug
changed files:processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactDataHandlerModel.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/SortParameters.java
core/src/main/java/org/apache/carbondata/core/scan/wrappers/ByteArrayWrapper.java
processing/src/main/java/org/apache/carbondata/processing/util/CarbonDataProcessorUtil.java
processing/src/main/java/org/apache/carbondata/processing/loading/CarbonDataLoadConfiguration.java
processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactHandlerFactory.java
processing/src/main/java/org/apache/carbondata/processing/merger/CompactionResultSortProcessor.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/TableFieldStat.java
texts:Compaction Failing for Complex datatypes with Dictionary Include
Steps to reproduce: Create Table with Complex type and Dictionary Include Complex type. Load data into the table 2-3 times. Alter table compact 'major'
issueID:CARBONDATA-32
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/scan/expression/conditional/NotEqualsExpression.java
core/src/main/java/org/apache/carbondata/core/scan/expression/conditional/NotInExpression.java
core/src/main/java/org/apache/carbondata/core/scan/expression/ColumnExpression.java
core/src/main/java/org/apache/carbondata/core/scan/expression/UnknownExpression.java
core/src/main/java/org/apache/carbondata/core/util/DataTypeUtil.java
core/src/main/java/org/apache/carbondata/core/scan/expression/conditional/EqualToExpression.java
core/src/main/java/org/apache/carbondata/core/scan/expression/ExpressionResult.java
core/src/main/java/org/apache/carbondata/core/scan/expression/conditional/InExpression.java
core/src/main/java/org/apache/carbondata/core/scan/expression/conditional/LessThanExpression.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/scan/expression/logical/OrExpression.java
core/src/main/java/org/apache/carbondata/core/scan/expression/conditional/GreaterThanExpression.java
core/src/main/java/org/apache/carbondata/core/scan/expression/LiteralExpression.java
core/src/main/java/org/apache/carbondata/core/scan/expression/logical/AndExpression.java
core/src/main/java/org/apache/carbondata/core/scan/expression/conditional/GreaterThanEqualToExpression.java
core/src/main/java/org/apache/carbondata/core/datastore/block/SegmentProperties.java
core/src/main/java/org/apache/carbondata/core/scan/filter/FilterUtil.java
core/src/main/java/org/apache/carbondata/core/scan/expression/conditional/LessThanEqualToExpression.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/ConditionalFilterResolverImpl.java
texts:Refactor DataType related code
1. Merge two DataType2. Merge tow DataTypeUtil3. Remove DataSetUtil
issueID:CARBONDATA-320
type:Bug
changed files:
texts:problem when dropped a table during all data nodes are down.
when dropped a table during all data nodes are down. sometimes the drop is failing and the files will remain in hdfs. but from hive the table will be removed.
issueID:CARBONDATA-3200
type:New Feature
changed files:core/src/main/java/org/apache/carbondata/core/metadata/blocklet/BlockletInfo.java
processing/src/main/java/org/apache/carbondata/processing/merger/CarbonCompactionUtil.java
processing/src/main/java/org/apache/carbondata/processing/merger/CarbonCompactionExecutor.java
processing/src/main/java/org/apache/carbondata/processing/merger/RowResultMergerProcessor.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockletDataMap.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/SingleThreadFinalSortFilesMerger.java
processing/src/main/java/org/apache/carbondata/processing/store/writer/AbstractFactDataWriter.java
core/src/main/java/org/apache/carbondata/core/scan/executor/impl/AbstractQueryExecutor.java
core/src/main/java/org/apache/carbondata/core/datastore/block/TableBlockInfo.java
processing/src/main/java/org/apache/carbondata/processing/store/writer/v3/CarbonFactDataWriterImplV3.java
processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactDataHandlerModel.java
core/src/main/java/org/apache/carbondata/core/util/DataFileFooterConverterV3.java
core/src/main/java/org/apache/carbondata/core/metadata/blocklet/DataFileFooter.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/InMemorySortTempChunkHolder.java
processing/src/main/java/org/apache/carbondata/processing/merger/CompactionResultSortProcessor.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/SortTempFileChunkHolder.java
core/src/main/java/org/apache/carbondata/core/util/AbstractDataFileFooterConverter.java
processing/src/main/java/org/apache/carbondata/processing/merger/AbstractResultProcessor.java
texts:No-Sort Compaction
When the data is loaded with SORT_SCOPE as NO_SORT, and done compaction upon, the data still remains unsorted. This does not affect much in query. The major purpose of compaction, is better pack the data and improve query performance. Now, the expected behaviour of compaction is sort to the data, so that after compaction, query performance becomes better. The columns to sort upon are provided by SORT_COLUMNS.
issueID:CARBONDATA-3201
type:New Feature
changed files:core/src/main/java/org/apache/carbondata/core/util/SessionParams.java
processing/src/main/java/org/apache/carbondata/processing/loading/events/LoadEvents.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonLoadOptionConstants.java
texts:SORT_SCOPE in LOAD_OPTIONS
Prerequisite: CARBONDATA-3200 If the Compaction always sort the data, then we can take advantage of the faster loading speed. If we provide SORT_COLUMNS in CREATE TABLE command, then we can load some data with SORT_SCOPE as NO_SORT. This helps in faster loading speed. But during off-peak time, user can COMPACT the data, and thus improving the subsequent query perfrmance. 
issueID:CARBONDATA-3202
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/metadata/converter/ThriftWrapperSchemaConverterImpl.java
texts:updated schema is not updated in session catalog after add, drop or rename column.
updated schema is not updated in session catalog after add, drop or rename column.  Spark does not support drop column , rename column, and supports add column from spark2.2 onwards, so after rename, or add or drop column, the new updated schema is not updated in catalog
issueID:CARBONDATA-3203
type:Bug
changed files:processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactDataHandlerModel.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/SortParameters.java
core/src/main/java/org/apache/carbondata/core/scan/wrappers/ByteArrayWrapper.java
processing/src/main/java/org/apache/carbondata/processing/util/CarbonDataProcessorUtil.java
processing/src/main/java/org/apache/carbondata/processing/loading/CarbonDataLoadConfiguration.java
processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactHandlerFactory.java
processing/src/main/java/org/apache/carbondata/processing/merger/CompactionResultSortProcessor.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/TableFieldStat.java
texts:Compaction failing for table which is retstructured
Steps to reproduce: Create table with complex and primitive types. Load data 2-3 times. Drop one column. Trigger Compaction.
issueID:CARBONDATA-3205
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/datastore/chunk/impl/DimensionRawColumnChunk.java
texts:Fix Get Local Dictionary for empty Array of Struct
Steps to Reproduce: Generate json data for complex type with more than 15 levels having Array of Struct value as empty. Create table through 'using carbon' Perform any query on that table and find the below Exception. 
issueID:CARBONDATA-3206
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/util/CarbonProperties.java
processing/src/main/java/org/apache/carbondata/processing/datatypes/ArrayDataType.java
processing/src/main/java/org/apache/carbondata/processing/datatypes/GenericDataType.java
processing/src/main/java/org/apache/carbondata/processing/datatypes/PrimitiveDataType.java
processing/src/main/java/org/apache/carbondata/processing/loading/converter/impl/MeasureFieldConverterImpl.java
core/src/main/java/org/apache/carbondata/core/scan/filter/GenericQueryType.java
processing/src/main/java/org/apache/carbondata/processing/util/CarbonLoaderUtil.java
core/src/main/java/org/apache/carbondata/core/scan/complextypes/StructQueryType.java
core/src/main/java/org/apache/carbondata/core/scan/complextypes/ArrayQueryType.java
core/src/main/java/org/apache/carbondata/core/scan/complextypes/MapQueryType.java
processing/src/main/java/org/apache/carbondata/processing/datatypes/StructDataType.java
core/src/main/java/org/apache/carbondata/core/scan/complextypes/ComplexQueryType.java
processing/src/main/java/org/apache/carbondata/processing/loading/converter/impl/NonDictionaryFieldConverterImpl.java
core/src/main/java/org/apache/carbondata/core/scan/complextypes/PrimitiveQueryType.java
texts:Fix some spell error in CarbonData
There are some spell error in CarbonData: numberofColumnPerIOString. => numberOfColumnPerIOString numberofColumnPerIO => numberOfColumnPerIO iexpectedMinSizePerNode => expectedMinSizePerNodeIntparentname => parentNamenullformat => nullFormat carbonsession Reset sesionaste and show session. ...space error： presto prepare go ahead
issueID:CARBONDATA-3208
type:Bug
changed files:processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/SortParameters.java
core/src/main/java/org/apache/carbondata/core/util/path/CarbonTablePath.java
processing/src/main/java/org/apache/carbondata/processing/merger/CarbonDataMergerUtil.java
processing/src/main/java/org/apache/carbondata/processing/loading/steps/CarbonRowDataWriterProcessorStepImpl.java
core/src/main/java/org/apache/carbondata/core/datastore/page/ColumnPage.java
core/src/main/java/org/apache/carbondata/core/datastore/block/SegmentProperties.java
processing/src/main/java/org/apache/carbondata/processing/loading/steps/DataWriterProcessorStepImpl.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonInputFormat.java
processing/src/main/java/org/apache/carbondata/processing/store/TablePage.java
datamap/lucene/src/main/java/org/apache/carbondata/datamap/lucene/LuceneDataMapWriter.java
texts:Remove unused parameter from code
Remove unused parameter from code：for example， sparkSession parameter in：org.apache.carbondata.spark.rdd.CarbonColumnDictGenerateRDD
issueID:CARBONDATA-3209
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/constants/SortScopeOptions.java
texts:Remove unused import
Remove unused import，for example：import java.io.Fileimport org.apache.commons.lang3.StringUtilsimport scala.util.Randomimport org.apache.spark.SparkEnvin org.apache.spark.sql.execution.datasources.SparkCarbonTableFormat。There are some unused import in org.apache.carbondata.spark.util.GlobalDictionaryUtil，org.apache.carbondata.core.util.DataTypeUtilTest。Please find other unused import in other class
issueID:CARBONDATA-3211
type:Improvement
changed files:datamap/lucene/src/main/java/org/apache/carbondata/datamap/lucene/LuceneFineGrainDataMapFactory.java
texts:Optimize the documentation
Optimize the documentation:1. Optimize the upper/lower case problem,we should unify the expression by using correct format , for example:Creating BloomFilter DataMapDropping specified datamapDisable Datamapin https://github.com/apache/carbondata/blob/master/docs/datamap/bloomfilter-datamap-guide.mdwe should change olap to OLAP in https://github.com/apache/carbondata/blob/master/docs/datamap-developer-guide.md2. Optimize the space error in project, for example: @param dbName the database name , if not a default databasein org.apache.carbondata.presto.server.PrestoServer#startServer=> after database name, it shouldn't have space, need change it to "database name,"others: org.apache.spark.sql.CarbonDatasourceHadoopRelation#toString failed to get lucene datamap ,
issueID:CARBONDATA-3212
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/datastore/page/LocalDictColumnPage.java
texts:Select * is failing with java.lang.NegativeArraySizeException in SDK flow

issueID:CARBONDATA-3213
type:Improvement
changed files:
texts:Add License for all doc
There are some doc no license in project, we should add License for all doc, for example: datamap-developer-guide.md introduction.md usecases.mdand so on, please check all doc
issueID:CARBONDATA-3215
type:Improvement
changed files:
texts:Optimize the documentation
Optimize the documentation: describe  Global dictionary local dictionary，non-dictionary together in doc list mvdataMap list
issueID:CARBONDATA-3216
type:Bug
changed files:
texts:There are some bugs in CSDK
There are some bugs in CSDK： 1.enableLocalDictionary can' t set falsecode：   writer.enableLocalDictionary(false);excepton：libc++abi.dylib: terminating with uncaught exception of type std::runtime_error: enableLocalDictionary parameter can't be NULL.2.
issueID:CARBONDATA-3217
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/scan/filter/executer/ImplicitIncludeFilterExecutorImpl.java
core/src/main/java/org/apache/carbondata/core/scan/filter/FilterUtil.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonInputFormat.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/resolverinfo/visitor/ImplicitColumnVisitor.java
core/src/main/java/org/apache/carbondata/hadoop/CarbonInputSplit.java
core/src/main/java/org/apache/carbondata/core/scan/expression/conditional/ImplicitExpression.java
core/src/main/java/org/apache/carbondata/core/scan/filter/ColumnFilterInfo.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockDataMap.java
core/src/main/java/org/apache/carbondata/core/scan/filter/intf/ExpressionType.java
texts:Optimize implicit filter expression performance by removing extra serialization
Currently all the filter values are getting serialized for all the tasks which is increasing the schedular delay thereby impacting the query performance. For each task 2 times deserialization is taking place in the executor side which is not required. 1 time is suficient
issueID:CARBONDATA-3218
type:New Feature
changed files:integration/presto/src/main/java/org/apache/carbondata/presto/readers/BooleanStreamReader.java
integration/presto/src/main/java/org/apache/carbondata/presto/CarbondataPageSourceProvider.java
integration/presto/src/main/java/org/apache/carbondata/presto/readers/IntegerStreamReader.java
integration/presto/src/main/java/org/apache/carbondata/presto/readers/DecimalSliceStreamReader.java
integration/presto/src/main/java/org/apache/carbondata/presto/readers/DoubleStreamReader.java
integration/presto/src/main/java/org/apache/carbondata/presto/impl/CarbonTableCacheModel.java
integration/presto/src/main/java/org/apache/carbondata/presto/readers/LongStreamReader.java
integration/presto/src/main/java/org/apache/carbondata/presto/readers/ObjectStreamReader.java
integration/presto/src/main/java/org/apache/carbondata/presto/readers/ShortStreamReader.java
integration/presto/src/main/java/org/apache/carbondata/presto/readers/SliceStreamReader.java
integration/presto/src/main/java/org/apache/carbondata/presto/readers/TimestampStreamReader.java
integration/presto/src/main/java/org/apache/carbondata/presto/impl/CarbonTableReader.java
integration/presto/src/main/java/org/apache/carbondata/presto/CarbondataSplitManager.java
texts:Schema is not refreshing in presto which is changed in spark carbon.
Schema which is updated in spark is not reflecting in presto. which results in wrong query result in presto.
issueID:CARBONDATA-3219
type:Improvement
changed files:processing/src/main/java/org/apache/carbondata/processing/loading/parser/impl/RangeColumnParserImpl.java
processing/src/main/java/org/apache/carbondata/processing/loading/csvinput/CSVInputFormat.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonLoadOptionConstants.java
processing/src/main/java/org/apache/carbondata/processing/loading/model/CarbonLoadModelBuilder.java
processing/src/main/java/org/apache/carbondata/processing/loading/model/LoadOption.java
processing/src/main/java/org/apache/carbondata/processing/loading/model/CarbonLoadModel.java
texts:support range partition the input data for local_sort/global sort data loading
For global_sort/local_sort table, load data command add RANGE_COLUMN optionload data inpath '<path>' into table <table name> options('RANGE_COLUMN'='<a column>')It will range partition the input data.Design doc:RANGE_COLUMN.pdf
issueID:CARBONDATA-322
type:New Feature
changed files:
texts:Integration with  spark 2.x
Since spark 2.0 released. there are many nice features such as more efficient parser, vectorized execution, adaptive execution. It is good to integrate with spark 2.xcurrent integration up to Spark v1.6 is tightly coupled with spark, we would like to cleanup the interface with following design points in mind: 1. decoupled with Spark, integration based on Spark's v2 datasource API2. Enable vectorized carbon reader3. Support saving DataFrame to Carbondata file through Carbondata's output format....
issueID:CARBONDATA-3220
type:Improvement
changed files:hadoop/src/main/java/org/apache/carbondata/hadoop/stream/StreamRecordReader.java
hadoop/src/main/java/org/apache/carbondata/hadoop/stream/CarbonStreamUtils.java
integration/presto/src/main/java/org/apache/carbondata/presto/CarbondataPageSourceProvider.java
integration/presto/src/main/java/org/apache/carbondata/presto/readers/IntegerStreamReader.java
integration/presto/src/main/java/org/apache/carbondata/presto/readers/DecimalSliceStreamReader.java
integration/presto/src/main/java/org/apache/carbondata/presto/readers/DoubleStreamReader.java
integration/presto/src/main/java/org/apache/carbondata/presto/readers/LongStreamReader.java
integration/presto/src/main/java/org/apache/carbondata/presto/CarbondataPageSource.java
integration/presto/src/main/java/org/apache/carbondata/presto/impl/CarbonLocalInputSplit.java
integration/presto/src/main/java/org/apache/carbondata/presto/readers/BooleanStreamReader.java
integration/spark2/src/main/scala/org/apache/carbondata/stream/CarbonStreamRecordReader.java
hadoop/src/main/java/org/apache/carbondata/hadoop/stream/StreamBlockletReader.java
integration/presto/src/main/java/org/apache/carbondata/presto/CarbonVectorBatch.java
core/src/main/java/org/apache/carbondata/hadoop/CarbonInputSplit.java
integration/presto/src/main/java/org/apache/carbondata/presto/impl/CarbonLocalMultiBlockSplit.java
hadoop/src/main/java/org/apache/carbondata/hadoop/stream/CarbonStreamInputFormat.java
integration/presto/src/main/java/org/apache/carbondata/presto/readers/ShortStreamReader.java
integration/presto/src/main/java/org/apache/carbondata/presto/readers/SliceStreamReader.java
integration/presto/src/main/java/org/apache/carbondata/presto/readers/TimestampStreamReader.java
integration/presto/src/main/java/org/apache/carbondata/presto/impl/CarbonTableReader.java
texts:Should support presto to read stream segment data
now the integration of presto doesn't support to read streaming segment data.
issueID:CARBONDATA-3221
type:Bug
changed files:examples/spark2/src/main/java/org/apache/carbondata/examples/sdk/SDKS3Example.java
hadoop/src/main/java/org/apache/carbondata/hadoop/util/CarbonVectorizedRecordReader.java
texts:SDK don&#39;t support read multiple file from S3
SDK don't support multiple file from S3:code:  // Read without filter        CarbonReader reader2 = CarbonReader            .builder(path, "_temp")            .projection(new String[]{"name", "age"})            .withHadoopConf(ACCESS_KEY, args[0])            .withHadoopConf(SECRET_KEY, args[1])            .withHadoopConf(ENDPOINT, args[2])            .build();        System.out.println("\nData:");         int i = 0;        while (i < 20 && reader2.hasNext()) {            Object[] row = (Object[]) reader2.readNextRow();            System.out.println(row[0] + " " + row[1]);            i++;        }        System.out.println("\nFinished");        reader2.close();exception:2019-01-02 12:07:29 INFO  AmazonHttpClient:448 - Unable to execute HTTP request: Timeout waiting for connection from poolorg.apache.http.conn.ConnectionPoolTimeoutException: Timeout waiting for connection from pool at org.apache.http.impl.conn.PoolingClientConnectionManager.leaseConnection(PoolingClientConnectionManager.java:226) at org.apache.http.impl.conn.PoolingClientConnectionManager$1.getConnection(PoolingClientConnectionManager.java:195) at sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at com.amazonaws.http.conn.ClientConnectionRequestFactory$Handler.invoke(ClientConnectionRequestFactory.java:70) at com.amazonaws.http.conn.$Proxy7.getConnection(Unknown Source) at org.apache.http.impl.client.DefaultRequestDirector.execute(DefaultRequestDirector.java:423) at org.apache.http.impl.client.AbstractHttpClient.doExecute(AbstractHttpClient.java:863) at org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:82) at org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:57) at com.amazonaws.http.AmazonHttpClient.executeHelper(AmazonHttpClient.java:384) at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:232) at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:3528) at com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:976) at com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:956) at org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:892) at org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:77) at org.apache.carbondata.core.datastore.filesystem.AbstractDFSCarbonFile.<init>(AbstractDFSCarbonFile.java:75) at org.apache.carbondata.core.datastore.filesystem.AbstractDFSCarbonFile.<init>(AbstractDFSCarbonFile.java:66) at org.apache.carbondata.core.datastore.filesystem.HDFSCarbonFile.<init>(HDFSCarbonFile.java:41) at org.apache.carbondata.core.datastore.filesystem.S3CarbonFile.<init>(S3CarbonFile.java:41) at org.apache.carbondata.core.datastore.impl.DefaultFileTypeProvider.getCarbonFile(DefaultFileTypeProvider.java:53) at org.apache.carbondata.core.datastore.impl.FileFactory.getCarbonFile(FileFactory.java:100) at org.apache.carbondata.core.datastore.impl.FileFactory.getDataInputStream(FileFactory.java:126) at org.apache.carbondata.core.datastore.impl.FileFactory.getDataInputStream(FileFactory.java:117) at org.apache.carbondata.core.reader.ThriftReader.open(ThriftReader.java:101) at org.apache.carbondata.core.reader.CarbonHeaderReader.readHeader(CarbonHeaderReader.java:60) at org.apache.carbondata.core.util.DataFileFooterConverterV3.readDataFileFooter(DataFileFooterConverterV3.java:63) at org.apache.carbondata.core.util.CarbonUtil.getDataFileFooter(CarbonUtil.java:926) at org.apache.carbondata.core.util.CarbonUtil.readMetadataFile(CarbonUtil.java:902) at org.apache.carbondata.core.scan.executor.impl.AbstractQueryExecutor.getDataBlocks(AbstractQueryExecutor.java:217) at org.apache.carbondata.core.scan.executor.impl.AbstractQueryExecutor.initQuery(AbstractQueryExecutor.java:138) at org.apache.carbondata.core.scan.executor.impl.AbstractQueryExecutor.getBlockExecutionInfos(AbstractQueryExecutor.java:403) at org.apache.carbondata.core.scan.executor.impl.VectorDetailQueryExecutor.execute(VectorDetailQueryExecutor.java:44) at org.apache.carbondata.hadoop.util.CarbonVectorizedRecordReader.initialize(CarbonVectorizedRecordReader.java:105) at org.apache.carbondata.sdk.file.CarbonReaderBuilder.build(CarbonReaderBuilder.java:215) at org.apache.carbondata.examples.sdk.SDKS3ReadExample.main(SDKS3ReadExample.java:82)2019-01-02 12:07:34 INFO  AmazonHttpClient:448 - Unable to execute HTTP request: Timeout waiting for connection from poolorg.apache.http.conn.ConnectionPoolTimeoutException: Timeout waiting for connection from pool at org.apache.http.impl.conn.PoolingClientConnectionManager.leaseConnection(PoolingClientConnectionManager.java:226) at org.apache.http.impl.conn.PoolingClientConnectionManager$1.getConnection(PoolingClientConnectionManager.java:195) at sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at com.amazonaws.http.conn.ClientConnectionRequestFactory$Handler.invoke(ClientConnectionRequestFactory.java:70) at com.amazonaws.http.conn.$Proxy7.getConnection(Unknown Source) at org.apache.http.impl.client.DefaultRequestDirector.execute(DefaultRequestDirector.java:423) at org.apache.http.impl.client.AbstractHttpClient.doExecute(AbstractHttpClient.java:863) at org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:82) at org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:57) at com.amazonaws.http.AmazonHttpClient.executeHelper(AmazonHttpClient.java:384) at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:232) at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:3528) at com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:976) at com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:956) at org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:892) at org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:77) at org.apache.carbondata.core.datastore.filesystem.AbstractDFSCarbonFile.<init>(AbstractDFSCarbonFile.java:75) at org.apache.carbondata.core.datastore.filesystem.AbstractDFSCarbonFile.<init>(AbstractDFSCarbonFile.java:66) at org.apache.carbondata.core.datastore.filesystem.HDFSCarbonFile.<init>(HDFSCarbonFile.java:41) at org.apache.carbondata.core.datastore.filesystem.S3CarbonFile.<init>(S3CarbonFile.java:41) at org.apache.carbondata.core.datastore.impl.DefaultFileTypeProvider.getCarbonFile(DefaultFileTypeProvider.java:53) at org.apache.carbondata.core.datastore.impl.FileFactory.getCarbonFile(FileFactory.java:100) at org.apache.carbondata.core.datastore.impl.FileFactory.getDataInputStream(FileFactory.java:126) at org.apache.carbondata.core.datastore.impl.FileFactory.getDataInputStream(FileFactory.java:117) at org.apache.carbondata.core.reader.ThriftReader.open(ThriftReader.java:101) at org.apache.carbondata.core.reader.CarbonHeaderReader.readHeader(CarbonHeaderReader.java:60) at org.apache.carbondata.core.util.DataFileFooterConverterV3.readDataFileFooter(DataFileFooterConverterV3.java:63) at org.apache.carbondata.core.util.CarbonUtil.getDataFileFooter(CarbonUtil.java:926) at org.apache.carbondata.core.util.CarbonUtil.readMetadataFile(CarbonUtil.java:902) at org.apache.carbondata.core.scan.executor.impl.AbstractQueryExecutor.getDataBlocks(AbstractQueryExecutor.java:217) at org.apache.carbondata.core.scan.executor.impl.AbstractQueryExecutor.initQuery(AbstractQueryExecutor.java:138) at org.apache.carbondata.core.scan.executor.impl.AbstractQueryExecutor.getBlockExecutionInfos(AbstractQueryExecutor.java:403) at org.apache.carbondata.core.scan.executor.impl.VectorDetailQueryExecutor.execute(VectorDetailQueryExecutor.java:44) at org.apache.carbondata.hadoop.util.CarbonVectorizedRecordReader.initialize(CarbonVectorizedRecordReader.java:105) at org.apache.carbondata.sdk.file.CarbonReaderBuilder.build(CarbonReaderBuilder.java:215) at org.apache.carbondata.examples.sdk.SDKS3ReadExample.main(SDKS3ReadExample.java:82)2019-01-02 12:07:39 INFO  AmazonHttpClient:448 - Unable to execute HTTP request: Timeout waiting for connection from poolorg.apache.http.conn.ConnectionPoolTimeoutException: Timeout waiting for connection from pool at org.apache.http.impl.conn.PoolingClientConnectionManager.leaseConnection(PoolingClientConnectionManager.java:226) at org.apache.http.impl.conn.PoolingClientConnectionManager$1.getConnection(PoolingClientConnectionManager.java:195) at sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at com.amazonaws.http.conn.ClientConnectionRequestFactory$Handler.invoke(ClientConnectionRequestFactory.java:70) at com.amazonaws.http.conn.$Proxy7.getConnection(Unknown Source) at org.apache.http.impl.client.DefaultRequestDirector.execute(DefaultRequestDirector.java:423) at org.apache.http.impl.client.AbstractHttpClient.doExecute(AbstractHttpClient.java:863) at org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:82) at org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:57) at com.amazonaws.http.AmazonHttpClient.executeHelper(AmazonHttpClient.java:384) at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:232) at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:3528) at com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:976) at com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:956) at org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:892) at org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:77) at org.apache.carbondata.core.datastore.filesystem.AbstractDFSCarbonFile.<init>(AbstractDFSCarbonFile.java:75) at org.apache.carbondata.core.datastore.filesystem.AbstractDFSCarbonFile.<init>(AbstractDFSCarbonFile.java:66) at org.apache.carbondata.core.datastore.filesystem.HDFSCarbonFile.<init>(HDFSCarbonFile.java:41) at org.apache.carbondata.core.datastore.filesystem.S3CarbonFile.<init>(S3CarbonFile.java:41) at org.apache.carbondata.core.datastore.impl.DefaultFileTypeProvider.getCarbonFile(DefaultFileTypeProvider.java:53) at org.apache.carbondata.core.datastore.impl.FileFactory.getCarbonFile(FileFactory.java:100) at org.apache.carbondata.core.datastore.impl.FileFactory.getDataInputStream(FileFactory.java:126) at org.apache.carbondata.core.datastore.impl.FileFactory.getDataInputStream(FileFactory.java:117) at org.apache.carbondata.core.reader.ThriftReader.open(ThriftReader.java:101) at org.apache.carbondata.core.reader.CarbonHeaderReader.readHeader(CarbonHeaderReader.java:60) at org.apache.carbondata.core.util.DataFileFooterConverterV3.readDataFileFooter(DataFileFooterConverterV3.java:63) at org.apache.carbondata.core.util.CarbonUtil.getDataFileFooter(CarbonUtil.java:926) at org.apache.carbondata.core.util.CarbonUtil.readMetadataFile(CarbonUtil.java:902) at org.apache.carbondata.core.scan.executor.impl.AbstractQueryExecutor.getDataBlocks(AbstractQueryExecutor.java:217) at org.apache.carbondata.core.scan.executor.impl.AbstractQueryExecutor.initQuery(AbstractQueryExecutor.java:138) at org.apache.carbondata.core.scan.executor.impl.AbstractQueryExecutor.getBlockExecutionInfos(AbstractQueryExecutor.java:403) at org.apache.carbondata.core.scan.executor.impl.VectorDetailQueryExecutor.execute(VectorDetailQueryExecutor.java:44) at org.apache.carbondata.hadoop.util.CarbonVectorizedRecordReader.initialize(CarbonVectorizedRecordReader.java:105) at org.apache.carbondata.sdk.file.CarbonReaderBuilder.build(CarbonReaderBuilder.java:215) at org.apache.carbondata.examples.sdk.SDKS3ReadExample.main(SDKS3ReadExample.java:82)2019-01-02 12:07:45 INFO  AmazonHttpClient:448 - Unable to execute HTTP request: Timeout waiting for connection from poolorg.apache.http.conn.ConnectionPoolTimeoutException: Timeout waiting for connection from pool at org.apache.http.impl.conn.PoolingClientConnectionManager.leaseConnection(PoolingClientConnectionManager.java:226) at org.apache.http.impl.conn.PoolingClientConnectionManager$1.getConnection(PoolingClientConnectionManager.java:195) at sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at com.amazonaws.http.conn.ClientConnectionRequestFactory$Handler.invoke(ClientConnectionRequestFactory.java:70) at com.amazonaws.http.conn.$Proxy7.getConnection(Unknown Source) at org.apache.http.impl.client.DefaultRequestDirector.execute(DefaultRequestDirector.java:423) at org.apache.http.impl.client.AbstractHttpClient.doExecute(AbstractHttpClient.java:863) at org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:82) at org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:57) at com.amazonaws.http.AmazonHttpClient.executeHelper(AmazonHttpClient.java:384) at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:232) at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:3528) at com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:976) at com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:956) at org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:892) at org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:77) at org.apache.carbondata.core.datastore.filesystem.AbstractDFSCarbonFile.<init>(AbstractDFSCarbonFile.java:75) at org.apache.carbondata.core.datastore.filesystem.AbstractDFSCarbonFile.<init>(AbstractDFSCarbonFile.java:66) at org.apache.carbondata.core.datastore.filesystem.HDFSCarbonFile.<init>(HDFSCarbonFile.java:41) at org.apache.carbondata.core.datastore.filesystem.S3CarbonFile.<init>(S3CarbonFile.java:41) at org.apache.carbondata.core.datastore.impl.DefaultFileTypeProvider.getCarbonFile(DefaultFileTypeProvider.java:53) at org.apache.carbondata.core.datastore.impl.FileFactory.getCarbonFile(FileFactory.java:100) at org.apache.carbondata.core.datastore.impl.FileFactory.getDataInputStream(FileFactory.java:126) at org.apache.carbondata.core.datastore.impl.FileFactory.getDataInputStream(FileFactory.java:117) at org.apache.carbondata.core.reader.ThriftReader.open(ThriftReader.java:101) at org.apache.carbondata.core.reader.CarbonHeaderReader.readHeader(CarbonHeaderReader.java:60) at org.apache.carbondata.core.util.DataFileFooterConverterV3.readDataFileFooter(DataFileFooterConverterV3.java:63) at org.apache.carbondata.core.util.CarbonUtil.getDataFileFooter(CarbonUtil.java:926) at org.apache.carbondata.core.util.CarbonUtil.readMetadataFile(CarbonUtil.java:902) at org.apache.carbondata.core.scan.executor.impl.AbstractQueryExecutor.getDataBlocks(AbstractQueryExecutor.java:217) at org.apache.carbondata.core.scan.executor.impl.AbstractQueryExecutor.initQuery(AbstractQueryExecutor.java:138) at org.apache.carbondata.core.scan.executor.impl.AbstractQueryExecutor.getBlockExecutionInfos(AbstractQueryExecutor.java:403) at org.apache.carbondata.core.scan.executor.impl.VectorDetailQueryExecutor.execute(VectorDetailQueryExecutor.java:44) at org.apache.carbondata.hadoop.util.CarbonVectorizedRecordReader.initialize(CarbonVectorizedRecordReader.java:105) at org.apache.carbondata.sdk.file.CarbonReaderBuilder.build(CarbonReaderBuilder.java:215) at org.apache.carbondata.examples.sdk.SDKS3ReadExample.main(SDKS3ReadExample.java:82)2019-01-02 12:07:53 INFO  AmazonHttpClient:448 - Unable to execute HTTP request: Timeout waiting for connection from poolorg.apache.http.conn.ConnectionPoolTimeoutException: Timeout waiting for connection from pool at org.apache.http.impl.conn.PoolingClientConnectionManager.leaseConnection(PoolingClientConnectionManager.java:226) at org.apache.http.impl.conn.PoolingClientConnectionManager$1.getConnection(PoolingClientConnectionManager.java:195) at sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at com.amazonaws.http.conn.ClientConnectionRequestFactory$Handler.invoke(ClientConnectionRequestFactory.java:70) at com.amazonaws.http.conn.$Proxy7.getConnection(Unknown Source) at org.apache.http.impl.client.DefaultRequestDirector.execute(DefaultRequestDirector.java:423) at org.apache.http.impl.client.AbstractHttpClient.doExecute(AbstractHttpClient.java:863) at org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:82) at org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:57) at com.amazonaws.http.AmazonHttpClient.executeHelper(AmazonHttpClient.java:384) at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:232) at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:3528) at com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:976) at com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:956) at org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:892) at org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:77) at org.apache.carbondata.core.datastore.filesystem.AbstractDFSCarbonFile.<init>(AbstractDFSCarbonFile.java:75) at org.apache.carbondata.core.datastore.filesystem.AbstractDFSCarbonFile.<init>(AbstractDFSCarbonFile.java:66) at org.apache.carbondata.core.datastore.filesystem.HDFSCarbonFile.<init>(HDFSCarbonFile.java:41) at org.apache.carbondata.core.datastore.filesystem.S3CarbonFile.<init>(S3CarbonFile.java:41) at org.apache.carbondata.core.datastore.impl.DefaultFileTypeProvider.getCarbonFile(DefaultFileTypeProvider.java:53) at org.apache.carbondata.core.datastore.impl.FileFactory.getCarbonFile(FileFactory.java:100) at org.apache.carbondata.core.datastore.impl.FileFactory.getDataInputStream(FileFactory.java:126) at org.apache.carbondata.core.datastore.impl.FileFactory.getDataInputStream(FileFactory.java:117) at org.apache.carbondata.core.reader.ThriftReader.open(ThriftReader.java:101) at org.apache.carbondata.core.reader.CarbonHeaderReader.readHeader(CarbonHeaderReader.java:60) at org.apache.carbondata.core.util.DataFileFooterConverterV3.readDataFileFooter(DataFileFooterConverterV3.java:63) at org.apache.carbondata.core.util.CarbonUtil.getDataFileFooter(CarbonUtil.java:926) at org.apache.carbondata.core.util.CarbonUtil.readMetadataFile(CarbonUtil.java:902) at org.apache.carbondata.core.scan.executor.impl.AbstractQueryExecutor.getDataBlocks(AbstractQueryExecutor.java:217) at org.apache.carbondata.core.scan.executor.impl.AbstractQueryExecutor.initQuery(AbstractQueryExecutor.java:138) at org.apache.carbondata.core.scan.executor.impl.AbstractQueryExecutor.getBlockExecutionInfos(AbstractQueryExecutor.java:403) at org.apache.carbondata.core.scan.executor.impl.VectorDetailQueryExecutor.execute(VectorDetailQueryExecutor.java:44) at org.apache.carbondata.hadoop.util.CarbonVectorizedRecordReader.initialize(CarbonVectorizedRecordReader.java:105) at org.apache.carbondata.sdk.file.CarbonReaderBuilder.build(CarbonReaderBuilder.java:215) at org.apache.carbondata.examples.sdk.SDKS3ReadExample.main(SDKS3ReadExample.java:82)2019-01-02 12:08:03 INFO  AmazonHttpClient:448 - Unable to execute HTTP request: Timeout waiting for connection from poolorg.apache.http.conn.ConnectionPoolTimeoutException: Timeout waiting for connection from pool at org.apache.http.impl.conn.PoolingClientConnectionManager.leaseConnection(PoolingClientConnectionManager.java:226) at org.apache.http.impl.conn.PoolingClientConnectionManager$1.getConnection(PoolingClientConnectionManager.java:195) at sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at com.amazonaws.http.conn.ClientConnectionRequestFactory$Handler.invoke(ClientConnectionRequestFactory.java:70) at com.amazonaws.http.conn.$Proxy7.getConnection(Unknown Source) at org.apache.http.impl.client.DefaultRequestDirector.execute(DefaultRequestDirector.java:423) at org.apache.http.impl.client.AbstractHttpClient.doExecute(AbstractHttpClient.java:863) at org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:82) at org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:57) at com.amazonaws.http.AmazonHttpClient.executeHelper(AmazonHttpClient.java:384) at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:232) at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:3528) at com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:976) at com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:956) at org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:892) at org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:77) at org.apache.carbondata.core.datastore.filesystem.AbstractDFSCarbonFile.<init>(AbstractDFSCarbonFile.java:75) at org.apache.carbondata.core.datastore.filesystem.AbstractDFSCarbonFile.<init>(AbstractDFSCarbonFile.java:66) at org.apache.carbondata.core.datastore.filesystem.HDFSCarbonFile.<init>(HDFSCarbonFile.java:41) at org.apache.carbondata.core.datastore.filesystem.S3CarbonFile.<init>(S3CarbonFile.java:41) at org.apache.carbondata.core.datastore.impl.DefaultFileTypeProvider.getCarbonFile(DefaultFileTypeProvider.java:53) at org.apache.carbondata.core.datastore.impl.FileFactory.getCarbonFile(FileFactory.java:100) at org.apache.carbondata.core.datastore.impl.FileFactory.getDataInputStream(FileFactory.java:126) at org.apache.carbondata.core.datastore.impl.FileFactory.getDataInputStream(FileFactory.java:117) at org.apache.carbondata.core.reader.ThriftReader.open(ThriftReader.java:101) at org.apache.carbondata.core.reader.CarbonHeaderReader.readHeader(CarbonHeaderReader.java:60) at org.apache.carbondata.core.util.DataFileFooterConverterV3.readDataFileFooter(DataFileFooterConverterV3.java:63) at org.apache.carbondata.core.util.CarbonUtil.getDataFileFooter(CarbonUtil.java:926) at org.apache.carbondata.core.util.CarbonUtil.readMetadataFile(CarbonUtil.java:902) at org.apache.carbondata.core.scan.executor.impl.AbstractQueryExecutor.getDataBlocks(AbstractQueryExecutor.java:217) at org.apache.carbondata.core.scan.executor.impl.AbstractQueryExecutor.initQuery(AbstractQueryExecutor.java:138) at org.apache.carbondata.core.scan.executor.impl.AbstractQueryExecutor.getBlockExecutionInfos(AbstractQueryExecutor.java:403) at org.apache.carbondata.core.scan.executor.impl.VectorDetailQueryExecutor.execute(VectorDetailQueryExecutor.java:44) at org.apache.carbondata.hadoop.util.CarbonVectorizedRecordReader.initialize(CarbonVectorizedRecordReader.java:105) at org.apache.carbondata.sdk.file.CarbonReaderBuilder.build(CarbonReaderBuilder.java:215) at org.apache.carbondata.examples.sdk.SDKS3ReadExample.main(SDKS3ReadExample.java:82)2019-01-02 12:08:17 INFO  AmazonHttpClient:448 - Unable to execute HTTP request: Timeout waiting for connection from poolorg.apache.http.conn.ConnectionPoolTimeoutException: Timeout waiting for connection from pool at org.apache.http.impl.conn.PoolingClientConnectionManager.leaseConnection(PoolingClientConnectionManager.java:226) at org.apache.http.impl.conn.PoolingClientConnectionManager$1.getConnection(PoolingClientConnectionManager.java:195) at sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at com.amazonaws.http.conn.ClientConnectionRequestFactory$Handler.invoke(ClientConnectionRequestFactory.java:70) at com.amazonaws.http.conn.$Proxy7.getConnection(Unknown Source) at org.apache.http.impl.client.DefaultRequestDirector.execute(DefaultRequestDirector.java:423) at org.apache.http.impl.client.AbstractHttpClient.doExecute(AbstractHttpClient.java:863) at org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:82) at org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:57) at com.amazonaws.http.AmazonHttpClient.executeHelper(AmazonHttpClient.java:384) at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:232) at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:3528) at com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:976) at com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:956) at org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:892) at org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:77) at org.apache.carbondata.core.datastore.filesystem.AbstractDFSCarbonFile.<init>(AbstractDFSCarbonFile.java:75) at org.apache.carbondata.core.datastore.filesystem.AbstractDFSCarbonFile.<init>(AbstractDFSCarbonFile.java:66) at org.apache.carbondata.core.datastore.filesystem.HDFSCarbonFile.<init>(HDFSCarbonFile.java:41) at org.apache.carbondata.core.datastore.filesystem.S3CarbonFile.<init>(S3CarbonFile.java:41) at org.apache.carbondata.core.datastore.impl.DefaultFileTypeProvider.getCarbonFile(DefaultFileTypeProvider.java:53) at org.apache.carbondata.core.datastore.impl.FileFactory.getCarbonFile(FileFactory.java:100) at org.apache.carbondata.core.datastore.impl.FileFactory.getDataInputStream(FileFactory.java:126) at org.apache.carbondata.core.datastore.impl.FileFactory.getDataInputStream(FileFactory.java:117) at org.apache.carbondata.core.reader.ThriftReader.open(ThriftReader.java:101) at org.apache.carbondata.core.reader.CarbonHeaderReader.readHeader(CarbonHeaderReader.java:60) at org.apache.carbondata.core.util.DataFileFooterConverterV3.readDataFileFooter(DataFileFooterConverterV3.java:63) at org.apache.carbondata.core.util.CarbonUtil.getDataFileFooter(CarbonUtil.java:926) at org.apache.carbondata.core.util.CarbonUtil.readMetadataFile(CarbonUtil.java:902) at org.apache.carbondata.core.scan.executor.impl.AbstractQueryExecutor.getDataBlocks(AbstractQueryExecutor.java:217) at org.apache.carbondata.core.scan.executor.impl.AbstractQueryExecutor.initQuery(AbstractQueryExecutor.java:138) at org.apache.carbondata.core.scan.executor.impl.AbstractQueryExecutor.getBlockExecutionInfos(AbstractQueryExecutor.java:403) at org.apache.carbondata.core.scan.executor.impl.VectorDetailQueryExecutor.execute(VectorDetailQueryExecutor.java:44) at org.apache.carbondata.hadoop.util.CarbonVectorizedRecordReader.initialize(CarbonVectorizedRecordReader.java:105) at org.apache.carbondata.sdk.file.CarbonReaderBuilder.build(CarbonReaderBuilder.java:215) at org.apache.carbondata.examples.sdk.SDKS3ReadExample.main(SDKS3ReadExample.java:82)2019-01-02 12:08:41 INFO  AmazonHttpClient:448 - Unable to execute HTTP request: Timeout waiting for connection from poolorg.apache.http.conn.ConnectionPoolTimeoutException: Timeout waiting for connection from pool at org.apache.http.impl.conn.PoolingClientConnectionManager.leaseConnection(PoolingClientConnectionManager.java:226) at org.apache.http.impl.conn.PoolingClientConnectionManager$1.getConnection(PoolingClientConnectionManager.java:195) at sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at com.amazonaws.http.conn.ClientConnectionRequestFactory$Handler.invoke(ClientConnectionRequestFactory.java:70) at com.amazonaws.http.conn.$Proxy7.getConnection(Unknown Source) at org.apache.http.impl.client.DefaultRequestDirector.execute(DefaultRequestDirector.java:423) at org.apache.http.impl.client.AbstractHttpClient.doExecute(AbstractHttpClient.java:863) at org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:82) at org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:57) at com.amazonaws.http.AmazonHttpClient.executeHelper(AmazonHttpClient.java:384) at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:232) at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:3528) at com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:976) at com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:956) at org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:892) at org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:77) at org.apache.carbondata.core.datastore.filesystem.AbstractDFSCarbonFile.<init>(AbstractDFSCarbonFile.java:75) at org.apache.carbondata.core.datastore.filesystem.AbstractDFSCarbonFile.<init>(AbstractDFSCarbonFile.java:66) at org.apache.carbondata.core.datastore.filesystem.HDFSCarbonFile.<init>(HDFSCarbonFile.java:41) at org.apache.carbondata.core.datastore.filesystem.S3CarbonFile.<init>(S3CarbonFile.java:41) at org.apache.carbondata.core.datastore.impl.DefaultFileTypeProvider.getCarbonFile(DefaultFileTypeProvider.java:53) at org.apache.carbondata.core.datastore.impl.FileFactory.getCarbonFile(FileFactory.java:100) at org.apache.carbondata.core.datastore.impl.FileFactory.getDataInputStream(FileFactory.java:126) at org.apache.carbondata.core.datastore.impl.FileFactory.getDataInputStream(FileFactory.java:117) at org.apache.carbondata.core.reader.ThriftReader.open(ThriftReader.java:101) at org.apache.carbondata.core.reader.CarbonHeaderReader.readHeader(CarbonHeaderReader.java:60) at org.apache.carbondata.core.util.DataFileFooterConverterV3.readDataFileFooter(DataFileFooterConverterV3.java:63) at org.apache.carbondata.core.util.CarbonUtil.getDataFileFooter(CarbonUtil.java:926) at org.apache.carbondata.core.util.CarbonUtil.readMetadataFile(CarbonUtil.java:902) at org.apache.carbondata.core.scan.executor.impl.AbstractQueryExecutor.getDataBlocks(AbstractQueryExecutor.java:217) at org.apache.carbondata.core.scan.executor.impl.AbstractQueryExecutor.initQuery(AbstractQueryExecutor.java:138) at org.apache.carbondata.core.scan.executor.impl.AbstractQueryExecutor.getBlockExecutionInfos(AbstractQueryExecutor.java:403) at org.apache.carbondata.core.scan.executor.impl.VectorDetailQueryExecutor.execute(VectorDetailQueryExecutor.java:44) at org.apache.carbondata.hadoop.util.CarbonVectorizedRecordReader.initialize(CarbonVectorizedRecordReader.java:105) at org.apache.carbondata.sdk.file.CarbonReaderBuilder.build(CarbonReaderBuilder.java:215) at org.apache.carbondata.examples.sdk.SDKS3ReadExample.main(SDKS3ReadExample.java:82)2019-01-02 12:09:06 INFO  AmazonHttpClient:448 - Unable to execute HTTP request: Timeout waiting for connection from poolorg.apache.http.conn.ConnectionPoolTimeoutException: Timeout waiting for connection from pool
issueID:CARBONDATA-3222
type:Bug
changed files:
texts:Fix dataload failure after creation of preaggregate datamap on main table with long_string_columns
Fix dataload failure after creation of preaggregate datamap on main table with long_string_columns. Dataload is gettling failed because child table properties are not getting modified according to the parent table for long_string_columns.This occurs only when we dont pass long_string_columns is not specified in dmproperties for preaggregate datamap.
issueID:CARBONDATA-3223
type:Bug
changed files:hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonInputFormat.java
texts:Datasize and Indexsize showing 0B for 1.1 store when show segments is done
Create table and load in 1.1 store. Refresh and Load in 1.5.1 version. Show Segments on the table will give 0B for the older segment.
issueID:CARBONDATA-3224
type:Bug
changed files:store/sdk/src/main/java/org/apache/carbondata/sdk/file/CarbonWriterBuilder.java
texts:SDK should validate the improper value
a)"BAD_RECORDS_ACTION","jfie"  b)"BAD_RECORDS_LOGGER_ENABLE","FLSE"For both the above cases test case is passing, which is improper, Validation is not done for the values to the corresponding key which is expected.Basically, it is accepting any string values.
issueID:CARBONDATA-3226
type:Bug
changed files:
texts:Remove duplicated and useless files
Remove duplicated and useless files from the project.For example, org/apache/carbondata/spark/rdd/CarbonMergeFilesRDD.scala has duplication of name with org/apache/spark/rdd/CarbonMergeFilesRDD.scala, but without any content at all. 
issueID:CARBONDATA-3227
type:Bug
changed files:
texts:There are some spell errors in the project
There are some spell errors in the project:escapecharoptionlisthivedefaultpartitionpvalueerrormsgisDetectAsDimentionDatatypePlease fix it if there are other spell error.
issueID:CARBONDATA-3230
type:Improvement
changed files:
texts:Add ALTER  test case with datasource for using parquet and carbon
Add ALTER  test case with datasource for using parquet and carbon
issueID:CARBONDATA-3233
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/datastore/page/UnsafeFixLengthColumnPage.java
texts:JVM is getting crashed during dataload while compressing in snappy
when huge dataload is done, some times dataload is failed and jvm is crashed during snappy compression Below is the logs:Java frames: (J=compiled Java code, j=interpreted, Vv=VM code) j org.xerial.snappy.SnappyNative.rawCompress(JJJ)J+0 j org.apache.carbondata.core.datastore.compression.SnappyCompressor.rawCompress(JIJ)J+9 j org.apache.carbondata.core.datastore.page.UnsafeFixLengthColumnPage.compress(Lorg/apache/carbondata/core/datastore/compression/Compressor[B+50 j org.apache.carbondata.core.datastore.page.encoding.adaptive.AdaptiveCodec.encodeAndCompressPage(Lorg/apache/carbondata/core/datastore/page/ColumnPage;Lorg/apache/carbondata/core/datastore/page/ColumnPageValueConverter;Lorg/apache/carbondata/core/datastore/compression/Compressor[B+85 j org.apache.carbondata.core.datastore.page.encoding.adaptive.AdaptiveDeltaIntegralCodec$1.encodeData(Lorg/apache/carbondata/core/datastore/page/ColumnPage[B+45 j org.apache.carbondata.core.datastore.page.encoding.ColumnPageEncoder.encode(Lorg/apache/carbondata/core/datastore/page/ColumnPage;)Lorg/apache/carbondata/core/datastore/page/encoding/EncodedColumnPage;+2 j org.apache.carbondata.processing.store.TablePage.encodeAndCompressMeasures()[Lorg/apache/carbondata/core/datastore/page/encoding/EncodedColumnPage;+54 j org.apache.carbondata.processing.store.TablePage.encode()V+6 j org.apache.carbondata.processing.store.CarbonFactDataHandlerColumnar.processDataRows(Ljava/util/List;)Lorg/apache/carbondata/processing/store/TablePage;+86 j org.apache.carbondata.processing.store.CarbonFactDataHandlerColumnar.access$500(Lorg/apache/carbondata/processing/store/CarbonFactDataHandlerColumnar;Ljava/util/List;)Lorg/apache/carbondata/processing/store/TablePage;+2 j org.apache.carbondata.processing.store.CarbonFactDataHandlerColumnar$Producer.call()Ljava/lang/Void;+8 j org.apache.carbondata.processing.store.CarbonFactDataHandlerColumnar$Producer.call()Ljava/lang/Object;+1 j java.util.concurrent.FutureTask.run()V+42 j java.util.concurrent.ThreadPoolExecutor.runWorker(Ljava/util/concurrent/ThreadPoolExecutor$Worker;)V+95 j java.util.concurrent.ThreadPoolExecutor$Worker.run()V+5 j java.lang.Thread.run()V+11 v ~StubRoutines::call_stub
issueID:CARBONDATA-3235
type:Bug
changed files:
texts:AlterTableRename and PreAgg Datamap Fail Issue
Alter Table Rename Table Fail When table rename is success in hive, but failed in carbon data store, it would throw exception, but would not go back and undo rename in hive. 
issueID:CARBONDATA-3236
type:Bug
changed files:
texts:JVM Crash for insert into new table from old table

issueID:CARBONDATA-3237
type:Bug
changed files:integration/presto/src/main/java/org/apache/carbondata/presto/readers/DecimalSliceStreamReader.java
integration/presto/src/main/java/org/apache/carbondata/presto/CarbonVectorBatch.java
integration/presto/src/main/java/org/apache/carbondata/presto/readers/SliceStreamReader.java
texts:optimize presto query time for dictionary include string column
optimize presto query time for dictionary include string column. problem: currently, for each query, presto carbon creates dictionary block for string columns.This happens for each query and if cardinality is more , it takes more time to build. This is not required. we can lookup using normal dictionary lookup. 
issueID:CARBONDATA-3238
type:Bug
changed files:
texts:Throw StackOverflowError exception using MV datamap
Exception:java.lang.StackOverflowError at org.apache.spark.sql.catalyst.expressions.AttributeMap$$anonfun$get$1.apply(AttributeMap.scala:34) at org.apache.spark.sql.catalyst.expressions.AttributeMap$$anonfun$get$1.apply(AttributeMap.scala:34) at scala.Option.map(Option.scala:146) at org.apache.spark.sql.catalyst.expressions.AttributeMap.get(AttributeMap.scala:34) at org.apache.spark.sql.catalyst.expressions.AttributeMap.contains(AttributeMap.scala:36)TestCase:    sql("drop datamap if exists all_table_mv")    sql("drop table if exists all_table")    sql("create table all_table(x1 bigint,x2 bigint,x3 string,x4 bigint,x5 bigint,x6 int,x7 string,x8 int, x9 int,x10 bigint," +      "x11 bigint, x12 bigint,x13 bigint,x14 bigint,x15 bigint,x16 bigint,x17 bigint,x18 bigint,x19 bigint) stored by 'carbondata'")    sql("insert into all_table select 1,1,null,1,1,1,null,1,1,1,1,1,1,1,1,1,1,1,1")    sql("create datamap all_table_mv on table all_table using 'mv' " +      "as select sum(x12) as y1, sum(x13) as y2, sum(x14) as y3,sum(x15) as y4,X8,x9,x2 from all_table group by X8,x9,x2")    sql("rebuild datamap all_table_mv")    sql("explain select sum(x12) as y1, sum(x13) as y2, sum(x14) as y3,sum(x15) as y4,X8,x9,x2 from all_table group by X8,x9,x2")
issueID:CARBONDATA-3239
type:Bug
changed files:
texts:Throwing ArrayIndexOutOfBoundsException in DataSkewRangePartitioner
2019-01-10 15:31:21 ERROR DataLoadProcessorStepOnSpark$:367 - Data Loading failed for table carbon_range_column4java.lang.ArrayIndexOutOfBoundsException: 1 at org.apache.spark.DataSkewRangePartitioner$$anonfun$initialize$1.apply$mcVI$sp(DataSkewRangePartitioner.scala:223) at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:160) at org.apache.spark.DataSkewRangePartitioner.initialize(DataSkewRangePartitioner.scala:222) at org.apache.spark.DataSkewRangePartitioner.getPartition(DataSkewRangePartitioner.scala:234) at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:151) at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96) at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53) at org.apache.spark.scheduler.Task.run(Task.scala:108) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748)
issueID:CARBONDATA-324
type:Bug
changed files:
texts:Decimal and Bigint type columns contains Null, after load data
Using Thrift server and Beeling client, i am trying to create a table and load the data from CSV. My tables contains BigInt and Decimal Column types, After load the data using Load Data command, The BigInt and Decimal Column contains Null Value. Bellow are the steps:Step 1: > create database wednesday;> use wednesday;> CREATE TABLE one (id int, age iNt, name String, salary decimal, data bigInt, weight double, dob timeStamp) STORED BY 'carbondata';Step 2: Create a csv file which contains column values as below: id, age, name, salary, data, weight, dob1, 54, james, 900000, 292092, 34.2, 2016-05-04 22:55:00Step 3: Load the data from CSV file as below: > LOAD DATA INPATH 'hdfs://localhost:54310/home/harmeet/sample3.csv' INTO TABLE one;Step 4: Select the data from table one, and BigInt and Decimal column contains Null value.
issueID:CARBONDATA-3241
type:Improvement
changed files:
texts:Refactor the requested scan columns and the projection columns

issueID:CARBONDATA-3242
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/CarbonTable.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonLoadOptionConstants.java
core/src/main/java/org/apache/carbondata/core/util/CarbonProperties.java
processing/src/main/java/org/apache/carbondata/processing/loading/model/CarbonLoadModelBuilder.java
texts:Range_Column should be table level property

issueID:CARBONDATA-3243
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/metadata/schema/table/CarbonTable.java
core/src/main/java/org/apache/carbondata/core/constants/SortScopeOptions.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/SingleThreadFinalSortFilesMerger.java
texts:CarbonTable.getSortScope() is not considering session property CARBON.TABLE.LOAD.SORT.SCOPE

issueID:CARBONDATA-3244
type:Improvement
changed files:
texts:Add benchmark for Change Data Capture scenario
CDC (change data capture) is a common scenario for analyzing slowly changed table in data warehouse.It is good to add benchmark test comparing two update methods:1. hive_solution, which uses INSERT OVERWRITE. This is a popular method for hive warehouse.2. carbon_solution, which uses CarbonData's update syntax to update the history table directly.
issueID:CARBONDATA-3246
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
core/src/main/java/org/apache/carbondata/core/scan/result/iterator/AbstractDetailQueryResultIterator.java
core/src/main/java/org/apache/carbondata/core/util/CarbonProperties.java
texts:SDK reader fails if vectorReader is false for concurrent read scenario and batch size is zero.
SDK reader fails if vectorReader is false for concurrent read scenario and batch size is zero.If the batch size is zero or less , we should throw a proper exception stating that batch size cannot be less than zero.
issueID:CARBONDATA-3247
type:Bug
changed files:
texts:Support to select all columns when creating MV datamap
create table all_table(name string, age int, height int)  stored by 'carbondata'create datamap all_table_mv on table all_table using 'mv' as select avg(age),avg(height),name from all_table group by namethrow UnsupportedOperationException("MV is not supported for this query")
issueID:CARBONDATA-325
type:Bug
changed files:
texts:Create table with columns contains spaces in name.
I want to create table, using columns that contains spaces. I am using Thrift Server and Beeline client for accessing carbon data. Whenever i am trying to create a table, and their columns name contains spaces i am getting an error. Below are the steps:Step 1:create table three (`first name` string, `age` int) stored by 'carbondata';Whenever i am executing above query, i am getting below error:Error: org.apache.carbondata.spark.exception.MalformedCarbonCommandException: Unsupported data type : FieldSchema(name:first name, type:string, comment:null).getType (state=,code=0)The above error is pretending to be wrong data types are using. If I am removing `stored by 'carbondata'` from query, then this will work fine because it is run on Hive.
issueID:CARBONDATA-3252
type:Bug
changed files:
texts:Remove  unused import and optimize the import order
Remove  unused import and fix some spell error org.apache.carbondata.spark.testsuite.badrecordloger.BadRecordLoggerTest:    remove CarbonLoadOptionConstants in line 27 org.apache.carbondata.spark.testsuite.directdictionary.TimestampNoDictionaryColumnTestCase:remove line 23 and 26
issueID:CARBONDATA-3253
type:Improvement
changed files:
texts:Remove test case of bloom datamap using search mode

issueID:CARBONDATA-3257
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
texts:Data Load is in No sort flowwhen version is upgraded even if sort columns are given. Also describe formatted displays wrong sort scope after refresh.

issueID:CARBONDATA-3258
type:Test
changed files:
texts:Add more test case for mv datamap
Add more test case for mv datamap
issueID:CARBONDATA-3259
type:Sub-task
changed files:
texts:Documentation Update

issueID:CARBONDATA-3260
type:Bug
changed files:
texts:Broadcast join is not properly in carbon with spark-2.3.2
It seems stats which come from catalog table of hive gives wrong data sizes for carbon table. Because of that even large tables are also going to broadcast join.
issueID:CARBONDATA-3261
type:Improvement
changed files:integration/presto/src/main/java/org/apache/carbondata/presto/PrestoCarbonVectorizedRecordReader.java
integration/presto/src/main/java/org/apache/carbondata/presto/CarbonVectorBatch.java
integration/presto/src/main/java/org/apache/carbondata/presto/readers/FloatStreamReader.java
texts:support float and byte reading from presto
problem: support float and byte reading from prestocause: currently float and byte cannot be read in presto due to code issue. It was going as double data type. Hence array out of bound issue used to come as float/byte read from double stream reader.solution: Implement a new stream reader for float and byte.
issueID:CARBONDATA-3262
type:Bug
changed files:processing/src/main/java/org/apache/carbondata/processing/loading/model/CarbonLoadModel.java
core/src/main/java/org/apache/carbondata/core/writer/CarbonIndexFileMergeWriter.java
texts:Failure to write merge index file results in merged segment being deleted when cleanup happens

issueID:CARBONDATA-3263
type:Improvement
changed files:
texts:Should update doc for RANGE_COLUMN feature

issueID:CARBONDATA-3264
type:New Feature
changed files:
texts:Support SORT_SCOPE in ALTER TABLE SET Command

issueID:CARBONDATA-3265
type:Bug
changed files:
texts:Memory Leak and Low Query Performance Issues in Range Partition

issueID:CARBONDATA-3267
type:Bug
changed files:processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/SortParameters.java
core/src/main/java/org/apache/carbondata/core/memory/IntPointerBuffer.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/merger/UnsafeIntermediateMerger.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/UnsafeCarbonRowPage.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/UnsafeSortDataRows.java
core/src/main/java/org/apache/carbondata/core/memory/UnsafeSortMemoryManager.java
texts:Data loading is failing with OOM using range sort
Problem:Range sort is failing with OOM.Root cause:This is because UnsafeSortStorageMemory is not able to control the off heap memory because of this when huge data is loaded it OOM exception is coming fron UnsafeMemoryAllocator.allocate.Solution:Control Sort Storage memory. After sorting the rows if memory is available then only add sorted records to sort storage memory otherwise write to disk
issueID:CARBONDATA-3268
type:Bug
changed files:integration/presto/src/main/java/org/apache/carbondata/presto/CarbonVectorBatch.java
texts:Query on Varchar showing as Null in Presto

issueID:CARBONDATA-3269
type:Bug
changed files:
texts:Range_column throwing ArrayIndexOutOfBoundsException when using KryoSerializer
Reproduce:For range_column feature,When we set "spark.serializer" to "org.apache.spark.serializer.KryoSerializer", data loading will throw ArrayIndexOutOfBoundsException.Excpetion:2019-01-25 13:00:19 ERROR DataLoadProcessorStepOnSpark$:367 - Data Loading failed for table carbon_range_column4 java.lang.ArrayIndexOutOfBoundsException: 5 at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:151) at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96) at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53) at org.apache.spark.scheduler.Task.run(Task.scala:108) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748) 2019-01-25 13:00:19 ERROR TaskContextImpl:91 - Error in TaskFailureListener org.apache.carbondata.processing.loading.exception.CarbonDataLoadingException: Data Loading failed for table carbon_range_column4 at org.apache.carbondata.spark.load.DataLoadProcessorStepOnSpark$.org$apache$carbondata$spark$load$DataLoadProcessorStepOnSpark$$wrapException(DataLoadProcessorStepOnSpark.scala:368) at org.apache.carbondata.spark.load.DataLoadProcessorStepOnSpark$$anonfun$convertFunc$3.apply(DataLoadProcessorStepOnSpark.scala:215) at org.apache.carbondata.spark.load.DataLoadProcessorStepOnSpark$$anonfun$convertFunc$3.apply(DataLoadProcessorStepOnSpark.scala:210) at org.apache.spark.TaskContext$$anon$2.onTaskFailure(TaskContext.scala:144) at org.apache.spark.TaskContextImpl$$anonfun$markTaskFailed$1.apply(TaskContextImpl.scala:107) at org.apache.spark.TaskContextImpl$$anonfun$markTaskFailed$1.apply(TaskContextImpl.scala:107) at org.apache.spark.TaskContextImpl$$anonfun$invokeListeners$1.apply(TaskContextImpl.scala:130) at org.apache.spark.TaskContextImpl$$anonfun$invokeListeners$1.apply(TaskContextImpl.scala:128) at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59) at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48) at org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:128) at org.apache.spark.TaskContextImpl.markTaskFailed(TaskContextImpl.scala:106) at org.apache.spark.scheduler.Task.run(Task.scala:113) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748) Caused by: java.lang.ArrayIndexOutOfBoundsException: 5 at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:151) at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96) at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53) at org.apache.spark.scheduler.Task.run(Task.scala:108) ... 4 more
issueID:CARBONDATA-3273
type:Bug
changed files:
texts:For table without SORT_COLUMNS, Loading data is showing SORT_SCOPE=LOCAL_SORT instead of NO_SORT

issueID:CARBONDATA-3275
type:Bug
changed files:
texts:There are 4 errors in CI after PR 3094 merged
There are 4 errors in CI after PR 3094 merged
issueID:CARBONDATA-3276
type:Improvement
changed files:
texts:Compacting table that do not exist should throw NoSuchTableException instead of MalformedCarbonCommandException
Compacting table that do not exist should throw NoSuchTableException instead of MalformedCarbonCommandException("Operation not allowed : ALTER TABLE table_name COMPACT 'MAJOR'")it's confused
issueID:CARBONDATA-3278
type:Improvement
changed files:datamap/bloom/src/main/java/org/apache/carbondata/datamap/bloom/BloomCoarseGrainDataMap.java
texts:Remove duplicate code to get filter string of date/timestamp

issueID:CARBONDATA-328
type:Improvement
changed files:
texts:Improve Code and Fix Warnings
Remove compiler warning and improve the existing code according to the standards.
issueID:CARBONDATA-3280
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/util/CarbonProperties.java
texts:SDK batch read failed
SDK batch read failed
issueID:CARBONDATA-3281
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/cache/CarbonLRUCache.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
texts:Limit the LRU cache size
If configure the LRU bigger than jvm xmx size, then use CARBON_MAX_LRU_CACHE_SIZE_DEFAULT replace.because if setting LRU bigger than xmx size,if we query for a big table with many more carbonfiles, may cause "Error: java.io.IOException: Problem in loading segment blocks: GC overhead limit exceeded (state=,code=0)" the jdbc server will restart.
issueID:CARBONDATA-3282
type:Bug
changed files:integration/presto/src/main/java/org/apache/carbondata/presto/CarbondataSplitManager.java
texts:presto carbon doesn&#39;t work with Hadoop conf in cluster.
problem:when datamap path is given , presto carbon throws 'hacluster' unkown host exception even when hdfs configuration is present. solution: from HDFS environment, set the hadoop configuration to thread local, so that FileFactory can use this configuration.
issueID:CARBONDATA-3284
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
texts:Workaround for Create-PreAgg Datamap Fail
If for some reason^&#91;1&#93;^, creating PreAgg datamap failed and its dropping also failed.Then dropping datamap also cannot be done, as the datamap was not registered to the parent table schema file, but got registered in spark-hive, so it shows it as a table, but won't let us drop it as carbon throws error if we try to drop it as a table.Workaround:After this change, we can at lease drop that as a hive folder by command{{drop table table_datamap; }}&#91;1&#93; - Reason could be something like setting HDFS Quota on database folder, so that parent table schema file cound not be modified.
issueID:CARBONDATA-3287
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/datamap/dev/cgdatamap/CoarseGrainDataMap.java
core/src/main/java/org/apache/carbondata/core/util/BlockletDataMapUtil.java
core/src/main/java/org/apache/carbondata/core/datamap/DataMapStoreManager.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/CarbonTable.java
core/src/main/java/org/apache/carbondata/core/datamap/dev/DataMap.java
core/src/main/java/org/apache/carbondata/core/scan/model/QueryModel.java
core/src/main/java/org/apache/carbondata/core/scan/executor/impl/AbstractQueryExecutor.java
core/src/main/java/org/apache/carbondata/core/datamap/TableDataMap.java
core/src/main/java/org/apache/carbondata/core/datamap/dev/fgdatamap/FineGrainDataMap.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockDataMap.java
core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
texts:Remove the validation of  same chema data files in location for external table and file format
Currently we have a validation that if there are two carbondata files in a location with different schema, then we fail the query. I think there is no need to fail. If you see the parquet behavior also we cna understand.  Here i think failing is not good, we can read the latets schema from latest carbondata file in the given location and based on that read all the files and give query output. For the columns which are not present in some data files, it wil have null values for the new column. But here basically we do not merge schema. we can maintain the same now also, only thing is can take latest schma. for example:1. one data file with columns a,b and c. 2nd file is with columns a,b,c,d,e. then can read and create table with 5 columns or 3 columns which ever is latest and create table(This will be when user does not specify schema). If he species table will be created with specified schema.
issueID:CARBONDATA-329
type:Improvement
changed files:
texts:constant final class changed to interface
Constant file's is final class and it is now changed to interface. Implicitly fields are static and final.
issueID:CARBONDATA-3290
type:Improvement
changed files:
texts:No need to apply MV/Preaggregate rules for Command

issueID:CARBONDATA-3291
type:Bug
changed files:
texts:MV datamap doesn&#39;t take affect when the same table join

issueID:CARBONDATA-3293
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/datamap/dev/cgdatamap/CoarseGrainDataMap.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstantsInternal.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockletDataMapRowIndexes.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonTableInputFormat.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockletDataMap.java
core/src/main/java/org/apache/carbondata/core/indexstore/schema/SchemaGenerator.java
core/src/main/java/org/apache/carbondata/core/mutate/CarbonUpdateUtil.java
core/src/main/java/org/apache/carbondata/core/datamap/dev/DataMap.java
core/src/main/java/org/apache/carbondata/core/datamap/TableDataMap.java
core/src/main/java/org/apache/carbondata/core/datamap/dev/fgdatamap/FineGrainDataMap.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockDataMap.java
texts:Prune datamaps improvement for count(*)
Problem:(1) Currently for count ( *) , the prune is same as select * query.  Blocklet and ExtendedBlocklet are formed from the DataMapRow and that is of no need and it is a time consuming process.(2) Pruning in select * query consumes time in convertToSafeRow() - converting the DataMapRow to safe as in an unsafe row to get the position of data, we need to traverse through the whole row to reach a position.(3) In case of filter queries, even if the blocklet is valid or invalid, we are converting the DataMapRow to safeRow. This conversion is time consuming increasing the number of blocklets. Solution:(1) We have the blocklet row count in the DataMapRow itself, so it is just enough to read the count. With this count ( *) query performance can be improved.(2) Maintain the data length also to the DataMapRow, so that traversing the whole row can be avoided. With the length we can directly hit the data position.(3) Read only the MinMax from the DataMapRow, decide whether scan is required on that blocklet, if required only then it can be converted to safeRow, if needed.
issueID:CARBONDATA-3294
type:Bug
changed files:
texts:MV datamap throw error when using count(1) and case when expression
Query SQL ``` sql(s"""SELECT MT.`3600` AS `3600`, MT.`2250410101` AS `2250410101`, count(1) over() as countNum, (CASE WHEN (SUM(COALESCE(seq_c, 0))) = 0 THEN NULL ELSE (CASE WHEN (CAST((SUM(COALESCE(seq_c, 0))) AS int)) = 0 THEN 0 ELSE ((CAST((SUM(COALESCE(succ_c, 0))) AS double)) / (CAST((SUM(COALESCE(seq_c, 0))) AS double))) END) * 100 END) AS rate FROM ( SELECT sum_result.*, H_REGION.`2250410101` FROM (SELECT cast(floor((starttime + 28800) / 3600) * 3600 - 28800 as int) AS `3600`, LAYER4ID, COALESCE(SUM(seq), 0) AS seq_c, COALESCE(SUM(succ), 0) AS succ_c FROM data_table WHERE STARTTIME >= 1549866600 AND STARTTIME < 1549899900 GROUP BY cast(floor((STARTTIME + 28800) / 3600) * 3600 - 28800 as int),LAYER4ID )sum_result LEFT JOIN (SELECT l4id AS `225040101`, l4name AS `2250410101`, l4name AS NAME_2250410101 FROM region GROUP BY l4id, l4name) H_REGION ON sum_result.LAYER4ID = H_REGION.`225040101` WHERE H_REGION.NAME_2250410101 IS NOT NULL ) MT GROUP BY MT.`3600`, MT.`2250410101` ORDER BY `3600` ASC LIMIT 5000""".stripMargin) ``` ERROR:mismatched input 'FROM' expecting {<EOF>, 'WHERE', 'GROUP', 'ORDER', 'HAVING', 'LIMIT', 'LATERAL', 'WINDOW', 'UNION', 'EXCEPT', 'MINUS', 'INTERSECT', 'SORT', 'CLUSTER', 'DISTRIBUTE'}(line 2, pos 0)== SQL ==SELECT MT.`3600`, MT.`2250410101`, `countNum`, `rate` FROM^^^
issueID:CARBONDATA-3295
type:Bug
changed files:
texts:MV datamap throw exception because its rewrite algorithm when multiply subquery
error:java.lang.UnsupportedOperationException was thrown.java.lang.UnsupportedOperationException at org.apache.carbondata.mv.plans.util.SQLBuildDSL$Fragment.productArity(SQLBuildDSL.scala:36) at scala.runtime.ScalaRunTime$$anon$1.<init>(ScalaRunTime.scala:174) at scala.runtime.ScalaRunTime$.typedProductIterator(ScalaRunTime.scala:172) mv sql:sql(s"""create datamap data_table_mv using 'mv' as SELECT STARTTIME,LAYER4ID, COALESCE (SUM(seq),0) AS seq_c, COALESCE (SUM(succ),0) AS succ_c FROM data_table GROUP BY STARTTIME,LAYER4ID""".stripMargin) Query sql:sql(s"""SELECT MT.`3600` AS `3600`, MT.`2250410101` AS `2250410101`, (CASE WHEN (SUM(COALESCE(seq_c, 0))) = 0 THEN NULL ELSE (CASE WHEN (CAST((SUM(COALESCE(seq_c, 0))) AS int)) = 0 THEN 0 ELSE ((CAST((SUM(COALESCE(succ_c, 0))) AS double)) / (CAST((SUM(COALESCE(seq_c, 0))) AS double))) END) * 100 END) AS rate FROM ( SELECT sum_result.*, H_REGION.`2250410101` FROM (SELECT cast(floor((starttime + 28800) / 3600) * 3600 - 28800 as int) AS `3600`, LAYER4ID, COALESCE(SUM(seq), 0) AS seq_c, COALESCE(SUM(succ), 0) AS succ_c FROM data_table WHERE STARTTIME >= 1549866600 AND STARTTIME < 1549899900 GROUP BY cast(floor((STARTTIME + 28800) / 3600) * 3600 - 28800 as int),LAYER4ID )sum_result LEFT JOIN (SELECT l4id AS `225040101`, l4name AS `2250410101`, l4name AS NAME_2250410101 FROM region GROUP BY l4id, l4name) H_REGION ON sum_result.LAYER4ID = H_REGION.`225040101` WHERE H_REGION.NAME_2250410101 IS NOT NULL ) MT GROUP BY MT.`3600`, MT.`2250410101` ORDER BY `3600` ASC LIMIT 5000""".stripMargin)
issueID:CARBONDATA-3297
type:Bug
changed files:
texts:Throw IndexOutOfBoundsException when creating table and drop table at the same time
java.lang.IndexOutOfBoundsException: 179        at scala.collection.mutable.ResizableArray$class.apply(ResizableArray.scala:43)        at scala.collection.mutable.ArrayBuffer.apply(ArrayBuffer.scala:48)        at scala.collection.IndexedSeqOptimized$class.segmentLength(IndexedSeqOptimized.scala:195)        at scala.collection.mutable.ArrayBuffer.segmentLength(ArrayBuffer.scala:48)        at scala.collection.GenSeqLike$class.prefixLength(GenSeqLike.scala:93)        at scala.collection.AbstractSeq.prefixLength(Seq.scala:41)        at scala.collection.IndexedSeqOptimized$class.find(IndexedSeqOptimized.scala:50)        at scala.collection.mutable.ArrayBuffer.find(ArrayBuffer.scala:48)        at org.apache.spark.sql.hive.CarbonFileMetastore.getTableFromMetadataCache(CarbonFileMetastore.scala:203)        at org.apache.spark.sql.CarbonEnv$.getCarbonTable(CarbonEnv.scala:203)        at org.apache.spark.sql.CarbonEnv$.getTablePath(CarbonEnv.scala:288)        at org.apache.spark.sql.execution.command.table.CarbonCreateTableCommand$$anonfun$1.apply(CarbonCreateTableCommand.scala:74)
issueID:CARBONDATA-3298
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/util/DeleteLoadFolders.java
texts:Logs are getting printed when clean files is executed for old stores
 Please run the below queries to reproduce the issue.In old store:create table log(a int, b string) stored by 'carbondata';insert into log values(1, 'abc');update table log set (a) =(2) where b = 'abc';delete from log where b = 'abc';clean files for table log;In new store , refresh and perform insert & delete operation and execute clean files command.In this case, logs are getting printed for already deleted segments as below"Files are not found in segment "location" it seems, files are already being deleted" 
issueID:CARBONDATA-3299
type:Bug
changed files:
texts:Updated value is not being reflected in the Desc formatted Command.
After changing the carbon properties related to Compaction, changed value is not being reflected in the desc formatted command.
issueID:CARBONDATA-33
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/util/path/CarbonTablePath.java
texts:Test cases fails when storePath contains &#39;-&#39; like &#39;incubator-carbon/store&#39;
If storePath contains '-' like "incubator-carbon/store" , testcases fails and also potential bug to fail the queries.
issueID:CARBONDATA-330
type:Improvement
changed files:
texts:Fix compiler warnings - Java related
Fix java compiler warnings and code cleanup.
issueID:CARBONDATA-3300
type:New Feature
changed files:core/src/main/java/org/apache/carbondata/core/metadata/blocklet/BlockletInfo.java
core/src/main/java/org/apache/carbondata/core/util/ObjectSerializationUtil.java
core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
texts:ClassNotFoundException when using UDF on spark-shell
create table x1 (imei string, deviceInformationId int, mac string, productdate timestamp, updatetime timestamp, gamePointId double, contractNumber double) STORED BY 'org.apache.carbondata.format';Load the data to x1:LOAD DATA inpath 'hdfs://localhost/x1_without_header.csv' into table x1 options('DELIMITER'=',', 'QUOTECHAR'='"','FILEHEADER'='imei, deviceinformationid,mac, productdate,updatetime, gamepointid,contractnumber');Create another table res_1 using following sql:create table res_1 as select * from x1 limit 2;2. Login spark-shell, register udf and run the join queryimport java.sql.Dateimport java.sql.Timestamp;spark.udf.register("castTimestampToDate", (x: Timestamp) =>  try {    Some(new Date(x.getTime - x.toLocalDateTime.getHour * 3600 * 1000L - x.toLocalDateTime.getMinute * 60 * 1000L - x.toLocalDateTime.getSecond * 1000L))  } catch {    case _: Exception => None  })spark.sql("select res_1.* from x1, res_1 where castTimestampToDate(x1.productdate) = castTimestampToDate(res_1.productdate) and x1.deviceInformationId = res_1.deviceInformationId").show(false) java.lang.RuntimeException: Error while reading filter expression  at org.apache.carbondata.hadoop.api.CarbonInputFormat.getFilterPredicates(CarbonInputFormat.java:392)  at org.apache.carbondata.hadoop.api.CarbonTableInputFormat.getSplits(CarbonTableInputFormat.java:204)  at org.apache.carbondata.spark.rdd.CarbonScanRDD.internalGetPartitions(CarbonScanRDD.scala:139)  at org.apache.carbondata.spark.rdd.CarbonRDD.getPartitions(CarbonRDD.scala:66)  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)  at scala.Option.getOrElse(Option.scala:121)  at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)  at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:46)  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)  at scala.Option.getOrElse(Option.scala:121)  at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)  at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:46)  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)  at scala.Option.getOrElse(Option.scala:121)  at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)  at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:46)  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)  at scala.Option.getOrElse(Option.scala:121)  at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)  at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:340)  at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)  at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3278)  at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2489)  at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2489)  at org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3259)  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)  at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3258)  at org.apache.spark.sql.Dataset.head(Dataset.scala:2489)  at org.apache.spark.sql.Dataset.take(Dataset.scala:2703)  at org.apache.spark.sql.Dataset.showString(Dataset.scala:254)  at org.apache.spark.sql.Dataset.show(Dataset.scala:725)  at org.apache.spark.sql.Dataset.show(Dataset.scala:702)  ... 49 elidedCaused by: java.io.IOException: Could not read object  at org.apache.carbondata.core.util.ObjectSerializationUtil.convertStringToObject(ObjectSerializationUtil.java:100)  at org.apache.carbondata.hadoop.api.CarbonInputFormat.getFilterPredicates(CarbonInputFormat.java:389)  ... 84 moreCaused by: java.lang.ClassNotFoundException: $anonfun$1  at java.net.URLClassLoader.findClass(URLClassLoader.java:381)  at java.lang.ClassLoader.loadClass(ClassLoader.java:424)  at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:338)  at java.lang.ClassLoader.loadClass(ClassLoader.java:357)  at java.lang.Class.forName0(Native Method)  at java.lang.Class.forName(Class.java:348)  at java.io.ObjectInputStream.resolveClass(ObjectInputStream.java:682)  at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1859)  at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1745)  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2033)  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1567)  at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2278)  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2202)  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2060)  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1567)  at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2278)  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2202)  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2060)  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1567)  at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2278)  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2202)  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2060)  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1567)  at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2278)  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2202)  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2060)  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1567)  at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2278)  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2202)  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2060)  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1567)  at java.io.ObjectInputStream.readObject(ObjectInputStream.java:427)  at java.util.ArrayList.readObject(ArrayList.java:797)  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)  at java.lang.reflect.Method.invoke(Method.java:498)  at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1158)  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2060)  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1567)  at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2278)  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2202)  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2060)  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1567)  at java.io.ObjectInputStream.readObject(ObjectInputStream.java:427)  at java.util.ArrayList.readObject(ArrayList.java:797)  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)  at java.lang.reflect.Method.invoke(Method.java:498)  at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1158)  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2060)  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1567)  at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2278)  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2202)  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2060)  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1567)  at java.io.ObjectInputStream.readObject(ObjectInputStream.java:427)  at org.apache.carbondata.core.util.ObjectSerializationUtil.convertStringToObject(ObjectSerializationUtil.java:98)  ... 85 more
issueID:CARBONDATA-3301
type:Bug
changed files:processing/src/main/java/org/apache/carbondata/processing/datatypes/PrimitiveDataType.java
texts:Array<date> column is giving null data in case of spark carbon file format
Steps to reproduce  drop table if exists issue create table issue(name string, dob array<date>) using carbon insert into issue select 'abc',array('2017-11-11') select * from issue"output is --------------+namedob --------------+abc []--------------+ But parquet gives correct output--------------+namedob --------------+abc &#91;2017-11-11&#93;--------------+
issueID:CARBONDATA-3302
type:Improvement
changed files:
texts:code cleaning related to CarbonCreateTable command
Extra check to validate whether the stream relation is not null , moreover condition can be optimized further, currently the condition has path validation whether path is part of s3 file system and then system is  checking whether the stream relation is not null, thi check can be first as this overall condition has to be evaluated for stream table
issueID:CARBONDATA-3303
type:Bug
changed files:
texts:MV datamap return wrong results when using coalesce and less groupby columns
SQL:create table coalesce_test_main(id int,name string,height int,weight int using carbondatainsert into coalesce_test_main select 1,'tom',170,130insert into coalesce_test_main select 2,'tom',170,120insert into coalesce_test_main select 3,'lily',160,100create datamap coalesce_test_main_mv using 'mv' as select coalesce(sum(id),0) as sum_id,name as myname,weight from coalesce_test_main group by name,weightselect coalesce(sum(id),0) as sumid,name from coalesce_test_main group by nameResult:1 tom2 tom3 lily
issueID:CARBONDATA-3305
type:New Feature
changed files:core/src/main/java/org/apache/carbondata/core/cache/CacheProvider.java
core/src/main/java/org/apache/carbondata/core/cache/CarbonLRUCache.java
texts:DDLs to Operate on CarbonLRUCache
New DDLs SHOW METACACHE SHOW METACACHE FOR TABLE tableName DROP METACACHE FOR TABLE tableName
issueID:CARBONDATA-3306
type:Sub-task
changed files:core/src/main/java/org/apache/carbondata/core/util/BlockletDataMapUtil.java
core/src/main/java/org/apache/carbondata/core/datamap/DataMapJob.java
core/src/main/java/org/apache/carbondata/core/datamap/DataMapStoreManager.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockletDataMapFactory.java
core/src/main/java/org/apache/carbondata/core/indexstore/PartitionSpec.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/CarbonTable.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonInputFormat.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockletDataMapDistributable.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonFileInputFormat.java
core/src/main/java/org/apache/carbondata/core/util/CarbonProperties.java
core/src/main/java/org/apache/carbondata/core/datamap/DataMapUtil.java
core/src/main/java/org/apache/carbondata/core/datamap/Segment.java
core/src/main/java/org/apache/carbondata/core/datamap/DistributableDataMapFormat.java
integration/spark2/src/main/java/org/apache/carbondata/datamap/IndexDataMapProvider.java
datamap/bloom/src/main/java/org/apache/carbondata/datamap/bloom/BloomCoarseGrainDataMapFactory.java
core/src/main/java/org/apache/carbondata/core/metadata/SegmentFileStore.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonTableInputFormat.java
core/src/main/java/org/apache/carbondata/core/indexstore/TableBlockIndexUniqueIdentifierWrapper.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
hadoop/src/main/java/org/apache/carbondata/hadoop/util/CarbonInputFormatUtil.java
core/src/main/java/org/apache/carbondata/core/indexstore/BlockletDetailsFetcher.java
core/src/main/java/org/apache/carbondata/core/cache/CarbonLRUCache.java
core/src/main/java/org/apache/carbondata/core/util/SessionParams.java
core/src/main/java/org/apache/carbondata/hadoop/CarbonInputSplit.java
core/src/main/java/org/apache/carbondata/core/datamap/dev/DataMapFactory.java
core/src/main/java/org/apache/carbondata/core/indexstore/Blocklet.java
core/src/main/java/org/apache/carbondata/core/datamap/TableDataMap.java
datamap/lucene/src/main/java/org/apache/carbondata/datamap/lucene/LuceneDataMapFactoryBase.java
core/src/main/java/org/apache/carbondata/core/datamap/DataMapChooser.java
core/src/main/java/org/apache/carbondata/core/indexstore/ExtendedBlocklet.java
texts:Implement a DistributableIndexPruneRDD and IndexPruneFileFormat
○ The RDD will accept list of segments and the filter expression for pruning.○ The pruning job is handled by the executors in a distributed way for the allocated segments.○ The splits are equally divided based on the size on the index files.
issueID:CARBONDATA-3307
type:Bug
changed files:processing/src/main/java/org/apache/carbondata/processing/loading/steps/CarbonRowDataWriterProcessorStepImpl.java
texts:when we creating the table without sort_columns and loading the data into it. Its generating more carbondata files than expected.

issueID:CARBONDATA-3309
type:Improvement
changed files:
texts:MV datamap adapt to spark 2.1 version

issueID:CARBONDATA-331
type:New Feature
changed files:
texts:Support no compression option while loading
Modify the compressor inteface and add a DummyCompressor for not doing compression.This interface can be extend later for adding new compressors.
issueID:CARBONDATA-3313
type:Bug
changed files:hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonTableInputFormat.java
texts:count(*) is not invalidating the invalid segments cache

issueID:CARBONDATA-3314
type:Bug
changed files:
texts:Index Cache Size printed in SHOW METACACHE ON TABLE DDL is not accurate
Index Cache Size printed in SHOW METACACHE ON TABLE DDL is not accurate.Ex : If the index cache size is 1 .99 MB , output is printed as 1 MB.
issueID:CARBONDATA-3315
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/scan/expression/RangeExpressionEvaluator.java
texts:Range Filter query with two between clauses with an OR gives wrong results
Create table t1(c1 string, c2 int) stored by 'carbondata' tblproperties('sort_columns'='c2') insert some values into table t1 select * from t1 where c2 between 2 and 3 or c2 between 3 and 4
issueID:CARBONDATA-3317
type:Bug
changed files:
texts:Executing &#39;show segments&#39; command throws NPE when spark streaming app write data to new stream segment.
When spark streaming app starts to create new stream segment, it does not create carbondataindex file before writing data successfully, and now if execute 'show segments' command, it will throw NPE.
issueID:CARBONDATA-3318
type:Improvement
changed files:datamap/bloom/src/main/java/org/apache/carbondata/datamap/bloom/BloomCacheKeyValue.java
datamap/bloom/src/main/java/org/apache/carbondata/datamap/bloom/BloomCoarseGrainDataMapFactory.java
texts:Decoupling of Cache Commands
Decoupling of CarbonDropCacheCommand and CarbonShowCacheCommands for Bloom filters and Pre-Aggregate tables.
issueID:CARBONDATA-3320
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/metadata/SegmentFileStore.java
texts:number of partitions are always zero in describe formatted for hive native partition
number of partitions are always zero create table desc(name string) partitioned by (num int) stored by 'carbondata'insert into desc select 'abc',3insert into desc select 'abc',5describe formatted desc
issueID:CARBONDATA-3321
type:Improvement
changed files:hadoop/src/main/java/org/apache/carbondata/hadoop/CarbonMultiBlockSplit.java
core/src/main/java/org/apache/carbondata/core/datamap/DataMapStoreManager.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockletDataMapFactory.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockletDataMap.java
core/src/main/java/org/apache/carbondata/core/indexstore/UnsafeMemoryDMStore.java
core/src/main/java/org/apache/carbondata/core/indexstore/TableBlockIndexUniqueIdentifier.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonInputFormat.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonFileInputFormat.java
integration/spark-common/src/main/java/org/apache/carbondata/spark/util/Util.java
integration/presto/src/main/java/org/apache/carbondata/presto/impl/CarbonLocalInputSplit.java
core/src/main/java/org/apache/carbondata/core/datamap/Segment.java
core/src/main/java/org/apache/carbondata/core/datamap/DistributableDataMapFormat.java
core/src/main/java/org/apache/carbondata/core/datastore/impl/FileFactory.java
core/src/main/java/org/apache/carbondata/core/indexstore/SegmentPropertiesFetcher.java
core/src/main/java/org/apache/carbondata/hadoop/internal/index/Block.java
hadoop/src/main/java/org/apache/carbondata/hadoop/util/CarbonVectorizedRecordReader.java
core/src/main/java/org/apache/carbondata/core/indexstore/schema/CarbonRowSchema.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonTableInputFormat.java
core/src/main/java/org/apache/carbondata/core/indexstore/row/DataMapRow.java
core/src/main/java/org/apache/carbondata/core/indexstore/schema/SchemaGenerator.java
core/src/main/java/org/apache/carbondata/core/scan/model/QueryModel.java
core/src/main/java/org/apache/carbondata/core/indexstore/row/UnsafeDataMapRow.java
core/src/main/java/org/apache/carbondata/hadoop/CarbonInputSplit.java
hadoop/src/main/java/org/apache/carbondata/hadoop/CarbonRecordReader.java
core/src/main/java/org/apache/carbondata/core/datamap/TableDataMap.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockDataMap.java
core/src/main/java/org/apache/carbondata/core/indexstore/ExtendedBlocklet.java
core/src/main/java/org/apache/carbondata/hadoop/internal/ObjectArrayWritable.java
texts:Improve Single/Concurrent query performance
When number of Segments are high Single/Concurrent query is slow. because of below reason  Memory footprint is more because of this gc is more and reducing query performance Converting to Unsafe data map row to safe data map during pruning  Multi threaded pruning in case of non filter query is not supported  Retrieval from unsafe data map row is slower 
issueID:CARBONDATA-3322
type:Bug
changed files:
texts:After renaming table, "SHOW METACACHE ON TABLE" still works for old table
Problem:After we alter table name from t1 to t2, "SHOW METACACHE ON TABLE" works for both old table name "t1" and new table name "t2" Fix:Added check for table.
issueID:CARBONDATA-3323
type:Bug
changed files:
texts:Output is null when cache is empty
Problem:When "SHOW METACACHE ON TABLE" is executed and carbonLRUCAche is null, output is empty sequence, which is not standard. Fix:Return standard output even when carbonLRUCache is not initalised (null) with size for index and dictionary as 0. 
issueID:CARBONDATA-3324
type:Bug
changed files:
texts:The parameter CarbonCommonConstants.CARBON_TIMESTAMP_FORMAT and CarbonCommonConstants.CARBON_DATE_FORMAT don&#39;t have effect in streaming table writing
When I write data into streaming table. I use such code:CarbonProperties.getInstance() .addProperty(CarbonCommonConstants.CARBON_TIMESTAMP_FORMAT, "yyyy/MM/dd HH:mm:ss") .addProperty(CarbonCommonConstants.CARBON_DATE_FORMAT, "yyyy/MM/dd") but don't have effect
issueID:CARBONDATA-3328
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/indexstore/ExtendedBlocklet.java
core/src/main/java/org/apache/carbondata/hadoop/CarbonInputSplit.java
hadoop/src/main/java/org/apache/carbondata/hadoop/CarbonMultiBlockSplit.java
texts:Performance issue with merge small files distribution
After PR#3154 in case of merge small files split length was coming 0 because of this it was merging all the files because of this query with merge small files was slow
issueID:CARBONDATA-3329
type:New Feature
changed files:
texts:DeadLock is observed when a query fails.
"HiveServer2-Handler-Pool: Thread-303" #303 prio=5 os_prio=0 tid=0x00007fcfe129f800 nid=0x59eb9 waiting for monitor entry &#91;0x00007fcfd3c42000&#93; java.lang.Thread.State: BLOCKED (on object monitor) at org.apache.log4j.Category.callAppenders(Category.java:204) waiting to lock <0x00007fd046f9ed60> (a org.apache.log4j.spi.RootLogger) at org.apache.log4j.Category.forcedLog(Category.java:391) at org.apache.log4j.Category.log(Category.java:856) at org.slf4j.impl.Log4jLoggerAdapter.log(Log4jLoggerAdapter.java:581) at org.apache.commons.logging.impl.SLF4JLocationAwareLog.info(SLF4JLocationAwareLog.java:155) at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.close(HiveMetaStoreClient.java:622) at sun.reflect.GeneratedMethodAccessor33.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:156) at com.sun.proxy.$Proxy28.close(Unknown Source) at sun.reflect.GeneratedMethodAccessor33.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.hadoop.hive.metastore.HiveMetaStoreClient$SynchronizedHandler.invoke(HiveMetaStoreClient.java:2107) locked <0x00007fd05611ef38> (a org.apache.hadoop.hive.metastore.HiveMetaStoreClient$SynchronizedHandler) at com.sun.proxy.$Proxy28.close(Unknown Source) at org.apache.hadoop.hive.ql.metadata.Hive.close(Hive.java:294) at org.apache.hadoop.hive.ql.metadata.Hive.access$000(Hive.java:141) at org.apache.hadoop.hive.ql.metadata.Hive$1.remove(Hive.java:161) locked <0x00007fd051ba0bb0> (a org.apache.hadoop.hive.ql.metadata.Hive$1) at org.apache.hadoop.hive.ql.metadata.Hive.closeCurrent(Hive.java:264) at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$withHiveState$1.apply(HiveClientImpl.scala:294) at org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:246) at org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:245) locked <0x00007fd04cbc4c78> (a org.apache.spark.sql.hive.client.hiveClientObject) at org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:292) at org.apache.spark.sql.hive.client.HiveClientImpl.databaseExists(HiveClientImpl.scala:388) at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply$mcZ$sp(HiveExternalCatalog.scala:178) at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:178) at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:178) at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97) locked <0x00007fd04ce5ff48> (a org.apache.spark.sql.hive.HiveExternalCatalog) at org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:177) at org.apache.spark.sql.catalyst.catalog.SessionCatalog.databaseExists(SessionCatalog.scala:198) at org.apache.spark.sql.catalyst.catalog.SessionCatalog.org$apache$spark$sql$catalyst$catalog$SessionCatalog$$requireDbExists(SessionCatalog.scala:138) at org.apache.spark.sql.catalyst.catalog.SessionCatalog.getDatabaseMetadata(SessionCatalog.scala:192) at org.apache.spark.sql.getDB$.getDBLocation(CarbonCatalystOperators.scala:107) at org.apache.spark.sql.hive.CarbonMetastore$$anonfun$loadMetadata$1.apply(CarbonMetastore.scala:253) at org.apache.spark.sql.hive.CarbonMetastore$$anonfun$loadMetadata$1.apply(CarbonMetastore.scala:251) at scala.collection.Iterator$class.foreach(Iterator.scala:893) at scala.collection.AbstractIterator.foreach(Iterator.scala:1336) at scala.collection.IterableLike$class.foreach(IterableLike.scala:72) at scala.collection.AbstractIterable.foreach(Iterable.scala:54) at org.apache.spark.sql.hive.CarbonMetastore.loadMetadata(CarbonMetastore.scala:251) at org.apache.spark.sql.hive.CarbonMetastore.<init>(CarbonMetastore.scala:134) at org.apache.spark.sql.CarbonEnv.init(CarbonEnv.scala:83) at org.apache.spark.sql.hive.CarbonSessionCatalog.carbonEnv$lzycompute(CarbonSessionState.scala:72) locked <0x00007fd058434528> (a org.apache.spark.sql.hive.CarbonSessionCatalog) at org.apache.spark.sql.hive.CarbonSessionCatalog.carbonEnv(CarbonSessionState.scala:70) at org.apache.spark.sql.CarbonEnv$.getInstance(CarbonEnv.scala:123) at org.apache.spark.sql.CarbonSession$.updateSessionInfoToCurrentThread(CarbonSession.scala:192) at org.apache.spark.sql.parser.CarbonSparkSqlParser.parsePlan(CarbonSparkSqlParser.scala:46) at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:600) at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:699) at org.apache.spark.sql.hive.thriftserver.SparkSQLSessionManager.openSession(SparkSQLSessionManager.scala:100) at org.apache.hive.service.cli.CLIService.openSession(CLIService.java:194) at org.apache.hive.service.cli.thrift.ThriftCLIService.getSessionHandle(ThriftCLIService.java:652) at org.apache.hive.service.cli.thrift.ThriftCLIService.OpenSession(ThriftCLIService.java:473) at org.apache.hive.service.cli.thrift.TCLIService$Processor$OpenSession.getResult(TCLIService.java:1253) at org.apache.hive.service.cli.thrift.TCLIService$Processor$OpenSession.getResult(TCLIService.java:1238) at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39) at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39) at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor.process(HadoopThriftAuthBridge.java:690) at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:286) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748)"pool-26-thread-3" #302 prio=5 os_prio=0 tid=0x00007fd03b231800 nid=0x59dc8 waiting for monitor entry &#91;0x00007fcfd282d000&#93; java.lang.Thread.State: BLOCKED (on object monitor) at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97) waiting to lock <0x00007fd04ce5ff48> (a org.apache.spark.sql.hive.HiveExternalCatalog) at org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:177) at org.apache.spark.sql.catalyst.catalog.SessionCatalog.databaseExists(SessionCatalog.scala:198) at org.apache.spark.sql.catalyst.catalog.SessionCatalog.org$apache$spark$sql$catalyst$catalog$SessionCatalog$$requireDbExists(SessionCatalog.scala:138) at org.apache.spark.sql.catalyst.catalog.SessionCatalog.getTableMetadata(SessionCatalog.scala:312) at org.apache.spark.sql.hive.CarbonMetastore.indexInfoFromHive(CarbonMetastore.scala:363) at org.apache.spark.sql.hive.CarbonMetastore.refreshIndexInfo(CarbonMetastore.scala:303) at org.apache.spark.sql.hive.CarbonMetastore.lookupRelation(CarbonMetastore.scala:162) at org.apache.spark.sql.hive.CarbonMetastore.lookupRelation(CarbonMetastore.scala:147) at org.apache.spark.sql.CarbonDatasourceHadoopRelation.carbonRelation$lzycompute(CarbonDatasourceHadoopRelation.scala:65) locked <0x00007fd056400268> (a org.apache.spark.sql.CarbonDatasourceHadoopRelation) at org.apache.spark.sql.CarbonDatasourceHadoopRelation.carbonRelation(CarbonDatasourceHadoopRelation.scala:63) at org.apache.spark.sql.CarbonDatasourceHadoopRelation.carbonTable$lzycompute(CarbonDatasourceHadoopRelation.scala:62) locked <0x00007fd056400268> (a org.apache.spark.sql.CarbonDatasourceHadoopRelation) at org.apache.spark.sql.CarbonDatasourceHadoopRelation.carbonTable(CarbonDatasourceHadoopRelation.scala:62) at org.apache.spark.sql.CarbonDatasourceHadoopRelation.toString(CarbonDatasourceHadoopRelation.scala:88) at java.lang.String.valueOf(String.java:2994) at java.lang.StringBuilder.append(StringBuilder.java:131) at scala.StringContext.standardInterpolator(StringContext.scala:125) at scala.StringContext.s(StringContext.scala:95) at org.apache.spark.sql.execution.datasources.LogicalRelation.simpleString(LogicalRelation.scala:106) at org.apache.spark.sql.catalyst.plans.QueryPlan.verboseString(QueryPlan.scala:337) at org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:552) at org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:572) at org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:572) at org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:572) at org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:572) at org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:572) at org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:572) at org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:572) at org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:572) at org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:572) at org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:572) at org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:572) at org.apache.spark.sql.catalyst.trees.TreeNode.treeString(TreeNode.scala:476) at org.apache.spark.sql.catalyst.trees.TreeNode.treeString(TreeNode.scala:473) at org.apache.spark.sql.catalyst.trees.TreeNode.toString(TreeNode.scala:470) at java.lang.String.valueOf(String.java:2994) at java.lang.StringBuilder.append(StringBuilder.java:131) at scala.StringContext.standardInterpolator(StringContext.scala:125) at scala.StringContext.s(StringContext.scala:95) at org.apache.spark.sql.AnalysisException$$anonfun$1.apply(AnalysisException.scala:46) at org.apache.spark.sql.AnalysisException$$anonfun$1.apply(AnalysisException.scala:46) at scala.Option.map(Option.scala:146) at org.apache.spark.sql.AnalysisException.getMessage(AnalysisException.scala:46) at java.lang.Throwable.getLocalizedMessage(Throwable.java:391) at java.lang.Throwable.toString(Throwable.java:480) at java.lang.String.valueOf(String.java:2994) at java.io.PrintWriter.println(PrintWriter.java:754) at java.lang.Throwable$WrappedPrintWriter.println(Throwable.java:764) at java.lang.Throwable.printStackTrace(Throwable.java:655) locked <0x00007fd0582f3a50> (a java.io.PrintWriter) at java.lang.Throwable.printStackTrace(Throwable.java:721) at org.apache.log4j.DefaultThrowableRenderer.render(DefaultThrowableRenderer.java:60) at org.apache.log4j.spi.ThrowableInformation.getThrowableStrRep(ThrowableInformation.java:87) locked <0x00007fd0582f3af8> (a org.apache.log4j.spi.ThrowableInformation) at org.apache.log4j.spi.LoggingEvent.getThrowableStrRep(LoggingEvent.java:413) at org.apache.log4j.WriterAppender.subAppend(WriterAppender.java:313) at org.apache.log4j.RollingFileAppender.subAppend(RollingFileAppender.java:276) at org.apache.log4j.WriterAppender.append(WriterAppender.java:162) at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251) locked <0x00007fd046feecb0> (a org.apache.log4j.RollingFileAppender) at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66) at org.apache.log4j.Category.callAppenders(Category.java:206) locked <0x00007fd046f9ed60> (a org.apache.log4j.spi.RootLogger) at org.apache.log4j.Category.forcedLog(Category.java:391) at org.apache.log4j.Category.log(Category.java:856) at org.slf4j.impl.Log4jLoggerAdapter.error(Log4jLoggerAdapter.java:576) at org.apache.spark.internal.Logging$class.logError(Logging.scala:91) at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.logError(SparkExecuteStatementOperation.scala:51) at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:324) at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1$$anon$2.run(SparkExecuteStatementOperation.scala:183) at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1$$anon$2.run(SparkExecuteStatementOperation.scala:180) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1.run(SparkExecuteStatementOperation.scala:193) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748)
issueID:CARBONDATA-333
type:Bug
changed files:
texts:Unable to perform compaction
Hello,There is some issue with compaction. Auto and force compaction are not working.In spark logs I got this error:Copying /tmp/5609139520219/0/database/mytable/Fact/Part0/Segment_0.1/0/part-0-0-1476956184000.carbondata --> hdfs://localhost:54310/user/hive/warehouse/carbon.store/database/mytable/Fact/Part0/Segment_0.1ERROR 20-10 15:06:26,009 - &#91;Executor task launch worker-0&#93;&#91;partitionID:mytable;queryID:5609201850014&#93; Exception while closing the handler in compaction merger Problem while copying file from local store to carbon store
issueID:CARBONDATA-3330
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/datamap/DataMapStoreManager.java
texts:Fix Invalid exception when SDK reader is trying to clear the datamap
java.io.IOException: File does not exist: /opt/csdk/out/cmplx_Schema/Metadata/schema at org.apache.carbondata.core.metadata.schema.SchemaReader.readCarbonTableFromStore(SchemaReader.java:60) at org.apache.carbondata.core.metadata.schema.table.CarbonTable.buildFromTablePath(CarbonTable.java:302) at org.apache.carbondata.core.datamap.DataMapStoreManager.getCarbonTable(DataMapStoreManager.java:512) at org.apache.carbondata.core.datamap.DataMapStoreManager.clearDataMaps(DataMapStoreManager.java:476) at org.apache.carbondata.hadoop.CarbonRecordReader.close(CarbonRecordReader.java:164) at org.apache.carbondata.sdk.file.CarbonReader.close(CarbonReader.java:219)
issueID:CARBONDATA-3331
type:Bug
changed files:
texts:Database index size is more than overall index size in SHOW METADATA command

issueID:CARBONDATA-3332
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/locks/LockUsage.java
texts:Concurrent update and compaction failure
Job aborted due to stage failure: Task 0 in stage 114.0 failed 4 times, most recent failure: Lost task 0.3 in stage 114.0 (TID 257, linux-53, executor 19): java.util.NoSuchElementException: key not found: 0at scala.collection.MapLike$class.default(MapLike.scala:228)at scala.collection.AbstractMap.default(Map.scala:59)at scala.collection.MapLike$class.apply(MapLike.scala:141)at scala.collection.AbstractMap.apply(Map.scala:59)at org.apache.carbondata.spark.rdd.CarbonDataRDDFactory$SegmentPartitioner$1.getPartition(CarbonDataRDDFactory.scala:709)at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)at org.apache.spark.scheduler.Task.run(Task.scala:99)at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:325)at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)at java.lang.Thread.run(Thread.java:748)
issueID:CARBONDATA-3333
type:Bug
changed files:processing/src/main/java/org/apache/carbondata/processing/loading/steps/CarbonRowDataWriterProcessorStepImpl.java
core/src/main/java/org/apache/carbondata/core/datastore/TableSpec.java
processing/src/main/java/org/apache/carbondata/processing/store/TablePage.java
texts:Fixed No Sort Store Size issue and Compatibility issue after alter addd column done in 1.1 and load in 1.5
Issue 1: Load is failing in latest version with alter in older versionIssue 2: After PR#3140 store size got increased  
issueID:CARBONDATA-3334
type:Bug
changed files:
texts:multiple segment files created for partition table for one segment
unnecessary file 0.tmp.  also  created along with duplicate segment files (and 0_1553762464817.segment,0_1553762469148.segment)   create table f (name string) partitioned by (b int) stored by 'carbondata' ;insert into f select 'a',10;check hdfs store location hadoop fs -ls /user/hive/warehouse/carbon.store/carbon1_1_test/f/Metadata/segments/Found 3 itemsrw-rw-r-+ 3 carbon hive 163 2019-03-28 14:11 /user/hive/warehouse/carbon.store/carbon1_1_test/f/Metadata/segments/0_1553762464817.segmentrw-rw-r-+ 3 carbon hive 158 2019-03-28 14:11 /user/hive/warehouse/carbon.store/carbon1_1_test/f/Metadata/segments/0_1553762469148.segmentdrwxrwx---+ - carbon hive 0 2019-03-28 14:11 /user/hive/warehouse/carbon.store/carbon1_1_test/f/Metadata/segments/_0.tmp 
issueID:CARBONDATA-3337
type:Sub-task
changed files:core/src/main/java/org/apache/carbondata/core/util/BlockletDataMapUtil.java
core/src/main/java/org/apache/carbondata/core/datamap/DataMapJob.java
core/src/main/java/org/apache/carbondata/core/datamap/DataMapStoreManager.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockletDataMapFactory.java
core/src/main/java/org/apache/carbondata/core/indexstore/PartitionSpec.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/CarbonTable.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonInputFormat.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockletDataMapDistributable.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonFileInputFormat.java
core/src/main/java/org/apache/carbondata/core/util/CarbonProperties.java
core/src/main/java/org/apache/carbondata/core/datamap/DataMapUtil.java
core/src/main/java/org/apache/carbondata/core/datamap/Segment.java
core/src/main/java/org/apache/carbondata/core/datamap/DistributableDataMapFormat.java
integration/spark2/src/main/java/org/apache/carbondata/datamap/IndexDataMapProvider.java
datamap/bloom/src/main/java/org/apache/carbondata/datamap/bloom/BloomCoarseGrainDataMapFactory.java
core/src/main/java/org/apache/carbondata/core/metadata/SegmentFileStore.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonTableInputFormat.java
core/src/main/java/org/apache/carbondata/core/indexstore/TableBlockIndexUniqueIdentifierWrapper.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
hadoop/src/main/java/org/apache/carbondata/hadoop/util/CarbonInputFormatUtil.java
core/src/main/java/org/apache/carbondata/core/indexstore/BlockletDetailsFetcher.java
core/src/main/java/org/apache/carbondata/core/cache/CarbonLRUCache.java
core/src/main/java/org/apache/carbondata/core/util/SessionParams.java
core/src/main/java/org/apache/carbondata/hadoop/CarbonInputSplit.java
core/src/main/java/org/apache/carbondata/core/datamap/dev/DataMapFactory.java
core/src/main/java/org/apache/carbondata/core/indexstore/Blocklet.java
core/src/main/java/org/apache/carbondata/core/datamap/TableDataMap.java
datamap/lucene/src/main/java/org/apache/carbondata/datamap/lucene/LuceneDataMapFactoryBase.java
core/src/main/java/org/apache/carbondata/core/datamap/DataMapChooser.java
core/src/main/java/org/apache/carbondata/core/indexstore/ExtendedBlocklet.java
texts:Implement a Hadoop RPC framwork for communication

issueID:CARBONDATA-3338
type:Sub-task
changed files:integration/spark2/src/main/java/org/apache/carbondata/datamap/IndexDataMapProvider.java
processing/src/main/java/org/apache/carbondata/processing/merger/CarbonDataMergerUtil.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/CarbonTable.java
core/src/main/java/org/apache/carbondata/core/datamap/dev/DataMapSyncStatus.java
core/src/main/java/org/apache/carbondata/core/datamap/status/DataMapSegmentStatusUtil.java
core/src/main/java/org/apache/carbondata/core/datamap/status/DataMapStatusManager.java
core/src/main/java/org/apache/carbondata/core/datamap/status/DiskBasedDataMapStatusProvider.java
processing/src/main/java/org/apache/carbondata/processing/util/CarbonLoaderUtil.java
core/src/main/java/org/apache/carbondata/core/datamap/DataMapUtil.java
core/src/main/java/org/apache/carbondata/core/statusmanager/LoadMetadataDetails.java
core/src/main/java/org/apache/carbondata/core/datamap/DataMapProvider.java
texts:Incremental dat load support to datamap on single table
Incremental dat load support to datamap on single tableprojection cases, aggregates, groupby cases
issueID:CARBONDATA-3339
type:Bug
changed files:
texts:Incremental data load support for datamap on multiple tables
Incremental data load support for datamap on multiple tables Join scenarios
issueID:CARBONDATA-334
type:Bug
changed files:integration/spark-common/src/main/java/org/apache/carbondata/spark/load/CarbonLoaderUtil.java
texts:Correct Some Spelling Mistakes

issueID:CARBONDATA-3341
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/datastore/chunk/store/ColumnPageWrapper.java
texts:Query is giving NULL in result
Steps to reproduce:1 set bad_records_action=force and carbon.push.rowfilters.for.vector=true2. create table t1(a bigint) stored by 'carbondata' TBLPROPERTIES('sort_columns'='a');3. insert into t1 select 'k';4. insert into t1 select 1;5. select * from t1 where a = 1 or a = 0; -------+a-------+NULL1-------+ 
issueID:CARBONDATA-3343
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/scan/result/impl/NonFilterQueryScannedResult.java
processing/src/main/java/org/apache/carbondata/processing/merger/CarbonCompactionUtil.java
core/src/main/java/org/apache/carbondata/core/scan/result/BlockletScannedResult.java
core/src/main/java/org/apache/carbondata/core/scan/expression/Expression.java
core/src/main/java/org/apache/carbondata/core/scan/model/QueryModelBuilder.java
core/src/main/java/org/apache/carbondata/core/scan/result/impl/FilterQueryScannedResult.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
processing/src/main/java/org/apache/carbondata/processing/merger/CarbonCompactionExecutor.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/CarbonTable.java
processing/src/main/java/org/apache/carbondata/processing/merger/RowResultMergerProcessor.java
core/src/main/java/org/apache/carbondata/core/scan/filter/FilterExpressionProcessor.java
core/src/main/java/org/apache/carbondata/core/scan/filter/FilterUtil.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/ConditionalFilterResolverImpl.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/RowLevelRangeFilterResolverImpl.java
texts:Support Compaction for Range Sort
CarbonData supports Compaction for all sort scopes based on their taskIds, i.e, we group the partitions(carbondata files) of different segments which have the same taskId to one task and then compact. But this would not be the correct way to handle the compaction in the case of Range Sort where we have data divided into different ranges for different segments.
issueID:CARBONDATA-3344
type:Bug
changed files:
texts:Fix Drop column not present in table
Steps to Reproduce: create table test1(col1 int) stored by 'carbondata' Try to Drop column not present in table => alter table test1 drop columns(name) Find the null pointer exception
issueID:CARBONDATA-3345
type:Bug
changed files:integration/presto/src/main/java/org/apache/carbondata/presto/impl/CarbonTableReader.java
texts:Presto query in Carbondata-streaming failed
I use streaming for saving switch' s syslog, here' s my table ddlCREATE TABLE IF NOT EXISTS syslog(id LONG, device_id LONG, ip STRING, message STRING, level SHORT, message_type CHAR(1), port_index INT, area_id LONG, createdon TIMESTAMP) STORED AS carbondata TBLPROPERTIES ('INVERTED_INDEX'='device_id,level,area_id','SORT_COLUMNS'='device_id,level,area_id,id','streaming'='true') Here' s a record example I mock ten thousand switchs，each one produced a record every 10 seconds on a day.On the early time, the presto' s query is normally.However, when the data growing, the presto query result is diffent from the spark sql.       I am looking forward to resolving it, thanks !
issueID:CARBONDATA-3348
type:Sub-task
changed files:core/src/main/java/org/apache/carbondata/core/metadata/schema/table/TableInfo.java
core/src/main/java/org/apache/carbondata/core/scan/model/QueryModelBuilder.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
core/src/main/java/org/apache/carbondata/core/scan/executor/impl/QueryExecutorProperties.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/CarbonTable.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonInputFormat.java
core/src/main/java/org/apache/carbondata/core/scan/executor/util/RestructureUtil.java
core/src/main/java/org/apache/carbondata/core/scan/executor/impl/AbstractQueryExecutor.java
core/src/main/java/org/apache/carbondata/core/datamap/TableDataMap.java
core/src/main/java/org/apache/carbondata/core/datamap/DataMapFilter.java
core/src/main/java/org/apache/carbondata/core/datamap/dev/expr/DataMapExprWrapperImpl.java
texts:support alter table property SORT_COLUMNS

issueID:CARBONDATA-3349
type:Sub-task
changed files:core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/SegmentIndexFileStore.java
tools/cli/src/main/java/org/apache/carbondata/tool/CarbonCli.java
tools/cli/src/main/java/org/apache/carbondata/tool/FileCollector.java
texts:add is_sorted and sort_columns information into show segments

issueID:CARBONDATA-335
type:Bug
changed files:
texts:Load data command options &#39;QUOTECHAR&#39; perform unexpected behavior.
Hey Team,I am using load data command with specific 'QUOTECHAR' option. But after loading the data, the behavior of quote character is not working as expected. Below is my example:create table one (name string, description string, salary double, age int, dob timestamp) stored by 'carbondata';CSV File Content >>name, description, salary, age, dobtammy, $my name$, 900000, 22, 19/10/20190: jdbc:hive2://127.0.0.1:10000> load data local inpath 'hdfs://localhost:54310/home/harmeet/dollarquote.csv' into table one OPTIONS('QUOTECHAR'="$");Results >> 0: jdbc:hive2://127.0.0.1:10000> select * from one; Actual Output >>> --------------------- tammy                                       $my name$    NULL   900000.0   22    -------------------------------------------------------Expected Output >>> --------------------- tammy                                       my name    NULL   900000.0   22    -------------------------------------------------------
issueID:CARBONDATA-3350
type:Sub-task
changed files:core/src/main/java/org/apache/carbondata/core/readcommitter/TableStatusReadCommittedScope.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockletDataMapFactory.java
processing/src/main/java/org/apache/carbondata/processing/merger/CarbonCompactionUtil.java
texts:enhance custom compaction to support resort single segment

issueID:CARBONDATA-3351
type:Sub-task
changed files:processing/src/main/java/org/apache/carbondata/processing/loading/converter/impl/FieldEncoderFactory.java
core/src/main/java/org/apache/carbondata/core/datastore/page/LazyColumnPage.java
store/sdk/src/main/java/org/apache/carbondata/sdk/file/RowUtil.java
core/src/main/java/org/apache/carbondata/core/datastore/page/VarLengthColumnPageBase.java
core/src/main/java/org/apache/carbondata/core/util/DataTypeUtil.java
core/src/main/java/org/apache/carbondata/core/metadata/datatype/DataType.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/store/impl/safe/SafeVariableLengthDimensionDataChunkStore.java
processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactDataHandlerColumnar.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/ColumnPageEncoderMeta.java
store/sdk/src/main/java/org/apache/carbondata/sdk/file/CSVCarbonWriter.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/SortStepRowHandler.java
core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
hadoop/src/main/java/org/apache/carbondata/hadoop/util/CarbonVectorizedRecordReader.java
store/sdk/src/main/java/org/apache/carbondata/sdk/file/utils/SDKUtil.java
core/src/main/java/org/apache/carbondata/core/datastore/page/ColumnPage.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/store/impl/safe/AbstractNonDictionaryVectorFiller.java
integration/spark-datasource/src/main/scala/org/apache/carbondata/spark/vectorreader/VectorizedCarbonRecordReader.java
processing/src/main/java/org/apache/carbondata/processing/store/TablePage.java
core/src/main/java/org/apache/carbondata/core/metadata/datatype/DataTypes.java
integration/spark-datasource/src/main/scala/org/apache/carbondata/converter/SparkDataTypeConverterImpl.java
core/src/main/java/org/apache/carbondata/core/datastore/page/UnsafeVarLengthColumnPage.java
core/src/main/java/org/apache/carbondata/core/scan/result/vector/impl/CarbonColumnVectorImpl.java
processing/src/main/java/org/apache/carbondata/processing/loading/converter/impl/BinaryFieldConverterImpl.java
core/src/main/java/org/apache/carbondata/core/datastore/row/CarbonRow.java
store/sdk/src/main/java/org/apache/carbondata/sdk/file/CarbonReader.java
store/sdk/src/main/java/org/apache/carbondata/sdk/file/Field.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/TableSchemaBuilder.java
store/sdk/src/main/java/org/apache/carbondata/sdk/file/JsonCarbonWriter.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
core/src/main/java/org/apache/carbondata/core/metadata/datatype/BinaryType.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/DefaultEncodingFactory.java
store/sdk/src/main/java/org/apache/carbondata/sdk/file/CarbonWriterBuilder.java
core/src/main/java/org/apache/carbondata/core/metadata/converter/ThriftWrapperSchemaConverterImpl.java
texts:Support Binary Data Type
Background :Binary is basic data type and widely used in various scenarios. So it’s better to support binary data type in CarbonData. Download data from S3 will be slow when dataset has lots of small binary data. The majority of application scenarios are  related to storage small binary data type into CarbonData, which can avoid small binary files problem and speed up S3 access performance, also can decrease cost of accessing OBS by decreasing the number of calling S3 API. It also will easier to manage structure data and Unstructured data(binary) by storing them into CarbonData. Goals:1. Supporting write binary data type by Carbon Java SDK.2. Supporting read binary data type by Spark Carbon file format(carbon datasource) and CarbonSession.3. Supporting read binary data type by Carbon SDK4. Supporting write binary by sparkApproach and Detail: 1.Supporting write binary data type by Carbon Java SDK &#91;Formal&#93;:     1.1 Java SDK needs support write data with specific data types, like int, double, byte[ ] data type, no need to convert all data type to string array. User read binary file as byte[], then SDK writes byte[] into binary column.=>Done     1.2 CarbonData compress binary column because now the compressor is table level.=>Done     1.3 CarbonData stores binary as dimension. => Done     1.4 Support configure page size for binary data type because binary data usually is big, such as 200k. Otherwise it will be very big for one blocklet (32000 rows). =>Done     1.5 Avro, JSON convert need consider  • AVRO fixed and variable length binary can be supported    => Avro don't support binary data type => No need   Support read binary from JSON  => done.     1.6 Binay data type as a child columns in Struct, Map                                                        => support it in the future, but priority is not very high, not in 1.5.4     1.7 Verify what is the maximum size of the binary value supportred      => snappy only support about 1.71 G, the max data size should be 2 GB, but need confirm             2. Supporting read and manage binary data type by Spark Carbon file format(carbon DataSource) and CarbonSession.&#91;Formal&#93;     2.1 Supporting read binary data type from non-transaction table, read binary column and return as byte[] =>Done     2.2 Support create table with binary column, table property doesn’t support sort_columns, dictionary, COLUMN_META_CACHE, RANGE_COLUMN for binary column => Done       => CARBON Datasource don't support dictionary include column       =>support  carbon.column.compressor= snappy,zstd,gzip for binary, compress is for all columns(table level)     2.3 Support CTAS for binary=> transaction/non-transaction,  Carbon/Hive/Parquet => Done      2.4 Support external table for binary=> Done     2.5 Support projection for binary column=> Done     2.6 Support desc formatted=> Done                   => Carbon Datasource don't support  ALTER TABLE add columns sql                   support  ALTER TABLE for(add column, rename, drop column) binary data type in carbon session=> Done                   Don't support change the data type for binary by alter table => Done     2.7 Don’t support PARTITION, BUCKETCOLUMNS  for binary => Done     2.8 Support compaction for binary=> Done     2.9 datamap? Don’t support bloomfilter, lucene, timeseries datamap,  no need min max datamap for binary, support mv and pre-aggregate in the future=> TODO     2.10 CSDK / python SDK support binary in the future.=> TODO     2.11 Support S3=> Done            2.12 support UDF, hex, base64, cast:.=> TODO                   select hex(bin) from carbon_table..=> TODO            2.15 support filter for binary => Done            2.16 select CAST(s AS BINARY) from carbon_table. => Done 3. Supporting read binary data type by Carbon SDK     3.1 Supporting read binary data type from non-transaction table, read binary column and return as byte[]=> Done     3.2 Supporting projection for binary column=> Done     3.3 Supporting S3=> Done     3.4 no need to support filter.=> to be discussd, not in this PR 4. Supporting write binary by spark (carbon file format / carbonsession, POC??)     4.1 Convert binary to String and storage in CSV=> Done     4.2 Spark load CSV and convert string to byte[], and storage in CarbonData. read binary column and return as byte[]=> Done            4.3 Support insert into/update/delete for binary data type => Done     4.4 Don’t support stream table. => TODO     4.5 Verify given value for binary column is Base64 encoded,Plain String and byte[] for SDK,fileformat,caronsession.            =>xubo: I think we should support configurable decode for binary, like support base64 and Hex, is it ok? Hive also add TODO for configurable. Hive don’t support Hex and normal string now.  => TODO, not in this PR           4.6  Local dictionary can be excluded =>Done           4.7 verify Binary type behavior with data having null values, Verify the bad records logger behavior with binary column, better to keep the badrecords file readable, should  we encode to base64 ?                         => support it in the future. Carbon doesn’t encode/decode base64 now, carbon will keep the same for output and input. Carbon can support configurable encode/decode for binary.  CarbonSession  only support load data from files(csv), so it’s already readable for bad record. How to confirm which is bad record for binary? For CarbonSDK, we can support encode to base64 default and add configure parameter to convert to other format, like Hex. Is it ok?  TODO, not in this PR           4.8 Verify with both unsafe true and false configurations for load and query       => Done 5. CLI tool support binary data type column => Done
issueID:CARBONDATA-3353
type:Bug
changed files:
texts:Fix MinMax Pruning for Measure column in case of Legacy store
For tables created and loaded in legacy store with measure column, when we query measure column with current version, query returns wrong results
issueID:CARBONDATA-3356
type:Bug
changed files:store/sdk/src/main/java/org/apache/carbondata/sdk/file/Field.java
core/src/main/java/org/apache/carbondata/core/util/BlockletDataMapUtil.java
texts:There are some exception when  carbonData DataSource read SDK files with varchar
There are some exception when  carbonData DataSource read SDK files with varchar     write data:  public void testReadSchemaFromDataFileArrayString() {    String path = "./testWriteFiles";    try {      FileUtils.deleteDirectory(new File(path));      Field[] fields = new Field[11];      fields[0] = new Field("stringField", DataTypes.STRING);      fields[1] = new Field("shortField", DataTypes.SHORT);      fields[2] = new Field("intField", DataTypes.INT);      fields[3] = new Field("longField", DataTypes.LONG);      fields[4] = new Field("doubleField", DataTypes.DOUBLE);      fields[5] = new Field("boolField", DataTypes.BOOLEAN);      fields[6] = new Field("dateField", DataTypes.DATE);      fields[7] = new Field("timeField", DataTypes.TIMESTAMP);      fields[8] = new Field("decimalField", DataTypes.createDecimalType(8, 2));      fields[9] = new Field("varcharField", DataTypes.VARCHAR);      fields[10] = new Field("arrayField", DataTypes.createArrayType(DataTypes.STRING));      Map<String, String> map = new HashMap<>();      map.put("complex_delimiter_level_1", "#");      CarbonWriter writer = CarbonWriter.builder()          .outputPath(path)          .withLoadOptions(map)          .withCsvInput(new Schema(fields))          .writtenBy("CarbonReaderTest")          .build();      for (int i = 0; i < 10; i++) {        String[] row2 = new String[]{            "robot" + (i % 10),            String.valueOf(i % 10000),            String.valueOf(i),            String.valueOf(Long.MAX_VALUE - i),            String.valueOf((double) i / 2),            String.valueOf(true),            "2019-03-02",            "2019-02-12 03:03:34",            "12.345",            "varchar",            "Hello#World#From#Carbon"        };        writer.write(row2);      }      writer.close();      File[] dataFiles = new File(path).listFiles(new FilenameFilter() {        @Override        public boolean accept(File dir, String name) {          if (name == null) {            return false;          }          return name.endsWith("carbondata");        }      });      if (dataFiles == null || dataFiles.length < 1) {        throw new RuntimeException("Carbon data file not exists.");      }      Schema schema = CarbonSchemaReader          .readSchema(dataFiles[0].getAbsolutePath())          .asOriginOrder();      // Transform the schema      String[] strings = new String[schema.getFields().length];      for (int i = 0; i < schema.getFields().length; i++) {        strings[i] = (schema.getFields())[i].getFieldName();      }      // Read data      CarbonReader reader = CarbonReader          .builder(path, "_temp")          .projection(strings)          .build();      int i = 0;      while (reader.hasNext()) {        Object[] row = (Object[]) reader.readNextRow();        assert (row[0].equals("robot" + i));        assert (row[2].equals(i));        assert (row[6].equals(17957));        Object[] arr = (Object[]) row[10];        assert (arr[0].equals("Hello"));        assert (arr[3].equals("Carbon"));        i++;      }      reader.close();//      FileUtils.deleteDirectory(new File(path));    } catch (Throwable e) {      e.printStackTrace();      Assert.fail(e.getMessage());    }  }      read data      test("Test read image carbon with spark carbon file format, generate by sdk, CTAS") {        sql("DROP TABLE IF EXISTS binaryCarbon")        sql("DROP TABLE IF EXISTS binaryCarbon3")        if (SparkUtil.isSparkVersionEqualTo("2.1")) {            sql(s"CREATE TABLE binaryCarbon USING CARBON OPTIONS(PATH '$writerPath')")            sql(s"CREATE TABLE binaryCarbon3 USING CARBON OPTIONS(PATH '$outputPath')" + " AS SELECT * FROM binaryCarbon")        } else {//            sql(s"CREATE TABLE binaryCarbon USING CARBON LOCATION '$writerPath'")            sql(s"CREATE TABLE binaryCarbon USING CARBON LOCATION '/Users/xubo/Desktop/xubo/git/carbondata3/store/sdk/testWriteFiles'")            sql("SELECT COUNT(*) FROM binaryCarbon").show()}}    exception:  java.io.IOException: All common columns present in the files doesn't have same datatype. Unsupported operation on nonTransactional table. Check logs. at org.apache.carbondata.core.scan.executor.impl.AbstractQueryExecutor.updateColumns(AbstractQueryExecutor.java:290) at org.apache.carbondata.core.scan.executor.impl.AbstractQueryExecutor.getDataBlocks(AbstractQueryExecutor.java:234) at org.apache.carbondata.core.scan.executor.impl.AbstractQueryExecutor.initQuery(AbstractQueryExecutor.java:138) at org.apache.carbondata.core.scan.executor.impl.AbstractQueryExecutor.getBlockExecutionInfos(AbstractQueryExecutor.java:407) at org.apache.carbondata.core.scan.executor.impl.VectorDetailQueryExecutor.execute(VectorDetailQueryExecutor.java:44) at org.apache.carbondata.spark.vectorreader.VectorizedCarbonRecordReader.initialize(VectorizedCarbonRecordReader.java:146) at org.apache.spark.sql.carbondata.execution.datasources.SparkCarbonFileFormat$$anonfun$buildReaderWithPartitionValues$2.apply(SparkCarbonFileFormat.scala:422) at org.apache.spark.sql.carbondata.execution.datasources.SparkCarbonFileFormat$$anonfun$buildReaderWithPartitionValues$2.apply(SparkCarbonFileFormat.scala:383) at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:128) at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:182) at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:109) at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.scan_nextBatch_0$(Unknown Source) at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithoutKey_0$(Unknown Source) at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source) at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43) at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$1.hasNext(WholeStageCodegenExec.scala:614) at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408) at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125) at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96) at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53) at org.apache.spark.scheduler.Task.run(Task.scala:109) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748)2019-04-20 10:42:08 ERROR TaskSetManager:70 - Task 0 in stage 0.0 failed 1 times; aborting jobJob aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0, localhost, executor driver): java.io.IOException: All common columns present in the files doesn't have same datatype. Unsupported operation on nonTransactional table. Check logs. at org.apache.carbondata.core.scan.executor.impl.AbstractQueryExecutor.updateColumns(AbstractQueryExecutor.java:290) at org.apache.carbondata.core.scan.executor.impl.AbstractQueryExecutor.getDataBlocks(AbstractQueryExecutor.java:234) at org.apache.carbondata.core.scan.executor.impl.AbstractQueryExecutor.initQuery(AbstractQueryExecutor.java:138) at org.apache.carbondata.core.scan.executor.impl.AbstractQueryExecutor.getBlockExecutionInfos(AbstractQueryExecutor.java:407) at org.apache.carbondata.core.scan.executor.impl.VectorDetailQueryExecutor.execute(VectorDetailQueryExecutor.java:44) at org.apache.carbondata.spark.vectorreader.VectorizedCarbonRecordReader.initialize(VectorizedCarbonRecordReader.java:146) at org.apache.spark.sql.carbondata.execution.datasources.SparkCarbonFileFormat$$anonfun$buildReaderWithPartitionValues$2.apply(SparkCarbonFileFormat.scala:422) at org.apache.spark.sql.carbondata.execution.datasources.SparkCarbonFileFormat$$anonfun$buildReaderWithPartitionValues$2.apply(SparkCarbonFileFormat.scala:383) at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:128) at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:182) at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:109) at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.scan_nextBatch_0$(Unknown Source) at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithoutKey_0$(Unknown Source) at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source) at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43) at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$1.hasNext(WholeStageCodegenExec.scala:614) at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408) at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125) at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96) at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53) at org.apache.spark.scheduler.Task.run(Task.scala:109) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748)Driver stacktrace:org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0, localhost, executor driver): java.io.IOException: All common columns present in the files doesn't have same datatype. Unsupported operation on nonTransactional table. Check logs. at org.apache.carbondata.core.scan.executor.impl.AbstractQueryExecutor.updateColumns(AbstractQueryExecutor.java:290) at org.apache.carbondata.core.scan.executor.impl.AbstractQueryExecutor.getDataBlocks(AbstractQueryExecutor.java:234) at org.apache.carbondata.core.scan.executor.impl.AbstractQueryExecutor.initQuery(AbstractQueryExecutor.java:138) at org.apache.carbondata.core.scan.executor.impl.AbstractQueryExecutor.getBlockExecutionInfos(AbstractQueryExecutor.java:407) at org.apache.carbondata.core.scan.executor.impl.VectorDetailQueryExecutor.execute(VectorDetailQueryExecutor.java:44) at org.apache.carbondata.spark.vectorreader.VectorizedCarbonRecordReader.initialize(VectorizedCarbonRecordReader.java:146) at org.apache.spark.sql.carbondata.execution.datasources.SparkCarbonFileFormat$$anonfun$buildReaderWithPartitionValues$2.apply(SparkCarbonFileFormat.scala:422) at org.apache.spark.sql.carbondata.execution.datasources.SparkCarbonFileFormat$$anonfun$buildReaderWithPartitionValues$2.apply(SparkCarbonFileFormat.scala:383) at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:128) at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:182) at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:109) at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.scan_nextBatch_0$(Unknown Source) at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithoutKey_0$(Unknown Source) at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source) at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43) at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$1.hasNext(WholeStageCodegenExec.scala:614) at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408) at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125) at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96) at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53) at org.apache.spark.scheduler.Task.run(Task.scala:109) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748)Driver stacktrace: at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1651) at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1639) at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1638) at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59) at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48) at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1638) at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831) at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831) at scala.Option.foreach(Option.scala:257) at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831) at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1872) at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1821) at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1810) at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48) at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642) at org.apache.spark.SparkContext.runJob(SparkContext.scala:2034) at org.apache.spark.SparkContext.runJob(SparkContext.scala:2055) at org.apache.spark.SparkContext.runJob(SparkContext.scala:2074) at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:363) at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38) at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3278) at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2489) at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2489) at org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3259) at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77) at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3258) at org.apache.spark.sql.Dataset.head(Dataset.scala:2489) at org.apache.spark.sql.Dataset.take(Dataset.scala:2703) at org.apache.spark.sql.Dataset.showString(Dataset.scala:254) at org.apache.spark.sql.Dataset.show(Dataset.scala:723) at org.apache.spark.sql.Dataset.show(Dataset.scala:682) at org.apache.spark.sql.Dataset.show(Dataset.scala:691) at org.apache.spark.sql.carbondata.datasource.SparkCarbonDataSourceBinaryTest$$anonfun$2.apply$mcV$sp(SparkCarbonDataSourceBinaryTest.scala:93) at org.apache.spark.sql.carbondata.datasource.SparkCarbonDataSourceBinaryTest$$anonfun$2.apply(SparkCarbonDataSourceBinaryTest.scala:84) at org.apache.spark.sql.carbondata.datasource.SparkCarbonDataSourceBinaryTest$$anonfun$2.apply(SparkCarbonDataSourceBinaryTest.scala:84) at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22) at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85) at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104) at org.scalatest.Transformer.apply(Transformer.scala:22) at org.scalatest.Transformer.apply(Transformer.scala:20) at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166) at org.scalatest.Suite$class.withFixture(Suite.scala:1122) at org.scalatest.FunSuite.withFixture(FunSuite.scala:1555) at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:163) at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175) at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175) at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306) at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:175) at org.scalatest.FunSuite.runTest(FunSuite.scala:1555) at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208) at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208) at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413) at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401) at scala.collection.immutable.List.foreach(List.scala:381) at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401) at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396) at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483) at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:208) at org.scalatest.FunSuite.runTests(FunSuite.scala:1555) at org.scalatest.Suite$class.run(Suite.scala:1424) at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1555) at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212) at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212) at org.scalatest.SuperEngine.runImpl(Engine.scala:545) at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:212) at org.apache.spark.sql.carbondata.datasource.SparkCarbonDataSourceBinaryTest.org$scalatest$BeforeAndAfterAll$$super$run(SparkCarbonDataSourceBinaryTest.scala:32) at org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:257) at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:256) at org.apache.spark.sql.carbondata.datasource.SparkCarbonDataSourceBinaryTest.run(SparkCarbonDataSourceBinaryTest.scala:32) at org.scalatest.tools.SuiteRunner.run(SuiteRunner.scala:55) at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2563) at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2557) at scala.collection.immutable.List.foreach(List.scala:381) at org.scalatest.tools.Runner$.doRunRunRunDaDoRunRun(Runner.scala:2557) at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1044) at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1043) at org.scalatest.tools.Runner$.withClassLoaderAndDispatchReporter(Runner.scala:2722) at org.scalatest.tools.Runner$.runOptionallyWithPassFailReporter(Runner.scala:1043) at org.scalatest.tools.Runner$.run(Runner.scala:883) at org.scalatest.tools.Runner.run(Runner.scala) at org.jetbrains.plugins.scala.testingSupport.scalaTest.ScalaTestRunner.runScalaTest2(ScalaTestRunner.java:131) at org.jetbrains.plugins.scala.testingSupport.scalaTest.ScalaTestRunner.main(ScalaTestRunner.java:28)Caused by: java.io.IOException: All common columns present in the files doesn't have same datatype. Unsupported operation on nonTransactional table. Check logs. at org.apache.carbondata.core.scan.executor.impl.AbstractQueryExecutor.updateColumns(AbstractQueryExecutor.java:290) at org.apache.carbondata.core.scan.executor.impl.AbstractQueryExecutor.getDataBlocks(AbstractQueryExecutor.java:234) at org.apache.carbondata.core.scan.executor.impl.AbstractQueryExecutor.initQuery(AbstractQueryExecutor.java:138) at org.apache.carbondata.core.scan.executor.impl.AbstractQueryExecutor.getBlockExecutionInfos(AbstractQueryExecutor.java:407) at org.apache.carbondata.core.scan.executor.impl.VectorDetailQueryExecutor.execute(VectorDetailQueryExecutor.java:44) at org.apache.carbondata.spark.vectorreader.VectorizedCarbonRecordReader.initialize(VectorizedCarbonRecordReader.java:146) at org.apache.spark.sql.carbondata.execution.datasources.SparkCarbonFileFormat$$anonfun$buildReaderWithPartitionValues$2.apply(SparkCarbonFileFormat.scala:422) at org.apache.spark.sql.carbondata.execution.datasources.SparkCarbonFileFormat$$anonfun$buildReaderWithPartitionValues$2.apply(SparkCarbonFileFormat.scala:383) at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:128) at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:182) at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:109) at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.scan_nextBatch_0$(Unknown Source) at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithoutKey_0$(Unknown Source) at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source) at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43) at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$1.hasNext(WholeStageCodegenExec.scala:614) at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408) at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125) at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96) at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53) at org.apache.spark.scheduler.Task.run(Task.scala:109) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748)    Analysis:  In carbon DataSource, will change carbon Varchar to spark String, inorg.apache.spark.sql.util.SparkTypeConverter#convertCarbonToSparkDataTypeparquet doesn't support varchar in Spark sql("CREATE TABLE hivetable(intField INT,stringField STRING,varcharField VARCHAR,timestampField TIMESTAMP," +"decimalField DECIMAL(6,2))")   sql("CREATE TABLE hivetable(intField INT,stringField STRING,varcharField VARCHAR,timestampField TIMESTAMP," +"decimalField DECIMAL(6,2)) using parquet")   sql("CREATE TABLE hivetable(intField INT,stringField STRING,varcharField VARCHAR,timestampField TIMESTAMP," +"decimalField DECIMAL(6,2)) stored as parquet")         exception:      DataType varchar is not supported.(line 1, pos 68)
issueID:CARBONDATA-3357
type:Sub-task
changed files:core/src/main/java/org/apache/carbondata/core/datamap/DataMapStoreManager.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/CarbonTable.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/DataMapSchema.java
core/src/main/java/org/apache/carbondata/core/datamap/DataMapUtil.java
texts:Support TableProperties from single parent table and restrict alter/delete/partition on mv

issueID:CARBONDATA-3358
type:Sub-task
changed files:
texts:Support configurable decode for loading binary data, support base64 and Hex decode.
Support configurable decode for loading binary data, support base64 and Hex decode.1. support configurable decode for loading2. test datamap: mv, preaggregate, timeseries, bloomfilter, lucene3. test datamap and configurable decode
issueID:CARBONDATA-3359
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/metadata/datatype/DecimalConverterFactory.java
texts:Data mismatch issue for decimal column after delete operation
after delete operation, the decimal column data is incorrect sql( s"""create table decimal_table(smallIntField smallInt,intField int,bigIntField bigint,floatField float, doubleField double,decimalField decimal(25, 4),timestampField timestamp,dateField date,stringField string, varcharField varchar(10),charField char(10))stored as carbondata """.stripMargin) sql(s"load data local inpath '$resourcesPath/decimalData.csv' into table decimal_table")sql("drop table if exists decimal_table")   
issueID:CARBONDATA-336
type:Wish
changed files:
texts:Align the the name description
Align the the name description for databaseNameOp and tableName.
issueID:CARBONDATA-3360
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/mutate/CarbonUpdateUtil.java
texts:NullPointerException in clean files operation
when delete is failed due to hdfs quota exceeded or disk space is full, then tableUpdateStatus.write will be present in store.So after that if clean files operation is done, we were trying to assign null to primitive type long, which will throw runtime exception, and .write file will not be deleted, since we consider it as invalid file.
issueID:CARBONDATA-3362
type:Sub-task
changed files:
texts:Document Update for Scenario
It will be easy for the user to understand if we provide scenarios in which the user need to configure this configuration.  
issueID:CARBONDATA-3364
type:Bug
changed files:integration/hive/src/main/java/org/apache/carbondata/hive/MapredCarbonInputFormat.java
integration/hive/src/main/java/org/apache/carbondata/hive/CarbonHiveInputSplit.java
integration/hive/src/main/java/org/apache/carbondata/hive/MapredCarbonOutputFormat.java
integration/hive/src/main/java/org/apache/carbondata/hive/CarbonHiveSerDe.java
texts:Support Read from Hive. Queries are giving empty results from hive.

issueID:CARBONDATA-3365
type:Improvement
changed files:store/sdk/src/main/java/org/apache/carbondata/sdk/file/arrow/ArrowConverter.java
store/sdk/src/main/java/org/apache/carbondata/sdk/file/arrow/ArrowFieldWriter.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonInputFormat.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonFileInputFormat.java
store/sdk/src/main/java/org/apache/carbondata/sdk/file/CarbonSchemaReader.java
store/sdk/src/main/java/org/apache/carbondata/sdk/file/CarbonReaderBuilder.java
store/sdk/src/main/java/org/apache/carbondata/sdk/file/ArrowCarbonReader.java
store/sdk/src/main/java/org/apache/carbondata/sdk/file/arrow/ArrowUtils.java
store/sdk/src/main/java/org/apache/carbondata/sdk/file/arrow/ArrowWriter.java
store/sdk/src/main/java/org/apache/carbondata/sdk/file/CarbonReader.java
core/src/main/java/org/apache/carbondata/core/stream/ExtendedByteArrayOutputStream.java
texts:Support Apache arrow vector filling from carbondata SDK
Background: As we know Apache arrow is a cross-language development platform for in-memory data, It specifies a standardised language-independent columnar memory format for flat and hierarchical data, organised for efficient analytic operations on modern hardware. So, By integrating carbon to support filling arrow vector, contents read by carbondata files can be used for analytics in any programming language. say arrow vector filled from carbon java SDK can be read by python, c, c++ and many other languages supported by arrow. This will also increase the scope for carbondata use-cases and carbondata can be used for various applications as arrow is integrated already with many query engines. Implementation: Stage1: After SDK reading the carbondata file, convert carbon rows and fill the arrow vector. Stage2: Deep integration with carbon vector; for this, currently carbon SDK vector doesn't support filling complex columns. After supporting this, arrow vector can be wrapped around carbon SDK vector for deep integration. 
issueID:CARBONDATA-3366
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/readcommitter/ReadCommittedScope.java
core/src/main/java/org/apache/carbondata/core/readcommitter/TableStatusReadCommittedScope.java
store/sdk/src/main/java/org/apache/carbondata/sdk/file/CarbonReader.java
store/sdk/src/main/java/org/apache/carbondata/sdk/file/CarbonReaderBuilder.java
texts:Support SDK reader to read blocklet level split
To provide more flexibility in SDK reader, blocklet level read support for carbondata files from SDK reader is required.With this, SDK reader can be used in distributed environment or in multithread environment by creating carbon readers in each worker at split level (blocklet split) 
issueID:CARBONDATA-3367
type:Bug
changed files:store/sdk/src/main/java/org/apache/carbondata/sdk/file/CarbonReaderBuilder.java
store/sdk/src/main/java/org/apache/carbondata/sdk/file/CarbonReader.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/CarbonTable.java
texts:OOM when huge number of carbondata files are read from SDK reader
Currently, for each carbondata file, one CarbonRecordReader will be created. And list of CarbonRecordReader will be maintained in carbonReader. so even when CarbonRecordReader is closed, the GC will not happen for that reader as list is still referring that object. so, each CarbonRecordReader needs separate memory , instead of reusing the previous memory. 
issueID:CARBONDATA-3368
type:Bug
changed files:store/sdk/src/main/java/org/apache/carbondata/sdk/file/CarbonReaderBuilder.java
store/sdk/src/main/java/org/apache/carbondata/sdk/file/CarbonReader.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/CarbonTable.java
texts:InferSchema from datafile instead of index file
problem : In SDK, when multiple readers were created with same folder location with different file list, for inferschema all the readers refers same index file, which was causing bottle neck and JVM crash in case of JNI call.solution: Inferschema from the data file mentioned while building the reader. 
issueID:CARBONDATA-3369
type:Bug
changed files:
texts:Fix issues during concurrent execution of Create table If not exists
Create table if not exists has following problems if run concurrently from different drivers 1) Sometimes It fails with error "Table <db.table> already exists." 2) Create table failed driver still holds the table with wrong path or schema. Eventual operations refer the wrong path3) Stale path created during create table is not deleted for ever &#91;After 1.5.0 version table will be created in a new folder using UUID if folder with table name already exists&#93;
issueID:CARBONDATA-337
type:Improvement
changed files:integration/spark-common/src/main/java/org/apache/carbondata/spark/merger/RowResultMerger.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/column/CarbonColumn.java
texts:Correct Inverted Index spelling mistakes
Correct Inverted Index spelling mistakes in three files.
issueID:CARBONDATA-3371
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/scan/wrappers/ByteArrayWrapper.java
core/src/main/java/org/apache/carbondata/core/scan/result/iterator/RawResultIterator.java
processing/src/main/java/org/apache/carbondata/processing/merger/CarbonCompactionExecutor.java
core/src/main/java/org/apache/carbondata/core/scan/result/iterator/ColumnDriftRawResultIterator.java
core/src/main/java/org/apache/carbondata/core/scan/executor/impl/AbstractQueryExecutor.java
core/src/main/java/org/apache/carbondata/core/scan/executor/util/QueryUtil.java
core/src/main/java/org/apache/carbondata/core/datastore/block/SegmentPropertiesAndSchemaHolder.java
texts:Compaction show ArrayIndexOutOfBoundsException after sort_columns modification
2019-05-05 15:26:39 ERROR DataTypeUtil:619 - Cannot convert�  Z�w} to SHORT type valueWrong length: 8, expected 22019-05-05 15:26:39 ERROR DataTypeUtil:621 - Problem while converting data type�  Z�w} 2019-05-05 15:26:39 ERROR CompactionResultSortProcessor:185 - 3java.lang.ArrayIndexOutOfBoundsException: 3 at org.apache.carbondata.core.scan.wrappers.ByteArrayWrapper.getNoDictionaryKeyByIndex(ByteArrayWrapper.java:81) at org.apache.carbondata.processing.merger.CompactionResultSortProcessor.prepareRowObjectForSorting(CompactionResultSortProcessor.java:332) at org.apache.carbondata.processing.merger.CompactionResultSortProcessor.processResult(CompactionResultSortProcessor.java:250) at org.apache.carbondata.processing.merger.CompactionResultSortProcessor.execute(CompactionResultSortProcessor.java:175) at org.apache.carbondata.spark.rdd.CarbonMergerRDD$$anon$1.<init>(CarbonMergerRDD.scala:226) at org.apache.carbondata.spark.rdd.CarbonMergerRDD.internalCompute(CarbonMergerRDD.scala:84) at org.apache.carbondata.spark.rdd.CarbonRDD.compute(CarbonRDD.scala:82) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) at org.apache.spark.rdd.RDD.iterator(RDD.scala:287) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87) at org.apache.spark.scheduler.Task.run(Task.scala:108) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748)2019-05-05 15:26:39 ERROR CarbonMergerRDD:233 - Compaction Failed
issueID:CARBONDATA-3373
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/scan/filter/executer/BitSetUpdaterFactory.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/MeasureColumnExecuterFilterInfo.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/IncludeFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/scan/filter/FilterExecutorUtil.java
core/src/main/java/org/apache/carbondata/core/scan/filter/FilterUtil.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/FilterBitSetUpdater.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/ExcludeFilterExecuterImpl.java
texts:Optimize scenes with in numbers in SQL
when sql with 'in numbers'  and spark.sql.codegen.wholeStage is false，the query is slow, the reason is that canbonscan row level filter's time complexity is O(n^2), we can replace list with hashset to improve query performancesql example: select * from xx where filed in (1,2,3,4,5,6) 
issueID:CARBONDATA-3374
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/util/path/CarbonTablePath.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockletDataMapFactory.java
core/src/main/java/org/apache/carbondata/core/indexstore/schema/SchemaGenerator.java
datamap/lucene/src/main/java/org/apache/carbondata/datamap/lucene/LuceneFineGrainDataMapFactory.java
core/src/main/java/org/apache/carbondata/core/indexstore/BlockletDetailsFetcher.java
core/src/main/java/org/apache/carbondata/core/datamap/dev/DataMapFactory.java
datamap/lucene/src/main/java/org/apache/carbondata/datamap/lucene/LuceneDataMapFactoryBase.java
texts:Optimize documentation and fix some spell errors.
Optimize documentation and fix some spell errors.
issueID:CARBONDATA-3375
type:Bug
changed files:
texts:GC Overhead limit exceeded error for huge data in Range Compaction
When only single data item is present then it will be launched as one single task wich results in one executor getting overloaded.
issueID:CARBONDATA-3377
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
core/src/main/java/org/apache/carbondata/core/util/CarbonProperties.java
texts:String Type Column with huge strings and null values fails Range Compaction
String Type Column with huge strings and null values fails giving NullPointerException when it is a range column and compaction is done.
issueID:CARBONDATA-3378
type:Sub-task
changed files:core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
core/src/main/java/org/apache/carbondata/core/datamap/DistributableDataMapFormat.java
texts:Display original query in Indexserver Job
When any query fired in main jdbcserver , in Index server there is no mapping of it. it is difficult to find which job in index server belong to which query specially in concurrent queries.
issueID:CARBONDATA-338
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/datastore/impl/FileFactory.java
core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
texts:Remove the method arguments as they are never used inside the method

issueID:CARBONDATA-3380
type:Bug
changed files:
texts:Fix missing appName and AnalysisException bug in DirectSQLExample
Fix two bug in DirectSQLExample fix missing appName which is writing the carbondata files. fix AnalysisException bug because of carbonfile
issueID:CARBONDATA-3381
type:Sub-task
changed files:
texts:Large response size Exception is thrown from index server.

issueID:CARBONDATA-3382
type:Bug
changed files:
texts:Fix compressor type displayed in desc formatted
When in carbon.properties the carbon.column.compressor is configured as gzip the describe formatted table does not display the compression type configured in carbon.properties instead it always displays the default compressor "snappy".
issueID:CARBONDATA-3384
type:Bug
changed files:hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonTableInputFormat.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockletDataMapFactory.java
texts:Delete/Update is throwing NullPointerException when index server is enabled.

issueID:CARBONDATA-3385
type:Bug
changed files:
texts:Merge index files are nosidered during clean files in case of compatibility case
Merge index files are nosidered during clean files in case of compatibility case
issueID:CARBONDATA-3386
type:Bug
changed files:
texts:Concurrent Merge index and query is failing
Concurrent merge index and query is failing. Load is triggered on a table, at the end of the load Merge index will be triggered. But this is triggered after the table status is updated as SUCCESS/PARTIAL SUCCESS for that segments. So for the concurrent query, this segment is available for query. Once the merge index is done, it deletes the index files, which are still referred by the query, this leads to the query failure.
issueID:CARBONDATA-3387
type:Sub-task
changed files:core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
processing/src/main/java/org/apache/carbondata/processing/util/CarbonLoaderUtil.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/DataMapSchema.java
core/src/main/java/org/apache/carbondata/core/datamap/DataMapProvider.java
texts:Support Partition with MV datamap & Show DataMap Status

issueID:CARBONDATA-339
type:Bug
changed files:
texts:Align storePath name in generateGlobalDictionary() of GlobalDictionaryUtil.scala
Align storePath name in generateGlobalDictionary() of GlobalDictionaryUtil.scala: Change all "hdfsLocation" to "storePath".It can support any path, not only hdfs path,need to change.
issueID:CARBONDATA-3392
type:Sub-task
changed files:hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonTableInputFormat.java
core/src/main/java/org/apache/carbondata/core/datamap/DataMapUtil.java
core/src/main/java/org/apache/carbondata/core/util/BlockletDataMapUtil.java
texts:Make use of LRU mandatory when using IndexServer
BackgroundCurrently LRU is optional for the user to configure, but this will raise some concerns in case of index server because the invalid segments have to be constantly removed from the cache in case of update/delete/compaction scenarios.Therefore if clear segment job is failed then the job would not fail bu there has to be a mechanism to prevent that segment from being in cache forever.
issueID:CARBONDATA-3393
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/metadata/SegmentFileStore.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
texts:Merge Index Job Failure should not trigger the merge index job again. Exception propagation should be decided by the User.
If the merge index job is failed, LOAD is also failing. Load should not consider the merge index job status to decide the LOAD status.
issueID:CARBONDATA-3394
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/statusmanager/SegmentStatusManager.java
core/src/main/java/org/apache/carbondata/core/mutate/CarbonUpdateUtil.java
texts:Clean files taking lot of time to finish
Clean files taking lot of time to finish, even though there are no marked for delete segment
issueID:CARBONDATA-3395
type:Bug
changed files:store/sdk/src/main/java/org/apache/carbondata/sdk/file/CarbonReaderBuilder.java
core/src/main/java/org/apache/carbondata/core/indexstore/BlockletDetailInfo.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonFileInputFormat.java
texts:When same split object is passed to concurrent readers, build() fails randomly with Exception.
When same split object is passed to concurrent readers, build() fails randomly with Exception 2019-05-24 13:51:55 ERROR CarbonVectorizedRecordReader:116 - java.lang.ArrayIndexOutOfBoundsException: 4
issueID:CARBONDATA-3396
type:Bug
changed files:processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactDataHandlerModel.java
core/src/main/java/org/apache/carbondata/core/scan/result/iterator/RawResultIterator.java
processing/src/main/java/org/apache/carbondata/processing/merger/CarbonCompactionUtil.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
core/src/main/java/org/apache/carbondata/core/scan/result/iterator/AbstractDetailQueryResultIterator.java
core/src/main/java/org/apache/carbondata/core/util/CarbonProperties.java
texts:Range Compaction Data mismatch

issueID:CARBONDATA-3397
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/scan/filter/FilterExpressionProcessor.java
core/src/main/java/org/apache/carbondata/core/datamap/DistributableDataMapFormat.java
texts:Remove SparkUnknown Expression to Index Server
if Query has UDF , it is registered into the Main driver and UDF function will not be available in Index server So it is better to remove the  SparkUnknown  Expression because anyway for pruning we select all blocksorg.apache.carbondata.core.scan.filter.executer.RowLevelFilterExecuterImpl#isScanRequired. 
issueID:CARBONDATA-3398
type:Sub-task
changed files:datamap/bloom/src/main/java/org/apache/carbondata/datamap/bloom/BloomCoarseGrainDataMapFactory.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonTableInputFormat.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockletDataMapFactory.java
core/src/main/java/org/apache/carbondata/core/metadata/SegmentFileStore.java
core/src/main/java/org/apache/carbondata/core/indexstore/BlockletDetailsFetcher.java
core/src/main/java/org/apache/carbondata/core/datamap/dev/DataMapFactory.java
texts:Implement Show Cache for IndexServer and MV

issueID:CARBONDATA-3399
type:Sub-task
changed files:
texts:Implement Executor ID based task distribution for Index Server

issueID:CARBONDATA-34
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/util/path/CarbonTablePath.java
texts:Drop table fails when mentions along with db name.
Drop table fails when mentions along with db name. LikeDROP TABLE default.tablename
issueID:CARBONDATA-340
type:Test
changed files:core/src/main/java/org/apache/carbondata/core/statusmanager/LoadMetadataDetails.java
texts:Implement test cases for load package in core module

issueID:CARBONDATA-3400
type:Bug
changed files:
texts:Support IndexSever for Spark-Shell for in secure KERBROSE mode
Support IndexSever for Spark-Shell for in secure KERBROSE mode
issueID:CARBONDATA-3402
type:Sub-task
changed files:
texts:Block complex data types and validate dmproperties in mv

issueID:CARBONDATA-3403
type:Bug
changed files:
texts:MV is not working for like and filter AND and OR queries
MV is not working for like and filter AND and OR queries Steps:create table brinjal (imei string,AMSize string,channelsId string,ActiveCountry string, Activecity string,gamePointId double,deviceInformationId double,productionDate Timestamp,deliveryDate timestamp,deliverycharge double) STORED BY 'org.apache.carbondata.format' ; create datamap brinjal_mv_tab_nlz_aa016 on table brinjal using 'mv' as select imei,AMSize,channelsId from brinjal where ActiveCountry NOT LIKE 'US' group by imei,AMSize,channelsId;create datamap brinjal_mv_tab_nlz_aa018 on table brinjal using 'mv' as select imei,AMSize,channelsId,ActiveCountry from brinjal where ActiveCountry ='Chinese' or channelsId =4 group by imei,AMSize,channelsId,ActiveCountry; then select imei,AMSize,channelsId from brinjal where ActiveCountry NOT LIKE 'US' group by imei,AMSize,channelsId; and   select imei,AMSize,channelsId,ActiveCountry from brinjal where ActiveCountry ='Chinese' or channelsId =4 group by imei,AMSize,channelsId,ActiveCountry;are not hitting the datamap cretaed def updatePlan(plan: ModularPlan): ModularPlan = {    // get all the outputlist from relation    val dmOutputList = plan.collect {      case s@modular.Select(_, _, _, _, _, children, _, _, _, _) if children        .forall(_.isInstanceOf[modular.LeafNode]) =>        s.outputList    }.flatten    val newPlan = plan.transform {      case plan =>        plan.transformExpressions {          case attr: Attribute =>            val existingCol = dmOutputList.find(_.exprId.equals(attr.exprId))            if (existingCol.isDefined) {              AttributeReference(existingCol.get.name, attr.dataType)(exprId = attr.exprId,                qualifier = attr.qualifier)            } else {              attr            }        }    }    newPlan  }
issueID:CARBONDATA-3404
type:New Feature
changed files:core/src/main/java/org/apache/carbondata/core/datastore/impl/FileFactory.java
core/src/main/java/org/apache/carbondata/core/datastore/filesystem/LocalCarbonFile.java
core/src/main/java/org/apache/carbondata/core/datastore/impl/DefaultFileTypeProvider.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/SchemaReader.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
core/src/main/java/org/apache/carbondata/core/datastore/impl/FileTypeInterface.java
core/src/main/java/org/apache/carbondata/core/locks/CarbonLockFactory.java
core/src/main/java/org/apache/carbondata/core/datastore/filesystem/AbstractDFSCarbonFile.java
core/src/main/java/org/apache/carbondata/core/datastore/filesystem/CarbonFile.java
core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
texts:Support CarbonFile API for coniguring custom file systems
Currently CarbonData supports few set of FileSystems like HDFS,S3,VIEWFS schemes.If user configures table path from different file systems apart from supported, FileFactory takes CarbonLocalFile as default and casues errors.This Jira proposes to support a API for user to extend CarbonFile & give the correct instance of CarbonFile from existing implementations or a new own implementation of CarbonFile interface.
issueID:CARBONDATA-3405
type:Bug
changed files:store/sdk/src/main/java/org/apache/carbondata/sdk/file/CarbonReaderBuilder.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/CarbonTable.java
texts:SDK reader getSplits() must clear the cache.
a. cache key is not filled during sdk reader, its always with null table name. fill thisb. clear the cache, after splits are obtained in getSplits()
issueID:CARBONDATA-3406
type:Bug
changed files:integration/hive/src/main/java/org/apache/carbondata/hive/CarbonObjectInspector.java
integration/hive/src/main/java/org/apache/carbondata/hive/CarbonDictionaryDecodeReadSupport.java
integration/hive/src/main/java/org/apache/carbondata/hive/CarbonHiveRecordReader.java
texts:Support Binary, Boolean,Varchar, Complex data types read and Dictionary columns read
(1) Support all data types read(2) Support dictionary columns read
issueID:CARBONDATA-3407
type:Bug
changed files:
texts:distinct, count, Sum query fails when MV is created on single projection column
distinct, count, Sum query fails when MV is created on single projection column sql("drop table if exists maintable")    sql("create table maintable(name string, age int, add string) stored by 'carbondata'")    sql("create datamap single_mv using 'mv' as select age from maintable")    sql("insert into maintable select 'pheobe',31,'NY'")    sql("insert into maintable select 'rachel',32,'NY'")     sql("select distinct(age) from maintable")     sql("select sum(age) from maintable")    sql("select count(age) from maintable")Fails with below Exception:requirement failed: Fragment is not supported.  Current frag:org.apache.carbondata.mv.plans.util.SQLBuildDSL$$anon$1@1f7f2e76java.lang.IllegalArgumentException: requirement failed: Fragment is not supported.  Current frag:org.apache.carbondata.mv.plans.util.SQLBuildDSL$$anon$1@1f7f2e76 at scala.Predef$.require(Predef.scala:224) at org.apache.carbondata.mv.plans.util.Printers$SQLFragmentCompactPrinter.printFragment(Printers.scala:248) at org.apache.carbondata.mv.plans.util.Printers$FragmentPrinter$$anonfun$print$1.apply(Printers.scala:82) at org.apache.carbondata.mv.plans.util.Printers$FragmentPrinter$$anonfun$print$1.apply(Printers.scala:80) at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33) at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35) at org.apache.carbondata.mv.plans.util.Printers$FragmentPrinter.print(Printers.scala:80) at org.apache.carbondata.mv.plans.util.Printers$class.render(Printers.scala:318) at org.apache.carbondata.mv.plans.modular.ModularPlan.render(ModularPlan.scala:35) at org.apache.carbondata.mv.plans.util.Printers$class.asCompactString(Printers.scala:323) at org.apache.carbondata.mv.plans.modular.ModularPlan.asCompactString(ModularPlan.scala:35) at org.apache.carbondata.mv.plans.modular.ModularPlan.asCompactSQL(ModularPlan.scala:156) at org.apache.carbondata.mv.datamap.MVAnalyzerRule.apply(MVAnalyzerRule.scala:83) at org.apache.carbondata.mv.datamap.MVAnalyzerRule.apply(MVAnalyzerRule.scala:43) at org.apache.spark.sql.hive.CarbonAnalyzer.execute(CarbonAnalyzer.scala:46)
issueID:CARBONDATA-3408
type:Sub-task
changed files:processing/src/main/java/org/apache/carbondata/processing/loading/model/CarbonLoadModelBuilder.java
processing/src/main/java/org/apache/carbondata/processing/util/CarbonLoaderUtil.java
core/src/main/java/org/apache/carbondata/core/util/DataTypeUtil.java
texts:CarbonSession partition support binary data type
CarbonSession partition support binary data type
issueID:CARBONDATA-3409
type:Sub-task
changed files:core/src/main/java/org/apache/carbondata/core/datamap/DataMapProvider.java
texts:Fix Concurrent dataloading Issue with mv

issueID:CARBONDATA-341
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/metadata/AbsoluteTableIdentifier.java
processing/src/main/java/org/apache/carbondata/processing/loading/DataLoadExecutor.java
texts:CarbonTableIdentifier being passed to the query flow has wrong tableid
CarbonTableIdentifier being passed to the query flow has wrong tableid.While creating the table the CarbonData system assign a uniqueID to the table. In all the places CarbonTableIdentifier  should have the same tableId.But CarbonTableIdentifier  is having the currentTimeStamp as tableId which is not correct.
issueID:CARBONDATA-3410
type:Sub-task
changed files:
texts:Add UDF, Hex/Base64 SQL functions for binary
Add UDF, Hex/Base64 SQL functions for binary
issueID:CARBONDATA-3411
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/datamap/DataMapStoreManager.java
store/sdk/src/main/java/org/apache/carbondata/sdk/file/CarbonReaderBuilder.java
texts:ClearDatamaps logs an exception in SDK
problem: In sdk when datamaps are cleared, below exception is loggedjava.io.IOException: File does not exist: /home/root1/Documents/ab/workspace/carbonFile/carbondata/store/sdk/testWriteFiles/771604793030370/Metadata/schema at org.apache.carbondata.core.metadata.schema.SchemaReader.readCarbonTableFromStore(SchemaReader.java:60) at org.apache.carbondata.core.metadata.schema.table.CarbonTable.buildFromTablePath(CarbonTable.java:272) at org.apache.carbondata.core.datamap.DataMapStoreManager.getCarbonTable(DataMapStoreManager.java:566) at org.apache.carbondata.core.datamap.DataMapStoreManager.clearDataMaps(DataMapStoreManager.java:514) at org.apache.carbondata.core.datamap.DataMapStoreManager.clearDataMaps(DataMapStoreManager.java:504) at org.apache.carbondata.sdk.file.CarbonReaderBuilder.getSplits(CarbonReaderBuilder.java:419) at org.apache.carbondata.sdk.file.CarbonReaderTest.testGetSplits(CarbonReaderTest.java:2605) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at junit.framework.TestCase.runTest(TestCase.java:176) at junit.framework.TestCase.runBare(TestCase.java:141) at junit.framework.TestResult$1.protect(TestResult.java:122) at junit.framework.TestResult.runProtected(TestResult.java:142) at junit.framework.TestResult.run(TestResult.java:125) at junit.framework.TestCase.run(TestCase.java:129) at junit.framework.TestSuite.runTest(TestSuite.java:255) at junit.framework.TestSuite.run(TestSuite.java:250) at org.junit.internal.runners.JUnit38ClassRunner.run(JUnit38ClassRunner.java:84) at org.junit.runner.JUnitCore.run(JUnitCore.java:160) at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:68) at com.intellij.rt.execution.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:47) at com.intellij.rt.execution.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:242) at com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:70)cause: CarbonTable is required for only launching the job, SDK there is no need to launch job. so , no need to build a carbon table.solution: build carbon table only when need to launch job.
issueID:CARBONDATA-3412
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/metadata/schema/table/CarbonTable.java
core/src/main/java/org/apache/carbondata/core/datamap/DistributableDataMapFormat.java
texts:For Non-transactional tables empty results are displayed with index server enabled

issueID:CARBONDATA-3413
type:Bug
changed files:store/sdk/src/main/java/org/apache/carbondata/sdk/file/ArrowCarbonReader.java
store/sdk/src/main/java/org/apache/carbondata/sdk/file/arrow/ArrowConverter.java
texts:Arrow allocators gives  OutOfMemory error when test with hugedata
Arrow allocators gives  OutOfMemory error when test with hugedata problem: OOM exception when in arrow with huge datacause: In ArrowConverter, allocator is not closed solution: close the allocator in arrowConverter.Also handle the problems in test utility API     
issueID:CARBONDATA-3414
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/datamap/DataMapStoreManager.java
store/sdk/src/main/java/org/apache/carbondata/sdk/file/CarbonReaderBuilder.java
texts:when Insert into partition table fails exception doesn&#39;t print reason.
problem: when Insert into partition table fails, exception doesn't print reason. cause: Exception was caught , but error message was not from that exception. solution: throw the exception directly  Steps to reproduce:  Open multiple spark beeline (say 10) Create a carbon table with partition  Insert overwrite to carbon table from all the 10 beeline concurrently some insert overwrite will be success and some will fail due to non-availability of lock even after retry. For the failed insert into sql, Exception is just "DataLoadFailure: " no error reason is printed.Need to print the valid error reason for the failure.    
issueID:CARBONDATA-3415
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/metadata/SegmentFileStore.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/SegmentIndexFileStore.java
core/src/main/java/org/apache/carbondata/core/writer/CarbonIndexFileMergeWriter.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonOutputCommitter.java
processing/src/main/java/org/apache/carbondata/processing/util/CarbonLoaderUtil.java
texts:Merge index is not working for partition table. Merge index for partition table is taking significantly longer time than normal table.
Issues:(1) Merge index is not working on partition table(2) Time taken for merge index is significantly more than the normal carbon table
issueID:CARBONDATA-3416
type:Bug
changed files:
texts:When new analyzer rule added in spark, not reflecting in carbon
When new analyzer rule added in spark, not reflecting in carbon.Carbon prepares the session state builder by extendeding the hivesession stet builder, and create new analyzer by overiding all the rules added by spark, so when new rule is added in spark, it will not be reflected in carbon as we have overriden the complete analyzer
issueID:CARBONDATA-3417
type:Bug
changed files:processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactDataHandlerModel.java
texts:Load time degrade for Range column due to cores configured

issueID:CARBONDATA-3418
type:Bug
changed files:
texts:Inherit Column Compressor Property from parent table to its child table&#39;s

issueID:CARBONDATA-3419
type:Bug
changed files:
texts:Desc Formatted not showing Range Column

issueID:CARBONDATA-3421
type:Improvement
changed files:
texts:Create table without column with properties failed, but throw incorrect exception
1. Create table without column but with correct tblproperties: create table ll stored by 'carbondata' tblproperties('sort_columns'='');2. Create table will fail with exception: Invalid table properties sort_columns, this is incorrect. Should throw correct exception like no schema is specified.
issueID:CARBONDATA-3423
type:Sub-task
changed files:
texts:Validate dictionary for binary data type

issueID:CARBONDATA-3424
type:Bug
changed files:
texts:There are improper exception when query with avg(substr(binary data type)).
Code:CREATE TABLE uniqdata (CUST_ID int,CUST_NAME binary,ACTIVE_EMUI_VERSION string, DOB timestamp, DOJ timestamp, BIGINT_COLUMN1 bigint,BIGINT_COLUMN2 bigint,DECIMAL_COLUMN1 decimal(30,10), DECIMAL_COLUMN2 decimal(36,10),Double_COLUMN1 double, Double_COLUMN2 double,INTEGER_COLUMN1 int) STORED BY 'org.apache.carbondata.format' TBLPROPERTIES('table_blocksize'='2000');LOAD DATA inpath 'hdfs://hacluster/chetan/2000_UniqData.csv' into table uniqdata OPTIONS('DELIMITER'=',' ,'QUOTECHAR'='"','BAD_RECORDS_ACTION'='FORCE','FILEHEADER'='CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1,Double_COLUMN2,INTEGER_COLUMN1');Select query with average function for substring of binary column is executed.select max(substr(CUST_NAME,1,2)),min(substr(CUST_NAME,1,2)),avg(substr(CUST_NAME,1,2)),count(substr(CUST_NAME,1,2)),sum(substr(CUST_NAME,1,2)),variance(substr(CUST_NAME,1,2)) from uniqdata where CUST_ID IS NULL or DOB IS NOT NULL or BIGINT_COLUMN1 =1233720368578 or DECIMAL_COLUMN1 = 12345678901.1234000058 or Double_COLUMN1 = 1.12345674897976E10 or INTEGER_COLUMN1 IS NULL limit 10;select max(substring(CUST_NAME,1,2)),min(substring(CUST_NAME,1,2)),avg(substring(CUST_NAME,1,2)),count(substring(CUST_NAME,1,2)),sum(substring(CUST_NAME,1,2)),variance(substring(CUST_NAME,1,2)) from uniqdata where CUST_ID IS NULL or DOB IS NOT NULL or BIGINT_COLUMN1 =1233720368578 or DECIMAL_COLUMN1 = 12345678901.1234000058 or Double_COLUMN1 = 1.12345674897976E10 or INTEGER_COLUMN1 IS NULL limit 10;Improper exception:"Invalid call to name on unresolved object, tree: unresolvedalias(avg(substring(CUST_NAME#73, 1, 2)), None)" did not contain "cannot resolve 'avg(substring(uniqdata.`CUST_NAME`, 1, 2))' due to data type mismatch: function average requires numeric types, not BinaryType"ScalaTestFailureLocation: org.apache.carbondata.integration.spark.testsuite.binary.TestBinaryDataType$$anonfun$27 at (TestBinaryDataType.scala:1410)org.scalatest.exceptions.TestFailedException: "Invalid call to name on unresolved object, tree: unresolvedalias(avg(substring(CUST_NAME#73, 1, 2)), None)" did not contain "cannot resolve 'avg(substring(uniqdata.`CUST_NAME`, 1, 2))' due to data type mismatch: function average requires numeric types, not BinaryType" at org.scalatest.Assertions$class.newAssertionFailedException(Assertions.scala:500) at org.scalatest.FunSuite.newAssertionFailedException(FunSuite.scala:1555) at org.scalatest.Assertions$AssertionsHelper.macroAssert(Assertions.scala:466) at org.apache.carbondata.integration.spark.testsuite.binary.TestBinaryDataType$$anonfun$27.apply$mcV$sp(TestBinaryDataType.scala:1410) at org.apache.carbondata.integration.spark.testsuite.binary.TestBinaryDataType$$anonfun$27.apply(TestBinaryDataType.scala:1352) at org.apache.carbondata.integration.spark.testsuite.binary.TestBinaryDataType$$anonfun$27.apply(TestBinaryDataType.scala:1352) at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22) at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85) at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104) at org.scalatest.Transformer.apply(Transformer.scala:22) at org.scalatest.Transformer.apply(Transformer.scala:20) at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166) at org.apache.spark.sql.test.util.CarbonFunSuite.withFixture(CarbonFunSuite.scala:41) at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:163) at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175) at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175) at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306) at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:175) at org.scalatest.FunSuite.runTest(FunSuite.scala:1555) at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208) at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208) at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413) at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401) at scala.collection.immutable.List.foreach(List.scala:381) at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401) at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396) at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483) at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:208) at org.scalatest.FunSuite.runTests(FunSuite.scala:1555) at org.scalatest.Suite$class.run(Suite.scala:1424) at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1555) at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212) at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212) at org.scalatest.SuperEngine.runImpl(Engine.scala:545) at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:212) at org.apache.carbondata.integration.spark.testsuite.binary.TestBinaryDataType.org$scalatest$BeforeAndAfterAll$$super$run(TestBinaryDataType.scala:33) at org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:257) at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:256) at org.apache.carbondata.integration.spark.testsuite.binary.TestBinaryDataType.run(TestBinaryDataType.scala:33) at org.scalatest.tools.SuiteRunner.run(SuiteRunner.scala:55) at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2563) at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2557) at scala.collection.immutable.List.foreach(List.scala:381) at org.scalatest.tools.Runner$.doRunRunRunDaDoRunRun(Runner.scala:2557) at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1044) at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1043) at org.scalatest.tools.Runner$.withClassLoaderAndDispatchReporter(Runner.scala:2722) at org.scalatest.tools.Runner$.runOptionallyWithPassFailReporter(Runner.scala:1043) at org.scalatest.tools.Runner$.run(Runner.scala:883) at org.scalatest.tools.Runner.run(Runner.scala) at org.jetbrains.plugins.scala.testingSupport.scalaTest.ScalaTestRunner.runScalaTest2(ScalaTestRunner.java:131) at org.jetbrains.plugins.scala.testingSupport.scalaTest.ScalaTestRunner.main(ScalaTestRunner.java:28)
issueID:CARBONDATA-3425
type:Sub-task
changed files:
texts:Add Documentation for MV datamap

issueID:CARBONDATA-3426
type:Bug
changed files:
texts:Fix Load performance degrade by fixing task distribution
Problem: Load performance degrade by fixing task distribution issue.Cause: Consider 3 node cluster (host name a,b,c with IP1, IP2, IP3 as ip address), to launch load task, host name is required from NewCarbonDataLoadRDD in getPreferredLocations(). But if the driver is a (IP1), result is IP1, b,c instead of a,b,c. Hence task was not launching to one executor which is same ip as driver.getLocalhostIPs is modified in current version recently and instead of IP it was returning address, hence local ip hostanme was removed instead of address.solution: Revert the change in getLocalhostIPs as it is not used in any other flow.   
issueID:CARBONDATA-3427
type:Improvement
changed files:
texts:Beautify DAG by showing less text

issueID:CARBONDATA-3429
type:Bug
changed files:tools/cli/src/main/java/org/apache/carbondata/tool/FileCollector.java
texts:CarbonCli on wrong segment path wrong error message is displayed
User executes the cli command to view the sort columns present in table segment but inputs incorrect segment path.The correct error message should be displayed stating that the "segment does not exist".
issueID:CARBONDATA-343
type:Improvement
changed files:
texts:Optimize the duplicated definition code in GlobalDictionaryUtil.scala
The two rows code have some duplicated definition:-----------------val table = carbonLoadModel.getCarbonDataLoadSchema.getCarbonTable.getAbsoluteTableIdentifier.getCarbonTableIdentifierval carbonTable = carbonLoadModel.getCarbonDataLoadSchema.getCarbonTable
issueID:CARBONDATA-3432
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
core/src/main/java/org/apache/carbondata/core/util/CarbonProperties.java
texts:Range Column compaction sending all the splits to all the executors one by one
Range Column compaction sending all the splits to all the executors one by one, instead can broadcast at once to all executors
issueID:CARBONDATA-3433
type:Bug
changed files:
texts:MV has issues when create on constant column, dupicate columns and limit queries
MV has issues when create on constant column, dupicate columns and limit queriestest case to reproducetest("test mv with duplicate columns in query and constant column") {    sql("drop table if exists maintable")    sql("create table maintable(name string, age int, add string) stored by 'carbondata'")    sql("create datamap dupli_mv using 'mv' as select name, sum(age),sum(age) from maintable group by name")    sql("create datamap constant_mv using 'mv' as select name, sum(1) ex1 from maintable group by name")    sql("insert into maintable select 'pheobe',31,'NY'")    val df1 = sql("select sum(age),name from maintable group by name")    val df2 = sql("select sum(age),sum(age),name from maintable group by name")    val df3 = sql("select name, sum(1) ex1 from maintable group by name")    val df4 = sql("select sum(1) ex1 from maintable group by name")    val analyzed1 = df1.queryExecution.analyzed    val analyzed2 = df2.queryExecution.analyzed    val analyzed3 = df3.queryExecution.analyzed    val analyzed4 = df4.queryExecution.analyzed    assert(TestUtil.verifyMVDataMap(analyzed1, "dupli_mv"))    assert(TestUtil.verifyMVDataMap(analyzed2, "dupli_mv"))    assert(TestUtil.verifyMVDataMap(analyzed3, "constant_mv"))    assert(TestUtil.verifyMVDataMap(analyzed4, "constant_mv"))  }
issueID:CARBONDATA-3434
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/datamap/DataMapProvider.java
texts:Fix Data Mismatch between MainTable and MV DataMap table during compaction

issueID:CARBONDATA-3436
type:Bug
changed files:
texts:update pre insert into rule as per spark
Problem: Carbon is not following hive syntax for one insert into strictly.Cause: For insert into, carbon has its own rule. CarbonAnalysisRule&#91;CarbonPreInsertionCasts&#93;In CarbonPreInsertionCasts, data type cast is missed. Hence carbon was not following hive syntax.solution:Add data type cast rule for insert into.Note: Change syntax for insert single record(for binary and complex type(array,struct,map)) Binary cannot accept value as integer, need to pass in quotes only. Earlier complex type was accepting null values for complex datatypes, now it won't accept null values for complex types as per Hive rule. Need to pass in default format only for date type and timestamp type for Complex type in insert command. For boolean type 't' is a valid entry for true and 'f' is a valid entry for false. For boolean type , any integer greater than 0 will be considered as true and 0 will be considered as false. In case of map,for same key. As per hive, the latter value should be inserted for the same key.Steps to reproduce:create table test(name string,id binary) stored by 'carbondata';insert into test select 'a',1;insert should throw cast exception as 1 is not binary. But it is passing now. 
issueID:CARBONDATA-3437
type:Bug
changed files:processing/src/main/java/org/apache/carbondata/processing/loading/parser/impl/MapParserImpl.java
texts:Map Implementation not correct
**Insert into map should override the old values in case of new duplicate value, which it was not doing.
issueID:CARBONDATA-344
type:Bug
changed files:
texts:Exists query not working
Whenever I m trying to execute 'exists' query, I'm getting the following error:Error: org.apache.spark.sql.AnalysisException:Unsupported language features in query: select id from emp2 where exists(select id from emp3 where emp2.id=emp3.id)TOK_QUERY 1, 0,29, 15  TOK_FROM 1, 4,6, 15    TOK_TABREF 1, 6,6, 15      TOK_TABNAME 1, 6,6, 15        emp2 1, 6,6, 15  TOK_INSERT 0, -1,29, 0    TOK_DESTINATION 0, -1,-1, 0      TOK_DIR 0, -1,-1, 0        TOK_TMP_FILE 0, -1,-1, 0    TOK_SELECT 1, 0,2, 7      TOK_SELEXPR 1, 2,2, 7        TOK_TABLE_OR_COL 1, 2,2, 7          id 1, 2,2, 7    TOK_WHERE 1, 8,29, 26      TOK_SUBQUERY_EXPR 1, 10,29, 26        TOK_SUBQUERY_OP 1, 10,10, 26          exists 1, 10,10, 26        TOK_QUERY 1, 11,29, 48          TOK_FROM 1, 16,18, 48            TOK_TABREF 1, 18,18, 48              TOK_TABNAME 1, 18,18, 48                emp3 1, 18,18, 48          TOK_INSERT 0, -1,28, 0            TOK_DESTINATION 0, -1,-1, 0              TOK_DIR 0, -1,-1, 0                TOK_TMP_FILE 0, -1,-1, 0            TOK_SELECT 1, 12,14, 40              TOK_SELEXPR 1, 14,14, 40                TOK_TABLE_OR_COL 1, 14,14, 40                  id 1, 14,14, 40            TOK_WHERE 1, 20,28, 66              = 1, 22,28, 66                . 1, 22,24, 63                  TOK_TABLE_OR_COL 1, 22,22, 59                    emp2 1, 22,22, 59                  id 1, 24,24, 64                . 1, 26,28, 71                  TOK_TABLE_OR_COL 1, 26,26, 67                    emp3 1, 26,26, 67                  id 1, 28,28, 72scala.NotImplementedError: No parse rules for ASTNode type: 864, text: TOK_SUBQUERY_EXPR :TOK_SUBQUERY_EXPR 1, 10,29, 26  TOK_SUBQUERY_OP 1, 10,10, 26    exists 1, 10,10, 26  TOK_QUERY 1, 11,29, 48    TOK_FROM 1, 16,18, 48      TOK_TABREF 1, 18,18, 48        TOK_TABNAME 1, 18,18, 48          emp3 1, 18,18, 48    TOK_INSERT 0, -1,28, 0      TOK_DESTINATION 0, -1,-1, 0        TOK_DIR 0, -1,-1, 0          TOK_TMP_FILE 0, -1,-1, 0      TOK_SELECT 1, 12,14, 40        TOK_SELEXPR 1, 14,14, 40          TOK_TABLE_OR_COL 1, 14,14, 40            id 1, 14,14, 40      TOK_WHERE 1, 20,28, 66        = 1, 22,28, 66          . 1, 22,24, 63            TOK_TABLE_OR_COL 1, 22,22, 59              emp2 1, 22,22, 59            id 1, 24,24, 64          . 1, 26,28, 71            TOK_TABLE_OR_COL 1, 26,26, 67              emp3 1, 26,26, 67            id 1, 28,28, 72" +org.apache.spark.sql.hive.HiveQl$.nodeToExpr(HiveQl.scala:1721)          ; (state=,code=0)
issueID:CARBONDATA-3440
type:Sub-task
changed files:core/src/main/java/org/apache/carbondata/core/indexstore/ExtendedBlockletWrapper.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockletDataMapFactory.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/CarbonTable.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonInputFormat.java
processing/src/main/java/org/apache/carbondata/processing/merger/CompactionType.java
core/src/main/java/org/apache/carbondata/core/util/SessionParams.java
core/src/main/java/org/apache/carbondata/hadoop/CarbonInputSplit.java
core/src/main/java/org/apache/carbondata/core/datamap/Segment.java
core/src/main/java/org/apache/carbondata/core/indexstore/ExtendedBlocklet.java
core/src/main/java/org/apache/carbondata/core/datamap/DistributableDataMapFormat.java
texts:Expose a DDL to add index size and data size to tableStatus for legacy segments
For legacy segments index size in not written to tablestatus due to which the distribution will go for a toss.To counter this a DDL will be exposed which will write the index size to the tablestatus for all the segments for a specified table.
issueID:CARBONDATA-3441
type:Bug
changed files:integration/hive/src/main/java/org/apache/carbondata/hive/MapredCarbonInputFormat.java
integration/hive/src/main/java/org/apache/carbondata/hive/CarbonHiveRecordReader.java
texts:Aggregate queries are failing on Reading from Hive
Aggregate queries are failing on Reading from Hive
issueID:CARBONDATA-3442
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
texts:Fix creating mv datamap with column name having length more than 128

issueID:CARBONDATA-3443
type:Improvement
changed files:
texts:Update hive guide with Read from hive

issueID:CARBONDATA-3444
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/datamap/DataMapStoreManager.java
texts:MV query fails for table having same col name and table name and other issues
1. MV creation fails for in join case for table having col name as table name2. MV query fails for query with cast expression presnt as projection in ctas query with aliasuse below test cases to reproduce the issue test("test cast expression with mv") {    sql("drop table IF EXISTS maintable")    sql("create table maintable (m_month bigint, c_code string, " +        "c_country smallint, d_dollar_value double, q_quantity double, u_unit smallint, b_country smallint, i_id int, y_year smallint) stored by 'carbondata'")    sql("insert into maintable select 10, 'xxx', 123, 456, 45, 5, 23, 1, 2000")    sql("drop datamap if exists da_cast")    sql("create datamap da_cast using 'mv' as select cast(floor((m_month +1000) / 900) * 900 - 2000 AS INT) as a, c_code as abc,m_month from maintable")    val df1 = sql(" select cast(floor((m_month +1000) / 900) * 900 - 2000 AS INT) as a ,c_code as abc  from maintable")    val df2 = sql(" select cast(floor((m_month +1000) / 900) * 900 - 2000 AS INT),c_code as abc  from maintable")    val analyzed1 = df1.queryExecution.analyzed    assert(TestUtil.verifyMVDataMap(analyzed1, "da_cast"))  }  test("test mv query when the column names and table name same in join scenario") {    sql("drop table IF EXISTS price")    sql("drop table IF EXISTS quality")    sql("create table price(product string,price int) stored by 'carbondata'")    sql("create table quality(product string,quality string) stored by 'carbondata'")    sql("create datamap same_mv using 'mv' as select price.product,price.price,quality.product,quality.quality from price,quality where price.product = quality.product")    val df1 = sql("select price.product from price,quality where price.product = quality.product")    val analyzed1 = df1.queryExecution.analyzed    assert(TestUtil.verifyMVDataMap(analyzed1, "same_mv"))  }
issueID:CARBONDATA-3445
type:Bug
changed files:
texts:In Aggregate query, CountStarPlan throws head of empty list error

issueID:CARBONDATA-3447
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/indexstore/ExtendedBlockletWrapper.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockletDataMapModel.java
hadoop/src/main/java/org/apache/carbondata/hadoop/CarbonMultiBlockSplit.java
processing/src/main/java/org/apache/carbondata/processing/merger/CarbonCompactionUtil.java
core/src/main/java/org/apache/carbondata/core/util/BlockletDataMapUtil.java
core/src/main/java/org/apache/carbondata/core/datamap/DataMapStoreManager.java
store/sdk/src/main/java/org/apache/carbondata/sdk/file/arrow/ArrowConverter.java
core/src/main/java/org/apache/carbondata/core/indexstore/BlockletDataMapIndexStore.java
core/src/main/java/org/apache/carbondata/core/stream/ExtendedByteArrayInputStream.java
core/src/main/java/org/apache/carbondata/core/scan/executor/impl/AbstractQueryExecutor.java
core/src/main/java/org/apache/carbondata/core/datastore/block/TableBlockInfo.java
core/src/main/java/org/apache/carbondata/core/statusmanager/FileFormat.java
core/src/main/java/org/apache/carbondata/core/util/CarbonProperties.java
integration/spark-common/src/main/java/org/apache/carbondata/spark/util/Util.java
core/src/main/java/org/apache/carbondata/core/datamap/DataMapUtil.java
core/src/main/java/org/apache/carbondata/core/scan/executor/util/QueryUtil.java
core/src/main/java/org/apache/carbondata/core/indexstore/BlockletDetailInfo.java
core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
core/src/main/java/org/apache/carbondata/core/stream/ExtendedByteArrayOutputStream.java
core/src/main/java/org/apache/carbondata/core/datamap/DistributableDataMapFormat.java
core/src/main/java/org/apache/carbondata/core/indexstore/ExtendedBlockletWrapperContainer.java
hadoop/src/main/java/org/apache/carbondata/hadoop/util/CarbonVectorizedRecordReader.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonTableInputFormat.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
core/src/main/java/org/apache/carbondata/core/stream/ExtendedDataInputStream.java
hadoop/src/main/java/org/apache/carbondata/hadoop/CarbonRecordReader.java
core/src/main/java/org/apache/carbondata/hadoop/CarbonInputSplit.java
core/src/main/java/org/apache/carbondata/core/indexstore/Blocklet.java
core/src/main/java/org/apache/carbondata/core/datamap/TableDataMap.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockDataMap.java
core/src/main/java/org/apache/carbondata/core/indexstore/ExtendedBlocklet.java
texts:Index Server Performance Improvement
Problem:When number of splits are high, index server performance is slow as compared to old flow(Driver caching). This is because data is transferred over network is more and causing performance bottleneck.   Solution: If data transferred is less we can sent through network, but when it grows we can write to file and only send file name and in Main driver it will read the file and construct input split.  Use snappy to compress the data, so data transferred through network/written to file size will be less, so IO time wont impact performance  In main driver pruning is done in multiple thread, added same for index executor as now index executor will do the pruning In case of block cache no need to send blockletdetailinfo object as size is more and same can be constructed in executor from file footer
issueID:CARBONDATA-3448
type:Bug
changed files:
texts:Wrong results in preaggregate query with spark adaptive execution
problem: Wrong results in preaggregate query with spark adaptive executionSpark2TestQueryExecutor.conf.set(SQLConf.ADAPTIVE_EXECUTION_ENABLED.key, "true") cause: For preaggreagate, segment info is set into threadLocal. when adaptive execution is called, spark is calling getInternalPartition in another thread where updated segment conf  is not set. Hence it is not using the updated segments. solution: CarbonScanRdd is already having the sessionInfo, use it instead of taking session info from the current thread.
issueID:CARBONDATA-3449
type:Bug
changed files:core/src/main/java/org/apache/carbondata/events/OperationListenerBus.java
texts:Initialization of listeners in case of concurrent scenrios is not synchronized

issueID:CARBONDATA-345
type:Test
changed files:
texts:improve code-coverage for core.carbon
Write Unit test case for core.carbon
issueID:CARBONDATA-3452
type:Bug
changed files:processing/src/main/java/org/apache/carbondata/processing/util/CarbonDataProcessorUtil.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonTableOutputFormat.java
texts:select query failure when substring on dictionary column with join
select query failure when substring on dictionary column with join"select a.ch from (select substring(s1,1,2) as ch from t1) a join t2 h on (a.ch = h.t2)"problem: select query failure when substring on dictionary column with join.cause: when dictionary include is present, data type is updated to int from string in plan attribute. so substring was unresolved on int column. Join operation try to reference this attribute which is unresolved.solution: skip updating datatype if dictionary is included in the plan 
issueID:CARBONDATA-3453
type:Bug
changed files:
texts:Set segment doesn&#39;t work with adaptive execution
steps:enable adaptive executioncreate table and load one segment data.set segment 0 and query --> gives dataset segment 1 and query --> gives data (but should not) problem: Set segment doesn't work with adaptive execution.cause: For set segments, driver will check carbon property and carbon property will look for segments in session params, which is not set in current thread incase of adaptive excution. solution: Use the session params from RDD's session info.
issueID:CARBONDATA-3454
type:Sub-task
changed files:core/src/main/java/org/apache/carbondata/core/indexstore/ExtendedBlockletWrapperContainer.java
core/src/main/java/org/apache/carbondata/core/indexstore/ExtendedBlockletWrapper.java
core/src/main/java/org/apache/carbondata/core/datamap/DataMapJob.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonTableInputFormat.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonInputFormat.java
core/src/main/java/org/apache/carbondata/core/datamap/DataMapUtil.java
core/src/main/java/org/apache/carbondata/core/indexstore/ExtendedBlocklet.java
core/src/main/java/org/apache/carbondata/core/datamap/DistributableDataMapFormat.java
texts:Optimize the performance of select count(*) for index server
Currently all the extended blocklets are being returned to the main driver in case of count . But all this information is not required for count case, therefore the optimal thing would be to send only the requried info.
issueID:CARBONDATA-3455
type:Bug
changed files:
texts:Job Group ID is not displayed in the IndexServer
Job Group ID is not displayed in the IndexServer
issueID:CARBONDATA-3456
type:Bug
changed files:integration/spark2/src/main/java/org/apache/carbondata/datamap/IndexDataMapProvider.java
datamap/bloom/src/main/java/org/apache/carbondata/datamap/bloom/BloomCoarseGrainDataMapFactory.java
processing/src/main/java/org/apache/carbondata/processing/merger/CarbonDataMergerUtil.java
core/src/main/java/org/apache/carbondata/core/metadata/SegmentFileStore.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonTableInputFormat.java
core/src/main/java/org/apache/carbondata/core/datamap/dev/DataMapSyncStatus.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonOutputCommitter.java
core/src/main/java/org/apache/carbondata/core/statusmanager/SegmentStatusManager.java
core/src/main/java/org/apache/carbondata/core/datamap/DataMapUtil.java
datamap/lucene/src/main/java/org/apache/carbondata/datamap/lucene/LuceneDataMapFactoryBase.java
core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
core/src/main/java/org/apache/carbondata/core/datamap/DataMapProvider.java
texts:Fix DataLaoding on MV when Yarn-Application is killed

issueID:CARBONDATA-3457
type:Bug
changed files:
texts:[MV]Fix Column not found with Cast Expression

issueID:CARBONDATA-3458
type:Bug
changed files:
texts:Running load, insert , CTAS command on carbon table sets double Execution ID info, and ID of CTAS is null

issueID:CARBONDATA-3459
type:Sub-task
changed files:
texts:Fixed id based distribution for show cache command
Currently tasks are not being fired based on the executor ID because getPrefferedLocation was not overridden.
issueID:CARBONDATA-346
type:Test
changed files:
texts:Update unit test for core module
Improve code coverage for util package.
issueID:CARBONDATA-3460
type:Bug
changed files:core/src/main/java/org/apache/carbondata/hadoop/CarbonInputSplit.java
core/src/main/java/org/apache/carbondata/core/indexstore/ExtendedBlocklet.java
texts:EOF exception is thrown when quering using index server
java.lang.UnsupportedOperationException: Unsupported columnar format version: 29556 at org.apache.carbondata.core.metadata.ColumnarFormatVersion.valueOf(ColumnarFormatVersion.java:52) at org.apache.carbondata.hadoop.CarbonInputSplit.readFields(CarbonInputSplit.java:357) at org.apache.carbondata.hadoop.CarbonMultiBlockSplit.readFields(CarbonMultiBlockSplit.java:154) at org.apache.hadoop.io.ObjectWritable.readObject(ObjectWritable.java:285) at org.apache.hadoop.io.ObjectWritable.readFields(ObjectWritable.java:77) at org.apache.spark.SerializableWritable$$anonfun$readObject$1.apply$mcV$sp(SerializableWritable.scala:45) at org.apache.spark.SerializableWritable$$anonfun$readObject$1.apply(SerializableWritable.scala:41) at org.apache.spark.SerializableWritable$$anonfun$readObject$1.apply(SerializableWritable.scala:41) at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1390) at org.apache.spark.SerializableWritable.readObject(SerializableWritable.scala:41) at sun.reflect.GeneratedMethodAccessor65.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:497) at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1017) at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1896) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351) at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1993) at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1918) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351) at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1993) at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1918) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351) at java.io.ObjectInputStream.readObject(ObjectInputStream.java:371) at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75) at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:114) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:313) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745)2019-06-27 12:44:07 ERROR Executor:91 - Exception in task 1472.0 in stage 10.0 (TID 3317)java.lang.IllegalStateException: unread block data
issueID:CARBONDATA-3462
type:Sub-task
changed files:core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
texts:Add usage and deployment document for index server

issueID:CARBONDATA-3466
type:Bug
changed files:tools/cli/src/main/java/org/apache/carbondata/tool/CarbonCli.java
texts:Fix NPE for carboncli command

issueID:CARBONDATA-3467
type:Bug
changed files:
texts:Fix count(*) with filter on string value

issueID:CARBONDATA-347
type:Improvement
changed files:
texts:Remove HadoopFileInputMeta
in kettle step,  HadoopFileInputMeta is useless, so, should remove related  HadoopFileInputStep from GrapGenerator and remove related codes.
issueID:CARBONDATA-3473
type:Bug
changed files:tools/cli/src/main/java/org/apache/carbondata/tool/DataFile.java
texts:Fix data size calcution of the last column in CarbonCli

issueID:CARBONDATA-3474
type:Bug
changed files:
texts:Fix validate mvQuery having filter expression and correct error message

issueID:CARBONDATA-3476
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/scan/scanner/impl/BlockletFilterScanner.java
texts:Read time and scan time stats shown wrong in executor log for filter query
Problem: Read time and scan time stats shown wrong in executor log for filter queryRoot cause: Projection read time is added in scan time because of this scan time and read time is not correct in statsSolution: Added projection read time for both measure and dimension column in read stats
issueID:CARBONDATA-3477
type:Bug
changed files:
texts:Throw out exception when use sql: &#39;update table select\n...&#39;
When use below sql to update table:UPDATE IUD_table2 a SET (a.IUD_table2_country, a.IUD_table2_salary) = (select b.IUD_table1_country, b.IUD_table1_salary from IUD_table1 b where b.IUD_table1_id = 8) WHERE a.IUD_table2_id < 6 or a.IUD_table2_id > 15It will throw out exception: Exception in thread "main" org.apache.spark.sql.AnalysisException: == Parse1 ==mismatched input '.' expecting <EOF>(line 2, pos 1)== SQL == select select b.IUD_table1_country, b.IUD_table1_salary from IUD_table1 b where b.IUD_table1_id = 8 from iud_table2 a -^^^== Parse2 == [1.1] failure: identifier matching regex (?i)ALTER expectedselect select 
issueID:CARBONDATA-3478
type:Bug
changed files:processing/src/main/java/org/apache/carbondata/processing/merger/CarbonCompactionExecutor.java
texts:Fix ArrayIndexOutOfBoundsException issue on compaction after alter rename operation
Please find the steps to reproduce the issue: Create table having dimension and measure columns load data rename / add column/ drop column in table alter set tblproperties('sort_columns'=measure column', 'sort_scope'='local_sort') load data perform compaction and find the exception below   Driver stacktrace: 2019-07-26 19:34:03 ERROR CarbonAlterTableCompactionCommand:345 - Exception in start compaction thread. java.lang.Exception: Exception in compaction Job aborted due to stage failure: Task 0 in stage 6.0 failed 1 times, most recent failure: Lost task 0.0 in stage 6.0 (TID 6, localhost, executor driver): java.lang.ArrayIndexOutOfBoundsException: 3 at org.apache.carbondata.core.scan.wrappers.ByteArrayWrapper.getNoDictionaryKeyByIndex(ByteArrayWrapper.java:81) 
issueID:CARBONDATA-348
type:Improvement
changed files:
texts:Remove useless step in kettle and delete them in plugin.xml
Remove useless step in kettle and delete related plugin.xmllike CarbonSortKeyAndGroupByStepMeta and so on.
issueID:CARBONDATA-3480
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/datamap/DataMapStoreManager.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/CarbonTable.java
core/src/main/java/org/apache/carbondata/core/metadata/CarbonMetadata.java
core/src/main/java/org/apache/carbondata/core/datamap/DataMapFilter.java
core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/TableSchema.java
texts:Remove Modified MDT and make relation refresh only when schema file is modified.

issueID:CARBONDATA-3481
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/datamap/TableDataMap.java
texts:Multi-thread pruning fails when datamaps count is just near numOfThreadsForPruning
Problem : Multi-thread pruning fails when datamaps count is just near numOfThreadsForPruning.Cause : When the datamaps count is just near numOfThreadsForPruning,As code is checking '>= ', last thread may not get the datamaps for prune. Hence array out of index exception is thrown in this scenario.There is no issues with higher number of datamaps.solution: In this scenario launch threads based on the distribution value, not on the hardcoded value steps to reproduce:5 index files with total 0.32 million data file (each has 50k, 120k, 50k, 60k, 50k each)with default thread count as 4, array of bound index is observed for below line.final List<SegmentDataMapGroup> segmentDataMapGroups = datamapListForEachThread.get;
issueID:CARBONDATA-3482
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockDataMap.java
core/src/main/java/org/apache/carbondata/core/datastore/block/SegmentPropertiesAndSchemaHolder.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockletDataMap.java
core/src/main/java/org/apache/carbondata/core/indexstore/BlockletDataMapIndexStore.java
texts:Null pointer exception when concurrent select queries are executed from different beeline terminals.
Beeline1: => create tables (1K )2. Beeline2 => insert into table t2 (only 1 records ) till 7K3. Concurrent queriesq1 : select count from t1q2 : select * from t1 limit 1q3 : select count from t2q2 : select * from t2 limit 1 Exception:java.lang.NullPointerException at org.apache.carbondata.core.indexstore.blockletindex.BlockDataMap.getFileFooterEntrySchema(BlockDataMap.java:1061) at org.apache.carbondata.core.indexstore.blockletindex.BlockDataMap.prune(BlockDataMap.java:727) at org.apache.carbondata.core.indexstore.blockletindex.BlockDataMap.prune(BlockDataMap.java:821) at org.apache.carbondata.core.indexstore.blockletindex.BlockletDataMapFactory.getAllBlocklets(BlockletDataMapFactory.java:446) at org.apache.carbondata.core.datamap.TableDataMap.pruneWithoutFilter(TableDataMap.java:156) at org.apache.carbondata.core.datamap.TableDataMap.prune(TableDataMap.java:143) at org.apache.carbondata.hadoop.api.CarbonInputFormat.getPrunedBlocklets(CarbonInputFormat.java:563) at org.apache.carbondata.hadoop.api.CarbonInputFormat.getDataBlocksOfSegment(CarbonInputFormat.java:471) at org.apache.carbondata.hadoop.api.CarbonTableInputFormat.getSplits(CarbonTableInputFormat.java:471) at org.apache.carbondata.hadoop.api.CarbonTableInputFormat.getSplits(CarbonTableInputFormat.java:199) at org.apache.carbondata.spark.rdd.CarbonScanRDD.internalGetPartitions(CarbonScanRDD.scala:141) at org.apache.carbondata.spark.rdd.CarbonRDD.getPartitions(CarbonRDD.scala:66) at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:256) at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:254)
issueID:CARBONDATA-3483
type:Bug
changed files:
texts:Can not run horizontal compaction when execute update sql
After PR#3166, horizontal compaction will not actually run when execute update sql.When it runs update sql and will run horizontal compaction if needs, it will require update.lock and compaction.lock when execute CarbonAlterTableCompactionCommand.alterTableForCompaction, but these two locks already are locked when it starts to execute update sql. so it will require locks failed and can't execute compaction.
issueID:CARBONDATA-3486
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/scan/filter/FilterUtil.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/resolverinfo/MeasureColumnResolvedFilterInfo.java
texts:Serialization/ deserialization issue with Datatype
When we use old store and do alter add sort columns on it then query on the old segment, serialization/de-serialization issue comes for Filter Column of Measure type which has been changed in Sort Column as it is being de-serialized by ObjectSerialization. This fails the check and the query.
issueID:CARBONDATA-3487
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/util/TaskMetricsMap.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
integration/spark-common/src/main/scala/org/apache/carbondata/spark/InitInputMetrics.java
core/src/main/java/org/apache/carbondata/core/util/CarbonProperties.java
texts:wrong Input metrics (size/record) displayed in spark UI during insert into
create a carbon table insert huge data (2 Billion row)  to carbon table.observe the metrics in spark UI. Both the size and record count in input metrics is wrong during insert into scenario.
issueID:CARBONDATA-3488
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
texts:Check the file size after move local file to carbon path
Problem:One user met an issue: the row num saved in carbonindex file is non zero but the file size of relevant carbondata file is 0. Solution:In CarbonUtil.copyCarbonDataFileToCarbonStorePath, check the file size of carbon file whether is the same as the size fo local file after move local file to carbon path.
issueID:CARBONDATA-3489
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/util/comparator/Comparator.java
texts:Optimizing the performance of sorting
Optimizing the performance of sortingRoot cause: In case of sorting in the comparator classes(NewRowComparator, RawRowComparator, IntermediateSortTempRowComparator and  UnsafeRowComparator) a new SerializableComparator object is been created in the compare method everytime two objects are passed for comparison.Solution: We can reduce the number of SerializeableComparator objects that are been created by storing the SerializeableComparators of primitive datatypes  in a map and getting it from the map instead of creating a new SerializeableComparator everytime.
issueID:CARBONDATA-3490
type:Bug
changed files:
texts:Concurrent data load failure with carbondata FileNotFound exception
Caused by: org.apache.carbondata.core.datastore.exception.CarbonDataWriterException: Problem while copying file from local store to carbon store at org.apache.carbondata.core.util.CarbonUtil.copyCarbonDataFileToCarbonStorePath(CarbonUtil.java:2750) at org.apache.carbondata.processing.store.writer.AbstractFactDataWriter.commitCurrentFile(AbstractFactDataWriter.java:283) at org.apache.carbondata.processing.store.writer.v3.CarbonFactDataWriterImplV3.closeWriter(CarbonFactDataWriterImplV3.java:393) ... 11 moreCaused by: java.io.FileNotFoundException: /tmp/carbon865982118689228_1/Fact/Part0/Segment_6/1/part-0-1_batchno0-0-6-1565329654844.carbondata (No such file or directory) at java.io.FileInputStream.open0(Native Method) at java.io.FileInputStream.open(FileInputStream.java:195) at java.io.FileInputStream.<init>(FileInputStream.java:138) at java.io.FileInputStream.<init>(FileInputStream.java:93) at org.apache.carbondata.core.datastore.filesystem.LocalCarbonFile.getDataInputStream(LocalCarbonFile.java:309) at org.apache.carbondata.core.datastore.filesystem.LocalCarbonFile.getDataInputStream(LocalCarbonFile.java:299) at org.apache.carbondata.core.datastore.impl.FileFactory.getDataInputStream(FileFactory.java:179) at org.apache.carbondata.core.datastore.impl.FileFactory.getDataInputStream(FileFactory.java:175) at org.apache.carbondata.core.util.CarbonUtil.copyLocalFileToCarbonStore(CarbonUtil.java:2781) at org.apache.carbondata.core.util.CarbonUtil.copyCarbonDataFileToCarbonStorePath(CarbonUtil.java:2746) ... 13 moreproblem: When two load is happening concurrently, one load is cleaning the temp directory of the concurrent loadcause: temp directory to store the carbon files is created using system.get nano time, due to this two load have same store location. when one load is completed, it cleaned the temp directory. causing dataload failure for other load.solution:use UUID instead of nano time while creating the temp directory to have each load a unique directory.
issueID:CARBONDATA-3492
type:Bug
changed files:processing/src/main/java/org/apache/carbondata/processing/datamap/DataMapWriterListener.java
core/src/main/java/org/apache/carbondata/core/util/CarbonProperties.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonInputFormat.java
core/src/main/java/org/apache/carbondata/core/datamap/DataMapUtil.java
core/src/main/java/org/apache/carbondata/core/datamap/DistributableDataMapFormat.java
texts:Cache Pre-Priming
Currently, we have an index server which basically helps in distributed caching of the datamaps in a separate spark application.The caching of the datamaps in index server will start once the query is fired on the table for the first time, all the datamaps will be loadedif the count is fired and only required will be loaded for any filter query.Here the problem or the bottleneck is, until and unless the query is fired on table, the caching won’t be done for the table datamaps.So consider a scenario where we are just loading the data to table for whole day and then next day we query,so all the segments will start loading into cache. So first time the query will be slow.What if we load the datamaps into cache or preprime the cache without waititng for any query on the table?Yes, what if we load the cache after every load is done, what if we load the cache for all the segments at once,so that first time query need not do all this job, which makes it faster.Here i have attached the design document for the pre-priming of cache into index server. Please have a look at it
issueID:CARBONDATA-3493
type:Bug
changed files:
texts:Carbon query fails when enable.query.statistics is true in specific scenario.

issueID:CARBONDATA-3494
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/datamap/DataMapStoreManager.java
texts:Nullpointer exception in case of drop table
Drop table is failing with nullpointer in below scenario with datamapThis issue will happen once the PR #3339 is merged, because with that PR refresh table happens correctly and then this fails.CREATE TABLE datamap_test_1 (id int,name string,salary float,dob date)STORED BY 'carbondata' TBLPROPERTIES('SORT_COLUMNS'='id');CREATE DATAMAP dm_datamap_test_1_2 ON TABLE datamap_test_1 USING 'bloomfilter' DMPROPERTIES ('INDEX_COLUMNS' = 'salary,name', 'BLOOM_SIZE'='640000', 'BLOOM_FPP'='0.00001', 'BLOOM_COMPRESS'='true');CREATE DATAMAP dm_datamap_test3 ON TABLE datamap_test_1 USING 'bloomfilter' DMPROPERTIES ('INDEX_COLUMNS' = 'dob', 'BLOOM_SIZE'='640000', 'BLOOM_FPP'='0.00001', 'BLOOM_COMPRESS'='true');drop table if exists datamap_test_1;
issueID:CARBONDATA-3495
type:Bug
changed files:processing/src/main/java/org/apache/carbondata/processing/datatypes/PrimitiveDataType.java
core/src/main/java/org/apache/carbondata/core/datastore/page/ComplexColumnPage.java
core/src/main/java/org/apache/carbondata/core/util/DataTypeUtil.java
texts:Insert into Complex data type of Binary fails with Carbon & SparkFileFormat

issueID:CARBONDATA-3496
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/datamap/DataMapStoreManager.java
core/src/main/java/org/apache/carbondata/core/util/SessionParams.java
processing/src/main/java/org/apache/carbondata/processing/datamap/DataMapWriterListener.java
texts:FileNotFoundException thrown for concurrent queries with index server
/user/hive/warehouse/carbon.store/default/uniqdata_ec483d0d-dee2-44ce-8b3a-a829d445e6b6/Fact/Part0/Segment_1/part-0-1_batchno0-0-1-1566391022190.carbondata at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:74) at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:64) at org.apache.hadoop.hdfs.server.namenode.FSDirStatAndListingOp.getBlockLocations(FSDirStatAndListingOp.java:648) at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1736) at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:712) at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:402) at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java) at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:973) at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2260) at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2256) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1781) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2254)
issueID:CARBONDATA-3499
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/datastore/impl/DefaultFileTypeProvider.java
texts:Fix insert failure with customFileProvider
problem: Below exception is thrown when the custom file system is used with first time insert randomly.IllegalArgumentException("Path belongs to unsupported file system") from FileFactory.getFileType() cause:DefaultFileTypeProvider.initializeCustomFileProvider is called concurrently insert. Hence one thread got the provider and other thread didn't get as flag is set to true.so other thread failed as it tried with default provider. solution:synchronize the initialization of custom file provider
issueID:CARBONDATA-35
type:New Feature
changed files:
texts:generate global dict using pre-defined dict from external column file
user can set colName:columnfilePath in load DML, which can provide small amount of distinct values, then carbon can use these distinct values to generate dictionary and avoid reading from large raw csv file. this is a new feature and can improve the performance.
issueID:CARBONDATA-350
type:Improvement
changed files:
texts:Remove org.apache.carbondata.processing.sortdatastep
the content of  org.apache.carbondata.processing.sortdatastep is duplicated with org.apache.carbondata.processing.sortandgroupby.sortdatastep, so i think the content of both can be merged, and  org.apache.carbondata.processing.sortdatastep can be removed.
issueID:CARBONDATA-3501
type:Improvement
changed files:
texts:Support to execute update sql on table with long_string field (Not update long_string field)
When execute update sql (not update long_string field) on table with long_string field, it fail.
issueID:CARBONDATA-3502
type:Bug
changed files:
texts:Select query fails with UDF having Match expression inside IN expression
Select query fails with UDF having Match expression inside IN expression and throws ArrayIndexOutOfBounds exception
issueID:CARBONDATA-3505
type:Bug
changed files:
texts:Fixed drop database cascade issue when 2 database point to same location.
Steps to reproduce:  create database x location '/x/table1 create database x1 location '/x/table1' create table in x and x1 drop database x cascade drop database x1 cascade.
issueID:CARBONDATA-3506
type:Bug
changed files:
texts:Alter table add, drop, rename and datatype change fails with hive compatile property
1. Alter table add, drop, rename and datatype change fails on partition table with hive compatile propertywhen hive.metastore.disallow.incompatible.col.type.changes is set true, add column or any alter fails on parition table in spark 2.2 and above2. when table has only two columns with one as partition column, if we allow dropping of non partition column, then it will like table with all the columns as partition column , which is invalid, and fails with above property as true. so block this operation.
issueID:CARBONDATA-3507
type:Bug
changed files:
texts:Create Table As Select Fails in Spark-2.3
CTAS fails due to wrong file path
issueID:CARBONDATA-3508
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/indexstore/BlockletDataMapIndexStore.java
datamap/bloom/src/main/java/org/apache/carbondata/datamap/bloom/BloomCoarseGrainDataMap.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonInputFormat.java
texts:Select query fails when the cg datamap is dropped concurrently while running the select query on filter column on which datamap is created
In this scenario, the select query fails with an exception saying "Error occurs while reading bloom index"
issueID:CARBONDATA-351
type:Improvement
changed files:
texts:name of thrift file is not unified
in carbon-format module, some file name is not unified.for example,  carbondataindex.thrift can be changed to  carbondata_index.thrift ,  dictionary_meta.thrift  can be changed to dictionary_metadata.thrift.. and so on .
issueID:CARBONDATA-3512
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/datamap/DataMapUtil.java
texts:Index Server Enchantment
Remove the keytab dependency for IndexServer. Authentication Enchantment. Authorization(ACL) Enchantment.
issueID:CARBONDATA-3513
type:Bug
changed files:processing/src/main/java/org/apache/carbondata/processing/merger/AbstractResultProcessor.java
texts:can not run major compaction when using hive partition table
Major compaction command runs error.ERROR information:2019-09-03 13:35:49 INFO  BlockManagerInfo:54 - Added broadcast_0_piece0 in memory on czh-yhfx-redis1:41430 (size: 26.4 KB, free: 5.2 GB)2019-09-03 13:35:49 INFO  BlockManagerInfo:54 - Added broadcast_0_piece0 in memory on czh-yhfx-redis1:41430 (size: 26.4 KB, free: 5.2 GB)2019-09-03 13:35:52 WARN  TaskSetManager:66 - Lost task 1.0 in stage 0.0 (TID 1, czh-yhfx-redis1, executor 1): java.lang.NumberFormatException: For input string: "32881200100001100000" at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65) at java.lang.Long.parseLong(Long.java:592) at java.lang.Long.parseLong(Long.java:631) at org.apache.carbondata.core.util.path.CarbonTablePath$DataFileUtil.getTaskIdFromTaskNo(CarbonTablePath.java:503) at org.apache.carbondata.processing.store.CarbonFactDataHandlerModel.getCarbonFactDataHandlerModel(CarbonFactDataHandlerModel.java:396) at org.apache.carbondata.processing.merger.RowResultMergerProcessor.<init>(RowResultMergerProcessor.java:86) at org.apache.carbondata.spark.rdd.CarbonMergerRDD$$anon$1.<init>(CarbonMergerRDD.scala:213) at org.apache.carbondata.spark.rdd.CarbonMergerRDD.internalCompute(CarbonMergerRDD.scala:86) at org.apache.carbondata.spark.rdd.CarbonRDD.compute(CarbonRDD.scala:82) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) at org.apache.spark.rdd.RDD.iterator(RDD.scala:287) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87) at org.apache.spark.scheduler.Task.run(Task.scala:108) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:748) version:apache-carbondata-1.5.1-bin-spark2.2.1-hadoop2.7.2.jartable is a hive partition table. 
issueID:CARBONDATA-3515
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
core/src/main/java/org/apache/carbondata/core/localdictionary/dictionaryholder/MapBasedDictionaryStore.java
core/src/main/java/org/apache/carbondata/core/util/CarbonProperties.java
texts:Limit local dictionary size to 10% of allowed blocklet size
problem: currently local dictionary max size is 2GB, because of this for varchar columns or long string columns, local dictionary can be of 2GB size. so, as local dictionary is stored in blocklet. blocklet size will exceed 2 GB, even though configured maximum blocklet size is 64MB. some places inter overflow happens during casting. solution: limit the local dictionary size to 10% of maximum allowed blocklet size
issueID:CARBONDATA-3516
type:New Feature
changed files:core/src/main/java/org/apache/carbondata/core/metadata/SegmentFileStore.java
core/src/main/java/org/apache/carbondata/core/datamap/Segment.java
core/src/main/java/org/apache/carbondata/core/statusmanager/LoadMetadataDetails.java
core/src/main/java/org/apache/carbondata/core/datamap/TableDataMap.java
texts:Support heterogeneous format segments in carbondata
Already existing customers use other formats like parquet, orc etc., but if they want to migrate to carbon there is no proper solution at hand. So this feature allows all the old data to add as a segment to carbondata .  And during query, it reads old data in its respective format and all new segments will be read in carbon. 
issueID:CARBONDATA-3517
type:Sub-task
changed files:processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactDataHandlerModel.java
core/src/main/java/org/apache/carbondata/core/metadata/SegmentFileStore.java
processing/src/main/java/org/apache/carbondata/processing/loading/DataLoadProcessBuilder.java
core/src/main/java/org/apache/carbondata/core/statusmanager/FileFormat.java
streaming/src/main/java/org/apache/carbondata/streaming/segment/StreamSegment.java
processing/src/main/java/org/apache/carbondata/processing/loading/CarbonDataLoadConfiguration.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockDataMap.java
core/src/main/java/org/apache/carbondata/core/datamap/Segment.java
core/src/main/java/org/apache/carbondata/core/statusmanager/LoadMetadataDetails.java
core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
core/src/main/java/org/apache/carbondata/core/datastore/block/SegmentPropertiesAndSchemaHolder.java
texts:Support add segment feature in carbon
We have a scenario like carbondata files are generated externally through SDK and if we want to add that data to the existing table then there is no proper way.  And also already existing data in other formats cannot be added to carbondata. To support above both the requirements we need to support Add segment feature with the following syntax.Alter table test add segment options (‘path’= 'hdfs://usr/oldtable,'format'=parquet) 
issueID:CARBONDATA-3518
type:New Feature
changed files:core/src/main/java/org/apache/carbondata/core/metadata/schema/table/TableSchema.java
texts:Support create table like command

issueID:CARBONDATA-3520
type:Bug
changed files:
texts:CTAS should fail if select query contains duplicate columns

issueID:CARBONDATA-3521
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/util/CarbonProperties.java
texts:optimize read property file code
Some points1) change System.getProperty(key) to System.getProperty(key, default)2) optimize duplicated code, like CARBON_PROPERTIES_FILE_PATH_DEFAULT3) change default carbon properties file path
issueID:CARBONDATA-3523
type:Improvement
changed files:processing/src/main/java/org/apache/carbondata/processing/store/writer/v3/CarbonFactDataWriterImplV3.java
core/src/main/java/org/apache/carbondata/core/metadata/index/BlockIndexInfo.java
core/src/main/java/org/apache/carbondata/core/util/BlockletDataMapUtil.java
core/src/main/java/org/apache/carbondata/core/util/CarbonMetadataUtil.java
processing/src/main/java/org/apache/carbondata/processing/store/writer/AbstractFactDataWriter.java
core/src/main/java/org/apache/carbondata/core/datastore/block/TableBlockInfo.java
core/src/main/java/org/apache/carbondata/core/util/AbstractDataFileFooterConverter.java
texts:Should store file size into index file

issueID:CARBONDATA-3524
type:Improvement
changed files:
texts:support compaction by GLOBAL_SORT
&#91;Backgroud&#93;For GLOBAL_SORT table,  now the segments will be compact in LOCAL_SORT. &#91;Motivation&#93;After compaction,  maybe it will impact query performance. Better to use GLABOL_SORT compaction to improve the performance.
issueID:CARBONDATA-3526
type:Bug
changed files:
texts:Cache issue and select query failure with multiple updates
Problem:When multiple updates happen on table, cache is loaded during update operation, but since on second update the horizontal compaction happens inside the segment, already loaded into cache are invalid. So if we do clean files, physical deletion of horizontal compacted takes place, but still the cache contains old files.So when select query is fired, query fails with file not found exception.
issueID:CARBONDATA-3527
type:Bug
changed files:
texts:Throw &#39;String length cannot exceed 32000 characters&#39; exception when load data with &#39;GLOBAL_SORT&#39; from csv which include big complex type data
Problem:When complex type data is used more than 32000 characters to indicate in csv file, and load data with 'GLOBAL_SORT' from these csv files, it will throw 'String length cannot exceed 32000 characters' exception.Cause:Use 'GLOBAL_SORT' to load data from csv files, it reads files and firstly store data in StringArrayRow, the type of all data are string, when call 'CarbonScalaUtil.getString' in 'NewRddIterator.next', it will check the length of all data and throw 'String length cannot exceed 32000 characters' exception even if it's complex type data which store as more than 32000 characters in csv files.Solution:In 'FieldConverter.objectToString' (called in 'CarbonScalaUtil.getString'), if the data type of field is complex type, don't check the length.
issueID:CARBONDATA-3529
type:Bug
changed files:
texts:Block Add Partition directly on MV datamap partitioned table

issueID:CARBONDATA-353
type:Improvement
changed files:
texts:Update doc for dateformat option
Update doc for dateformat option
issueID:CARBONDATA-3530
type:Sub-task
changed files:core/src/main/java/org/apache/carbondata/core/datamap/DataMapStoreManager.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/DataMapSchema.java
texts:Create timeseries MV Datamap (Only Non-lazy) with the supported granularity levels.
Create timeseries MV Datamap (Only Non-lazy) with the supported granularity levels.
issueID:CARBONDATA-3531
type:Sub-task
changed files:core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
core/src/main/java/org/apache/carbondata/core/preagg/TimeSeriesFunctionEnum.java
core/src/main/java/org/apache/carbondata/core/preagg/DaysOfWeekEnum.java
core/src/main/java/org/apache/carbondata/core/preagg/TimeSeriesUDF.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/datamap/Granularity.java
texts:Dataload and rebuild to datamap(Auto dataloding to datamap or downsampling).
Dataload and rebuild to datamap(Auto dataloding to datamap or downsampling).
issueID:CARBONDATA-354
type:Bug
changed files:
texts:Query execute successfully even not argument given in count function
When I am executing following command:select count() from tableName;It gave me no error and execute successfully but it gives following exception when I execute the same in Hive:FAILED: UDFArgumentException Argument expected
issueID:CARBONDATA-3541
type:Bug
changed files:
texts:Select queries with Aggregation Functions such as variance, stddev,etc fails with MV datamap

issueID:CARBONDATA-3542
type:Improvement
changed files:integration/hive/src/main/java/org/apache/carbondata/hive/CarbonMapInspector.java
integration/hive/src/main/java/org/apache/carbondata/hive/CarbonObjectInspector.java
integration/hive/src/main/java/org/apache/carbondata/hive/CarbonDictionaryDecodeReadSupport.java
texts:Support Map data type reading through Hive

issueID:CARBONDATA-3544
type:Improvement
changed files:tools/cli/src/main/java/org/apache/carbondata/tool/DataSummary.java
tools/cli/src/main/java/org/apache/carbondata/tool/CarbonCli.java
tools/cli/src/main/java/org/apache/carbondata/tool/FileCollector.java
texts:CLI should support a option to show statistics for all columns
better to add option -C to show statistics for all columns
issueID:CARBONDATA-3549
type:Improvement
changed files:
texts:How to build carbondata-1.6.0 with spark-2.1.1
I'm building carbondata-1.6.0-rc3 with spark-2.1.1, and I found errors as below:[ERROR] /carbondata-root-1.6.0/integration/spark2/src/main/spark2.1/org/apache/spark/sql/hive/CreateCarbonSourceTableAsSelectCommand.scala:153: error: scrutinee is incompatible with pattern type;[ERROR] /carbondata-root-1.6.0/integration/spark2/src/main/spark2.1/org/apache/spark/sql/hive/CreateCarbonSourceTableAsSelectCommand.scala:153: error: scrutinee is incompatible with pattern type;[INFO] found : org.apache.spark.sql.execution.datasources.HadoopFsRelation[INFO] required: Unit[INFO] case fs:HadoopFsRelation if table.partitionColumnNames.nonEmpty &&[INFO] ^[WARNING] three warnings foundFinally I found the problem. In spark-2.1.1, org.apache.spark.sql.execution.datasources.Datasource#write has no return result, which in spark-2.1.0 has a BaseRelation as return.spark-2.1.0: /** Writes the given [[DataFrame]] out to this [[DataSource]]. */def write(    mode: SaveMode,    data: DataFrame): BaseRelation = {  if (data.schema.map(_.dataType).exists(_.isInstanceOf[CalendarIntervalType])) {    throw new AnalysisException("Cannot save interval data type into external storage.")  }spark-2.1.1 /** * Writes the given [[DataFrame]] out to this [[DataSource]]. */def write(mode: SaveMode, data: DataFrame): Unit = {  if (data.schema.map(_.dataType).exists(_.isInstanceOf[CalendarIntervalType])) {    throw new AnalysisException("Cannot save interval data type into external storage.")  } so when we build carbondata with spark-2.1.1, this method will give Exception in this code, because the val result's type is Unit in spark-2.1.1. I checked this method DataSource#write in spark-2.1.1 found it has been replaced by writeAndRead. So I have to modify org.apache.spark.sql.hive.CreateCarbonSourceTableAsSelectCommand on line 146, change dataSource.write(mode, df) to dataSource.writeAndRead(mode, df) After that the problem was resolved.val result = try {  // dataSource.write(mode, df) replace this code with below  dataSource.writeAndRead(mode, df)} catch {  case ex: AnalysisException =>    logError(s"Failed to write to table $tableName in $mode mode", ex)    throw ex}result match {  case fs: HadoopFsRelation if table.partitionColumnNames.nonEmpty &&                               sparkSession.sqlContext.conf.manageFilesourcePartitions =>    // Need to recover partitions into the metastore so our saved data is visible.    sparkSession.sessionState.executePlan(      AlterTableRecoverPartitionsCommand(table.identifier)).toRdd  case _ =>}     
issueID:CARBONDATA-355
type:Improvement
changed files:
texts:Remove unnecessary method argument columnIdentifier of PathService.getCarbonTablePath
Remove one of method arguments of PathService#getCarbonTablePath since it is not necessary pass columnIdentifier when get table path.
issueID:CARBONDATA-3552
type:Bug
changed files:processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/FileMergeSortComparator.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/holder/UnsafeInmemoryHolder.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/SortParameters.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/merger/UnsafeIntermediateFileMerger.java
processing/src/main/java/org/apache/carbondata/processing/util/CarbonDataProcessorUtil.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/holder/UnsafeSortTempFileChunkHolder.java
processing/src/main/java/org/apache/carbondata/processing/loading/DataLoadProcessBuilder.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/SortTempFileChunkHolder.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/TableFieldStat.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/holder/UnsafeFinalMergePageHolder.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/merger/UnsafeSingleThreadFinalSortFilesMerger.java
texts:dataload fails for column added in 1.1 which is a sort column in sort step in new version of carbondata
Problem:dataload fails for column added in 1.1 which is a sort column in sort step in new version of carbondataIn carbon 1.1 version, create a column with Dictionary exclude column, then add a timestamp column, By default it will be a sort column. Upgrade to latest version and load data. Data load fails with below exception org.apache.carbondata.processing.loading.exception.CarbonDataLoadingException:        at org.apache.carbondata.processing.loading.sort.impl.ParallelReadMergeSorterImpl.sort(ParallelReadMergeSorterImpl.java:120)        at org.apache.carbondata.processing.loading.steps.SortProcessorStepImpl.execute(SortProcessorStepImpl.java:55)        at org.apache.carbondata.processing.loading.steps.DataWriterProcessorStepImpl.execute(DataWriterProcessorStepImpl.java:109)        at org.apache.carbondata.processing.loading.DataLoadExecutor.execute(DataLoadExecutor.java:52)        at org.apache.carbondata.spark.rdd.NewCarbonDataLoadRDD$$anon$1.<init>(NewCarbonDataLoadRDD.scala:150)        at org.apache.carbondata.spark.rdd.NewCarbonDataLoadRDD.internalCompute(NewCarbonDataLoadRDD.scala:124)        at org.apache.carbondata.spark.rdd.CarbonRDD.compute(CarbonRDD.scala:82)        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)        at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)        at org.apache.spark.scheduler.Task.run(Task.scala:99)        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:325)        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)        at java.lang.Thread.run(Thread.java:748)Caused by: org.apache.carbondata.core.datastore.exception.CarbonDataWriterException:        at org.apache.carbondata.processing.sort.sortdata.SingleThreadFinalSortFilesMerger.checkFailure(SingleThreadFinalSortFilesMerger.java:226)        at org.apache.carbondata.processing.sort.sortdata.SingleThreadFinalSortFilesMerger.startSorting(SingleThreadFinalSortFilesMerger.java:217)        at org.apache.carbondata.processing.sort.sortdata.SingleThreadFinalSortFilesMerger.startFinalMerge(SingleThreadFinalSortFilesMerger.java:113)        at org.apache.carbondata.processing.loading.sort.impl.ParallelReadMergeSorterImpl.sort(ParallelReadMergeSorterImpl.java:118)        ... 14 moreCaused by: java.util.concurrent.ExecutionException: java.lang.ClassCastException: [B cannot be cast to java.lang.Long        at java.util.concurrent.FutureTask.report(FutureTask.java:122)        at java.util.concurrent.FutureTask.get(FutureTask.java:192)        at org.apache.carbondata.processing.sort.sortdata.SingleThreadFinalSortFilesMerger.checkFailure(SingleThreadFinalSortFilesMerger.java:224)        ... 17 moreCaused by: java.lang.ClassCastException: [B cannot be cast to java.lang.Long        at org.apache.carbondata.core.util.comparator.LongSerializableComparator.compare(Comparator.java:180)        at org.apache.carbondata.processing.sort.sortdata.IntermediateSortTempRowComparator.compare(IntermediateSortTempRowComparator.java:64)        at org.apache.carbondata.processing.sort.sortdata.IntermediateSortTempRowComparator.compare(IntermediateSortTempRowComparator.java:31)        at org.apache.carbondata.processing.sort.sortdata.SortTempFileChunkHolder.compareTo(SortTempFileChunkHolder.java:301)        at org.apache.carbondata.processing.sort.sortdata.SortTempFileChunkHolder.compareTo(SortTempFileChunkHolder.java:43)        at java.util.PriorityQueue.siftUpComparable(PriorityQueue.java:656)        at java.util.PriorityQueue.siftUp(PriorityQueue.java:647)        at java.util.PriorityQueue.offer(PriorityQueue.java:344)        at java.util.PriorityQueue.add(PriorityQueue.java:321)        at org.apache.carbondata.processing.sort.sortdata.SingleThreadFinalSortFilesMerger$2.call(SingleThreadFinalSortFilesMerger.java:204)        at org.apache.carbondata.processing.sort.sortdata.SingleThreadFinalSortFilesMerger$2.call(SingleThreadFinalSortFilesMerger.java:190)        at java.util.concurrent.FutureTask.run(FutureTask.java:266)        ... 3 more
issueID:CARBONDATA-3553
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/metadata/schema/SchemaReader.java
store/sdk/src/main/java/org/apache/carbondata/sdk/file/CarbonWriterBuilder.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/CarbonTable.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonInputFormat.java
store/sdk/src/main/java/org/apache/carbondata/sdk/file/Schema.java
hadoop/src/main/java/org/apache/carbondata/hadoop/readsupport/impl/CarbonRowReadSupport.java
store/sdk/src/main/java/org/apache/carbondata/sdk/file/CarbonSchemaReader.java
store/sdk/src/main/java/org/apache/carbondata/sdk/file/CarbonReaderBuilder.java
core/src/main/java/org/apache/carbondata/core/metadata/converter/SchemaConverter.java
processing/src/main/java/org/apache/carbondata/processing/loading/model/CarbonLoadModelBuilder.java
core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
texts:Support SDK Writer using existing schema file
Sometimes, user need to build SDK writer with schema file, rather than with scheme object. It's will be easier to use.val writer = CarbonWriter.builder.outputPath(...).withSchemaFile("/default/test/Metadata/schema").withCsvInput().build()
issueID:CARBONDATA-3555
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/metadata/schema/table/TableInfo.java
hadoop/src/main/java/org/apache/carbondata/hadoop/stream/StreamRecordReader.java
processing/src/main/java/org/apache/carbondata/processing/merger/CarbonCompactionExecutor.java
core/src/main/java/org/apache/carbondata/core/datamap/DataMapStoreManager.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/CarbonTable.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonInputFormat.java
core/src/main/java/org/apache/carbondata/core/scan/executor/impl/AbstractQueryExecutor.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonFileInputFormat.java
store/sdk/src/main/java/org/apache/carbondata/sdk/file/CarbonSchemaReader.java
core/src/main/java/org/apache/carbondata/core/datamap/DataMapFilter.java
store/sdk/src/main/java/org/apache/carbondata/sdk/file/CarbonReaderBuilder.java
integration/presto/src/main/java/org/apache/carbondata/presto/CarbondataPageSource.java
core/src/main/java/org/apache/carbondata/core/datamap/Segment.java
core/src/main/java/org/apache/carbondata/core/scan/model/QueryModelBuilder.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonTableInputFormat.java
hadoop/src/main/java/org/apache/carbondata/hadoop/util/CarbonInputFormatUtil.java
store/sdk/src/main/java/org/apache/carbondata/store/LocalCarbonStore.java
core/src/main/java/org/apache/carbondata/core/scan/model/QueryModel.java
integration/spark-datasource/src/main/scala/org/apache/carbondata/spark/vectorreader/VectorizedCarbonRecordReader.java
core/src/main/java/org/apache/carbondata/core/datamap/TableDataMap.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockDataMap.java
integration/presto/src/main/java/org/apache/carbondata/presto/impl/CarbonTableReader.java
texts:Refactor DataMapFilter to act as a filter provider.
Refactor DataMapFilter to act as a filter provider and should be used in all the API to get the expression and the filter resolver. All filter resolving related methods will be moved to datamap filter. change existing internal interfaces to take DataMapFilter instead of Expression.
issueID:CARBONDATA-3556
type:Test
changed files:
texts:Add testcases for Insert into Complex data type of all Primitive types with 2 levels
Add testcases for Insert into Complex data type of all Primitive types with 2 levels with CarbonSession and SparkFileFormatComplex Data Type include, Struct Array Map
issueID:CARBONDATA-3558
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/metadata/schema/table/CarbonTable.java
core/src/main/java/org/apache/carbondata/core/datamap/DataMapProvider.java
texts:Clean up codes for property "autoRefreshDataMap"

issueID:CARBONDATA-356
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/scan/expression/conditional/GreaterThanExpression.java
core/src/main/java/org/apache/carbondata/core/scan/model/QueryMeasure.java
core/src/main/java/org/apache/carbondata/core/scan/expression/conditional/NotInExpression.java
core/src/main/java/org/apache/carbondata/core/scan/model/QueryDimension.java
core/src/main/java/org/apache/carbondata/core/scan/expression/conditional/GreaterThanEqualToExpression.java
core/src/main/java/org/apache/carbondata/core/scan/expression/conditional/EqualToExpression.java
core/src/main/java/org/apache/carbondata/core/metadata/datatype/DataType.java
core/src/main/java/org/apache/carbondata/core/scan/model/QueryModel.java
core/src/main/java/org/apache/carbondata/core/scan/executor/impl/AbstractQueryExecutor.java
core/src/main/java/org/apache/carbondata/core/scan/expression/ExpressionResult.java
core/src/main/java/org/apache/carbondata/core/scan/expression/conditional/InExpression.java
core/src/main/java/org/apache/carbondata/core/scan/expression/conditional/LessThanExpression.java
core/src/main/java/org/apache/carbondata/core/scan/expression/conditional/LessThanEqualToExpression.java
core/src/main/java/org/apache/carbondata/core/scan/expression/conditional/NotEqualsExpression.java
texts:Remove Two Useless Files ConvertedType.java and QuerySchemaInfo.java
ConvertedType.java and QuerySchemaInfo.java are uselesss.
issueID:CARBONDATA-3560
type:Bug
changed files:
texts:When format is given in uppercase, add segment does not work

issueID:CARBONDATA-3561
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/datastore/chunk/store/impl/LocalDictDimensionDataChunkStore.java
core/src/main/java/org/apache/carbondata/core/scan/result/vector/impl/directread/ColumnarVectorWrapperDirectWithInvertedIndex.java
datamap/examples/src/minmaxdatamap/main/java/org/apache/carbondata/datamap/examples/MinMaxIndexDataMapFactory.java
core/src/main/java/org/apache/carbondata/core/scan/result/vector/impl/directread/ColumnarVectorWrapperDirectWithDeleteDelta.java
texts:After delete/update the data, the query results become incorrect
 CREATE TABLE `workqueue_zyy02` (`queuecode` STRING, `channelflag` STRING, `dt` STRING) USING org.apache.spark.sql.CarbonSource PARTITIONED BY (dt);   delete from workqueue_zyy02 m where m.queuecode ='1';before   after
issueID:CARBONDATA-3562
type:Bug
changed files:
texts:Fix for SDK filter queries not working when schema is given explicitly while Add Segment
Queries will not return correct result from added segment when the schema is given explicitly in case of SDK.
issueID:CARBONDATA-3563
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/metadata/schema/table/DataMapSchemaStorageProvider.java
core/src/main/java/org/apache/carbondata/core/scan/expression/MatchExpression.java
hadoop/src/main/java/org/apache/carbondata/hadoop/InputMetricsStats.java
texts:optimize java code checkstyle for RedundantImport rule
Checks for redundant import statements. An import statement is considered redundant if: It is a duplicate of another import. The class non-statically imported is from the java.lang package, e.g. importing java.lang.String. The class non-statically imported is from the same package as the current package.
issueID:CARBONDATA-3564
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/metadata/SegmentFileStore.java
core/src/main/java/org/apache/carbondata/core/profiler/TablePruningInfo.java
processing/src/main/java/org/apache/carbondata/processing/util/CarbonLoaderUtil.java
datamap/lucene/src/main/java/org/apache/carbondata/datamap/lucene/LuceneDataMapFactoryBase.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/store/impl/unsafe/UnsafeVariableLengthDimensionDataChunkStore.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/DiskBasedDMSchemaStorageProvider.java
texts:optimize java code checkstyle for EmptyStatement rule
Detects empty statements (standalone {@code ";"} semicolon).Empty statements often introduce bugs that are hard to spot
issueID:CARBONDATA-3566
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/metadata/SegmentFileStore.java
core/src/main/java/org/apache/carbondata/core/writer/CarbonIndexFileMergeWriter.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonOutputCommitter.java
texts:Support add segment for partition table
CarbonData supports ADD SEGMENT for non-partition table already, it should also support for Hive partition table.
issueID:CARBONDATA-3567
type:Bug
changed files:
texts:Added segment datasize and indexsize not correct and added mixed format segment compaction not correct

issueID:CARBONDATA-3568
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/metadata/blocklet/BlockletInfo.java
hadoop/src/main/java/org/apache/carbondata/hadoop/stream/StreamRecordReader.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/TableInfo.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockletDataMapFactory.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/store/impl/safe/SafeVariableShortLengthDimensionDataChunkStore.java
integration/presto/src/main/java/org/apache/carbondata/presto/readers/IntegerStreamReader.java
integration/presto/src/main/java/org/apache/carbondata/presto/readers/DecimalSliceStreamReader.java
core/src/main/java/org/apache/carbondata/core/util/DataFileFooterConverter.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonFileInputFormat.java
integration/hive/src/main/java/org/apache/carbondata/hive/CarbonHiveRecordReader.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/compress/DirectCompressCodec.java
core/src/main/java/org/apache/carbondata/core/localdictionary/generator/ColumnLocalDictionaryGenerator.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelRangeGrtrThanEquaToFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/scan/expression/logical/RangeExpression.java
core/src/main/java/org/apache/carbondata/core/datastore/page/SafeFixLengthColumnPage.java
core/src/main/java/org/apache/carbondata/core/scan/result/vector/impl/directread/ColumnarVectorWrapperDirectWithInvertedIndex.java
integration/spark-datasource/src/main/scala/org/apache/carbondata/spark/vectorreader/ColumnarVectorWrapperDirect.java
core/src/main/java/org/apache/carbondata/core/datastore/columnar/ColumnWithRowIdForHighCard.java
streaming/src/main/java/org/apache/carbondata/streaming/segment/StreamSegment.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/holder/UnsafeFinalMergePageHolder.java
core/src/main/java/org/apache/carbondata/core/datastore/TableSegmentUniqueIdentifier.java
core/src/main/java/org/apache/carbondata/core/datastore/block/BlockInfo.java
core/src/main/java/org/apache/carbondata/core/localdictionary/exception/DictionaryThresholdReachedException.java
core/src/main/java/org/apache/carbondata/core/locks/HdfsFileLock.java
integration/hive/src/main/java/org/apache/carbondata/hive/MapredCarbonOutputFormat.java
core/src/main/java/org/apache/carbondata/core/cache/dictionary/ColumnDictionaryInfo.java
core/src/main/java/org/apache/carbondata/core/scan/executor/impl/AbstractQueryExecutor.java
core/src/main/java/org/apache/carbondata/core/datastore/row/CarbonRow.java
processing/src/main/java/org/apache/carbondata/processing/util/CarbonLoaderUtil.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/reader/dimension/AbstractDimensionChunkReader.java
core/src/main/java/org/apache/carbondata/core/stats/TaskStatistics.java
core/src/main/java/org/apache/carbondata/core/datamap/dev/fgdatamap/FineGrainDataMap.java
processing/src/main/java/org/apache/carbondata/processing/loading/steps/InputProcessorStepImpl.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonTableOutputFormat.java
processing/src/main/java/org/apache/carbondata/processing/loading/steps/DataConverterProcessorStepImpl.java
processing/src/main/java/org/apache/carbondata/processing/loading/steps/JsonInputProcessorStepImpl.java
common/src/main/java/org/apache/carbondata/common/CarbonIterator.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/InMemorySortTempChunkHolder.java
core/src/main/java/org/apache/carbondata/core/cache/dictionary/DictionaryColumnUniqueIdentifier.java
core/src/main/java/org/apache/carbondata/core/indexstore/ExtendedBlocklet.java
integration/presto/src/main/java/org/apache/carbondata/presto/impl/CarbonTableReader.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/ImplicitIncludeFilterExecutorImpl.java
processing/src/main/java/org/apache/carbondata/processing/util/CarbonBadRecordUtil.java
core/src/main/java/org/apache/carbondata/core/datastore/page/statistics/DummyStatsCollector.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/AndFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/indexstore/PartitionSpec.java
core/src/main/java/org/apache/carbondata/core/scan/result/iterator/PartitionSpliterRawResultIterator.java
core/src/main/java/org/apache/carbondata/core/util/DataTypeUtil.java
core/src/main/java/org/apache/carbondata/core/scan/expression/ExpressionResult.java
processing/src/main/java/org/apache/carbondata/processing/loading/steps/InputProcessorStepWithNoConverterImpl.java
core/src/main/java/org/apache/carbondata/core/datastore/filesystem/AbstractDFSCarbonFile.java
store/sdk/src/main/java/org/apache/carbondata/store/CarbonRowReadSupport.java
common/src/main/java/org/apache/carbondata/common/constants/LoggerAction.java
core/src/main/java/org/apache/carbondata/core/datastore/page/statistics/LVStringStatsCollector.java
integration/spark-datasource/src/main/spark2.3plus/org/apache/spark/sql/CarbonDictionaryWrapper.java
integration/spark2/src/main/java/org/apache/carbondata/datamap/IndexDataMapProvider.java
hadoop/src/main/java/org/apache/carbondata/hadoop/util/CarbonVectorizedRecordReader.java
core/src/main/java/org/apache/carbondata/core/scan/result/iterator/DetailQueryResultIterator.java
processing/src/main/java/org/apache/carbondata/processing/merger/CarbonDataMergerUtilResult.java
core/src/main/java/org/apache/carbondata/core/indexstore/schema/CarbonRowSchema.java
processing/src/main/java/org/apache/carbondata/processing/loading/dictionary/DirectDictionary.java
integration/spark2/src/main/java/org/apache/carbondata/spark/readsupport/SparkRowReadSupportImpl.java
processing/src/main/java/org/apache/carbondata/processing/sort/exception/CarbonSortKeyAndGroupByException.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/impl/ParallelReadMergeSorterImpl.java
core/src/main/java/org/apache/carbondata/core/memory/HeapMemoryAllocator.java
integration/presto/src/main/java/org/apache/carbondata/presto/CarbondataColumnConstraint.java
core/src/main/java/org/apache/carbondata/core/scan/filter/FilterExpressionProcessor.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockDataMap.java
core/src/main/java/org/apache/carbondata/core/scan/result/vector/impl/CarbonColumnVectorImpl.java
store/sdk/src/main/java/org/apache/carbondata/sdk/file/arrow/ArrowFieldWriter.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/impl/FixedLengthDimensionColumnPage.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/resolverinfo/TrueConditionalResolverImpl.java
processing/src/main/java/org/apache/carbondata/processing/loading/AbstractDataLoadProcessorStep.java
core/src/main/java/org/apache/carbondata/core/keygenerator/mdkey/Bits.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/adaptive/AdaptiveDeltaIntegralCodec.java
core/src/main/java/org/apache/carbondata/core/scan/executor/exception/QueryExecutionException.java
core/src/main/java/org/apache/carbondata/core/datamap/AbstractDataMapJob.java
core/src/main/java/org/apache/carbondata/core/datastore/filesystem/LocalCarbonFile.java
core/src/main/java/org/apache/carbondata/core/mutate/data/RowCountDetailsVO.java
core/src/main/java/org/apache/carbondata/core/indexstore/BlockletDataMapIndexWrapper.java
store/sdk/src/main/java/org/apache/carbondata/sdk/file/JsonCarbonWriter.java
processing/src/main/java/org/apache/carbondata/processing/loading/row/CarbonSortBatch.java
processing/src/main/java/org/apache/carbondata/processing/loading/converter/impl/ComplexFieldConverterImpl.java
core/src/main/java/org/apache/carbondata/core/metadata/converter/ThriftWrapperSchemaConverterImpl.java
core/src/main/java/org/apache/carbondata/core/readcommitter/TableStatusReadCommittedScope.java
core/src/main/java/org/apache/carbondata/core/datastore/page/VarLengthColumnPageBase.java
core/src/main/java/org/apache/carbondata/core/datamap/dev/expr/AndDataMapExprWrapper.java
integration/presto/src/main/java/org/apache/carbondata/presto/readers/FloatStreamReader.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/RowLevelFilterResolverImpl.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/store/impl/safe/SafeFixedLengthDimensionDataChunkStore.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/column/CarbonMeasure.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/store/impl/unsafe/UnsafeFixedLengthDimensionDataChunkStore.java
core/src/main/java/org/apache/carbondata/core/datastore/compression/ZstdCompressor.java
core/src/main/java/org/apache/carbondata/core/datastore/exception/CarbonDataWriterException.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/FileMergeSortComparator.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/store/impl/safe/SafeAbsractDimensionDataChunkStore.java
core/src/main/java/org/apache/carbondata/core/util/path/CarbonTablePath.java
core/src/main/java/org/apache/carbondata/core/reader/CarbonFooterReader.java
streaming/src/main/java/org/apache/carbondata/streaming/CarbonStreamOutputFormat.java
core/src/main/java/org/apache/carbondata/core/datastore/block/TableTaskInfo.java
integration/presto/src/main/java/org/apache/carbondata/presto/readers/ObjectStreamReader.java
core/src/main/java/org/apache/carbondata/core/scan/result/vector/impl/directread/ColumnarVectorWrapperDirectWithDeleteDelta.java
datamap/lucene/src/main/java/org/apache/carbondata/datamap/lucene/LuceneDataMapFactoryBase.java
integration/spark-datasource/src/main/spark2.1andspark2.2/org/apache/spark/sql/CarbonDictionaryWrapper.java
core/src/main/java/org/apache/carbondata/core/datamap/dev/expr/DataMapDistributableWrapper.java
core/src/main/java/org/apache/carbondata/core/scan/result/vector/impl/directread/AbstractCarbonColumnarVector.java
core/src/main/java/org/apache/carbondata/core/scan/wrappers/ByteArrayWrapper.java
core/src/main/java/org/apache/carbondata/core/scan/scanner/impl/BlockletFullScanner.java
core/src/main/java/org/apache/carbondata/core/datastore/columnar/BlockIndexerStorageForNoDictionary.java
core/src/main/java/org/apache/carbondata/core/metadata/ColumnIdentifier.java
core/src/main/java/org/apache/carbondata/core/scan/expression/logical/FalseExpression.java
processing/src/main/java/org/apache/carbondata/processing/loading/partition/impl/HashPartitionerImpl.java
integration/presto/src/main/java/org/apache/carbondata/presto/readers/LongStreamReader.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/column/ColumnSchema.java
tools/cli/src/main/java/org/apache/carbondata/tool/FileCollector.java
core/src/main/java/org/apache/carbondata/core/indexstore/BlockletDetailInfo.java
core/src/main/java/org/apache/carbondata/core/keygenerator/mdkey/AbstractKeyGenerator.java
core/src/main/java/org/apache/carbondata/core/datamap/dev/expr/DataMapExprWrapperImpl.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/store/ColumnPageWrapper.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/IncludeFilterExecuterImpl.java
processing/src/main/java/org/apache/carbondata/processing/loading/row/CarbonRowBatch.java
core/src/main/java/org/apache/carbondata/core/datamap/TableDataMap.java
processing/src/main/java/org/apache/carbondata/processing/loading/converter/impl/MeasureFieldConverterImpl.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/DiskBasedDMSchemaStorageProvider.java
core/src/main/java/org/apache/carbondata/core/cache/dictionary/DictionaryByteArrayWrapper.java
integration/presto/src/main/java/org/apache/carbondata/presto/CarbonColumnVectorWrapper.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/impl/AbstractDimensionColumnPage.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/SortDataRows.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/column/ParentColumnTableRelation.java
core/src/main/java/org/apache/carbondata/core/locks/AlluxioFileLock.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/resolverinfo/MeasureColumnResolvedFilterInfo.java
core/src/main/java/org/apache/carbondata/core/indexstore/BlockletDataMapIndexStore.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/SingleThreadFinalSortFilesMerger.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/resolverinfo/visitor/ImplicitColumnVisitor.java
core/src/main/java/org/apache/carbondata/core/datastore/compression/AbstractCompressor.java
core/src/main/java/org/apache/carbondata/core/scan/result/iterator/AbstractDetailQueryResultIterator.java
core/src/main/java/org/apache/carbondata/core/datastore/page/SafeDecimalColumnPage.java
core/src/main/java/org/apache/carbondata/core/datastore/page/UnsafeFixLengthColumnPage.java
processing/src/main/java/org/apache/carbondata/processing/store/writer/v3/CarbonFactDataWriterImplV3.java
core/src/main/java/org/apache/carbondata/core/util/DataTypeConverterImpl.java
integration/hive/src/main/java/org/apache/carbondata/hive/CarbonArrayInspector.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/impl/ParallelReadMergeSorterWithColumnRangeImpl.java
integration/presto/src/main/java/org/apache/carbondata/presto/CarbondataConnectorFactory.java
core/src/main/java/org/apache/carbondata/core/keygenerator/mdkey/MultiDimKeyVarLengthGenerator.java
integration/spark-datasource/src/main/spark2.1andspark2.2/org/apache/spark/sql/CarbonVectorProxy.java
core/src/main/java/org/apache/carbondata/core/cache/dictionary/ForwardDictionary.java
core/src/main/java/org/apache/carbondata/core/metadata/datatype/DecimalConverterFactory.java
core/src/main/java/org/apache/carbondata/core/reader/CarbonIndexFileReader.java
core/src/main/java/org/apache/carbondata/core/keygenerator/columnar/impl/MultiDimKeyVarLengthVariableSplitGenerator.java
core/src/main/java/org/apache/carbondata/core/util/comparator/Comparator.java
processing/src/main/java/org/apache/carbondata/processing/merger/NodeMultiBlockRelation.java
integration/spark-datasource/src/main/spark2.3plus/org/apache/spark/sql/CarbonVectorProxy.java
integration/presto/src/main/java/org/apache/carbondata/presto/readers/SliceStreamReader.java
core/src/main/java/org/apache/carbondata/core/scan/filter/FilterUtil.java
datamap/examples/src/minmaxdatamap/main/java/org/apache/carbondata/datamap/examples/MinMaxIndexDataMap.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/UnsafeSortDataRows.java
core/src/main/java/org/apache/carbondata/core/datamap/DataMapMeta.java
processing/src/main/java/org/apache/carbondata/processing/loading/converter/impl/NonDictionaryFieldConverterImpl.java
processing/src/main/java/org/apache/carbondata/processing/sort/DummyRowUpdater.java
core/src/main/java/org/apache/carbondata/core/datastore/block/AbstractIndex.java
core/src/main/java/org/apache/carbondata/core/cache/dictionary/AbstractColumnDictionaryInfo.java
core/src/main/java/org/apache/carbondata/core/scan/expression/logical/TrueExpression.java
core/src/main/java/org/apache/carbondata/core/scan/executor/infos/DeleteDeltaInfo.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/column/CarbonImplicitDimension.java
core/src/main/java/org/apache/carbondata/core/util/DeleteLoadFolders.java
core/src/main/java/org/apache/carbondata/core/mutate/UpdateVO.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/BitSetUpdaterFactory.java
processing/src/main/java/org/apache/carbondata/processing/loading/parser/impl/MapParserImpl.java
integration/spark2/src/main/scala/org/apache/carbondata/stream/CarbonStreamRecordReader.java
integration/presto/src/main/java/org/apache/carbondata/presto/PrestoCarbonVectorizedRecordReader.java
processing/src/main/java/org/apache/carbondata/processing/datatypes/ArrayDataType.java
processing/src/main/java/org/apache/carbondata/processing/sort/SchemaBasedRowUpdater.java
core/src/main/java/org/apache/carbondata/core/datastore/compression/SnappyCompressor.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/impl/UnsafeParallelReadMergeSorterImpl.java
integration/presto/src/main/java/org/apache/carbondata/presto/readers/TimestampStreamReader.java
core/src/main/java/org/apache/carbondata/core/util/CarbonThreadFactory.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/IntermediateFileMerger.java
core/src/main/java/org/apache/carbondata/core/datastore/page/DecoderBasedFallbackEncoder.java
core/src/main/java/org/apache/carbondata/core/datastore/impl/DefaultFileTypeProvider.java
core/src/main/java/org/apache/carbondata/core/util/BlockletDataMapUtil.java
processing/src/main/java/org/apache/carbondata/processing/loading/steps/DataWriterProcessorStepImpl.java
integration/presto/src/main/java/org/apache/carbondata/presto/ColumnarVectorWrapperDirect.java
core/src/main/java/org/apache/carbondata/core/scan/result/iterator/ChunkRowIterator.java
core/src/main/java/org/apache/carbondata/core/scan/expression/conditional/ImplicitExpression.java
core/src/main/java/org/apache/carbondata/core/cache/dictionary/DictionaryChunksWrapper.java
core/src/main/java/org/apache/carbondata/core/datastore/columnar/ColumnWithRowId.java
core/src/main/java/org/apache/carbondata/core/writer/CarbonDeleteDeltaWriterImpl.java
processing/src/main/java/org/apache/carbondata/processing/datatypes/StructDataType.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/resolverinfo/DimColumnResolvedFilterInfo.java
streaming/src/main/java/org/apache/carbondata/streaming/parser/CSVStreamParserImp.java
core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
core/src/main/java/org/apache/carbondata/core/service/impl/ColumnUniqueIdGenerator.java
core/src/main/java/org/apache/carbondata/core/scan/filter/intf/RowImpl.java
integration/presto/src/main/java/org/apache/carbondata/presto/readers/BooleanStreamReader.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/impl/DimensionRawColumnChunk.java
core/src/main/java/org/apache/carbondata/core/reader/CarbonDeleteFilesDataReader.java
core/src/main/java/org/apache/carbondata/core/scan/result/vector/impl/CarbonDictionaryImpl.java
core/src/main/java/org/apache/carbondata/core/mutate/SegmentUpdateDetails.java
core/src/main/java/org/apache/carbondata/core/scan/result/vector/impl/directread/ColumnarVectorWrapperDirectWithDeleteDeltaAndInvertedIndex.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/rle/RLECodec.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/holder/UnsafeSortTempFileChunkHolder.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelRangeLessThanFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/datastore/filesystem/S3CarbonFile.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/TableSchema.java
core/src/main/java/org/apache/carbondata/core/reader/CarbonDeleteDeltaFileReaderImpl.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/holder/UnsafeInmemoryHolder.java
processing/src/main/java/org/apache/carbondata/processing/merger/RowResultMergerProcessor.java
processing/src/main/java/org/apache/carbondata/processing/loading/parser/impl/JsonRowParser.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/SegmentIndexFileStore.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonInputFormat.java
core/src/main/java/org/apache/carbondata/core/mutate/DeleteDeltaBlockletDetails.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/TableFieldStat.java
core/src/main/java/org/apache/carbondata/core/locks/ZooKeeperLocking.java
integration/hive/src/main/java/org/apache/carbondata/hive/CarbonHiveSerDe.java
core/src/main/java/org/apache/carbondata/hadoop/CarbonInputSplit.java
core/src/main/java/org/apache/carbondata/core/scan/complextypes/StructQueryType.java
core/src/main/java/org/apache/carbondata/core/datastore/columnar/ColumnWithRowIdForNoDictionary.java
core/src/main/java/org/apache/carbondata/core/scan/result/RowBatch.java
core/src/main/java/org/apache/carbondata/core/indexstore/ExtendedBlockletWrapper.java
core/src/main/java/org/apache/carbondata/core/scan/result/impl/NonFilterQueryScannedResult.java
processing/src/main/java/org/apache/carbondata/processing/loading/steps/CarbonRowDataWriterProcessorStepImpl.java
core/src/main/java/org/apache/carbondata/core/reader/CarbonHeaderReader.java
integration/hive/src/main/java/org/apache/carbondata/hive/CarbonObjectInspector.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/resolverinfo/FalseConditionalResolverImpl.java
datamap/examples/src/minmaxdatamap/main/java/org/apache/carbondata/datamap/examples/MinMaxIndexDataMapFactory.java
core/src/main/java/org/apache/carbondata/core/fileoperations/AtomicFileOperationsImpl.java
core/src/main/java/org/apache/carbondata/core/datastore/block/TableBlockInfo.java
core/src/main/java/org/apache/carbondata/core/scan/result/vector/ColumnVectorInfo.java
core/src/main/java/org/apache/carbondata/core/datamap/dev/expr/OrDataMapExprWrapper.java
integration/hive/src/main/java/org/apache/carbondata/hive/CarbonDictionaryDecodeReadSupport.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/reader/measure/v3/MeasureChunkPageReaderV3.java
core/src/main/java/org/apache/carbondata/core/datastore/impl/FileReaderImpl.java
integration/presto/src/main/java/org/apache/carbondata/presto/CarbondataModule.java
core/src/main/java/org/apache/carbondata/core/scan/wrappers/IntArrayWrapper.java
integration/hive/src/main/java/org/apache/carbondata/hive/CarbonMapInspector.java
core/src/main/java/org/apache/carbondata/core/fileoperations/AtomicFileOperationS3Impl.java
core/src/main/java/org/apache/carbondata/core/indexstore/row/DataMapRowImpl.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/adaptive/AdaptiveFloatingCodec.java
streaming/src/main/java/org/apache/carbondata/streaming/CarbonStreamRecordWriter.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/reader/measure/AbstractMeasureChunkReader.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/SortTempFileChunkHolder.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/reader/measure/v3/MeasureChunkReaderV3.java
core/src/main/java/org/apache/carbondata/core/datastore/filesystem/ViewFSCarbonFile.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/RowLevelRangeFilterResolverImpl.java
integration/spark-datasource/src/main/scala/org/apache/carbondata/spark/vectorreader/ColumnarVectorWrapper.java
integration/hive/src/main/java/org/apache/carbondata/hive/MapredCarbonInputFormat.java
core/src/main/java/org/apache/carbondata/core/datastore/page/LocalDictColumnPage.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/IndexWrapper.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/impl/VariableLengthDimensionColumnPage.java
core/src/main/java/org/apache/carbondata/core/util/DataFileFooterConverter2.java
core/src/main/java/org/apache/carbondata/core/mutate/DeleteDeltaBlockDetails.java
store/sdk/src/main/java/org/apache/carbondata/sdk/file/AvroCarbonWriter.java
processing/src/main/java/org/apache/carbondata/processing/datatypes/PrimitiveDataType.java
core/src/main/java/org/apache/carbondata/core/statusmanager/SegmentRefreshInfo.java
core/src/main/java/org/apache/carbondata/core/metadata/datatype/DataTypeAdapter.java
processing/src/main/java/org/apache/carbondata/processing/loading/converter/impl/RowConverterImpl.java
core/src/main/java/org/apache/carbondata/core/indexstore/ExtendedBlockletWrapperContainer.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/sort/UnsafeIntSortDataFormat.java
core/src/main/java/org/apache/carbondata/core/datastore/page/ActualDataBasedFallbackEncoder.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonOutputCommitter.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/column/CarbonDimension.java
integration/hive/src/main/java/org/apache/carbondata/hive/CarbonHiveInputSplit.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/DataMapSchema.java
datamap/examples/src/minmaxdatamap/main/java/org/apache/carbondata/datamap/examples/MinMaxDataWriter.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelRangeGrtThanFiterExecuterImpl.java
datamap/bloom/src/main/java/org/apache/carbondata/datamap/bloom/BloomCacheKeyValue.java
core/src/main/java/org/apache/carbondata/core/datastore/block/SegmentPropertiesAndSchemaHolder.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/merger/UnsafeIntermediateFileMerger.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/RelationIdentifier.java
core/src/main/java/org/apache/carbondata/core/indexstore/TableBlockIndexUniqueIdentifier.java
integration/presto/src/main/java/org/apache/carbondata/presto/readers/DoubleStreamReader.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/LogicalFilterResolverImpl.java
integration/presto/src/main/java/org/apache/carbondata/presto/CarbondataPageSource.java
core/src/main/java/org/apache/carbondata/core/scan/processor/DataBlockIterator.java
core/src/main/java/org/apache/carbondata/core/reader/CarbonFooterReaderV3.java
core/src/main/java/org/apache/carbondata/core/datastore/impl/DFSFileReaderImpl.java
core/src/main/java/org/apache/carbondata/core/metadata/CarbonTableIdentifier.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/store/impl/safe/AbstractNonDictionaryVectorFiller.java
integration/presto/src/main/java/org/apache/carbondata/presto/readers/ShortStreamReader.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/impl/MeasureRawColumnChunk.java
core/src/main/java/org/apache/carbondata/core/mutate/CarbonUpdateUtil.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockletDataRefNode.java
hadoop/src/main/java/org/apache/carbondata/hadoop/util/CarbonInputSplitTaskInfo.java
core/src/main/java/org/apache/carbondata/core/datamap/Segment.java
processing/src/main/java/org/apache/carbondata/processing/loading/steps/SortProcessorStepImpl.java
processing/src/main/java/org/apache/carbondata/processing/exception/SliceMergerException.java
core/src/main/java/org/apache/carbondata/core/datamap/dev/cgdatamap/CoarseGrainDataMap.java
core/src/main/java/org/apache/carbondata/core/keygenerator/directdictionary/timestamp/TimeStampDirectDictionaryGenerator.java
datamap/lucene/src/main/java/org/apache/carbondata/datamap/lucene/LuceneFineGrainDataMapFactory.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/store/impl/unsafe/UnsafeAbstractDimensionDataChunkStore.java
core/src/main/java/org/apache/carbondata/core/indexstore/row/UnsafeDataMapRow.java
core/src/main/java/org/apache/carbondata/core/keygenerator/directdictionary/timestamp/DateDirectDictionaryGenerator.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/reader/dimension/v3/DimensionChunkPageReaderV3.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/adaptive/AdaptiveIntegralCodec.java
core/src/main/java/org/apache/carbondata/core/datamap/DataMapChooser.java
core/src/main/java/org/apache/carbondata/core/locks/LocalFileLock.java
core/src/main/java/org/apache/carbondata/core/localdictionary/dictionaryholder/MapBasedDictionaryStore.java
core/src/main/java/org/apache/carbondata/core/scan/complextypes/PrimitiveQueryType.java
processing/src/main/java/org/apache/carbondata/processing/loading/TableProcessingOperations.java
core/src/main/java/org/apache/carbondata/core/locks/S3FileLock.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/reader/dimension/v3/DimensionChunkReaderV3.java
tools/cli/src/main/java/org/apache/carbondata/tool/ScanBenchmark.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/adaptive/AdaptiveDeltaFloatingCodec.java
processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactDataHandlerColumnar.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/store/impl/LocalDictDimensionDataChunkStore.java
core/src/main/java/org/apache/carbondata/core/statusmanager/FileFormat.java
core/src/main/java/org/apache/carbondata/core/datastore/page/SafeVarLengthColumnPage.java
core/src/main/java/org/apache/carbondata/core/datastore/page/DecimalColumnPage.java
core/src/main/java/org/apache/carbondata/core/scan/filter/optimizer/RangeFilterOptmizer.java
core/src/main/java/org/apache/carbondata/core/util/DataFileFooterConverterV3.java
processing/src/main/java/org/apache/carbondata/processing/loading/exception/CarbonDataLoadingException.java
processing/src/main/java/org/apache/carbondata/processing/loading/jsoninput/JsonInputFormat.java
core/src/main/java/org/apache/carbondata/core/metadata/SegmentFileStore.java
core/src/main/java/org/apache/carbondata/core/datastore/compression/GzipCompressor.java
core/src/main/java/org/apache/carbondata/core/scan/complextypes/ArrayQueryType.java
core/src/main/java/org/apache/carbondata/core/locks/CarbonLockUtil.java
core/src/main/java/org/apache/carbondata/core/statusmanager/LoadMetadataDetails.java
processing/src/main/java/org/apache/carbondata/processing/merger/NodeBlockRelation.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/ConditionalFilterResolverImpl.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/impl/UnsafeParallelReadMergeSorterWithColumnRangeImpl.java
datamap/bloom/src/main/java/org/apache/carbondata/datamap/bloom/BloomDataMapWriter.java
integration/hive/src/main/java/org/apache/carbondata/hive/CarbonStorageFormatDescriptor.java
core/src/main/java/org/apache/carbondata/core/datamap/dev/fgdatamap/FineGrainBlocklet.java
core/src/main/java/org/apache/carbondata/core/datastore/page/UnsafeVarLengthColumnPage.java
hadoop/src/main/java/org/apache/carbondata/hadoop/CarbonMultiBlockSplit.java
core/src/main/java/org/apache/carbondata/core/datastore/page/statistics/KeyPageStatsCollector.java
hadoop/src/main/java/org/apache/carbondata/hadoop/readsupport/impl/DictionaryDecodeReadSupport.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockletDataMap.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/CarbonTable.java
integration/presto/src/main/java/org/apache/carbondata/presto/CarbondataPlugin.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelRangeLessThanEqualFilterExecuterImpl.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/holder/UnsafeInmemoryMergeHolder.java
processing/src/main/java/org/apache/carbondata/processing/loading/converter/impl/DirectDictionaryFieldConverterImpl.java
core/src/main/java/org/apache/carbondata/core/keygenerator/columnar/impl/MultiDimKeyVarLengthEquiSplitGenerator.java
core/src/main/java/org/apache/carbondata/core/scan/result/impl/FilterQueryScannedResult.java
core/src/main/java/org/apache/carbondata/core/datastore/block/TableBlockUniqueIdentifier.java
core/src/main/java/org/apache/carbondata/core/datastore/page/statistics/PrimitivePageStatsCollector.java
hadoop/src/main/java/org/apache/carbondata/hadoop/CarbonRecordReader.java
core/src/main/java/org/apache/carbondata/core/metadata/AbsoluteTableIdentifier.java
core/src/main/java/org/apache/carbondata/core/datastore/exception/IndexBuilderException.java
texts:optimize java code checkstyle for @Override rule
@Override should at one line in java code file. for a bad example@Override public int hashCode() {  int result = order;  result = 31 * result + (dataMap != null ? dataMap.hashCode() : 0);  return result;} 
issueID:CARBONDATA-3569
type:Improvement
changed files:
texts:spark ui open exception
Spark 2.3.2 jars contains related packages of javax, and the carbondata package references another javax, which causes an exception in sparkui.
issueID:CARBONDATA-357
type:Sub-task
changed files:
texts:Write unit test for ValueCompressionUtil

issueID:CARBONDATA-3571
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/statusmanager/SegmentStatusManager.java
texts:Support table status file read retry when getting EOFException
When storing table status file in object store, reading of table status file may fail (receive EOFException) when table status file is being modifying.To protect from this scenario, we should add retry when reading table status file
issueID:CARBONDATA-3572
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/scan/executor/infos/BlockExecutionInfo.java
processing/src/main/java/org/apache/carbondata/processing/loading/csvinput/BlockDetails.java
processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactDataHandlerModel.java
processing/src/main/java/org/apache/carbondata/processing/merger/CarbonCompactionUtil.java
processing/src/main/java/org/apache/carbondata/processing/merger/RowResultMergerProcessor.java
core/src/main/java/org/apache/carbondata/core/scan/result/vector/impl/CarbonColumnVectorImpl.java
integration/presto/src/main/java/org/apache/carbondata/presto/impl/CarbonTableConfig.java
core/src/main/java/org/apache/carbondata/events/Event.java
core/src/main/java/org/apache/carbondata/core/scan/collector/impl/DictionaryBasedVectorResultCollector.java
core/src/main/java/org/apache/carbondata/hadoop/CarbonInputSplit.java
core/src/main/java/org/apache/carbondata/core/keygenerator/mdkey/Bits.java
core/src/main/java/org/apache/carbondata/core/datastore/compression/AbstractCompressor.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelFilterExecuterImpl.java
texts:optimize java code checkstyle for LeftCurly rule
Checks for the placement of left curly braces ('{') for code blocks.
issueID:CARBONDATA-3574
type:Bug
changed files:
texts:Delete segment by ID gives results from the deleted parquet segments before clean files

issueID:CARBONDATA-3575
type:Improvement
changed files:processing/src/main/java/org/apache/carbondata/processing/loading/sort/SortStepRowHandler.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonLoadOptionConstants.java
core/src/main/java/org/apache/carbondata/core/indexstore/BlockletDataMapIndexStore.java
texts:optimize java code checkstyle for OperatorWrap rule
optimize java code checkstyle for OperatorWrap rule, bad example@CarbonPropertypublic static final String ENABLE_CARBON_LOAD_SKEWED_DATA_OPTIMIZATION    = "carbon.load.skewedDataOptimization.enabled";
issueID:CARBONDATA-3576
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RestructureExcludeFilterExecutorImpl.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/TableInfo.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockletDataMapRowIndexes.java
core/src/main/java/org/apache/carbondata/core/datamap/status/DataMapStatus.java
core/src/main/java/org/apache/carbondata/core/datamap/DataMapStoreManager.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockletDataMapFactory.java
integration/presto/src/main/java/org/apache/carbondata/presto/readers/DecimalSliceStreamReader.java
core/src/main/java/org/apache/carbondata/core/util/DataFileFooterConverter.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonFileInputFormat.java
integration/hive/src/main/java/org/apache/carbondata/hive/CarbonHiveRecordReader.java
core/src/main/java/org/apache/carbondata/core/scan/collector/impl/RawBasedResultCollector.java
core/src/main/java/org/apache/carbondata/core/localdictionary/generator/ColumnLocalDictionaryGenerator.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelRangeGrtrThanEquaToFilterExecuterImpl.java
datamap/bloom/src/main/java/org/apache/carbondata/datamap/bloom/BloomIndexFileStore.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/FilterResolverIntf.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/SchemaEvolutionEntry.java
core/src/main/java/org/apache/carbondata/core/datastore/page/SafeFixLengthColumnPage.java
core/src/main/java/org/apache/carbondata/core/scan/result/vector/impl/directread/ColumnarVectorWrapperDirectWithInvertedIndex.java
core/src/main/java/org/apache/carbondata/core/util/CarbonTaskInfo.java
integration/spark-datasource/src/main/scala/org/apache/carbondata/spark/vectorreader/VectorizedCarbonRecordReader.java
streaming/src/main/java/org/apache/carbondata/streaming/segment/StreamSegment.java
store/sdk/src/main/java/org/apache/carbondata/sdk/file/arrow/ArrowUtils.java
core/src/main/java/org/apache/carbondata/core/datastore/block/BlockInfo.java
core/src/main/java/org/apache/carbondata/core/localdictionary/exception/DictionaryThresholdReachedException.java
streaming/src/main/java/org/apache/carbondata/streaming/StreamBlockletWriter.java
core/src/main/java/org/apache/carbondata/core/util/ByteUtil.java
integration/hive/src/main/java/org/apache/carbondata/hive/MapredCarbonOutputFormat.java
core/src/main/java/org/apache/carbondata/core/cache/dictionary/ColumnDictionaryInfo.java
core/src/main/java/org/apache/carbondata/core/datamap/dev/DataMapSyncStatus.java
core/src/main/java/org/apache/carbondata/core/readcommitter/ReadCommittedScope.java
core/src/main/java/org/apache/carbondata/core/scan/executor/impl/AbstractQueryExecutor.java
processing/src/main/java/org/apache/carbondata/processing/util/CarbonLoaderUtil.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/reader/dimension/AbstractDimensionChunkReader.java
core/src/main/java/org/apache/carbondata/core/datamap/dev/fgdatamap/FineGrainDataMap.java
processing/src/main/java/org/apache/carbondata/processing/loading/steps/InputProcessorStepImpl.java
core/src/main/java/org/apache/carbondata/core/stats/DriverQueryStatisticsRecorderDummy.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/FalseFilterExecutor.java
store/sdk/src/main/java/org/apache/carbondata/sdk/file/Field.java
processing/src/main/java/org/apache/carbondata/processing/loading/steps/DataConverterProcessorStepImpl.java
processing/src/main/java/org/apache/carbondata/processing/loading/steps/JsonInputProcessorStepImpl.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/TableSchemaBuilder.java
core/src/main/java/org/apache/carbondata/core/scan/expression/LiteralExpression.java
processing/src/main/java/org/apache/carbondata/processing/util/CarbonDataProcessorUtil.java
core/src/main/java/org/apache/carbondata/core/datastore/IndexKey.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RestructureIncludeFilterExecutorImpl.java
core/src/main/java/org/apache/carbondata/core/preagg/TimeSeriesUDF.java
core/src/main/java/org/apache/carbondata/core/util/SessionParams.java
core/src/main/java/org/apache/carbondata/core/datamap/dev/DataMapFactory.java
core/src/main/java/org/apache/carbondata/core/indexstore/BlockMetaInfo.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/InMemorySortTempChunkHolder.java
core/src/main/java/org/apache/carbondata/core/scan/complextypes/MapQueryType.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/FilterBitSetUpdater.java
core/src/main/java/org/apache/carbondata/core/scan/filter/ColumnFilterInfo.java
core/src/main/java/org/apache/carbondata/core/indexstore/ExtendedBlocklet.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstantsInternal.java
core/src/main/java/org/apache/carbondata/core/datastore/page/statistics/DummyStatsCollector.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/AndFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/datastore/page/FallbackEncodedColumnPage.java
core/src/main/java/org/apache/carbondata/core/indexstore/PartitionSpec.java
core/src/main/java/org/apache/carbondata/core/scan/result/iterator/PartitionSpliterRawResultIterator.java
core/src/main/java/org/apache/carbondata/core/util/DataTypeUtil.java
core/src/main/java/org/apache/carbondata/core/scan/model/ProjectionColumn.java
processing/src/main/java/org/apache/carbondata/processing/loading/steps/InputProcessorStepWithNoConverterImpl.java
core/src/main/java/org/apache/carbondata/core/keygenerator/directdictionary/DirectDictionaryGenerator.java
core/src/main/java/org/apache/carbondata/core/datastore/block/BlockletInfos.java
tools/cli/src/main/java/org/apache/carbondata/tool/CarbonCli.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/DimensionColumnPage.java
core/src/main/java/org/apache/carbondata/core/mutate/DeleteDeltaVo.java
integration/spark-datasource/src/main/spark2.3plus/org/apache/spark/sql/CarbonDictionaryWrapper.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/reader/MeasureColumnChunkReader.java
hadoop/src/main/java/org/apache/carbondata/hadoop/util/CarbonVectorizedRecordReader.java
core/src/main/java/org/apache/carbondata/core/scan/result/iterator/DetailQueryResultIterator.java
core/src/main/java/org/apache/carbondata/core/indexstore/schema/CarbonRowSchema.java
processing/src/main/java/org/apache/carbondata/processing/loading/dictionary/DirectDictionary.java
integration/spark2/src/main/java/org/apache/carbondata/spark/readsupport/SparkRowReadSupportImpl.java
core/src/main/java/org/apache/carbondata/core/datastore/blocklet/BlockletEncodedColumnPage.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/impl/ParallelReadMergeSorterImpl.java
integration/spark-common/src/main/scala/org/apache/carbondata/spark/InitInputMetrics.java
core/src/main/java/org/apache/carbondata/core/devapi/DictionaryGenerationException.java
core/src/main/java/org/apache/carbondata/core/indexstore/BlockletDetailsFetcher.java
integration/presto/src/main/java/org/apache/carbondata/presto/CarbondataColumnConstraint.java
core/src/main/java/org/apache/carbondata/core/scan/filter/FilterExpressionProcessor.java
core/src/main/java/org/apache/carbondata/core/indexstore/Blocklet.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockDataMap.java
core/src/main/java/org/apache/carbondata/core/metadata/converter/SchemaConverter.java
processing/src/main/java/org/apache/carbondata/processing/loading/model/CarbonLoadModel.java
core/src/main/java/org/apache/carbondata/core/datastore/columnar/BlockIndexerStorageForShort.java
core/src/main/java/org/apache/carbondata/core/scan/result/vector/impl/CarbonColumnVectorImpl.java
processing/src/main/java/org/apache/carbondata/processing/loading/parser/RowParser.java
store/sdk/src/main/java/org/apache/carbondata/sdk/file/arrow/ArrowFieldWriter.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/impl/FixedLengthDimensionColumnPage.java
core/src/main/java/org/apache/carbondata/core/scan/collector/impl/AbstractScannedResultCollector.java
core/src/main/java/org/apache/carbondata/core/scan/executor/infos/MeasureInfo.java
processing/src/main/java/org/apache/carbondata/processing/loading/parser/impl/RowParserImpl.java
core/src/main/java/org/apache/carbondata/core/scan/collector/impl/RestructureBasedRawResultCollector.java
store/sdk/src/main/java/org/apache/carbondata/sdk/file/CarbonReaderBuilder.java
core/src/main/java/org/apache/carbondata/core/scan/executor/exception/QueryExecutionException.java
core/src/main/java/org/apache/carbondata/core/datastore/impl/FileFactory.java
core/src/main/java/org/apache/carbondata/core/datastore/filesystem/LocalCarbonFile.java
processing/src/main/java/org/apache/carbondata/processing/loading/row/CarbonSortBatch.java
core/src/main/java/org/apache/carbondata/core/scan/collector/impl/DictionaryBasedVectorResultCollector.java
core/src/main/java/org/apache/carbondata/core/scan/filter/FilterExecutorUtil.java
core/src/main/java/org/apache/carbondata/core/writer/CarbonDeleteDeltaWriter.java
core/src/main/java/org/apache/carbondata/core/metadata/converter/ThriftWrapperSchemaConverterImpl.java
core/src/main/java/org/apache/carbondata/core/readcommitter/TableStatusReadCommittedScope.java
core/src/main/java/org/apache/carbondata/core/scan/collector/ResultCollectorFactory.java
processing/src/main/java/org/apache/carbondata/processing/loading/converter/impl/AbstractDictionaryFieldConverterImpl.java
core/src/main/java/org/apache/carbondata/core/stats/QueryStatisticsRecorder.java
core/src/main/java/org/apache/carbondata/core/datamap/dev/expr/AndDataMapExprWrapper.java
integration/presto/src/main/java/org/apache/carbondata/presto/readers/FloatStreamReader.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockletDataMapDistributable.java
hadoop/src/main/java/org/apache/carbondata/hadoop/CacheAccessClient.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/partition/PartitionType.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/resolverinfo/visitor/NoDictionaryTypeVisitor.java
core/src/main/java/org/apache/carbondata/core/util/path/CarbonTablePath.java
core/src/main/java/org/apache/carbondata/core/indexstore/TableBlockIndexUniqueIdentifierWrapper.java
core/src/main/java/org/apache/carbondata/core/datastore/page/statistics/ColumnPageStatsCollector.java
core/src/main/java/org/apache/carbondata/core/util/DataTypeConverter.java
core/src/main/java/org/apache/carbondata/core/metadata/datatype/StructField.java
datamap/lucene/src/main/java/org/apache/carbondata/datamap/lucene/LuceneDataMapFactoryBase.java
store/sdk/src/main/java/org/apache/carbondata/sdk/file/arrow/ArrowWriter.java
integration/spark-datasource/src/main/spark2.1andspark2.2/org/apache/spark/sql/CarbonDictionaryWrapper.java
core/src/main/java/org/apache/carbondata/core/scan/collector/ScannedResultCollector.java
core/src/main/java/org/apache/carbondata/hadoop/internal/ObjectArrayWritable.java
core/src/main/java/org/apache/carbondata/core/datamap/dev/expr/DataMapDistributableWrapper.java
core/src/main/java/org/apache/carbondata/core/scan/wrappers/ByteArrayWrapper.java
core/src/main/java/org/apache/carbondata/core/datamap/dev/BlockletSerializer.java
core/src/main/java/org/apache/carbondata/core/stats/QueryStatisticsRecorderImpl.java
core/src/main/java/org/apache/carbondata/core/scan/scanner/impl/BlockletFullScanner.java
core/src/main/java/org/apache/carbondata/core/datastore/columnar/BlockIndexerStorageForNoDictionary.java
core/src/main/java/org/apache/carbondata/core/datastore/compression/CompressorFactory.java
core/src/main/java/org/apache/carbondata/core/metadata/ColumnIdentifier.java
core/src/main/java/org/apache/carbondata/core/localdictionary/dictionaryholder/DictionaryStore.java
core/src/main/java/org/apache/carbondata/core/scan/expression/logical/FalseExpression.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelRangeTypeExecuterFactory.java
processing/src/main/java/org/apache/carbondata/processing/loading/parser/impl/StructParserImpl.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/DimColumnExecuterFilterInfo.java
core/src/main/java/org/apache/carbondata/core/localdictionary/generator/LocalDictionaryGenerator.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/column/ColumnSchema.java
tools/cli/src/main/java/org/apache/carbondata/tool/FileCollector.java
core/src/main/java/org/apache/carbondata/core/indexstore/BlockletDetailInfo.java
core/src/main/java/org/apache/carbondata/core/scan/collector/impl/RestructureBasedDictionaryResultCollector.java
core/src/main/java/org/apache/carbondata/core/datamap/dev/expr/DataMapExprWrapperImpl.java
core/src/main/java/org/apache/carbondata/core/reader/CarbonDeleteDeltaFileReader.java
core/src/main/java/org/apache/carbondata/core/scan/result/iterator/VectorDetailQueryResultIterator.java
hadoop/src/main/java/org/apache/carbondata/hadoop/AbstractRecordReader.java
processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactDataHandlerModel.java
core/src/main/java/org/apache/carbondata/core/stats/QueryStatistic.java
core/src/main/java/org/apache/carbondata/core/mutate/TupleIdEnum.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/store/ColumnPageWrapper.java
core/src/main/java/org/apache/carbondata/core/datastore/FileReader.java
hadoop/src/main/java/org/apache/carbondata/hadoop/CarbonProjection.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/IncludeFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/datamap/TableDataMap.java
processing/src/main/java/org/apache/carbondata/processing/loading/csvinput/StringArrayWritable.java
processing/src/main/java/org/apache/carbondata/processing/loading/converter/impl/MeasureFieldConverterImpl.java
processing/src/main/java/org/apache/carbondata/processing/loading/converter/BadRecordLogHolder.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/impl/AbstractDimensionColumnPage.java
core/src/main/java/org/apache/carbondata/core/metadata/index/BlockIndexInfo.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/column/ParentColumnTableRelation.java
core/src/main/java/org/apache/carbondata/core/scan/expression/RangeExpressionEvaluator.java
integration/presto/src/main/java/org/apache/carbondata/presto/impl/CarbonTableConfig.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/resolverinfo/MeasureColumnResolvedFilterInfo.java
core/src/main/java/org/apache/carbondata/core/indexstore/BlockletDataMapIndexStore.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/resolverinfo/visitor/ImplicitColumnVisitor.java
core/src/main/java/org/apache/carbondata/core/datastore/compression/AbstractCompressor.java
core/src/main/java/org/apache/carbondata/core/scan/result/iterator/AbstractDetailQueryResultIterator.java
datamap/bloom/src/main/java/org/apache/hadoop/util/bloom/CarbonBloomFilter.java
core/src/main/java/org/apache/carbondata/core/scan/executor/util/QueryUtil.java
core/src/main/java/org/apache/carbondata/core/datastore/page/UnsafeFixLengthColumnPage.java
processing/src/main/java/org/apache/carbondata/processing/loading/parser/CarbonParserFactory.java
processing/src/main/java/org/apache/carbondata/processing/store/writer/v3/CarbonFactDataWriterImplV3.java
integration/hive/src/main/java/org/apache/carbondata/hive/CarbonArrayInspector.java
core/src/main/java/org/apache/carbondata/core/datastore/DataRefNode.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/impl/ParallelReadMergeSorterWithColumnRangeImpl.java
store/sdk/src/main/java/org/apache/carbondata/sdk/file/utils/SDKUtil.java
core/src/main/java/org/apache/carbondata/core/scan/result/BlockletScannedResult.java
datamap/bloom/src/main/java/org/apache/carbondata/datamap/bloom/BloomCoarseGrainDataMapFactory.java
core/src/main/java/org/apache/carbondata/core/util/ThreadLocalTaskInfo.java
processing/src/main/java/org/apache/carbondata/processing/loading/FailureCauses.java
core/src/main/java/org/apache/carbondata/core/util/ObjectSerializationUtil.java
processing/src/main/java/org/apache/carbondata/processing/loading/parser/impl/ArrayParserImpl.java
core/src/main/java/org/apache/carbondata/core/datastore/TableSpec.java
core/src/main/java/org/apache/carbondata/core/metadata/datatype/DecimalConverterFactory.java
integration/spark-datasource/src/main/spark2.1andspark2.2/org/apache/spark/sql/CarbonVectorProxy.java
core/src/main/java/org/apache/carbondata/core/reader/CarbonIndexFileReader.java
processing/src/main/java/org/apache/carbondata/processing/loading/jsoninput/JsonStreamReader.java
core/src/main/java/org/apache/carbondata/core/keygenerator/columnar/impl/MultiDimKeyVarLengthVariableSplitGenerator.java
processing/src/main/java/org/apache/carbondata/processing/merger/NodeMultiBlockRelation.java
integration/spark-datasource/src/main/spark2.3plus/org/apache/spark/sql/CarbonVectorProxy.java
integration/presto/src/main/java/org/apache/carbondata/presto/readers/SliceStreamReader.java
core/src/main/java/org/apache/carbondata/core/scan/filter/FilterUtil.java
processing/src/main/java/org/apache/carbondata/processing/loading/converter/impl/NonDictionaryFieldConverterImpl.java
core/src/main/java/org/apache/carbondata/core/datastore/columnar/BlockIndexerStorageForNoInvertedIndexForShort.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/column/CarbonColumn.java
processing/src/main/java/org/apache/carbondata/processing/loading/parser/impl/PrimitiveParserImpl.java
core/src/main/java/org/apache/carbondata/core/datastore/block/AbstractIndex.java
processing/src/main/java/org/apache/carbondata/processing/loading/events/LoadEvents.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/DataMapSchemaFactory.java
core/src/main/java/org/apache/carbondata/core/scan/expression/logical/TrueExpression.java
core/src/main/java/org/apache/carbondata/core/scan/executor/infos/DeleteDeltaInfo.java
processing/src/main/java/org/apache/carbondata/processing/loading/model/CarbonLoadModelBuilder.java
processing/src/main/java/org/apache/carbondata/processing/loading/row/IntermediateSortTempRow.java
core/src/main/java/org/apache/carbondata/core/mutate/UpdateVO.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/resolverinfo/visitor/DictionaryColumnVisitor.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/BitSetUpdaterFactory.java
processing/src/main/java/org/apache/carbondata/processing/loading/parser/impl/MapParserImpl.java
core/src/main/java/org/apache/carbondata/core/datamap/status/DataMapStatusStorageProvider.java
processing/src/main/java/org/apache/carbondata/processing/datatypes/ArrayDataType.java
core/src/main/java/org/apache/carbondata/core/scan/result/vector/CarbonColumnVector.java
integration/presto/src/main/java/org/apache/carbondata/presto/PrestoCarbonVectorizedRecordReader.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/resolverinfo/visitor/CustomTypeDictionaryVisitor.java
integration/presto/src/main/java/org/apache/carbondata/presto/Types.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonV3DataFormatConstants.java
core/src/main/java/org/apache/carbondata/core/scan/executor/impl/DetailQueryExecutor.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/impl/UnsafeParallelReadMergeSorterImpl.java
core/src/main/java/org/apache/carbondata/core/util/CarbonThreadFactory.java
core/src/main/java/org/apache/carbondata/core/datastore/page/DecoderBasedFallbackEncoder.java
store/sdk/src/main/java/org/apache/carbondata/sdk/file/CarbonWriterBuilder.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/SortParameters.java
processing/src/main/java/org/apache/carbondata/processing/merger/CarbonCompactionUtil.java
core/src/main/java/org/apache/carbondata/core/datamap/DataMapJob.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/column/ColumnUniqueIdGenerator.java
processing/src/main/java/org/apache/carbondata/processing/loading/steps/DataWriterProcessorStepImpl.java
integration/presto/src/main/java/org/apache/carbondata/presto/ColumnarVectorWrapperDirect.java
core/src/main/java/org/apache/carbondata/core/enums/EscapeSequences.java
core/src/main/java/org/apache/carbondata/core/scan/expression/conditional/ImplicitExpression.java
core/src/main/java/org/apache/carbondata/core/cache/dictionary/DictionaryChunksWrapper.java
core/src/main/java/org/apache/carbondata/core/datastore/filesystem/CarbonFile.java
core/src/main/java/org/apache/carbondata/core/util/CarbonProperties.java
core/src/main/java/org/apache/carbondata/core/scan/scanner/LazyPageLoader.java
integration/presto/src/main/java/org/apache/carbondata/presto/impl/CarbonLocalInputSplit.java
core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
core/src/main/java/org/apache/carbondata/core/service/impl/ColumnUniqueIdGenerator.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/impl/DimensionRawColumnChunk.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/ImplicitColumnFilterExecutor.java
core/src/main/java/org/apache/carbondata/core/scan/processor/BlockScan.java
core/src/main/java/org/apache/carbondata/core/datastore/block/SegmentProperties.java
core/src/main/java/org/apache/carbondata/core/metadata/blocklet/SegmentInfo.java
core/src/main/java/org/apache/carbondata/core/reader/CarbonDeleteFilesDataReader.java
core/src/main/java/org/apache/carbondata/core/scan/result/vector/impl/CarbonDictionaryImpl.java
core/src/main/java/org/apache/carbondata/core/datastore/blocklet/EncodedBlocklet.java
processing/src/main/java/org/apache/carbondata/processing/merger/CompactionResultSortProcessor.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelRangeLessThanFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/datastore/filesystem/S3CarbonFile.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/TableSchema.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RangeValueFilterExecuterImpl.java
processing/src/main/java/org/apache/carbondata/processing/loading/parser/ComplexParser.java
core/src/main/java/org/apache/carbondata/core/datamap/dev/fgdatamap/FineGrainDataMapFactory.java
core/src/main/java/org/apache/carbondata/core/stats/DriverQueryStatisticsRecorderImpl.java
core/src/main/java/org/apache/carbondata/core/service/ColumnUniqueIdService.java
processing/src/main/java/org/apache/carbondata/processing/loading/parser/impl/JsonRowParser.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/SegmentIndexFileStore.java
processing/src/main/java/org/apache/carbondata/processing/merger/RowResultMergerProcessor.java
core/src/main/java/org/apache/carbondata/core/indexstore/UnsafeMemoryDMStore.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonInputFormat.java
core/src/main/java/org/apache/carbondata/core/util/DataFileFooterConverterFactory.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/TableFieldStat.java
processing/src/main/java/org/apache/carbondata/processing/loading/exception/BadRecordFoundException.java
core/src/main/java/org/apache/carbondata/core/memory/UnsafeSortMemoryManager.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/ExcludeFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/stream/ExtendedByteArrayOutputStream.java
integration/hive/src/main/java/org/apache/carbondata/hive/CarbonHiveSerDe.java
core/src/main/java/org/apache/carbondata/core/locks/ZooKeeperLocking.java
processing/src/main/java/org/apache/carbondata/processing/loading/parser/impl/RangeColumnParserImpl.java
hadoop/src/main/java/org/apache/carbondata/hadoop/readsupport/CarbonReadSupport.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/reader/DimensionColumnChunkReader.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/FilterExecuter.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonTableInputFormat.java
core/src/main/java/org/apache/carbondata/core/scan/expression/exception/FilterIllegalMemberException.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/reader/CarbonDataReaderFactory.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/OrFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/datamap/dev/DataMap.java
core/src/main/java/org/apache/carbondata/core/scan/processor/RawBlockletColumnChunks.java
datamap/bloom/src/main/java/org/apache/carbondata/datamap/bloom/BloomDataMapModel.java
processing/src/main/java/org/apache/carbondata/processing/loading/model/CarbonDataLoadSchema.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/SortIntermediateFileMerger.java
core/src/main/java/org/apache/carbondata/core/metadata/blocklet/DataFileFooter.java
core/src/main/java/org/apache/carbondata/hadoop/CarbonInputSplit.java
core/src/main/java/org/apache/carbondata/core/datastore/columnar/ColumnWithRowIdForNoDictionary.java
core/src/main/java/org/apache/carbondata/core/scan/result/vector/MeasureDataVectorProcessor.java
core/src/main/java/org/apache/carbondata/core/util/TaskMetricsMap.java
core/src/main/java/org/apache/carbondata/core/scan/result/impl/NonFilterQueryScannedResult.java
core/src/main/java/org/apache/carbondata/core/scan/executor/QueryExecutorFactory.java
core/src/main/java/org/apache/carbondata/core/writer/CarbonIndexFileWriter.java
processing/src/main/java/org/apache/carbondata/processing/loading/steps/CarbonRowDataWriterProcessorStepImpl.java
core/src/main/java/org/apache/carbondata/core/reader/CarbonHeaderReader.java
integration/hive/src/main/java/org/apache/carbondata/hive/CarbonObjectInspector.java
integration/spark2/src/main/java/org/apache/carbondata/datamap/DataMapManager.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/resolverinfo/FalseConditionalResolverImpl.java
processing/src/main/java/org/apache/carbondata/processing/store/writer/v3/BlockletDataHolder.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/store/impl/safe/SafeVariableLengthDimensionDataChunkStore.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/AbstractRawColumnChunk.java
core/src/main/java/org/apache/carbondata/core/datamap/status/DataMapStatusDetail.java
core/src/main/java/org/apache/carbondata/core/datastore/block/TableBlockInfo.java
core/src/main/java/org/apache/carbondata/core/scan/executor/util/RestructureUtil.java
core/src/main/java/org/apache/carbondata/core/datamap/dev/expr/OrDataMapExprWrapper.java
integration/hive/src/main/java/org/apache/carbondata/hive/CarbonDictionaryDecodeReadSupport.java
integration/spark-datasource/src/main/spark2.3plus/org/apache/spark/sql/ColumnVectorFactory.java
core/src/main/java/org/apache/carbondata/core/scan/result/vector/ColumnVectorInfo.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/NewRowComparatorForNormalDims.java
core/src/main/java/org/apache/carbondata/core/scan/scanner/BlockletScanner.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/store/impl/unsafe/UnsafeVariableLengthDimensionDataChunkStore.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/reader/measure/v3/MeasureChunkPageReaderV3.java
core/src/main/java/org/apache/carbondata/core/fileoperations/AtomicFileOperationS3Impl.java
core/src/main/java/org/apache/carbondata/core/scan/result/vector/CarbonDictionary.java
core/src/main/java/org/apache/carbondata/core/indexstore/row/DataMapRowImpl.java
core/src/main/java/org/apache/carbondata/core/indexstore/row/DataMapRow.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/sort/TimSort.java
core/src/main/java/org/apache/carbondata/core/metadata/blocklet/datachunk/DataChunk.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/reader/measure/AbstractMeasureChunkReader.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/SchemaEvolution.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/reader/measure/v3/MeasureChunkReaderV3.java
processing/src/main/java/org/apache/carbondata/processing/util/Auditor.java
core/src/main/java/org/apache/carbondata/core/datastore/filesystem/ViewFSCarbonFile.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/RowLevelRangeFilterResolverImpl.java
integration/hive/src/main/java/org/apache/carbondata/hive/MapredCarbonInputFormat.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/IndexWrapper.java
hadoop/src/main/java/org/apache/carbondata/hadoop/stream/CarbonStreamUtils.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/impl/VariableLengthDimensionColumnPage.java
core/src/main/java/org/apache/carbondata/core/util/DataFileFooterConverter2.java
core/src/main/java/org/apache/carbondata/core/scan/collector/impl/RowIdRestructureBasedRawResultCollector.java
core/src/main/java/org/apache/carbondata/core/scan/executor/impl/QueryExecutorProperties.java
core/src/main/java/org/apache/carbondata/core/util/BitSetGroup.java
core/src/main/java/org/apache/carbondata/core/scan/collector/impl/DictionaryBasedResultCollector.java
core/src/main/java/org/apache/carbondata/core/keygenerator/directdictionary/DirectDictionaryKeyGeneratorFactory.java
core/src/main/java/org/apache/carbondata/core/metadata/CarbonMetadata.java
processing/src/main/java/org/apache/carbondata/processing/loading/converter/impl/BinaryFieldConverterImpl.java
processing/src/main/java/org/apache/carbondata/processing/loading/converter/impl/RowConverterImpl.java
processing/src/main/java/org/apache/carbondata/processing/loading/parser/GenericParser.java
core/src/main/java/org/apache/carbondata/core/stats/QueryStatisticsRecorderDummy.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/sort/UnsafeIntSortDataFormat.java
core/src/main/java/org/apache/carbondata/core/scan/collector/impl/RestructureBasedVectorResultCollector.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/SchemaReader.java
core/src/main/java/org/apache/carbondata/events/OperationEventListener.java
integration/presto/src/main/java/org/apache/carbondata/presto/CarbonVectorBatch.java
core/src/main/java/org/apache/carbondata/core/datastore/page/ActualDataBasedFallbackEncoder.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/resolverinfo/visitor/MeasureColumnVisitor.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/holder/UnsafeCarbonRowForMerge.java
integration/hive/src/main/java/org/apache/carbondata/hive/CarbonHiveInputSplit.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelRangeGrtThanFiterExecuterImpl.java
datamap/bloom/src/main/java/org/apache/carbondata/datamap/bloom/BloomCacheKeyValue.java
core/src/main/java/org/apache/carbondata/core/datastore/block/SegmentPropertiesAndSchemaHolder.java
core/src/main/java/org/apache/carbondata/core/datamap/dev/DataMapWriter.java
core/src/main/java/org/apache/carbondata/core/memory/MemoryBlock.java
core/src/main/java/org/apache/carbondata/core/scan/executor/infos/BlockExecutionInfo.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/Sorter.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/merger/UnsafeIntermediateMerger.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/RelationIdentifier.java
core/src/main/java/org/apache/carbondata/core/writer/CarbonIndexFileMergeWriter.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/LogicalFilterResolverImpl.java
datamap/bloom/src/main/java/org/apache/carbondata/datamap/bloom/BloomDataMapCache.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/AbstractMergeSorter.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/datamap/DataMapDistributable.java
datamap/bloom/src/main/java/org/apache/carbondata/datamap/bloom/BloomCoarseGrainDataMap.java
core/src/main/java/org/apache/carbondata/core/scan/processor/DataBlockIterator.java
core/src/main/java/org/apache/carbondata/core/datastore/impl/DFSFileReaderImpl.java
core/src/main/java/org/apache/carbondata/core/scan/result/iterator/RawResultIterator.java
core/src/main/java/org/apache/carbondata/core/keygenerator/directdictionary/timestamp/TimeStampGranularityTypeValue.java
processing/src/main/java/org/apache/carbondata/processing/loading/csvinput/CSVInputFormat.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/resolverinfo/visitor/FilterInfoTypeVisitorFactory.java
tools/cli/src/main/java/org/apache/carbondata/tool/DataSummary.java
core/src/main/java/org/apache/carbondata/core/memory/CarbonUnsafe.java
processing/src/main/java/org/apache/carbondata/processing/loading/CarbonDataLoadConfiguration.java
core/src/main/java/org/apache/carbondata/core/util/AbstractDataFileFooterConverter.java
core/src/main/java/org/apache/carbondata/core/datamap/DataMapLevel.java
processing/src/main/java/org/apache/carbondata/processing/loading/csvinput/BoundedInputStream.java
core/src/main/java/org/apache/carbondata/core/scan/collector/impl/RowIdRawBasedResultCollector.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/impl/MeasureRawColumnChunk.java
processing/src/main/java/org/apache/carbondata/processing/merger/CarbonCompactionExecutor.java
core/src/main/java/org/apache/carbondata/core/datamap/dev/expr/DataMapExprWrapper.java
processing/src/main/java/org/apache/carbondata/processing/store/writer/AbstractFactDataWriter.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockletDataRefNode.java
core/src/main/java/org/apache/carbondata/core/datamap/Segment.java
processing/src/main/java/org/apache/carbondata/processing/loading/steps/SortProcessorStepImpl.java
core/src/main/java/org/apache/carbondata/core/datamap/DistributableDataMapFormat.java
core/src/main/java/org/apache/carbondata/core/datamap/dev/cgdatamap/CoarseGrainDataMap.java
processing/src/main/java/org/apache/carbondata/processing/merger/CarbonDataMergerUtil.java
core/src/main/java/org/apache/carbondata/core/keygenerator/directdictionary/timestamp/TimeStampDirectDictionaryGenerator.java
core/src/main/java/org/apache/carbondata/core/util/CarbonMetadataUtil.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/MeasureColumnExecuterFilterInfo.java
core/src/main/java/org/apache/carbondata/core/indexstore/schema/SchemaGenerator.java
core/src/main/java/org/apache/carbondata/core/keygenerator/directdictionary/timestamp/DateDirectDictionaryGenerator.java
core/src/main/java/org/apache/carbondata/core/scan/executor/QueryExecutor.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/reader/dimension/v3/DimensionChunkPageReaderV3.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/resolverinfo/visitor/ResolvedFilterInfoVisitorIntf.java
core/src/main/java/org/apache/carbondata/core/datamap/DataMapChooser.java
core/src/main/java/org/apache/carbondata/core/localdictionary/dictionaryholder/MapBasedDictionaryStore.java
processing/src/main/java/org/apache/carbondata/processing/loading/converter/impl/FieldEncoderFactory.java
core/src/main/java/org/apache/carbondata/core/datastore/page/ColumnPageValueConverter.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/reader/dimension/v3/DimensionChunkReaderV3.java
core/src/main/java/org/apache/carbondata/core/scan/result/iterator/ColumnDriftRawResultIterator.java
core/src/main/java/org/apache/carbondata/core/datamap/status/DataMapStatusManager.java
core/src/main/java/org/apache/carbondata/core/metadata/encoder/Encoding.java
core/src/main/java/org/apache/carbondata/core/datamap/dev/DataMapBuilder.java
core/src/main/java/org/apache/carbondata/core/scan/executor/impl/VectorDetailQueryExecutor.java
core/src/main/java/org/apache/carbondata/core/scan/expression/conditional/GreaterThanExpression.java
core/src/main/java/org/apache/carbondata/core/util/DataFileFooterConverterV3.java
core/src/main/java/org/apache/carbondata/core/scan/filter/intf/FilterExecuterType.java
core/src/main/java/org/apache/carbondata/core/keygenerator/directdictionary/timestamp/TimeStampGranularityConstants.java
processing/src/main/java/org/apache/carbondata/processing/loading/jsoninput/JsonInputFormat.java
core/src/main/java/org/apache/carbondata/core/datastore/page/ColumnPage.java
core/src/main/java/org/apache/carbondata/core/metadata/SegmentFileStore.java
core/src/main/java/org/apache/carbondata/events/OperationContext.java
core/src/main/java/org/apache/carbondata/core/statusmanager/SegmentStatusManager.java
processing/src/main/java/org/apache/carbondata/processing/merger/NodeBlockRelation.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/ConditionalFilterResolverImpl.java
datamap/bloom/src/main/java/org/apache/carbondata/datamap/bloom/BloomDataMapWriter.java
integration/presto/src/main/java/org/apache/carbondata/presto/PrestoFilterUtil.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockletDataMapModel.java
core/src/main/java/org/apache/carbondata/core/datamap/dev/fgdatamap/FineGrainBlocklet.java
core/src/main/java/org/apache/carbondata/core/scan/expression/ColumnExpression.java
core/src/main/java/org/apache/carbondata/core/indexstore/blockletindex/BlockletDataMap.java
hadoop/src/main/java/org/apache/carbondata/hadoop/readsupport/impl/DictionaryDecodeReadSupport.java
core/src/main/java/org/apache/carbondata/core/datamap/SegmentDataMapGroup.java
core/src/main/java/org/apache/carbondata/core/localdictionary/PageLevelDictionary.java
processing/src/main/java/org/apache/carbondata/processing/loading/converter/RowConverter.java
core/src/main/java/org/apache/carbondata/core/datastore/page/encoding/ColumnPageEncoder.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/metadata/FilterResolverMetadata.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelRangeLessThanEqualFilterExecuterImpl.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/holder/UnsafeInmemoryMergeHolder.java
core/src/main/java/org/apache/carbondata/core/scan/result/impl/FilterQueryScannedResult.java
processing/src/main/java/org/apache/carbondata/processing/merger/CompactionType.java
core/src/main/java/org/apache/carbondata/core/metadata/datatype/BinaryType.java
core/src/main/java/org/apache/carbondata/core/datastore/block/TableBlockUniqueIdentifier.java
core/src/main/java/org/apache/carbondata/core/datastore/page/statistics/PrimitivePageStatsCollector.java
hadoop/src/main/java/org/apache/carbondata/hadoop/CarbonRecordReader.java
core/src/main/java/org/apache/carbondata/core/metadata/AbsoluteTableIdentifier.java
texts:optimize java code checkstyle for EmptyLineSeparator rule
optimize java code checkstyle for EmptyLineSeparator rule
issueID:CARBONDATA-3577
type:Improvement
changed files:
texts:Use Spark 2.3 as default version and upgrade Spark 2.3.2 to 2.3.4

issueID:CARBONDATA-3579
type:Improvement
changed files:
texts:Support merge index files when adding partition
Normally, application will use Carbon SDK to write files into a partition folder, then add the folder to partitioned carbon table. If there are many threads writes to the same partition folder, there will be many carbon index files, and it is not good for query performance since all index files need to be read to spark driver.So, a better way is to merge the index files when adding new partition to carbon table.
issueID:CARBONDATA-358
type:Bug
changed files:
texts:Compaction is not working in latest release
Compaction feature is not working in open source 0.2 release  (Major, Minor and Auto)
issueID:CARBONDATA-3583
type:Improvement
changed files:
texts:Upgrade default JDK version 1.7 to 1.8
Upgrade default JDK version 1.7 to 1.8 which provides some good features that makes code cleaner.
issueID:CARBONDATA-3584
type:Test
changed files:core/src/main/java/org/apache/carbondata/core/util/DataTypeUtil.java
texts:Select Query fails for Boolean dictionary column when Codegen is false
Please follow steps to reproduce the issue: create table carbon_table(a1 boolean,a2 binary,a3 int) stored by 'carbondata' tblproperties('dictionary_include'='a1') insert into carbon_table select true,'a',1 set spark.sql.codegen.wholestage=false select a1 from carbon_table
issueID:CARBONDATA-3589
type:Bug
changed files:
texts:insert into select from hive table into carbon table having partition fails with index server running
Steps :CREATE TABLE uniqdata_int_string(ACTIVE_EMUI_VERSION string, DOB timestamp, DOJ timestamp, BIGINT_COLUMN1 bigint,BIGINT_COLUMN2 bigint,DECIMAL_COLUMN1 decimal(30,10), DECIMAL_COLUMN2 decimal(36,10),Double_COLUMN1 double, Double_COLUMN2 double, INTEGER_COLUMN1 int) Partitioned by (cust_id int, cust_name string) STORED BY 'org.apache.carbondata.format' TBLPROPERTIES ("TABLE_BLOCKSIZE"= "256 MB");LOAD DATA INPATH 'hdfs://hacluster/chetan/2000_UniqData.csv' into table uniqdata_int_string partition(cust_id='1', cust_name='CUST_NAME_00002') OPTIONS ('FILEHEADER'='CUST_ID,CUST_NAME ,ACTIVE_EMUI_VERSION,DOB,DOJ, BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1, Double_COLUMN2,INTEGER_COLUMN1','BAD_RECORDS_ACTION'='FORCE');0: jdbc:hive2://10.18.98.225:22550/default> CREATE TABLE uniqdata_hive (CUST_ID int,CUST_NAME String,ACTIVE_EMUI_VERSION string, DOB timestamp, DOJ timestamp, BIGINT_COLUMN1 bigint,BIGINT_COLUMN2 bigint,DECIMAL_COLUMN1 decimal(30,10), DECIMAL_COLUMN2 decimal(36,10),Double_COLUMN1 double, Double_COLUMN2 double, INTEGER_COLUMN1 int)ROW FORMAT DELIMITED FIELDS TERMINATED BY ',';---------+Result ---------+---------+No rows selected (0.6 seconds) Issue : - insert into select from hive table into carbon table having partition fails with index server running.0: jdbc:hive2://10.18.98.225:22550/default> insert into uniqdata_int_string partition(cust_id='1',cust_name='CUST_NAME_00002') select * from uniqdata_hive limit 10;Error: java.util.NoSuchElementException: None.get (state=,code=0)Expected :- insert into select from hive table into carbon table having partition should be success with index server running.
issueID:CARBONDATA-359
type:Bug
changed files:
texts:is null & null functions are not working when data fetching from sub query
Is null & Not null functions are not working when data fetching from sub query.         select * from (select if( Latest_areaId=3,1,Latest_areaId) test from Carbon_automation) qq where test is  null;         select * from (select if( Latest_areaId=3,1,Latest_areaId) test from Carbon_automation) qq where test is  not null;
issueID:CARBONDATA-36
type:Bug
changed files:
texts:fix dictionary exception when data column count less then schema
when csv head is id,name,age,cityif the data record like as the following:1,a,10,china2,b3,c,11,usathen,when read this row:2,b,generating dicionary will failure,and the remain record (3,c,11,usa)will no generate dictionary.
issueID:CARBONDATA-360
type:Bug
changed files:
texts:On Dictionary excluded column, condition is not working if value is not in &#39;&#39;
On Dictionary excluded column, condition is not working if value is not in '' for   numeric
issueID:CARBONDATA-361
type:Bug
changed files:
texts:SMALL INT data showing unsupported datatype
When I tried executing the following query with SMALLINT datatypecreate table testTable2(id SMALLINT, name String) stored by 'carbondata' ;It is giving me following exception.Error: org.apache.carbondata.spark.exception.MalformedCarbonCommandException: Unsupported data type : FieldSchema(name:id, type:smallint, comment:null).getType (state=,code=0)
issueID:CARBONDATA-362
type:Bug
changed files:
texts:Optimize the parameters&#39; name in CarbonDataRDDFactory.scala
Optimize the parameters' name in CarbonDataRDDFactory.scala:changes the name of "hdfsStoreLocation"  to "storePath", because not only support hdfs path.
issueID:CARBONDATA-363
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/datastore/block/BlockInfo.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
texts:Block loading issue in case of blocklet distribution
Problem: In case of blocklet distribution same block is getting loaded multiple times this is because when blocklet distribution is enabled same block will divided inside a task so there is not synchronisation as block loading is done in different thread because of this same block is getting read from carbon data file footer multiple times and it is hitting the first time query performance.Solution: Need to add locking for above issue, if one thread is loading particulate block other thread need and once block loaded other thread need to use same reference of the block
issueID:CARBONDATA-364
type:Bug
changed files:
texts:Drop table is behaving inconsistently
ScenarioRun load command on table and then run drop table command for same table.Drop table will give message as table is locked for updation but actually its deleting all files table related files from store.
issueID:CARBONDATA-365
type:Bug
changed files:integration/spark-common/src/main/java/org/apache/carbondata/spark/merger/RowResultMerger.java
texts:Compaction fails when table is created with configured block size
When user creates table with configured block size as given below.Compaction fails after multiple load.create table T_Carbn06(Active_status String,Item_type_cd INT,Qty_day_avg INT,Qty_total INT,Sell_price BIGINT,Sell_pricep DOUBLE,Discount_price DOUBLE,Profit DECIMAL(3,2),Item_code String,Item_name String,Outlet_name String,Update_time TIMESTAMP,Create_date String)STORED BY 'org.apache.carbondata.format' TBLPROPERTIES('table_blocksize'='2047');
issueID:CARBONDATA-366
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
processing/src/main/java/org/apache/carbondata/processing/store/writer/AbstractFactDataWriter.java
texts:Incorrect load data behaviour in mentioned scenario
In some scenario, when user loads data system shows load is success full but when we query, there is no data.When we see at store location, 0 byte file is created for index file.
issueID:CARBONDATA-367
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/datastore/impl/FileFactory.java
core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
core/src/main/java/org/apache/carbondata/core/datastore/filesystem/AlluxioCarbonFile.java
texts:Add support alluxio(tachyon) file system(enhance ecosystem integration)
For supporting alluxio users to use higher performance file format(CarbonData), and enhance Apache CarbonData ecosystem integration.Can load alluxio file for example "alluxio://localhost:19998/data.csv" to Carbon Data.
issueID:CARBONDATA-368
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/metadata/schema/table/column/CarbonColumn.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/CarbonTable.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/column/CarbonDimension.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/column/ColumnSchema.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/column/CarbonMeasure.java
core/src/main/java/org/apache/carbondata/core/metadata/converter/ThriftWrapperSchemaConverterImpl.java
texts:Should improve performance of DataFrame loading

issueID:CARBONDATA-369
type:Improvement
changed files:
texts:Remove Useless Files in carbondata.scan.expression
Remove Useless Files in carbondata.scan.expressioncore/src/main/java/org/apache/carbondata/scan/expression/UnaryExpression.javacore/src/main/java/org/apache/carbondata/scan/expression/arithmetic/AddExpression.javacore/src/main/java/org/apache/carbondata/scan/expression/arithmetic/BinaryArithmeticExpression.javacore/src/main/java/org/apache/carbondata/scan/expression/arithmetic/DivideExpression.javacore/src/main/java/org/apache/carbondata/scan/expression/arithmetic/MultiplyExpression.javacore/src/main/java/org/apache/carbondata/scan/expression/arithmetic/SubstractExpression.javacore/src/main/java/org/apache/carbondata/scan/expression/logical/NotExpression.java
issueID:CARBONDATA-37
type:New Feature
changed files:core/src/main/java/org/apache/carbondata/core/keygenerator/directdictionary/timestamp/TimeStampDirectDictionaryGenerator.java
processing/src/main/java/org/apache/carbondata/processing/model/CarbonLoadModel.java
core/src/main/java/org/apache/carbondata/core/keygenerator/directdictionary/DirectDictionaryKeyGeneratorFactory.java
core/src/main/java/org/apache/carbondata/core/keygenerator/directdictionary/DirectDictionaryGenerator.java
integration/spark-common/src/main/java/org/apache/carbondata/spark/load/CarbonLoaderUtil.java
texts:Support Date/Time format for Timestamp columns to be defined at column level
Carbon support defining the Date/Time format. But the configuration for the same is present in carbon.properties and hence is global for all tables.This global configuration for timestamp format cannot support scenarios where different tables or different Timestamp columns in the same table.Suggest to provide option in the create table DDL itself to define the format for each Timestamp column. Also provide defaults so that users can create table with Timestamp columns without having to always define the Date/Time format.
issueID:CARBONDATA-370
type:Bug
changed files:
texts:Abnormal behaviour of datatype Int & bigInt
The specified data-type range for Int & bigInt is:INT –––> -2147483648 to 2147483647BIGINT ––> -922337203685477580807 to 922337203685477580806 but I found abnormal behaviour in these data-type . 1.When I insert beyond range in Int it allows range even beyond bigInt. 2.When I insert into bigInt it doesn’t accept all values following in its range.
issueID:CARBONDATA-371
type:Test
changed files:
texts:Write unit test for ColumnDictionaryInfo

issueID:CARBONDATA-373
type:Bug
changed files:
texts:Drop Database Cascade should be supported in Carbon
Currently drop database cascade command is restricted in CarbonSolution : Need to support drop database cascade command in carbon. Also below changes to be done in Carbon store.Create DB folder in carbon store when user triggers Create database commandRemoving DB folder from carbon store when user triggers Drop database commandRemoving Tables & DB folder from carbon store when user triggers Drop database cascade command
issueID:CARBONDATA-374
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/util/DataTypeUtil.java
core/src/main/java/org/apache/carbondata/core/scan/collector/impl/AbstractScannedResultCollector.java
core/src/main/java/org/apache/carbondata/core/metadata/datatype/DataType.java
texts:Short data type is not working.
Short datatype is not working as you have mentioned it is supported datatype in the below link:http://apache-carbondata-mailing-list-archive.1130556.n5.nabble.com/List-the-supported-datatypes-in-carbondata-td2419.htmle.g:create table testTable(id Short, name String) stored by 'carbondata' ;Error: org.apache.spark.sql.AnalysisException: cannot recognize input near 'Short' ',' 'name' in column type; line 1 pos 26 (state=,code=0)
issueID:CARBONDATA-375
type:Bug
changed files:
texts:Dictionary cache not getting cleared after task completion in dictionary decoder
Currently LRU cache eviction policy is based on dictionary access count. For cache to remove a entry its access count must be 0. In dictionary decoder after conversion of surrogate key to actual value the access count for dictionary columns in query is not getting decremented due to which it will never be cleared from memory when LRU cache size is configured.
issueID:CARBONDATA-376
type:Bug
changed files:
texts:Value beyond the integer range should not be allowed for storage in integer datatype
Integer datatype is supporting range beyond bigInt.
issueID:CARBONDATA-377
type:Sub-task
changed files:
texts:Improve code coverage for Core.Cache.Dictionary
Write Unit Test Cases for following files:DictionaryColumnUniqueIdentifier ForwardDictionaryReverseDictionary
issueID:CARBONDATA-378
type:Bug
changed files:
texts:Long datatype is not working
As it is mentioned in DataType.java file that long is supported datatype but when I run query using long datatype its displays an error.e.g:create table testTable(id long, name String) stored by 'carbondata' ;Error: org.apache.spark.sql.AnalysisException: cannot recognize input near 'long' ',' 'name' in column type; line 1 pos 29 (state=,code=0)
issueID:CARBONDATA-379
type:Test
changed files:
texts:Test Cases to be added for Scan package under org.apache.carbondata.core

issueID:CARBONDATA-38
type:Improvement
changed files:
texts:Cleanup carbon.properties
The carbon.properties.template file has several stale configurations which are not used from the code.Please clean up the same and retain only valid configuration properties.
issueID:CARBONDATA-380
type:Bug
changed files:
texts:Invalid output of Count function without arguments
Whenever I m trying to run the following query:select count() from emp5; It is giving invalid output (0) instead of error.
issueID:CARBONDATA-381
type:Bug
changed files:
texts:Unnecessary catalog metadata refresh and array index of bound exception in drop table
Problem:1. Whenever a catalog metadata is refreshed it modified the timestamp of modifiedTime.mdt file which leads to unnecessary refreshing the complete catalog metadata.2. Array Index of bound exception is thrown on failure of table creation.
issueID:CARBONDATA-385
type:Bug
changed files:
texts:Select query is giving cast exception
In below scenario, select query is giving error as belowError: java.lang.ClassCastException: java.lang.Integer cannot be cast to java.lang.String1. employee tablecreate table employee(name string, empid string, mgrid string, mobileno bigint) stored by 'carbondata'2.load below data in employee table tom,t23717,h2399,997802075263. manager tablecreate table manager(name string, empid string, mgrid string, mobileno bigint) stored by 'carbondata'4. load below data in manager table harry,h2399,v788232,998232302055.Run below queryselect e.empid from employee e inner join manager m on e.mgrid=m.empid#select empid,mgrid from employee
issueID:CARBONDATA-386
type:Test
changed files:
texts:Write unit test for Util Module

issueID:CARBONDATA-387
type:Bug
changed files:
texts:RLike and REGEXP showing all records in empty regex
When I'm trying to run RLIKE with empty regex, it is showing me all the records instead of no record.select * from knolders where age RLIKE '';It is behaving in a similar manner for REGEXP operator.
issueID:CARBONDATA-388
type:Improvement
changed files:
texts:Remove Useless File CarbonFileFolderComparator.java
Remove Useless File CarbonFileFolderComparator.java
issueID:CARBONDATA-389
type:Bug
changed files:
texts:Subquery not working
Whenever I execute select name from emp2 where exists (select id from emp3 where emp2.id=emp3.id);It gives me following exception:scala.NotImplementedError: No parse rules for ASTNode typeAnd also select name from emp4 where id in (select id from emp5);gives the same error
issueID:CARBONDATA-390
type:Bug
changed files:
texts:Float Data Type is Not Working
Float datatype is not working as you have mentioned it in the Carbon data types in the below link: https://cwiki.apache.org/confluence/display/CARBONDATA/Carbon+Data+Typese.g:Query: create table employee(id Int,name String,sal Float)stored by 'carbondata';Error:org.apache.carbondata.spark.exception.MalformedCarbonCommandException: Unsupported data type : FieldSchema(name:sal, type:float, comment:null).getType (state=,code=0)
issueID:CARBONDATA-391
type:Improvement
changed files:
texts:[Documentation] Information missing about how to load data from data source other than CSV

issueID:CARBONDATA-392
type:Bug
changed files:
texts:sql coalesce function is having problem in carbon for null data
When query is as belowselect * from table where coalesce( name ,1) != 'Lily';null row is not displayed.1. Create tableCREATE TABLE big_int(imei string,age int,task bigint,name string,country string,city string,sale int,num double,level decimal(10,3),quest bigint,productdate timestamp,enddate timestamp,PointId double,score decimal(10,3))STORED BY 'org.apache.carbondata.format' TBLPROPERTIES('dictionary_include' =  'imei,name,age,productdate,enddate,country ,city ,sale ,num ,PointId,level,score,task,quest',"COLUMNPROPERTIES.level.shared_column"= "share.level");2.LOAD DATA INPATH 'csvfile path'  INTO TABLE big_int options ('DELIMITER'=',', 'QUOTECHAR'='"');  below is content of csv fileimei0,2147,9279,fegt,china,hangzhou,10000,100.05,100.055,10,2016-05-01 12:25:36,2016-05-01 21:14:48,1,1.005imei1,-2148,-9807,lrhkr,America,NewYork,1000,10.05,100.05,100,2016-05-02 19:25:15,2016-05-02 22:25:46,2,1.05imei2,2147,9279,dfegt,china,hangzhou,10000,100.05,100.055,10,2016-05-01 12:25:36,2016-05-01 21:14:48,1,1.005imei3,-217,-9206,lrhkr,America,NewYork,1000,100.005,100.05,100,2016-05-02 19:25:15,2016-05-02 22:25:46,2,1.05imei4,10,0,Lily,America,LA,20,15.5,45,12,16,2016-06-12 12:25:12,2016-06-15 13:52:17,15.5,21imei5,3. run select query as belowselect imei,name,country,city,productdate,enddate,age,task,sale,num,level,quest,pointid,score from big_int where coalesce( name ,1) != 'Lily';There is null data in load  but same is not displayed
issueID:CARBONDATA-393
type:Test
changed files:
texts:Write Unit Test cases for core.keygenerator package

issueID:CARBONDATA-394
type:Bug
changed files:
texts:Carbon Loading data from files having invalid extensions or no extension
When I try to run the following queries :LOAD DATA inpath 'hdfs://localhost:54310/user/hive/warehouse/file1.csv.csv' INTO table empdata options('DELIMITER'=',', 'FILEHEADER'='id, name','QUOTECHAR'='"');LOAD DATA inpath 'hdfs://localhost:54310/user/hive/warehouse/file2.csv.csv.csv.csv' INTO table empdata options('DELIMITER'=',', 'FILEHEADER'='id, name','QUOTECHAR'='"'); LOAD DATA inpath 'hdfs://localhost:54310/user/hive/warehouse/file3.txttt' INTO table empdata options('DELIMITER'=',', 'FILEHEADER'='id, name','QUOTECHAR'='"');LOAD DATA inpath 'hdfs://localhost:54310/user/hive/warehouse/file4' INTO table empdata options('DELIMITER'=',', 'FILEHEADER'='id, name','QUOTECHAR'='"');LOAD DATA inpath 'hdfs://localhost:54310/user/hive/warehouse/file5.txt.bat.csv' INTO table empdata options('DELIMITER'=',', 'FILEHEADER'='id, name','QUOTECHAR'='"');We should get Input File Errors, but the data is loaded successfully into the Carbon table.
issueID:CARBONDATA-395
type:Test
changed files:
texts:Unit Test cases for package org.apache.carbondata.scan.expression.ExpressionResult

issueID:CARBONDATA-396
type:Test
changed files:
texts:Implement test cases for datastorage package

issueID:CARBONDATA-397
type:Improvement
changed files:
texts:Use of ANTLR instead of CarbonSqlParser for parsing queries
We are using CarbonSqlParser for parsing queries but we can use ANTLR for the same.  we could better handle query parsing with ANTLR.
issueID:CARBONDATA-398
type:Bug
changed files:
texts:In DropCarbonTable flow, Metadata lock should be acquired only if table exist
Issue : In drop table flow, we acquiring metadata lock even if table not exist which is creating table folder and creating meta.lock file.Solution : Check and acquire lock only if carbon table exist.
issueID:CARBONDATA-399
type:Bug
changed files:
texts:[Bad Records] Data Load is not FAILED even  bad_records_action="FAIL" .
Data Load is not FAILED when string data are loaded in the int column . 1. Create table  defect_5 (imei string ,deviceInformationId int,mac string,productdate timestamp,updatetime timestamp,gamePointId double,contractNumber double) stored by 'carbondata' TBLPROPERTIES('DICTIONARY_INCLUDE'='deviceInformationId') ;deviceInformationId  is int  ( it will handled as  dimension). Now load the data 2.  0: jdbc:hive2://ha-cluster/default> LOAD DATA  inpath 'hdfs://hacluster/tmp/100_default_date_11_header_2.csv' into table defect_5 options('DELIMITER'=',', 'bad_records_action'='FAIL',  'QUOTECHAR'='"','FILEHEADER'='imei,deviceinformationid,mac,productdate,updatetime,gamepointid,contractnumber');---------+ Result  ---------+---------+No rows selected (0.969 seconds)3. Data imei,deviceinformationid,mac,productdate,updatetime,gamepointid,contractnumber1AA1,babu,Mikaa1,2015-01-01 11:00:00,2015-01-01 13:00:00,10,2601AA2,3,Mikaa2,2015-01-02 12:00:00,2015-01-01 14:00:00,278,2301AA3,1,Mikaa1,2015-01-03 13:00:00,2015-01-01 15:00:00,2556,11AA4,10,Mikaa2,2015-01-04 14:00:00,2015-01-01 16:00:00,640,2541AA5,10,Mikaa,2015-01-05 15:00:00,2015-01-01 17:00:00,980,2561AA6,10,Mikaa,2015-01-06 16:00:00,2015-01-01 18:00:00,1,23781AA7,10,Mikaa,2015-01-07 17:00:00,2015-01-01 19:00:00,96,2341AA8,9,max,2015-01-08 18:00:00,2015-01-01 20:00:00,89,2361AA9,10,max,2015-01-09 19:00:00,2015-01-01 21:00:00,198.36,239.2Expect Outoput:- Data Load should FAIL
issueID:CARBONDATA-4
type:New Feature
changed files:
texts:CarbonData supports spark1.6.1
Currently CarbonData only supports spark1.5.2, need to support spark 1.6.1 also
issueID:CARBONDATA-40
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
texts:Make metastore_db location of derby configurable in CarbonContext
Currently metastore_db location of derby is not configurable and it is always created in current directory. Because of this constraint user not able to create multiple CarbonContext with derby db. This is the reason user cannot create CarbonContext in spark-shell with standalone derby db.To overcome the above issue we can make the derby db location configurable while creating CarbonContext.Case 1:val cc = new CarbonContext(sc)In above scenario, default store location at `./carbondata` it means store is created in current directory.And default metastore location `./carbonmetastore`, it means metastore is created at current directory.Case 2 :val cc = new CarbonContext(sc, storeLocation)In above scenario, default metastore location `./carbonmetastore`, it means metastore is created at current directory.Case 3: val cc = new CarbonContext(sc, storeLocation, metastoreLocation)In above scenario all parameters are passed, so store location and metastoreLocation would be created at desired location.
issueID:CARBONDATA-400
type:Bug
changed files:processing/src/main/java/org/apache/carbondata/processing/loading/csvinput/CSVInputFormat.java
texts:[Bad Records] Load data is fail and displaying the string value in beeline as exception
Steps1. Create tableCREATE TABLE String_test2 (string_col string) STORED BY 'org.apache.carbondata.format';2. Load the data with parameter 'BAD_RECORDS_ACTION'='FORCE' and csv contains a string value that is out of boundary.LOAD DATA INPATH 'hdfs://hacluster/Carbon/Priyal/string5.csv' into table String_test2 OPTIONS('DELIMITER'=',' , 'QUOTECHAR'='"','BAD_RECORDS_LOGGER_ENABLE'='TRUE', 'BAD_RECORDS_ACTION'='FORCE','FILEHEADER'='string_col');Actual Result: Load data is failed and displaying the string value in beeline as exception trace.Expected Result:Should display a correct error message and should  not print the exception trace on the console.Exception thrown on console is as shown below.Error: com.univocity.parsers.common.TextParsingException: Error processing input: Length of parsed input (100001) exceeds the maximum number of characters defined in your parser settings (100000).Hint: Number of characters processed may have exceeded limit of 100000 characters per column. Use settings.setMaxCharsPerColumn(int) to define the maximum number of characters a column can haveEnsure your configuration is correct, with delimiters, quotes and escape sequences that match the input format you are trying to parseParser Configuration: CsvParserSettings:        Column reordering enabled=true        Empty value=null        Header extraction enabled=false        Headers=null        Ignore leading whitespaces=true        Ignore trailing whitespaces=true        Input buffer size=128        Input reading on separate thread=false        Line separator detection enabled=false        Maximum number of characters per column=100000        Maximum number of columns=20480        Null value=        Number of records to read=all        Parse unescaped quotes=true        Row processor=none        Selected fields=none        Skip empty lines=trueFormat configuration:        CsvFormat:                Comment character=#                Field delimiter=,                Line separator (normalized)=\n                Line separator sequence=\n                Quote character="                Quote escape character=quote escape                Quote escape escape character=\0, line=0, char=100002. Content parsed: [hellohowareyouwelcomehellohellohellohellohellohellohellohelloheellooabcdefghijklmnopqrstuvwxyzabcqwertuyioplkjhgfdsazxcvbnmpoiuytrewqasdfghjklmnbvcxzasdghskhdgkhdbkshkjchskdhfssudkdjdudusdjhdshdshsjddshjdkdhgdhdshdhdududushdudududududududududududududududududuudududududududuudududududududududududududududududududududududududududuhellohowareyouwelcomehellohellohellohellohellohellohellohelloheellooabcdefghijklmnopqrstuvwxyzabcqwertuyioplkjhgfdsazxcvbnmpoiuytrewqasdfghjklmnbvcxzasdghskhdgkhdbkshkjchskdhfssudkdjdudusdjhdshdshsjddshjdkdhgdhdshdhdududushdudududududududududududududududududuudududududududuududududududududuu
issueID:CARBONDATA-401
type:Improvement
changed files:processing/src/main/java/org/apache/carbondata/processing/loading/converter/impl/FieldEncoderFactory.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/column/ColumnUniqueIdGenerator.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
processing/src/main/java/org/apache/carbondata/processing/datatypes/PrimitiveDataType.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/CarbonTable.java
core/src/main/java/org/apache/carbondata/core/cache/dictionary/ForwardDictionary.java
core/src/main/java/org/apache/carbondata/core/cache/dictionary/ColumnDictionaryInfo.java
processing/src/main/java/org/apache/carbondata/processing/loading/converter/impl/RowConverterImpl.java
core/src/main/java/org/apache/carbondata/core/cache/dictionary/AbstractColumnDictionaryInfo.java
processing/src/main/java/org/apache/carbondata/processing/loading/model/CarbonLoadModel.java
processing/src/main/java/org/apache/carbondata/processing/loading/DataLoadProcessBuilder.java
processing/src/main/java/org/apache/carbondata/processing/loading/CarbonDataLoadConfiguration.java
core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
texts:Look forward to support reading csv file only once in data loading
Now, In Carbon data loading module, generating global dictionary is independent.  Carbon read the csv file twice for generating global dictionary and loading carbon data, respectively. We look forward to read the csv file only once.
issueID:CARBONDATA-402
type:Improvement
changed files:
texts:carbon should support CreateAsSelect
provide support for CreateAsSelect,the syntax is hive Syntax,  like below:    Create  TABLE table4  STORED BY 'carbondata' AS SELECT * FROM table3
issueID:CARBONDATA-403
type:Improvement
changed files:
texts:add example for data load without using kettle
add example for data load without using kettle
issueID:CARBONDATA-404
type:Bug
changed files:
texts:Data loading from DataFrame to carbon table is FAILED
Data loading FAILED when   Loading data from DataFrame with tempCSV option =true    (Default option ) in 3 Node cluster .Steps val customSchema = StructType(Array(    StructField("imei", StringType, true),    StructField("deviceInformationId", IntegerType, true),    StructField("mac", StringType, true),    StructField("productdate", TimestampType , true),    StructField("updatetime", TimestampType, true),    StructField("gamePointId", DoubleType, true),    StructField("contractNumber", DoubleType, true)       ));val df = cc.read.format("com.databricks.spark.csv").option("header", "false").schema(customSchema).load("/opt/data/xyz/100_default_date_11_header.csv");Start data loading scala> df.write.format("carbondata").option("tableName","mycarbon2").save();INFO  10-11 23:24:35,970 - main Query [          CREATE TABLE IF NOT EXISTS DEFAULT.MYCARBON2          (IMEI STRING, DEVICEINFORMATIONID INT, MAC STRING, PRODUCTDATE TIMESTAMP, UPDATETIME TIMESTAMP, GAMEPOINTID DOUBLE, CONTRACTNUMBER DOUBLE)          STORED BY 'ORG.APACHE.CARBONDATA.FORMAT'      ]INFO  10-11 23:24:35,977 - Parsing command:          CREATE TABLE IF NOT EXISTS default.mycarbon2          (imei STRING, deviceInformationId INT, mac STRING, productdate TIMESTAMP, updatetime TIMESTAMP, gamePointId DOUBLE, contractNumber DOUBLE)          STORED BY 'org.apache.carbondata.format'INFO  10-11 23:24:35,978 - Parse CompletedINFO  10-11 23:24:36,227 - main Query [          LOAD DATA INPATH './TEMPCSV'          INTO TABLE DEFAULT.MYCARBON2          OPTIONS ('FILEHEADER' = 'IMEI,DEVICEINFORMATIONID,MAC,PRODUCTDATE,UPDATETIME,GAMEPOINTID,CONTRACTNUMBER')      ]INFO  10-11 23:24:36,233 - Successfully able to get the table metadata file lockAUDIT 10-11 23:24:36,234 - &#91;BLR1000007781&#93;&#91;root&#93;&#91;Thread-1&#93;Dataload failed for default.mycarbon2. The input file does not exist: ./tempCSVINFO  10-11 23:24:36,234 - main Successfully deleted the lock file /tmp/default/mycarbon2/meta.lockINFO  10-11 23:24:36,234 - Table MetaData Unlocked Successfully after data loadorg.apache.carbondata.processing.etl.DataLoadingException: The input file does not exist: ./tempCSV        at org.apache.spark.util.FileUtils$$anonfun$getPaths$1.apply$mcVI$sp(FileUtils.scala:66)CSV DATA1AA1,1,Mikaa1,2015-01-01 11:00:00,2015-01-01 13:00:00,198,2601AA2,3,Mikaa2,2015-01-02 12:00:00,2015-01-01 14:00:00,278,2301AA3,1,Mikaa1,2015-01-03 13:00:00,2015-01-01 15:00:00,2556,11AA4,10,Mikaa2,2015-01-04 14:00:00,2015-01-01 16:00:00,640,2541AA5,10,Mikaa,2015-01-05 15:00:00,2015-01-01 17:00:00,980,2561AA6,10,Mikaa,2015-01-06 16:00:00,2015-01-01 18:00:00,1,23781AA7,10,Mikaa,2015-01-07 17:00:00,2015-01-01 19:00:00,96,2341AA8,9,max,2015-01-08 18:00:00,2015-01-01 20:00:00,89,236
issueID:CARBONDATA-405
type:Bug
changed files:
texts:Data load fail if dataframe is created with LONG datatype column .
Loading data from dataframe having LONG data type is failling with below error . Follow the below stepsscala> val mydf = cc.read.format("json").load("/opt/data/people.json")scala> mydf.printSchemaroot&#8211; age: long (nullable = true)&#8211; name: string (nullable = true)scala> mydf.write.format("carbondata").option("tableName","mycarbon3").save();INFO  10-11 23:33:28,872 - main Query [          CREATE TABLE IF NOT EXISTS DEFAULT.MYCARBON3          (AGE LONG, NAME STRING)          STORED BY 'ORG.APACHE.CARBONDATA.FORMAT'      ]INFO  10-11 23:33:28,877 - Parsing command:          CREATE TABLE IF NOT EXISTS default.mycarbon3          (age LONG, name STRING)          STORED BY 'org.apache.carbondata.format'NoViableAltException(162@[])        at org.apache.hadoop.hive.ql.parse.HiveParser.type(HiveParser.java:38610)        at org.apache.hadoop.hive.ql.parse.HiveParser.colType(HiveParser.java:38367)        at org.apache.hadoop.hive.ql.parse.HiveParser.columnNameType(HiveParser.java:38051)Expected:- long data type is not supported so it should be converted to supported data type ( Long to bigint) Json file values{"name":"Michael"}{"name":"Andy", "age":30}{"name":"Justin", "age":19}
issueID:CARBONDATA-408
type:Bug
changed files:
texts:Unable to create view from a table
When we tried to execute the following query to create view in carbon :create view emp_view AS Select name,sal from demo2;NOTE :demo2 table contains following columns: id Int,name String, sal decimalwe got the following exception:Error: org.apache.spark.sql.execution.QueryExecutionException: FAILED: SemanticException &#91;Error 10004&#93;: Line 1:31 Invalid table alias or column reference 'name': (possible column names are: col) (state=,code=0)where as we are able to create view in hive using the same query.
issueID:CARBONDATA-409
type:Bug
changed files:
texts:Drop non-existing macro executes successfully while it must give an error.
I have created a macro :CREATE TEMPORARY MACRO simple_add (x int, y int) x + y;then i dropped the macro.     > drop temporary macro simple_add;OKTime taken: 0.038 secondshive>     >     > select simple_add(2,3);FAILED: SemanticException &#91;Error 10011&#93;: Line 1:7 Invalid function 'simple_add'then i again tried to drop the same macro and it again executed without any exception:    > drop temporary macro simple_add;OKTime taken: 0.016 seconds
issueID:CARBONDATA-41
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/scan/result/iterator/DetailQueryResultIterator.java
texts:Run example, system can&#39;t normally end
Reproduce steps:1.Run CarbonExample.scala in Intellij IDEA2.Get result, but system can't normally end.3.Run CarbonExample.scala again in Intellij IDEA, get the below errors: FAILED SelectChannelConnector@0.0.0.0:4040: java.net.BindException: Address already in usejava.net.BindException: Address already in useneed to manually stop it, then can run CarbonExample.scala again in Intellij IDEA
issueID:CARBONDATA-410
type:Test
changed files:
texts:Implement test cases for core.datastore.file system

issueID:CARBONDATA-411
type:Improvement
changed files:
texts:test

issueID:CARBONDATA-412
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
core/src/main/java/org/apache/carbondata/core/util/path/CarbonTablePath.java
texts:in windows, when load into table whose name has "_", the old segment will be deleted.
when carbon table name has "_", such as "load_test", then load into table twice, in the second load,  the first segment 0 will be deleted.
issueID:CARBONDATA-413
type:Improvement
changed files:
texts:Implement unit test cases for scan.expression package

issueID:CARBONDATA-414
type:Improvement
changed files:integration/spark-common/src/main/java/org/apache/carbondata/spark/merger/CarbonDataMergerUtil.java
texts:Access array elements using index than Loop

issueID:CARBONDATA-415
type:Improvement
changed files:
texts:Fixing of the Findbugs and other static issues.

issueID:CARBONDATA-416
type:Test
changed files:
texts:Add unit test case for result.impl package

issueID:CARBONDATA-417
type:Bug
changed files:
texts:[Bad Records] Not created and not writen log file when logger is True and action as Fail
Steps:1. Create Table:CREATE TABLE truefail (ID int,CUST_ID int,cust_name string) STORED BY 'org.apache.carbondata.format';2. Load Data having Logger as True and Action as FalseLOAD DATA INPATH 'hdfs://hacluster/Raghu/test2.csv' into table truefail OPTIONS('DELIMITER'=',' , 'QUOTECHAR'='"','BAD_RECORDS_LOGGER_ENABLE'='TRUE', 'BAD_RECORDS_ACTION'='FAIL','FILEHEADER'='ID,CUST_ID,cust_name');0: jdbc:hive2://ha-cluster/default>  CREATE TABLE truefail (ID int,CUST_ID int,cust_name string) STORED BY 'org.apache.carbondata.format';---------+ result  ---------+---------+No rows selected (0.679 seconds)0: jdbc:hive2://ha-cluster/default>  LOAD DATA INPATH 'hdfs://hacluster/Raghu/test2.csv' into table truefail OPTIONS('DELIMITER'=',' , 'QUOTECHAR'='"','BAD_RECORDS_LOGGER_ENABLE'='TRUE', 'BAD_RECORDS_ACTION'='FAIL','FILEHEADER'='ID,CUST_ID,cust_name');Error: java.lang.Exception: DataLoad failure: Data load failed due to bad record ,The value  "987654321010111213141516171819101122334455667788990012131415161718191909192939495969798" with column name CUST_ID and column data type INT is not a valid Record (state=,code=0)0: jdbc:hive2://ha-cluster/default>Actual Result: Not Creating and not written log file for bad records when BAD_RECORDS_LOGGER_ENABLE'='TRUE', 'BAD_RECORDS_ACTION'='FAIL'Expected Result: Should create and write log file when BAD_RECORDS_LOGGER_ENABLE'='TRUE', 'BAD_RECORDS_ACTION'='FAIL' for Bad records
issueID:CARBONDATA-418
type:Bug
changed files:
texts:Data Loading performance issue
In CarbonCsvBasedSeqGen step for each row we are calling dimension ids split method as String.split method is costly operation and every time it will  create a unnecessary object it will impact the performance, We can store it in a  local variable
issueID:CARBONDATA-419
type:Bug
changed files:
texts:Show tables in query for non-existing database do not show any exception
When executing command 'show tables in' for an non-existing database, it do not show any error,Example: show tables in test_tableMg;(where test_tableMg database do not exist)whereas when executing the same query on hive, it says : FAILED: SemanticException &#91;Error 10072&#93;: Database does not exist: test_tableMg
issueID:CARBONDATA-42
type:Bug
changed files:
texts:Missing Code on Github for Compilation
The package; Error:(35, 24) java: package org.carbondata.format does not existIs not available in the source code on Github and it is causing 100 error when trying to make the project in Intellij.
issueID:CARBONDATA-420
type:Improvement
changed files:
texts:Remove unused parameter in config template file
max.file.size is unused now, need to remove it.
issueID:CARBONDATA-421
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/resolverinfo/visitor/CustomTypeDictionaryVisitor.java
texts:Timestamp data type filter issue with format other than "-"
Problem: When time format is yyyy/mm/dd other than "" filter query is not working , As in filter only "" is allowed user need to give the filter value is "-" but as data loaded in in "/" filter is not working and returning 0 result. Soluntion: Problem is in filter we are taking default format but we need to take format used during data loaded while converting the filter values to surrogate key
issueID:CARBONDATA-422
type:Bug
changed files:
texts:[Bad Records]Select query failed with "NullPointerException" after data-load with options as MAXCOLUMN and BAD_RECORDS_ACTION
Description : Select query failed with "NullPointerException" after data-load with options as MAXCOLUMN and BAD_RECORDS_ACTIONSteps:1. Create table2. Load data into table with BAD_RECORDS_ACTION option [ Create Table &#8211; columns -9 ,CSV coulmn - 10 , Header - 9]3. Do select * query ,it will pass 4. Then Load data into table with BAD_RECORDS_ACTION and MAXCOLUMN option [ Create Table &#8211; columns -9 ,CSV coulmn - 10 , Header - 9,MAXCOLUMNS &#8211; 9]5. Do select * query ,it will fail with "NullPointerException"Log :- -------0: jdbc:hive2://ha-cluster/default> create table emp3(ID int,Name string,DOJ timestamp,Designation string,Salary double,Dept string,DOB timestamp,Addr string,Gender string) STORED BY 'org.apache.carbondata.format';---------+ result ---------+---------+No rows selected (0.589 seconds)0: jdbc:hive2://ha-cluster/default> LOAD DATA inpath 'hdfs://hacluster/chetan/emp11.csv' into table emp3 options('DELIMITER'=',', 'QUOTECHAR'='"','FILEHEADER'='ID,Name,DOJ,Designation,Salary,Dept,DOB,Addr,Gender', 'BAD_RECORDS_ACTION'='FORCE');---------+ Result ---------+---------+No rows selected (2.415 seconds)0: jdbc:hive2://ha-cluster/default> select * from emp3;----------------------------------------------------------------+ id  name  doj  designation  salary  dept  dob  addr  gender ----------------------------------------------------------------+ NULL  NULL  NULL  NULL  NULL  NULL  NULL  NULL  NULL  1  AAA  NULL  Trainee  10000.0  IT  NULL  Pune  Male  2  BBB  NULL  SE  30000.0  NW  NULL  Bangalore  Female  3  CCC  NULL  SSE  40000.0  DATA  NULL  Mumbai  Female  4  DDD  NULL  TL  60000.0  OPER  NULL  Delhi  Male  5  EEE  NULL  STL  80000.0  MAIN  NULL  Chennai  Female  6  FFF  NULL  Trainee  10000.0  IT  NULL  Pune  Male  7  GGG  NULL  SE  30000.0  NW  NULL  Bangalore  Female  8  HHH  NULL  SSE  40000.0  DATA  NULL  Mumbai  Female  9  III  NULL  TL  60000.0  OPER  NULL  Delhi  Male  10  JJJ  NULL  STL  80000.0  MAIN  NULL  Chennai  Female  NULL  Name  NULL  Designation  NULL  Dept  NULL  Addr  Gender ----------------------------------------------------------------+12 rows selected (0.418 seconds)0: jdbc:hive2://ha-cluster/default> LOAD DATA inpath 'hdfs://hacluster/chetan/emp11.csv' into table emp3 options('DELIMITER'=',', 'QUOTECHAR'='"','FILEHEADER'='ID,Name,DOJ,Designation,Salary,Dept,DOB,Addr,Gender','MAXCOLUMNS'='9', 'BAD_RECORDS_ACTION'='FORCE');---------+ Result ---------+---------+No rows selected (1.424 seconds)0: jdbc:hive2://ha-cluster/default> select * from emp3;Error: java.io.IOException: java.lang.NullPointerException (state=,code=0)0: jdbc:hive2://ha-cluster/default>
issueID:CARBONDATA-423
type:Improvement
changed files:
texts:Added Example to Load Data to carbon Table using case class

issueID:CARBONDATA-424
type:Bug
changed files:processing/src/main/java/org/apache/carbondata/processing/loading/steps/DataConverterProcessorStepImpl.java
processing/src/main/java/org/apache/carbondata/processing/loading/BadRecordsLogger.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/impl/ParallelReadMergeSorterImpl.java
processing/src/main/java/org/apache/carbondata/processing/loading/converter/impl/RowConverterImpl.java
common/src/main/java/org/apache/carbondata/common/constants/LoggerAction.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/impl/ThreadStatusObserver.java
texts:Data Load will fail for badrecord and "bad_records_action" is fail
Whenever there is a bad record found in csv file and the BAD_RECORDS_ACTION' is FAIL then data load will fail with an error message which gives information about the badrecord because of which data load is failed.
issueID:CARBONDATA-426
type:Improvement
changed files:
texts:replace if else with conditional operator

issueID:CARBONDATA-428
type:Improvement
changed files:
texts:Remove Redundant Condition Checks

issueID:CARBONDATA-429
type:Sub-task
changed files:core/src/main/java/org/apache/carbondata/core/cache/dictionary/DictionaryColumnUniqueIdentifier.java
core/src/main/java/org/apache/carbondata/core/metadata/datatype/DataType.java
texts:Eliminate unnecessary file name check in dictionary cache
1.there are currently many file name check for each column's dictionary cache, which cause unnecessary calls to HDFS getFileStatus.2.  in checkAndLoadDictionaryData, we get meta file's mtime from hdfs each time we invoke cache.get to check if the local cache is valid or not.  The local dictionary cache may be invalid after parallel data load.  This will in turn increase number of calls to getFileStatus as well.
issueID:CARBONDATA-43
type:Bug
changed files:
texts:query decimal filed with "select * from table where decimalFied=1234.12", the query result is empty.
create table with decimal field, and then query with "select ... from table where decimalFied=1234.12", the query result is empty, while in hive, we can get the correct result.
issueID:CARBONDATA-431
type:Sub-task
changed files:processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactDataHandlerColumnar.java
texts:Improve compression ratio for numeric datatype
Carbon has better compression ratio for String type, but worst for numeric data type, identify issues with current numeric datatype compression for carbon to get better compression ratio.DataType     Text Parquet   Orc  Carbondecimal   16G  | 11G      |  6G    |    13Gint           5G    |     1G      |    1G    |    3GString    24G  | 22G      |    11G   |  3G   （no dictionary）       -------    high cardinalityString 30G    | 4G      |    4G    |    1G  &#8211; Dictionary encode            1G  &#8211; Dictionary encode without inverted index            3G  &#8211; No dictionary encode              -----------low cardinality
issueID:CARBONDATA-434
type:Improvement
changed files:
texts:Update test cases for AllDataTypesTestCase2

issueID:CARBONDATA-435
type:Improvement
changed files:
texts:improve integration test case for AllDataTypesTestCase4

issueID:CARBONDATA-436
type:Sub-task
changed files:
texts:Make blocklet size configuration respect to the actual size (in terms of byte) of the blocklet
Currently, the blocklet size is based on the row counts within the blocklet. The default value(120000) is small for hdfs io. If we increase the value, which may cause too many Young-GC when we scan many columns, instead, we can extend the configuration with respect to the actual size of the blocklet.
issueID:CARBONDATA-438
type:Test
changed files:
texts:Add unit test for scan.scanner.impl package

issueID:CARBONDATA-439
type:Bug
changed files:
texts:Select query having combination of AND and OR operator with IN filter failed with "NullPointerException"
when a select query having a combination of AND followed by OR with IN filter , the query execution is failing with "NullPointerException"Error: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 46.0 failed 4 times, most recent failure: Lost task 0.3 in stage 46.0 (TID 141, BLR1000014274, 3): java.lang.RuntimeException: Exception occurred in query execution :: java.util.concurrent.ExecutionException: java.lang.NullPointerException        at scala.sys.package$.error(package.scala:27)        at org.apache.carbondata.spark.rdd.CarbonScanRDD$$anon$1.<init>(CarbonScanRDD.scala:241)        at org.apache.carbondata.spark.rdd.CarbonScanRDD.compute(CarbonScanRDD.scala:204)        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:334)        at org.apache.spark.rdd.RDD.iterator(RDD.scala:267)        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:334)        at org.apache.spark.rdd.RDD.iterator(RDD.scala:267)        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:334)        at org.apache.spark.rdd.RDD.iterator(RDD.scala:267)        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:334)        at org.apache.spark.rdd.RDD.iterator(RDD.scala:267)        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:334)        at org.apache.spark.rdd.RDD.iterator(RDD.scala:267)        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:334)        at org.apache.spark.rdd.RDD.iterator(RDD.scala:267)        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:68)        at org.apache.spark.scheduler.Task.run(Task.scala:90)        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:253)        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)        at java.lang.Thread.run(Thread.java:745)
issueID:CARBONDATA-44
type:Bug
changed files:integration/spark-common/src/main/java/org/apache/carbondata/spark/load/CarbonLoaderUtil.java
texts:Table can not many times load data
follow step the problem can be recreated1.cc.sql("create table if not exists tt5 (id string, name string, city string, age Int) stored by 'org.apache.carbondata.format'")2.cc.sql(s"load data inpath 'hdfs://127.0.0.1:9000/test/sample.csv' into table tt5")3.cc.sql("select * from tt5").show------- no problem with the above steps 4.cc.sql(s"load data inpath 'hdfs://127.0.0.1:9000/test/sample.csv' into table tt5")5.cc.sql("select * from tt5").show------- display error message:ERROR 07-07 17:56:52,995 - mainjava.io.IOException: java.io.FileNotFoundException: File does not exist: hdfs://127.0.0.1:9000/carbondata/default/tt5/Fact/Part0/Segment_0 at org.carbondata.hadoop.CarbonInputFormat.getSplits(CarbonInputFormat.java:286) at org.carbondata.spark.rdd.CarbonScanRDD.getPartitions(CarbonScanRDD.scala:84) at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239) at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237) at scala.Option.getOrElse(Option.scala:120) at org.apache.spark.rdd.RDD.partitions(RDD.scala:237)
issueID:CARBONDATA-440
type:New Feature
changed files:
texts:Provide Update/Delete functionality support in CarbonData
Currently, CarbonData does not support modification of existing rows in the table. This is a major limitation for many real world desirable use cases in data warehousing, such as slow changing dimension tables, data correction of fact tables or data cleanup, etc. Many users want to be able to update and delete rows from the CarbonData table.Following are some high level design goals to support this functionality,1. Provide a standard SQL interface to perform Update and Delete operations.2. Perform Update and Delete operations on CarbonData table without having to rewrite the entire CarbonData block (file) by making use of differential files (a.k.a delta files).3. After Update or Delete operation, CarbonData readers should skip deleted records and read updated records seamlessly without having to modify user applications.4. Perform compaction of Update and Delete delta files to maintain adequate query performance
issueID:CARBONDATA-441
type:New Feature
changed files:
texts:Add module for spark2
To start spark2.0 integration with CarbonData, new modules need to be added:1. spark-common, which will have all common classes for both spark1 and spark2 integration2. spark2, which have carbon-spark2 integrationAfter the spark2 integration is done, CarbonData master branch should be enable to work with both spark1 and spark2. The target assembling is build by using profile, like: To build integration with spark1.5: use -Pspark-1.5 To build integration with spark1.6: use -Pspark-1.6 To build integration with spark2.0: use -Pspark-2.0
issueID:CARBONDATA-442
type:Bug
changed files:
texts:Query result mismatching with Hive
=> I created table using following command : create table Carbon_automation_test5 (imei string,deviceInformationId int,MAC string,deviceColor string,device_backColor string,modelId string,marketName string,AMSize string,ROMSize string,CUPAudit string,CPIClocked string,series string,productionDate string,bomCode string,internalModels string, deliveryTime string, channelsId string,channelsName string , deliveryAreaId string, deliveryCountry string, deliveryProvince string, deliveryCity string,deliveryDistrict string, deliveryStreet string,oxSingleNumber string, ActiveCheckTime string, ActiveAreaId string, ActiveCountry string, ActiveProvince string, Activecity string, ActiveDistrict string, ActiveStreet string, ActiveOperatorId string, Active_releaseId string, Active_EMUIVersion string,Active_operaSysVersion string, Active_BacVerNumber string, Active_BacFlashVer string,Active_webUIVersion string, Active_webUITypeCarrVer string,Active_webTypeDataVerNumber string, Active_operatorsVersion string, Active_phonePADPartitionedVersions string,Latest_YEAR int, Latest_MONTH int, Latest_DAY int, Latest_HOUR string, Latest_areaId string, Latest_country string, Latest_province string, Latest_city string,Latest_district string, Latest_street string, Latest_releaseId string,Latest_EMUIVersion string, Latest_operaSysVersion string, Latest_BacVerNumber string,Latest_BacFlashVer string, Latest_webUIVersion string, Latest_webUITypeCarrVer string,Latest_webTypeDataVerNumber string, Latest_operatorsVersion string,Latest_phonePADPartitionedVersions string, Latest_operatorId string,gamePointDescription string, gamePointId int,contractNumber int) stored by 'org.apache.carbondata.format' => Load csv to table : LOAD DATA INPATH 'hdfs://localhost:54310/user/hduser/100_olap.csv' INTO table Carbon_automation_test5 OPTIONS('DELIMITER'= ',' ,'QUOTECHAR'= '"', 'FILEHEADER'= 'imei,deviceInformationId,MAC,deviceColor,device_backColor,modelId,marketName,AMSize,ROMSize,CUPAudit,CPIClocked,series,productionDate,bomCode,internalModels,deliveryTime,channelsId,channelsName,deliveryAreaId,deliveryCountry,deliveryProvince,deliveryCity,deliveryDistrict,deliveryStreet,oxSingleNumber,contractNumber,ActiveCheckTime,ActiveAreaId,ActiveCountry,ActiveProvince,Activecity,ActiveDistrict,ActiveStreet,ActiveOperatorId,Active_releaseId,Active_EMUIVersion,Active_operaSysVersion,Active_BacVerNumber,Active_BacFlashVer,Active_webUIVersion,Active_webUITypeCarrVer,Active_webTypeDataVerNumber,Active_operatorsVersion,Active_phonePADPartitionedVersions,Latest_YEAR,Latest_MONTH,Latest_DAY,Latest_HOUR,Latest_areaId,Latest_country,Latest_province,Latest_city,Latest_district,Latest_street,Latest_releaseId,Latest_EMUIVersion,Latest_operaSysVersion,Latest_BacVerNumber,Latest_BacFlashVer,Latest_webUIVersion,Latest_webUITypeCarrVer,Latest_webTypeDataVerNumber,Latest_operatorsVersion,Latest_phonePADPartitionedVersions,Latest_operatorId,gamePointId,gamePointDescription')=>now executed SELECT querry : SELECT Carbon_automation_test5.AMSize AS AMSize, Carbon_automation_test5.ActiveCountry AS ActiveCountry, Carbon_automation_test5.Activecity AS Activecity , SUM(Carbon_automation_test5.gamePointId) AS Sum_gamePointId FROM ( SELECT AMSize,ActiveCountry,gamePointId, Activecity FROM (select * from Carbon_automation_test5) SUB_QRY ) Carbon_automation_test5 INNER JOIN ( SELECT ActiveCountry, Activecity, AMSize FROM (select * from Carbon_automation_test5) SUB_QRY ) Carbon_automation_vmall_test1 ON Carbon_automation_test5.AMSize = Carbon_automation_vmall_test1.AMSize WHERE NOT(Carbon_automation_test5.AMSize <= '3RAM size') GROUP BY Carbon_automation_test5.AMSize, Carbon_automation_test5.ActiveCountry, Carbon_automation_test5.Activecity ORDER BY Carbon_automation_test5.AMSize ASC, Carbon_automation_test5.ActiveCountry ASC, Carbon_automation_test5.Activecity ASC;-----------------------------------------------------   AMSize    ActiveCountry   Activecity   Sum_gamePointId  ----------------------------------------------------- 4RAM size   Chinese         changsha     200860            4RAM size   Chinese         guangzhou    38016             4RAM size   Chinese         shenzhen     49610             4RAM size   Chinese         wuhan        117568            4RAM size   Chinese         xiangtan     254320            4RAM size   Chinese         yichang      29436             5RAM size   Chinese         changsha     13845             5RAM size   Chinese         guangzhou    23560             5RAM size   Chinese         wuhan        12390             6RAM size   Chinese         changsha     23697             6RAM size   Chinese         guangzhou    15912             6RAM size   Chinese         shenzhen     19278             6RAM size   Chinese         wuhan        29313             6RAM size   Chinese         xiangtan     7794              6RAM size   Chinese         zhuzhou      26568             7RAM size   Chinese         changsha     1057              7RAM size   Chinese         wuhan        27853             7RAM size   Chinese         yichang      14217             7RAM size   Chinese         zhuzhou      15673             8RAM size   Chinese         guangzhou    27380             8RAM size   Chinese         shenzhen     3550              8RAM size   Chinese         wuhan        29700             8RAM size   Chinese         xiangtan     31020             8RAM size   Chinese         yichang      51660             8RAM size   Chinese         zhuzhou      26840             9RAM size   Chinese         changsha     32390             9RAM size   Chinese         shenzhen     30650             9RAM size   Chinese         wuhan        15670             9RAM size   Chinese         xiangtan     58210             9RAM size   Chinese         yichang      5710             -----------------------------------------------------=> Also i have executed above command on hive as follow : create table Carbon_automation_test5_hive (imei string,deviceInformationId int,MAC string,deviceColor string,device_backColor string,modelId string,marketName string,AMSize string,ROMSize string,CUPAudit string,CPIClocked string,series string,productionDate string,bomCode string,internalModels string, deliveryTime string, channelsId string,channelsName string , deliveryAreaId string, deliveryCountry string, deliveryProvince string, deliveryCity string,deliveryDistrict string, deliveryStreet string,oxSingleNumber string, ActiveCheckTime string, ActiveAreaId string, ActiveCountry string, ActiveProvince string, Activecity string, ActiveDistrict string, ActiveStreet string, ActiveOperatorId string, Active_releaseId string, Active_EMUIVersion string,Active_operaSysVersion string, Active_BacVerNumber string, Active_BacFlashVer string,Active_webUIVersion string, Active_webUITypeCarrVer string,Active_webTypeDataVerNumber string, Active_operatorsVersion string, Active_phonePADPartitionedVersions string,Latest_YEAR int, Latest_MONTH int, Latest_DAY int, Latest_HOUR string, Latest_areaId string, Latest_country string, Latest_province string, Latest_city string,Latest_district string, Latest_street string, Latest_releaseId string,Latest_EMUIVersion string, Latest_operaSysVersion string, Latest_BacVerNumber string,Latest_BacFlashVer string, Latest_webUIVersion string, Latest_webUITypeCarrVer string,Latest_webTypeDataVerNumber string, Latest_operatorsVersion string,Latest_phonePADPartitionedVersions string, Latest_operatorId string,gamePointDescription string, gamePointId int,contractNumber int) row format delimited fields terminated by ',' LOAD DATA INPATH 'hdfs://localhost:54310/user/hduser/100_olap.csv' INTO table Carbon_automation_test5; SELECT Carbon_automation_test5_hive.AMSize AS AMSize, Carbon_automation_test5_hive.ActiveCountry AS ActiveCountry, Carbon_automation_test5_hive.Activecity AS Activecity , SUM(Carbon_automation_test5_hive.gamePointId) AS Sum_gamePointId FROM ( SELECT AMSize,ActiveCountry,gamePointId, Activecity FROM (select * from Carbon_automation_test5_hive) SUB_QRY ) Carbon_automation_test5_hive INNER JOIN ( SELECT ActiveCountry, Activecity, AMSize FROM (select * from Carbon_automation_test5_hive) SUB_QRY ) Carbon_automation_vmall_test1 ON Carbon_automation_test5_hive.AMSize = Carbon_automation_vmall_test1.AMSize WHERE NOT(Carbon_automation_test5_hive.AMSize <= '3RAM size') GROUP BY Carbon_automation_test5_hive.AMSize, Carbon_automation_test5_hive.ActiveCountry, Carbon_automation_test5_hive.Activecity ORDER BY Carbon_automation_test5_hive.AMSize ASC, Carbon_automation_test5_hive.ActiveCountry ASC, Carbon_automation_test5_hive.Activecity ASC;-------------------------------------------------------------   AMSize    ActiveCountry       Activecity       Sum_gamePointId  ------------------------------------------------------------- 4RAM size   1               Guangdong Province   49610             4RAM size   2               Guangdong Province   38016             4RAM size   3               Hunan Province       200860            4RAM size   4               Hunan Province       254320            4RAM size   6               Hubei Province       117568            4RAM size   7               Hubei Province       29436             5RAM size   2               Guangdong Province   23560             5RAM size   3               Hunan Province       13845             5RAM size   6               Hubei Province       12390             6RAM size   1               Guangdong Province   19278             6RAM size   2               Guangdong Province   15912             6RAM size   3               Hunan Province       23697             6RAM size   4               Hunan Province       7794              6RAM size   5               Hunan Province       26568             6RAM size   6               Hubei Province       29313             7RAM size   3               Hunan Province       1057              7RAM size   5               Hunan Province       15673             7RAM size   6               Hubei Province       27853             7RAM size   7               Hubei Province       14217             8RAM size   1               Guangdong Province   3550              8RAM size   2               Guangdong Province   27380             8RAM size   4               Hunan Province       31020             8RAM size   5               Hunan Province       26840             8RAM size   6               Hubei Province       29700             8RAM size   7               Hubei Province       51660             9RAM size   1               Guangdong Province   30650             9RAM size   3               Hunan Province       32390             9RAM size   4               Hunan Province       58210             9RAM size   6               Hubei Province       15670             9RAM size   7               Hubei Province       5710             -------------------------------------------------------------
issueID:CARBONDATA-443
type:Improvement
changed files:
texts:Enable non-sort data loading
Improving data ingestion rate for fast ingestion for special use cases with potential side-effect of compromising query performance
issueID:CARBONDATA-444
type:Task
changed files:
texts:Improved integration test-case for AllDataTypesTestCase1

issueID:CARBONDATA-445
type:Task
changed files:
texts:Improved integration test-case for AllDataTypesTestCase3

issueID:CARBONDATA-446
type:Test
changed files:
texts:Add Unit Tests For Scan.collector.impl package

issueID:CARBONDATA-447
type:Improvement
changed files:
texts:Use Carbon log service instead of spark Logging
Use Carbon log service instead of spark Logging
issueID:CARBONDATA-448
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/scan/complextypes/StructQueryType.java
texts:Solve compilation error for spark2 integration
Currently, carbon-core module compile will fail with -Pspark-2.0
issueID:CARBONDATA-449
type:Improvement
changed files:
texts:Remove unnecessary log property
When creating Log object, there are some unnecessary properties
issueID:CARBONDATA-45
type:New Feature
changed files:
texts:Support MAP type
>>CREATE TABLE table1 (                 deviceInformationId int,                 channelsId string,                 props map<key:int,value:string>)              STORED BY 'org.apache.carbondata.format'>>insert into table1 select 10,'channel1', map(1,'user1',101, 'root')format of data to be read from csv, with '$' as level 1 delimiter and map keys terminated by '#'>>load data local inpath '/tmp/data.csv' into table1 options ('COMPLEX_DELIMITER_LEVEL_1'='$', 'COMPLEX_DELIMITER_LEVEL_2'=':', 'COMPLEX_DELIMITER_FOR_KEY'='#')20,channel2,2#user2$100#usercommon30,channel3,3#user3$100#usercommon40,channel4,4#user3$100#usercommon>>select channelId, props[100] from table1 where deviceInformationId > 10;20, usercommon30, usercommon40, usercommon>>select channelId, props from table1 where props[2] = 'user2';20, {2,'user2', 100, 'usercommon'}Following cases needs to  be handled:Sub featurePending activityRemarksBasic Maptype supportDevelop Create table DDL, Load map data from CSV, select * from maptableMaptype lookup in projection and filterDevelopProjection and filters needs execution at sparkNULL values, UDFs, Describe supportDevelopCompaction support  Test + fix  As compaction works at byte level, no changes required. Needs to add test-casesInsert into table Develop  Source table data containing Map data needs to convert from spark datatype to string , as carbon takes string as input row Support DDL for Map fields Dictionary include and Dictionary Exclude  Develop  Also needs to handle CarbonDictionaryDecoder  to handle the same. Support multilevel Map  Develop  currently DDL is validated to allow only 2 levels, remove this restrictionSupport Map value to be a measure  Develop  Currently array and struct supports only dimensions which needs changeSupport Alter table to add and remove Map column  Develop  implement DDL and requires default value handling Projections of Map loopup push down to carbon  Develop  this is an optimization, when more number of values are present in Map Filter map loolup push down to carbon  Develop  this is an optimization, when more number of values are present in Map Update Map values  Develop  update map valueDesign suggestion:Map can be represented internally stored as Array<Struct<key,Value>>, So that conversion of data is required to Map data type while giving to spark. Schema will have new column of map type similar to Array.
issueID:CARBONDATA-450
type:Test
changed files:
texts:Increase Test Coverage for Core.reader module

issueID:CARBONDATA-451
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/metadata/AbsoluteTableIdentifier.java
texts:Can not run query on windows now
As tablePath on windows has '/' and not replaced when substring, it would throw error when execute query.I have fixed this and will raise a pr.
issueID:CARBONDATA-453
type:Sub-task
changed files:
texts:Implement DAT(Double Array Trie) for Dictionary
Implement DAT structure for Dictionary in order to reduce memory footprint and improve performance.
issueID:CARBONDATA-456
type:Bug
changed files:hadoop/src/main/java/org/apache/carbondata/hadoop/util/CarbonInputFormatUtil.java
texts:Select count(*) from table is slower.
Select count is slower in current master branch compare to previous versions.
issueID:CARBONDATA-458
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/metadata/blocklet/BlockletInfo.java
core/src/main/java/org/apache/carbondata/core/scan/executor/infos/BlockExecutionInfo.java
core/src/main/java/org/apache/carbondata/core/util/DataFileFooterConverter2.java
processing/src/main/java/org/apache/carbondata/processing/merger/CarbonCompactionUtil.java
core/src/main/java/org/apache/carbondata/core/scan/executor/impl/QueryExecutorProperties.java
processing/src/main/java/org/apache/carbondata/processing/store/writer/AbstractFactDataWriter.java
processing/src/main/java/org/apache/carbondata/processing/store/CarbonDataWriterFactory.java
core/src/main/java/org/apache/carbondata/core/util/DataFileFooterConverter.java
processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactDataHandlerColumnar.java
core/src/main/java/org/apache/carbondata/core/datastore/block/TableBlockInfo.java
core/src/main/java/org/apache/carbondata/core/scan/executor/impl/AbstractQueryExecutor.java
core/src/main/java/org/apache/carbondata/core/util/CarbonProperties.java
core/src/main/java/org/apache/carbondata/core/util/DataFileFooterConverterFactory.java
core/src/main/java/org/apache/carbondata/core/scan/executor/util/QueryUtil.java
core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/reader/MeasureColumnChunkReader.java
core/src/main/java/org/apache/carbondata/core/datastore/DataRefNode.java
core/src/main/java/org/apache/carbondata/core/scan/scanner/AbstractBlockletScanner.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/reader/DimensionColumnChunkReader.java
core/src/main/java/org/apache/carbondata/core/datastore/block/SegmentProperties.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
core/src/main/java/org/apache/carbondata/core/util/CarbonMetadataUtil.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/reader/CarbonDataReaderFactory.java
core/src/main/java/org/apache/carbondata/core/metadata/blocklet/DataFileFooter.java
core/src/main/java/org/apache/carbondata/hadoop/CarbonInputSplit.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/reader/measure/AbstractMeasureChunkReader.java
core/src/main/java/org/apache/carbondata/core/util/AbstractDataFileFooterConverter.java
core/src/main/java/org/apache/carbondata/core/scan/scanner/impl/FilterScanner.java
texts:Improving carbon first time query performance
Improving carbon first time query performanceReason:1. As file system cache is cleared file reading will make it slower to read and cache2. In first time query carbon will have to read the footer from file data file to form the btree3. Carbon reading more footer data than its required(data chunk)4. There are lots of random seek is happening in carbon as column data(data page, rle, inverted index) are not stored together.Solution: 1. Improve block loading time. This can be done by removing data chunk from blockletInfo and storing only offset and length of data chunk2. compress presence meta bitset stored for null values for measure column using snappy 3. Store the metadata and data of a column together and read together this reduces random seek and improve IO
issueID:CARBONDATA-459
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
core/src/main/java/org/apache/carbondata/core/util/CarbonProperties.java
texts:Block distribution is wrong in case of dynamic allocation=true
In case when dynamic allocation is true and configured max executors are more than the initial executors then carbon is not able to request the max number of executors configured. Due to this resources are getting under utilized and case when number of blocks increases, the distribution of blocks is limited to the number of nodes and the number of tasks launched are less. This leads to under utilization of resources and hence impacts the query and load performance.
issueID:CARBONDATA-46
type:Bug
changed files:
texts:Fix dataframe API of Carbon
Dataframe API is not working currently as there was some fields like tablePath made mandatory in Datasource relation. But it is not required make mandatory.
issueID:CARBONDATA-460
type:Test
changed files:
texts:Add Unit Tests For core.writer.sortindex package

issueID:CARBONDATA-461
type:Sub-task
changed files:integration/spark-common/src/main/java/org/apache/carbondata/spark/load/DeleteLoadFolders.java
integration/spark-common/src/main/java/org/apache/carbondata/spark/util/CarbonQueryUtil.java
integration/spark-common/src/main/java/org/apache/carbondata/spark/merger/CarbonDataMergerUtil.java
texts:Clean partitioner in RDD package
To make carbon RDDs reusable in spark2 integration, need to remove partitioner in RDDs
issueID:CARBONDATA-462
type:Sub-task
changed files:
texts:Clean up carbonTableSchema.scala before moving to spark-common package
Clean up code, prepare for moving to spark-common package
issueID:CARBONDATA-463
type:Sub-task
changed files:processing/src/main/java/org/apache/carbondata/processing/merger/CarbonCompactionUtil.java
processing/src/main/java/org/apache/carbondata/processing/merger/CarbonCompactionExecutor.java
processing/src/main/java/org/apache/carbondata/processing/merger/CarbonDataMergerUtil.java
processing/src/main/java/org/apache/carbondata/processing/merger/RowResultMergerProcessor.java
common/src/main/java/org/apache/carbondata/common/exceptions/sql/MalformedCarbonCommandException.java
processing/src/main/java/org/apache/carbondata/processing/merger/CompactionType.java
processing/src/main/java/org/apache/carbondata/processing/util/CarbonLoaderUtil.java
processing/src/main/java/org/apache/carbondata/processing/util/DeleteLoadFolders.java
processing/src/main/java/org/apache/carbondata/processing/merger/NodeMultiBlockRelation.java
processing/src/main/java/org/apache/carbondata/processing/util/CarbonQueryUtil.java
processing/src/main/java/org/apache/carbondata/processing/merger/NodeBlockRelation.java
texts:Extract spark-common module
Extracting spark module code to spark-common module, to make it reusable in spark2 integration
issueID:CARBONDATA-465
type:Improvement
changed files:processing/src/main/java/org/apache/carbondata/processing/util/CarbonDataProcessorUtil.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/impl/ParallelReadMergeSorterImpl.java
texts:Spark streaming dataframe support
Carbondata-1.0.0 support load data with spark data frame api. There is a limit that kettle is still required since DataFrameLoaderRDD still depends on kettle. We provide NewDataFrameLoaderRDD  to load data with new flow .Also,we discovered some bugs:1. CarbonMetastoreCatalog.createTableFromThrift```/** schemaFilePath starts with file:// will not create meta files successfully while thriftWriter will have no complains. This will cause some weired error eg. No table found.     */    val thriftWriter = new ThriftWriter(schemaFilePath, false)    thriftWriter.open()    thriftWriter.write(thriftTableInfo)    thriftWriter.close()``` 2. There are some exceptions raised  even when you have set useKettle to false.
issueID:CARBONDATA-467
type:Sub-task
changed files:processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/SortParameters.java
processing/src/main/java/org/apache/carbondata/processing/newflow/partition/impl/HashPartitionerImpl.java
hadoop/src/main/java/org/apache/carbondata/hadoop/CarbonMultiBlockSplit.java
processing/src/main/java/org/apache/carbondata/processing/loading/steps/DataWriterProcessorStepImpl.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/CarbonTable.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/SingleThreadFinalSortFilesMerger.java
processing/src/main/java/org/apache/carbondata/processing/store/writer/AbstractFactDataWriter.java
processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactDataHandlerColumnar.java
core/src/main/java/org/apache/carbondata/core/datastore/row/CarbonRow.java
core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
processing/src/main/java/org/apache/carbondata/processing/loading/steps/SortProcessorStepImpl.java
processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactDataHandlerModel.java
core/src/main/java/org/apache/carbondata/core/util/path/CarbonTablePath.java
processing/src/main/java/org/apache/carbondata/processing/newflow/partition/Partitioner.java
core/src/main/java/org/apache/carbondata/core/util/CarbonMetadataUtil.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/impl/ParallelReadMergeSorterWithBucketingImpl.java
core/src/main/java/org/apache/carbondata/hadoop/CarbonInputSplit.java
processing/src/main/java/org/apache/carbondata/processing/loading/DataLoadProcessBuilder.java
processing/src/main/java/org/apache/carbondata/processing/loading/CarbonDataLoadConfiguration.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/BucketingInfo.java
core/src/main/java/org/apache/carbondata/core/metadata/converter/ThriftWrapperSchemaConverterImpl.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/TableSchema.java
texts:CREATE TABLE extension to support bucket table.
1. CREATE TABLE Statement extension.CREATE TABLE test(user_id BIGINT, firstname STRING, lastname STRING)CLUSTERED BY(user_id) INTO 32 BUCKETS STORED BY 'carbondata';2. Carbon file format update (Thrift definition extension)3. Respect to bucket definition during data load. Store the bucketid to carbondata index file
issueID:CARBONDATA-469
type:Sub-task
changed files:
texts:Leveraging Carbondata&#39;s bucketing info for optimized Join operation
Optimize join in spark using bucketing information to avoid shuffling when possible.
issueID:CARBONDATA-47
type:Improvement
changed files:
texts:Simplified datasource format name and storage name in carbondata
It is part of usability changes. Simplified datasource format name and query storage name.Datasource format name :Current usage of datasource formatdf.write.format("org.apache.spark.sql.CarbonSource")We can also use as follow to simplifydf.write.format("carbondata")Query storage name :Current usage of query storage nameCREATE TABLE IF NOT EXISTS t3  (country String,name String, phonetype String, serialname String, salary Int)  STORED BY 'org.apache.carbondata.format'We can also use as follow to simplifyCREATE TABLE IF NOT EXISTS t3  (country String,name String, phonetype String, serialname String, salary Int)  STORED BY 'carbondata'
issueID:CARBONDATA-470
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/memory/MemoryBlock.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/holder/UnsafeInmemoryHolder.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/holder/UnsafeCarbonRow.java
core/src/main/java/org/apache/carbondata/core/memory/IntPointerBuffer.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/merger/UnsafeIntermediateFileMerger.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/merger/UnsafeIntermediateMerger.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/comparator/UnsafeRowComparator.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/UnsafeCarbonRowPage.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/holder/SortTempChunkHolder.java
core/src/main/java/org/apache/carbondata/core/memory/UnsafeMemoryAllocator.java
processing/src/main/java/org/apache/carbondata/processing/loading/steps/SortProcessorStepImpl.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/holder/UnsafeInmemoryMergeHolder.java
core/src/main/java/org/apache/carbondata/core/memory/MemoryAllocator.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/sort/UnsafeIntSortDataFormat.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/comparator/UnsafeRowComparatorForNormalDIms.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
core/src/main/java/org/apache/carbondata/core/memory/MemoryLocation.java
core/src/main/java/org/apache/carbondata/core/memory/UnsafeMemoryManager.java
core/src/main/java/org/apache/carbondata/core/memory/HeapMemoryAllocator.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/holder/UnsafeSortTempFileChunkHolder.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/holder/UnsafeCarbonRowForMerge.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/merger/UnsafeInMemoryIntermediateDataMerger.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/sort/TimSort.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/impl/UnsafeParallelReadMergeSorterImpl.java
core/src/main/java/org/apache/carbondata/core/memory/CarbonUnsafe.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/UnsafeSortDataRows.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/holder/UnsafeFinalMergePageHolder.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/merger/UnsafeSingleThreadFinalSortFilesMerger.java
texts:Add unsafe offheap and on-heap sort in carbodata loading
In the current carbondata system loading performance is not so encouraging since we need to sort the data at executor level for data loading. Carbondata collects batch of data and sorts before dumping to the temporary files and finally it does merge sort from those temporary files to finish sorting. Here we face two major issues , one is disk IO and second is GC issue. Even though we dump to the file still carbondata face lot of GC issue since we sort batch data in-memory before dumping to the temporary files.To solve the above problems we can introduce Unsafe Storage and Unsafe sort.Unsafe Storage : User can configure the memory limit to keep the amount of data to in-memory. Here we can keep all the data in continuous memory location either on off-heap or on-heap using Unsafe. Once configure limit exceeds remaining data will be spilled to disk.Unsafe Sort : The data which is store in-memory using Unsafe can be sorted using Unsafe sort.
issueID:CARBONDATA-471
type:Bug
changed files:processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/SortDataRows.java
processing/src/main/java/org/apache/carbondata/processing/csvload/CSVInputFormat.java
common/src/main/java/org/apache/carbondata/common/CarbonIterator.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/impl/ParallelReadMergeSorterImpl.java
processing/src/main/java/org/apache/carbondata/processing/csvload/CSVRecordReaderIterator.java
processing/src/main/java/org/apache/carbondata/processing/loading/DataLoadProcessBuilder.java
core/src/main/java/org/apache/carbondata/core/scan/result/iterator/AbstractDetailQueryResultIterator.java
processing/src/main/java/org/apache/carbondata/processing/loading/DataLoadExecutor.java
processing/src/main/java/org/apache/carbondata/processing/loading/steps/InputProcessorStepImpl.java
texts:Optimize no kettle flow and fix issues in cluster
1. In non kettle flow memory usage is very high in sort step, because it keeps lots of per every thread, it leads to GC issue.2. Close all record readers which are opened while loading data.3. Carbontable cannot found during data load because it is not loaded in executor, so add the carbon table in executor side to memory.4. Prefetch the data in inputprocessor step to avoid IO waiting.
issueID:CARBONDATA-472
type:Test
changed files:
texts:Improve code coverage for core.cache package.

issueID:CARBONDATA-473
type:Sub-task
changed files:hadoop/src/main/java/org/apache/carbondata/hadoop/CarbonProjection.java
hadoop/src/main/java/org/apache/carbondata/hadoop/readsupport/impl/DictionaryDecodeReadSupport.java
processing/src/main/java/org/apache/carbondata/processing/merger/CarbonCompactionUtil.java
texts:spark 2 stable datasource api integration

issueID:CARBONDATA-474
type:Bug
changed files:
texts:Implement unit test cases for core.datastorage package

issueID:CARBONDATA-475
type:Test
changed files:
texts:Implement unit test cases for core.carbon.querystatics package

issueID:CARBONDATA-476
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/datastore/impl/FileFactory.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
core/src/main/java/org/apache/carbondata/core/datastore/impl/DFSFileReaderImpl.java
core/src/main/java/org/apache/carbondata/core/metadata/AbsoluteTableIdentifier.java
texts:storeLocation start with file:/// cause table not found exceptioin
When you set carbondata's storelocation starts with file:///, carbondata will fail to create table meta,and  it do not raise any exceptionthe code if found is in CarbonMetastore.createTableFromThrift``` val thriftWriter = new ThriftWriter(schemaFilePath, false)    thriftWriter.open()    thriftWriter.write(thriftTableInfo)//if writing failed, it will be quiet.    thriftWriter.close()```
issueID:CARBONDATA-478
type:New Feature
changed files:integration/spark2/src/main/java/org/apache/carbondata/spark/readsupport/SparkRowReadSupportImpl.java
texts:Separate SparkRowReadSupportImpl implementation for integrating with Spark1.x vs. Spark 2.x

issueID:CARBONDATA-48
type:New Feature
changed files:
texts:Support Carbon sql cli to enhance user experiance
Support SQL cli to enhance user experience.  User can directly run the carbon-spark-sql and input the sql queries. These queries will run on CarbonContext and shows the output.Example:carbondata/bin$ ./carbon-spark-sqlspark-sql> CREATE TABLE test_table (country String, name String, phonetype String, serialname String, salary Int) STORED BY 'org.apache.carbondata.format';INFO  09-07 12:27:31,388 - Time taken: 1.729 secondsspark-sql> LOAD DATA INPATH '/home/root1/carbon/carbondata/examples/src/main/resources/data.csv' into table test_table;INFO  09-07 12:29:01,598 - Time taken: 1.951 secondsspark-sql> select count(*) from test_table;1000INFO  09-07 12:29:56,313 - Time taken: 0.82 seconds, Fetched 1 row(s)
issueID:CARBONDATA-480
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/util/DataFileFooterConverter2.java
core/src/main/java/org/apache/carbondata/core/metadata/ColumnarFormatVersion.java
processing/src/main/java/org/apache/carbondata/processing/store/CarbonDataWriterFactory.java
core/src/main/java/org/apache/carbondata/core/util/DataFileFooterConverter.java
processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactDataHandlerColumnar.java
core/src/main/java/org/apache/carbondata/core/datastore/block/TableBlockInfo.java
core/src/main/java/org/apache/carbondata/core/util/CarbonProperties.java
core/src/main/java/org/apache/carbondata/core/util/DataFileFooterConverterFactory.java
core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
core/src/main/java/org/apache/carbondata/core/util/CarbonMetadataUtil.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/reader/CarbonDataReaderFactory.java
core/src/main/java/org/apache/carbondata/core/metadata/blocklet/DataFileFooter.java
core/src/main/java/org/apache/carbondata/hadoop/CarbonInputSplit.java
core/src/main/java/org/apache/carbondata/core/util/AbstractDataFileFooterConverter.java
texts:Add file format version enum
Add file format version enum instead of using short value
issueID:CARBONDATA-481
type:Bug
changed files:
texts:[SPARK2]fix late decoder and support whole stage code gen

issueID:CARBONDATA-482
type:Test
changed files:
texts:improve integration test case of AllDataTypesTestCase6

issueID:CARBONDATA-483
type:Test
changed files:
texts:Add Unit Tests For core.carbon.metadata package

issueID:CARBONDATA-484
type:New Feature
changed files:core/src/main/java/org/apache/carbondata/core/datastore/block/BlockInfo.java
core/src/main/java/org/apache/carbondata/core/datastore/TableSegmentUniqueIdentifier.java
core/src/main/java/org/apache/carbondata/core/util/path/CarbonTablePath.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
core/src/main/java/org/apache/carbondata/core/cache/Cache.java
core/src/main/java/org/apache/carbondata/core/datastore/block/AbstractIndex.java
core/src/main/java/org/apache/carbondata/core/cache/CacheProvider.java
core/src/main/java/org/apache/carbondata/core/cache/CarbonLRUCache.java
hadoop/src/main/java/org/apache/carbondata/hadoop/CacheAccessClient.java
core/src/main/java/org/apache/carbondata/core/cache/CacheType.java
core/src/main/java/org/apache/carbondata/core/datastore/block/TableBlockUniqueIdentifier.java
core/src/main/java/org/apache/carbondata/core/scan/result/iterator/AbstractDetailQueryResultIterator.java
core/src/main/java/org/apache/carbondata/core/scan/executor/impl/AbstractQueryExecutor.java
core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
texts:Implement LRU cache for B-Tree
LRU Cache for B-Tree is proposed  to ensure to avoid out memory, when too many number of tables exits and all are not frequently used.Problem:CarbonData is maintaining two level of B-Tree cache, one at the driver level and another at executor level.  Currently CarbonData has the mechanism to invalidate the segments and blocks cache for the invalid table segments, but there is no eviction policy for the unused cached object. So the instance at which complete memory is utilized then the system will not be able to process any new requests.Solution:In the cache maintained at the driver level and at the executor there must be objects in cache currently not in use. Therefore system should have the mechanism to below mechanism.1.       Set the max memory limit till which objects could be hold in the memory.2.       When configured memory limit reached then identify the cached objects currently not in use so that the required memory could be freed without impacting the existing process.3.       Eviction should be done only till the required memory is not meet.For details please refer to attachments.
issueID:CARBONDATA-486
type:Bug
changed files:
texts:Reading dataframe concurrently will lead to wrong data

issueID:CARBONDATA-487
type:Bug
changed files:
texts:spark2 integration is not compiling
spark2 integration is not compiling
issueID:CARBONDATA-488
type:New Feature
changed files:
texts:add InsertInto feature for spark2

issueID:CARBONDATA-489
type:Sub-task
changed files:core/src/main/java/org/apache/carbondata/core/datastore/impl/FileFactory.java
texts:spark2 decimal issue
create a table with decimal field and query it will throw error, do not support decimal(0, 0)
issueID:CARBONDATA-49
type:Bug
changed files:
texts:Can not query 3 million rows data which be loaded through local store system(not HDFS)
CSV data be stored at local machine(not HDSF), test result as below.1.If the csv data is 1 million rows, all query is ok.2.If the csv data is 3 million rows, query of cc.sql("select * from tablename")  having the below errors:ERROR 11-07 20:56:54,131 - &#91;Executor task launch worker-12&#93;&#91;partitionID:connectdemo;queryID:33111337863067_0&#93;org.carbondata.scan.executor.exception.QueryExecutionException: at org.carbondata.scan.executor.impl.AbstractQueryExecutor.initQuery(AbstractQueryExecutor.java:99) at org.carbondata.scan.executor.impl.AbstractQueryExecutor.getBlockExecutionInfos(AbstractQueryExecutor.java:178) at org.carbondata.scan.executor.impl.DetailRawRecordQueryExecutor.execute(DetailRawRecordQueryExecutor.java:20) at org.carbondata.spark.rdd.CarbonScanRDD$$anon$1.<init>(CarbonScanRDD.scala:174) at org.carbondata.spark.rdd.CarbonScanRDD.compute(CarbonScanRDD.scala:155) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306) at org.apache.spark.rdd.RDD.iterator(RDD.scala:270) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306) at org.apache.spark.rdd.RDD.iterator(RDD.scala:270) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306) at org.apache.spark.rdd.RDD.iterator(RDD.scala:270) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306) at org.apache.spark.rdd.RDD.iterator(RDD.scala:270) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306) at org.apache.spark.rdd.RDD.iterator(RDD.scala:270) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306) at org.apache.spark.rdd.RDD.iterator(RDD.scala:270) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306) at org.apache.spark.rdd.RDD.iterator(RDD.scala:270) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66) at org.apache.spark.scheduler.Task.run(Task.scala:89) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745)Caused by: org.carbondata.core.carbon.datastore.exception.IndexBuilderException: at org.carbondata.core.carbon.datastore.BlockIndexStore.fillLoadedBlocks(BlockIndexStore.java:211) at org.carbondata.core.carbon.datastore.BlockIndexStore.loadAndGetBlocks(BlockIndexStore.java:191) at org.carbondata.scan.executor.impl.AbstractQueryExecutor.initQuery(AbstractQueryExecutor.java:96)
issueID:CARBONDATA-490
type:Improvement
changed files:
texts:Unify all RDD in carbon-spark and carbon-spark2 module
Currently there are duplicate RDD(CarbonScanRDD.scala,NewCarbonDataLoadRDD.scala,CarbonDataRDDFactory) in carbon-spark and carbon-spark2 modules
issueID:CARBONDATA-491
type:Sub-task
changed files:
texts:do not use runnablecommand in spark2
we should not use the runnablecommand in spark, that may leads to some compatibility issue.
issueID:CARBONDATA-492
type:Bug
changed files:
texts:When profile spark-2.0 is avtive , CarbonExample have error in intellij idea

issueID:CARBONDATA-493
type:Bug
changed files:
texts:Insert into select from a empty table cause exception
example:Insert into target_table select * from source_tableif source table is empty, data loading step will throw exception
issueID:CARBONDATA-494
type:Test
changed files:
texts:Implement unit test cases for filter.executer package

issueID:CARBONDATA-495
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/datastore/compression/Compressor.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
core/src/main/java/org/apache/carbondata/core/util/CarbonMetadataUtil.java
core/src/main/java/org/apache/carbondata/core/datastore/compression/CompressorFactory.java
processing/src/main/java/org/apache/carbondata/processing/store/writer/AbstractFactDataWriter.java
core/src/main/java/org/apache/carbondata/core/datastore/compression/SnappyCompressor.java
texts:Unify compressor interface
Use compressor factory to unify the interface and eliminate small objects
issueID:CARBONDATA-496
type:Test
changed files:
texts:Implement unit test cases for core.carbon.datastore package

issueID:CARBONDATA-497
type:Bug
changed files:
texts:[Spark2] fix datatype issue of CarbonLateDecoderRule
In spark2, LogicalPlan resolve method need to check input data type. If the data type is wrong, the logical plan will be unresolved. CarbonLateDecoderRule should correct the datatype of dictionary dimension to resolve the logical plan.
issueID:CARBONDATA-498
type:Improvement
changed files:processing/src/main/java/org/apache/carbondata/processing/store/writer/CarbonFactDataWriter.java
core/src/main/java/org/apache/carbondata/core/util/CarbonMetadataUtil.java
processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactDataHandlerColumnar.java
core/src/main/java/org/apache/carbondata/core/metadata/ValueEncoderMeta.java
core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
texts:Refactor compression model

issueID:CARBONDATA-499
type:Sub-task
changed files:
texts:CarbonData-DML-Delete-Record-Support
Provide SQL support for delete operation to delete a row in CarbonData table.
issueID:CARBONDATA-5
type:Bug
changed files:
texts:data mismatch between the carbon Table and Hive Table for columns having \N for non numeric data type

issueID:CARBONDATA-50
type:New Feature
changed files:
texts:Support Spark 1.6.2 in CarbonData
Current carbon cannot support latest Spark version 1.6.2.  It should be supported with new maven profile like below mvn clean -Pspark-1.6.2 package
issueID:CARBONDATA-500
type:Sub-task
changed files:
texts:CarbonData-DML-Update-Support
Provide SQL support for Update operation to update bulk data in CarbonData table.
issueID:CARBONDATA-501
type:Sub-task
changed files:
texts:CarbonData-Create-Delete-DeltaFile-Support
Add support to create Delete Delta files in order to support Update/Delete functionality for CarbonData table.
issueID:CARBONDATA-502
type:Sub-task
changed files:
texts:CarbonData-Create-Update-DeltaFile-Support
Add functionality to create Update Delta files to support update operation on CarbonData table.
issueID:CARBONDATA-503
type:Sub-task
changed files:
texts:CarbonData-Cleanup-DeltaFiles-Support
Add functionality to support cleanup of delta files as a result of Update/Delete operation.
issueID:CARBONDATA-504
type:Sub-task
changed files:
texts:CarbonData-Cleanup-DeltaFiles-Support
Add functionality to support cleanup of delta files as a result of Update/Delete operation.
issueID:CARBONDATA-505
type:Sub-task
changed files:
texts:CarbonData-Implicit-TupleID-Creation
Add functionality to generate implicit tuple ids for the rows being deleted or updated from CarbonData table.
issueID:CARBONDATA-506
type:Sub-task
changed files:
texts:CarbonData-Exclude-DeletedRecords-On-Query
Add functionality in CarbonData record scanner to exclude deleted records from the scan.
issueID:CARBONDATA-507
type:Sub-task
changed files:
texts:CarbonData-Include-UpdatedRecords-On-Query
Add functionality to CarbonData record scanner to include updated records from Update Delta files.
issueID:CARBONDATA-508
type:Sub-task
changed files:
texts:CarbonDatat-Compaction-Delete-DeltaFiles
Add functionality to compact multiple delta files into one delete delta file as part of Update/Delete feature.
issueID:CARBONDATA-509
type:Sub-task
changed files:
texts:CarbonDatat-Compaction-Update-DeltaFiles
Add functionality to compact multiple Update delta files as part of Update/Delete feature.
issueID:CARBONDATA-51
type:New Feature
changed files:processing/src/main/java/org/apache/carbondata/processing/model/CarbonLoadModel.java
texts:support generate global dictionary from the local dictionary files
Users can generate local dictionary files periodically  through streaming tool, and then it will triggers carbon to execute load task. Using local dictionary files to generate global dictionary, which can help reduce the cost of generating global dictionary and improve the load performance
issueID:CARBONDATA-510
type:Sub-task
changed files:
texts:CarbonData-Exclude-Invalid-Btree-After-Compaction
Add functionality to exclude invalid driver/executor block level btree after compaction.
issueID:CARBONDATA-512
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/util/DataTypeUtil.java
texts:Reduce number of Timestamp formatter
In DataTypeUtil.java, carbon is creating a SimpleDateFormat for every row, which creates too many objects.
issueID:CARBONDATA-513
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/util/DataTypeUtil.java
core/src/main/java/org/apache/carbondata/core/scan/collector/impl/AbstractScannedResultCollector.java
texts:Reduce number of BigDecimal objects for scan
While scanning BigDecimal column, currently Carbon creates too many java BigDecimal objects and scala BigDecimal objects.
issueID:CARBONDATA-514
type:Bug
changed files:
texts:Select string type columns will return error.
The data successfully loaded and count is OK, but when I tried to query the detail data, it returns below error:scala> cc.sql("desc carbontest_002").show ---------------------  col_namedata_typecomment---------------------       vin   string data_date   string --------------------- scala> cc.sql("load data inpath 'hdfs://nameservice2/user/appuser/lucao/mydata4.csv' into table default.carbontest_002 OPTIONS('DELIMITER'=',')") WARN  07-12 16:30:30,241 - main skip empty input file: hdfs://nameservice2/user/appuser/lucao/mydata4.csv/_SUCCESS AUDIT 07-12 16:30:34,338 - &#91;*.com&#93;&#91;appuser&#93;&#91;Thread-1&#93;Data load request has been received for table default.carbontest_002 AUDIT 07-12 16:30:38,410 - &#91;*.com&#93;&#91;appuser&#93;&#91;Thread-1&#93;Data load is successful for default.carbontest_002 res12: org.apache.spark.sql.DataFrame = [] scala> cc.sql("select count from carbontest_002") res14: org.apache.spark.sql.DataFrame = &#91;_c0: bigint&#93; scala> res14.show -------     _c0------- 1000000------- scala> cc.sql("select vin, count as cnt from carbontest_002 group by vin").show WARN  07-12 16:32:04,250 - Lost task 1.0 in stage 20.0 (TID 40, *.com): java.lang.ClassCastException: java.lang.String cannot be cast to java.lang.Integer at scala.runtime.BoxesRunTime.unboxToInt(BoxesRunTime.java:106) at org.apache.spark.sql.catalyst.expressions.BaseGenericInternalRow$class.getInt(rows.scala:41) at org.apache.spark.sql.catalyst.expressions.GenericMutableRow.getInt(rows.scala:248) at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source) at org.apache.spark.sql.CarbonScan$$anonfun$1$$anon$1.next(CarbonScan.scala:155) at org.apache.spark.sql.CarbonScan$$anonfun$1$$anon$1.next(CarbonScan.scala:149) at org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.processInputs(TungstenAggregationIterator.scala:512) at org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.<init>(TungstenAggregationIterator.scala:686) at org.apache.spark.sql.execution.aggregate.TungstenAggregate$$anonfun$doExecute$1$$anonfun$2.apply(TungstenAggregate.scala:95) at org.apache.spark.sql.execution.aggregate.TungstenAggregate$$anonfun$doExecute$1$$anonfun$2.apply(TungstenAggregate.scala:86) at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710) at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306) at org.apache.spark.rdd.RDD.iterator(RDD.scala:270) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306) at org.apache.spark.rdd.RDD.iterator(RDD.scala:270) at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73) at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41) at org.apache.spark.scheduler.Task.run(Task.scala:89) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) at java.lang.Thread.run(Thread.java:745) ERROR 07-12 16:32:04,516 - Task 1 in stage 20.0 failed 4 times; aborting job WARN  07-12 16:32:04,600 - Lost task 0.1 in stage 20.0 (TID 45, *): TaskKilled (killed intentionally) ERROR 07-12 16:32:04,604 - Listener SQLListener threw an exception java.lang.NullPointerException at org.apache.spark.sql.execution.ui.SQLListener.onTaskEnd(SQLListener.scala:167) at org.apache.spark.scheduler.SparkListenerBus$class.onPostEvent(SparkListenerBus.scala:42) at org.apache.spark.scheduler.LiveListenerBus.onPostEvent(LiveListenerBus.scala:31) at org.apache.spark.scheduler.LiveListenerBus.onPostEvent(LiveListenerBus.scala:31) at org.apache.spark.util.ListenerBus$class.postToAll(ListenerBus.scala:55) at org.apache.spark.util.AsynchronousListenerBus.postToAll(AsynchronousListenerBus.scala:37) at org.apache.spark.util.AsynchronousListenerBus$$anon$1$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(AsynchronousListenerBus.scala:80) at org.apache.spark.util.AsynchronousListenerBus$$anon$1$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(AsynchronousListenerBus.scala:65) at org.apache.spark.util.AsynchronousListenerBus$$anon$1$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(AsynchronousListenerBus.scala:65) at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57) at org.apache.spark.util.AsynchronousListenerBus$$anon$1$$anonfun$run$1.apply$mcV$sp(AsynchronousListenerBus.scala:64) at org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1181) at org.apache.spark.util.AsynchronousListenerBus$$anon$1.run(AsynchronousListenerBus.scala:63) org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 20.0 failed 4 times, most recent failure: Lost task 1.3 in stage 20.0 (TID 44, *): java.lang.ClassCastException: java.lang.String cannot be cast to java.lang.Integer at scala.runtime.BoxesRunTime.unboxToInt(BoxesRunTime.java:106) at org.apache.spark.sql.catalyst.expressions.BaseGenericInternalRow$class.getInt(rows.scala:41) at org.apache.spark.sql.catalyst.expressions.GenericMutableRow.getInt(rows.scala:248) at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source) at org.apache.spark.sql.CarbonScan$$anonfun$1$$anon$1.next(CarbonScan.scala:155) at org.apache.spark.sql.CarbonScan$$anonfun$1$$anon$1.next(CarbonScan.scala:149) at org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.processInputs(TungstenAggregationIterator.scala:512) at org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.<init>(TungstenAggregationIterator.scala:686) at org.apache.spark.sql.execution.aggregate.TungstenAggregate$$anonfun$doExecute$1$$anonfun$2.apply(TungstenAggregate.scala:95) at org.apache.spark.sql.execution.aggregate.TungstenAggregate$$anonfun$doExecute$1$$anonfun$2.apply(TungstenAggregate.scala:86) at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710) at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306) at org.apache.spark.rdd.RDD.iterator(RDD.scala:270) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306) at org.apache.spark.rdd.RDD.iterator(RDD.scala:270) at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73) at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41) at org.apache.spark.scheduler.Task.run(Task.scala:89) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) at java.lang.Thread.run(Thread.java:745) Driver stacktrace: at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431) at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419) at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418) at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59) at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47) at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418) at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799) at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799) at scala.Option.foreach(Option.scala:236) at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799) at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640) at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599) at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588) at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48) at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620) at org.apache.spark.SparkContext.runJob(SparkContext.scala:1843) at org.apache.spark.SparkContext.runJob(SparkContext.scala:1856) at org.apache.spark.SparkContext.runJob(SparkContext.scala:1869) at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:212) at org.apache.spark.sql.execution.Limit.executeCollect(basicOperators.scala:165) at org.apache.spark.sql.execution.SparkPlan.executeCollectPublic(SparkPlan.scala:174) at org.apache.spark.sql.DataFrame$$anonfun$org$apache$spark$sql$DataFrame$$execute$1$1.apply(DataFrame.scala:1499) at org.apache.spark.sql.DataFrame$$anonfun$org$apache$spark$sql$DataFrame$$execute$1$1.apply(DataFrame.scala:1499) at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:53) at org.apache.spark.sql.DataFrame.withNewExecutionId(DataFrame.scala:2086) at org.apache.spark.sql.DataFrame.org$apache$spark$sql$DataFrame$$execute$1(DataFrame.scala:1498) at org.apache.spark.sql.DataFrame.org$apache$spark$sql$DataFrame$$collect(DataFrame.scala:1505) at org.apache.spark.sql.DataFrame$$anonfun$head$1.apply(DataFrame.scala:1375) at org.apache.spark.sql.DataFrame$$anonfun$head$1.apply(DataFrame.scala:1374) at org.apache.spark.sql.DataFrame.withCallback(DataFrame.scala:2099) at org.apache.spark.sql.DataFrame.head(DataFrame.scala:1374) at org.apache.spark.sql.DataFrame.take(DataFrame.scala:1456) at org.apache.spark.sql.DataFrame.showString(DataFrame.scala:170) at org.apache.spark.sql.DataFrame.show(DataFrame.scala:350) at org.apache.spark.sql.DataFrame.show(DataFrame.scala:311) at org.apache.spark.sql.DataFrame.show(DataFrame.scala:319) at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:35) at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:40) at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:42) at $iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:44) at $iwC$$iwC$$iwC$$iwC.<init>(<console>:46) at $iwC$$iwC$$iwC.<init>(<console>:48) at $iwC$$iwC.<init>(<console>:50) at $iwC.<init>(<console>:52) at <init>(<console>:54) at .<init>(<console>:58) at .<clinit>(<console>) at .<init>(<console>:7) at .<clinit>(<console>) at $print(<console>) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:606) at org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:1045) at org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1326) at org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:821) at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:852) at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:800) at org.apache.spark.repl.SparkILoop.reallyInterpret$1(SparkILoop.scala:857) at org.apache.spark.repl.SparkILoop.interpretStartingWith(SparkILoop.scala:902) at org.apache.spark.repl.SparkILoop.command(SparkILoop.scala:814) at org.apache.spark.repl.SparkILoop.processLine$1(SparkILoop.scala:657) at org.apache.spark.repl.SparkILoop.innerLoop$1(SparkILoop.scala:665) at org.apache.spark.repl.SparkILoop.org$apache$spark$repl$SparkILoop$$loop(SparkILoop.scala:670) at org.apache.spark.repl.SparkILoop$$anonfun$org$apache$spark$repl$SparkILoop$$process$1.apply$mcZ$sp(SparkILoop.scala:997) at org.apache.spark.repl.SparkILoop$$anonfun$org$apache$spark$repl$SparkILoop$$process$1.apply(SparkILoop.scala:945) at org.apache.spark.repl.SparkILoop$$anonfun$org$apache$spark$repl$SparkILoop$$process$1.apply(SparkILoop.scala:945) at scala.tools.nsc.util.ScalaClassLoader$.savingContextLoader(ScalaClassLoader.scala:135) at org.apache.spark.repl.SparkILoop.org$apache$spark$repl$SparkILoop$$process(SparkILoop.scala:945) at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:1064) at org.apache.spark.repl.Main$.main(Main.scala:31) at org.apache.spark.repl.Main.main(Main.scala) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:606) at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731) at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181) at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206) at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121) at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala) Caused by: java.lang.ClassCastException: java.lang.String cannot be cast to java.lang.Integer at scala.runtime.BoxesRunTime.unboxToInt(BoxesRunTime.java:106) at org.apache.spark.sql.catalyst.expressions.BaseGenericInternalRow$class.getInt(rows.scala:41) at org.apache.spark.sql.catalyst.expressions.GenericMutableRow.getInt(rows.scala:248) at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source) at org.apache.spark.sql.CarbonScan$$anonfun$1$$anon$1.next(CarbonScan.scala:155) at org.apache.spark.sql.CarbonScan$$anonfun$1$$anon$1.next(CarbonScan.scala:149) at org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.processInputs(TungstenAggregationIterator.scala:512) at org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.<init>(TungstenAggregationIterator.scala:686) at org.apache.spark.sql.execution.aggregate.TungstenAggregate$$anonfun$doExecute$1$$anonfun$2.apply(TungstenAggregate.scala:95) at org.apache.spark.sql.execution.aggregate.TungstenAggregate$$anonfun$doExecute$1$$anonfun$2.apply(TungstenAggregate.scala:86) at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710) at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306) at org.apache.spark.rdd.RDD.iterator(RDD.scala:270) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306) at org.apache.spark.rdd.RDD.iterator(RDD.scala:270) at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73) at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41) at org.apache.spark.scheduler.Task.run(Task.scala:89) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) at java.lang.Thread.run(Thread.java:745)
issueID:CARBONDATA-515
type:Test
changed files:
texts:Implement unit test cases for processing.newflow.converter package

issueID:CARBONDATA-516
type:New Feature
changed files:integration/spark2/src/main/java/org/apache/carbondata/spark/readsupport/SparkRowReadSupportImpl.java
core/src/main/java/org/apache/carbondata/core/scan/collector/impl/DictionaryBasedResultCollector.java
texts:[SPARK2]update union class in CarbonLateDecoderRule for Spark 2.x integration
In spark2, Union class is no longer sub-class of BinaryNode.
issueID:CARBONDATA-517
type:Sub-task
changed files:core/src/main/java/org/apache/carbondata/core/util/CarbonProperties.java
texts:Use carbon property to get the store path/kettle home
to distinguish the carbon config with spark config. for carbon config we use carbon property to get them
issueID:CARBONDATA-518
type:Bug
changed files:
texts:CarbonExample of spark moudle can not run as kettlehome and storepath shoug get form carbonproperties now

issueID:CARBONDATA-519
type:New Feature
changed files:core/src/main/java/org/apache/carbondata/core/datastore/chunk/impl/FixedLengthDimensionDataChunk.java
core/src/main/java/org/apache/carbondata/core/scan/executor/infos/BlockExecutionInfo.java
core/src/main/java/org/apache/carbondata/core/scan/result/vector/CarbonColumnarBatch.java
core/src/main/java/org/apache/carbondata/core/scan/executor/QueryExecutorFactory.java
core/src/main/java/org/apache/carbondata/core/scan/result/AbstractScannedResult.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/DimensionColumnDataChunk.java
processing/src/main/java/org/apache/carbondata/processing/merger/CarbonCompactionExecutor.java
core/src/main/java/org/apache/carbondata/core/scan/result/vector/impl/CarbonColumnVectorImpl.java
core/src/main/java/org/apache/carbondata/core/scan/collector/impl/AbstractScannedResultCollector.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/impl/VariableLengthDimensionDataChunk.java
core/src/main/java/org/apache/carbondata/core/keygenerator/directdictionary/DirectDictionaryGenerator.java
core/src/main/java/org/apache/carbondata/core/scan/result/vector/ColumnVectorInfo.java
core/src/main/java/org/apache/carbondata/core/scan/result/iterator/AbstractDetailQueryResultIterator.java
core/src/main/java/org/apache/carbondata/core/scan/executor/impl/AbstractQueryExecutor.java
core/src/main/java/org/apache/carbondata/core/scan/result/iterator/VectorDetailQueryResultIterator.java
core/src/main/java/org/apache/carbondata/core/scan/executor/impl/VectorDetailQueryExecutor.java
core/src/main/java/org/apache/carbondata/core/scan/result/impl/FilterQueryScannedResult.java
core/src/main/java/org/apache/carbondata/core/keygenerator/directdictionary/timestamp/TimeStampDirectDictionaryGenerator.java
integration/spark2/src/main/java/org/apache/carbondata/spark/readsupport/SparkRowReadSupportImpl.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
core/src/main/java/org/apache/carbondata/core/scan/result/vector/CarbonColumnVector.java
core/src/main/java/org/apache/carbondata/core/scan/model/QueryModel.java
hadoop/src/main/java/org/apache/carbondata/hadoop/CarbonRecordReader.java
core/src/main/java/org/apache/carbondata/core/keygenerator/directdictionary/timestamp/DateDirectDictionaryGenerator.java
integration/spark-datasource/src/main/scala/org/apache/carbondata/spark/vectorreader/VectorizedCarbonRecordReader.java
core/src/main/java/org/apache/carbondata/core/scan/collector/impl/DictionaryBasedVectorResultCollector.java
core/src/main/java/org/apache/carbondata/core/scan/result/vector/MeasureDataVectorProcessor.java
integration/spark-datasource/src/main/scala/org/apache/carbondata/spark/vectorreader/ColumnarVectorWrapper.java
core/src/main/java/org/apache/carbondata/core/scan/collector/ScannedResultCollector.java
texts:Enable vector reader in Carbon-Spark 2.0 integration and Carbon layer
Spark 2.0 supports vectorized reader and uses whole codegen to improve performance, Carbon will enable vectorized reader integrating with Spark to take advantage of new features of Spark2.x
issueID:CARBONDATA-52
type:Bug
changed files:
texts:query  average value of bigInt filed, the query result is inaccurate
when query  average value of bigInt filed "column1", if the value of "sum(column1)/count(column1)" is double type with decimal,the result of avg(column1) is inaccurate, it lost precision.
issueID:CARBONDATA-520
type:Sub-task
changed files:
texts:Executor can not get the read support class
Executor can not get the read support class, this leads to cast exception when running carbon on spark2
issueID:CARBONDATA-521
type:Sub-task
changed files:
texts:Depends on more stable class of spark in spark2
avoid to use unstable class in spark2, otherwise it leads to compatible issue with spark
issueID:CARBONDATA-522
type:Bug
changed files:
texts:New data loading flowcauses testcase failures like big decimal etc
Pls check http://136.243.101.176:8080/job/ApacheCarbonPRBuilder/105/.new data flow cause test regressions.
issueID:CARBONDATA-524
type:Improvement
changed files:
texts:improve integration test case of AllDataTypesTestCase5

issueID:CARBONDATA-525
type:Test
changed files:
texts:Fix timestamp based test cases

issueID:CARBONDATA-526
type:Improvement
changed files:
texts:Write Unit TestCase For org.apache.carbondata.scan.filter.resolver

issueID:CARBONDATA-528
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
texts:to support octal escape delimiter char

issueID:CARBONDATA-529
type:Test
changed files:
texts:Add Unit Tests for processing.newflow.parser package

issueID:CARBONDATA-53
type:Bug
changed files:
texts:when No dictionary column has negative number, the query result is Null.
when use DICTIONARY_EXCLUDE to set column as No_Dictionary, and for this column, when the data has negative number, the query result of negative number will be null.
issueID:CARBONDATA-530
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
texts:Query with ordery by and limit is not optimized properly
for order by query having limit, spark optimizes the plan. But since we put Decoder in between Limit and TungstenSort  plan, check the plan as below, its not able to optimize the plan== Physical Plan ==                                                                                                                                                                                                                                                                                                                    Limit 2                                                                                                                                                                                                                                                                                                                                 ConvertToSafe                                                                                                                                                                                                                                                                                                                           CarbonDictionaryDecoder CarbonDecoderRelation(Map(name#3 -> name#3),CarbonDatasourceRelation(`default`.`dict`,None)), ExcludeProfile(ArrayBuffer(name#3)), CarbonAliasDecoderRelation()                                                                                                                                               TungstenSort name#3 ASC, true, 0                                                                                                                                                                                                                                                                                                      ConvertToUnsafe                                                                                                                                                                                                                                                                                                                         Exchange rangepartitioning(name#3 ASC)                                                                                                                                                                                                                                                                                                  ConvertToSafe                                                                                                                                                                                                                                                                                                                           CarbonDictionaryDecoder CarbonDecoderRelation(Map(name#3 -> name#3),CarbonDatasourceRelation(`default`.`dict`,None)), IncludeProfile(ArrayBuffer(name#3)), CarbonAliasDecoderRelation()                                                                                                                                               CarbonScan name#3, (CarbonRelation default, dict, CarbonMetaData(ArrayBuffer(name),ArrayBuffer(default_dummy_measure),org.apache.carbondata.core.carbon.metadata.schema.table.CarbonTable@6021d179,DictionaryMap(Map(name -> true))), org.apache.carbondata.spark.merger.TableMeta@4c3f903d, None), (name#3 = hello), false Code Generation: true                                                                                                                                                                                                                                                                                                                  ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------We should put outer decoder on top of limit
issueID:CARBONDATA-531
type:Improvement
changed files:
texts:Eliminate spark dependency in carbon core
Clean up the interface and take out Spark dependency on Carbon-core module.
issueID:CARBONDATA-532
type:Bug
changed files:
texts:When set use_kettle=false , the testcase [TestEmptyRows] run failed
When force set use_kettle=false, and run CI, the testcase &#91;TestEmptyRows&#93; run failed, and catch exception:ERROR 13-12 18:08:57,467 - &#91;Executor task launch worker-0&#93;&#91;partitionID:default_emptyrowcarbontable_26c7bf9e-c20e-40db-ada2-f776a8b34ee1&#93; org.apache.carbondata.processing.store.writer.exception.CarbonDataWriterException: java.math.BigDecimal cannot be cast to [Ljava.lang.Long;java.util.concurrent.ExecutionException: org.apache.carbondata.processing.store.writer.exception.CarbonDataWriterException: java.math.BigDecimal cannot be cast to [Ljava.lang.Long; at java.util.concurrent.FutureTask.report(FutureTask.java:122) at java.util.concurrent.FutureTask.get(FutureTask.java:188) at org.apache.carbondata.processing.store.CarbonFactDataHandlerColumnar.processWriteTaskSubmitList(CarbonFactDataHandlerColumnar.java:1114) at org.apache.carbondata.processing.store.CarbonFactDataHandlerColumnar.finish(CarbonFactDataHandlerColumnar.java:1081) at org.apache.carbondata.processing.newflow.steps.DataWriterProcessorStepImpl.close(DataWriterProcessorStepImpl.java:142) at org.apache.carbondata.processing.newflow.DataLoadExecutor.execute(DataLoadExecutor.java:57) at org.apache.carbondata.spark.rdd.NewCarbonDataLoadRDD$$anon$1.<init>(NewCarbonDataLoadRDD.scala:160) at org.apache.carbondata.spark.rdd.NewCarbonDataLoadRDD.compute(NewCarbonDataLoadRDD.scala:139) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:300) at org.apache.spark.rdd.RDD.iterator(RDD.scala:264) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66) at org.apache.spark.scheduler.Task.run(Task.scala:88) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) at java.lang.Thread.run(Thread.java:745)pls check pr: https://github.com/apache/incubator-carbondata/pull/418and the CI result: http://136.243.101.176:8080/job/ApacheCarbonPRBuilder/166/console
issueID:CARBONDATA-533
type:Improvement
changed files:
texts:Write Unit Test Case For carbondataprocessing.newflow.steps

issueID:CARBONDATA-534
type:Improvement
changed files:
texts:Write Unit Test Case For DataLoadExecuter and DataLoadProcessBuilder

issueID:CARBONDATA-535
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/scan/expression/conditional/NotEqualsExpression.java
core/src/main/java/org/apache/carbondata/core/scan/expression/conditional/NotInExpression.java
core/src/main/java/org/apache/carbondata/core/util/DataTypeUtil.java
core/src/main/java/org/apache/carbondata/core/scan/expression/conditional/EqualToExpression.java
core/src/main/java/org/apache/carbondata/core/cache/dictionary/ColumnDictionaryInfo.java
core/src/main/java/org/apache/carbondata/core/keygenerator/directdictionary/DirectDictionaryKeyGeneratorFactory.java
core/src/main/java/org/apache/carbondata/core/scan/expression/ExpressionResult.java
core/src/main/java/org/apache/carbondata/core/keygenerator/directdictionary/DirectDictionaryGenerator.java
core/src/main/java/org/apache/carbondata/core/scan/expression/conditional/InExpression.java
core/src/main/java/org/apache/carbondata/core/scan/expression/conditional/LessThanExpression.java
core/src/main/java/org/apache/carbondata/core/scan/expression/conditional/GreaterThanExpression.java
core/src/main/java/org/apache/carbondata/core/scan/expression/LiteralExpression.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/resolverinfo/visitor/CustomTypeDictionaryVisitor.java
integration/spark2/src/main/java/org/apache/carbondata/spark/readsupport/SparkRowReadSupportImpl.java
core/src/main/java/org/apache/carbondata/core/scan/expression/conditional/GreaterThanEqualToExpression.java
core/src/main/java/org/apache/carbondata/core/keygenerator/directdictionary/timestamp/DateDirectDictionaryGenerator.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/resolverinfo/visitor/ResolvedFilterInfoVisitorIntf.java
core/src/main/java/org/apache/carbondata/core/scan/filter/FilterUtil.java
core/src/main/java/org/apache/carbondata/core/util/AbstractDataFileFooterConverter.java
core/src/main/java/org/apache/carbondata/core/scan/expression/conditional/LessThanEqualToExpression.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/ConditionalFilterResolverImpl.java
core/src/main/java/org/apache/carbondata/core/metadata/converter/ThriftWrapperSchemaConverterImpl.java
core/src/main/java/org/apache/carbondata/core/scan/complextypes/PrimitiveQueryType.java
texts:Enable Date and Char datatype support for Carbondata
Add Date and Char datatype support for Carbondata
issueID:CARBONDATA-536
type:Bug
changed files:
texts:Initialize GlobalDictionaryUtil.updateTableMetadataFunc for Spark 2.x
GlobalDictionaryUtil.updateTableMetadataFunc needs to be initialized.
issueID:CARBONDATA-537
type:Bug
changed files:
texts:Bug fix for DICTIONARY_EXCLUDE option in spark2 integration
1. Fix bug for dictionary_exclude option in spark2 integration. In spark2, datat type name is changed from "string" to "stringtype", but `isStringAndTimestampColDictionaryExclude` is not modified.2. Fix bug for data loading with no-kettle. In no-kettle loading, should not ask user to set kettle home environment variable.3. clean up scala code style in `GlobalDictionaryUtil`
issueID:CARBONDATA-538
type:Improvement
changed files:
texts:Add test case to spark2 integration
Currently spark2 integration has very few test case, it should be improved
issueID:CARBONDATA-539
type:Bug
changed files:
texts:Return empty row in map reduce application
There is a bug that Carbon will return empty row in map reduce app if projection columns are not set.
issueID:CARBONDATA-54
type:Bug
changed files:
texts:Windows functions are not working in Carbon
Windows functions like Rank, Row_number etc are not working in Carbon.Example :SELECT country,name,salary FROM (SELECT country,name,salary,dense_rank() OVER (PARTITION BY country ORDER BY salary DESC) as rank FROM t3) tmp WHERE rank <= 2
issueID:CARBONDATA-540
type:New Feature
changed files:processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactDataHandlerColumnar.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/SingleThreadFinalSortFilesMerger.java
processing/src/main/java/org/apache/carbondata/processing/store/writer/AbstractFactDataWriter.java
texts:Support insertInto without kettle for spark2
Support inserInto without kettle for spark2
issueID:CARBONDATA-541
type:Test
changed files:
texts:Implement unit test cases for processing.newflow.dictionary package

issueID:CARBONDATA-542
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/util/DataTypeUtil.java
texts:Parsing values for measures and dimensions during data load should adopt a strict check
Currently in carbon we treat Short and Int as long and at the time of storing in carbon data files delta compression is used which compresses the data based on min and max values of the column.While parsing the values for these datatypes, we use Double data type parser and extract long value from that. Code snippet as below. Double.valueOf(msrValue).longValue()This has the following problems.1. Measure Values beyond the range of Int and Short are parsed successfully. This behavior conflicts when the same measure is included as dictionary_include and becomes a dimension. When we query then each dimension value is parsed for its datatype for result conversion and at that time NumberFormatException is thrown and null is displayed in the result while for measure the loaded values are displayed. This also impacts aggregate queries. That is why strict check mechanism is adopted for dimensions values parsing.2. Data inconsistency  in case of measures as for decimal values, the value before decimal will only be considered for Int and Short datatypes.3. For measures, if values beyond the datatype range are allowed the compression will decrease.Therefore we will have to adopt a strict behavior for both dimensions and measures.
issueID:CARBONDATA-543
type:Test
changed files:
texts:Implement unit test cases for DataBlockIteratorImpl, IntermediateFileMerger and SortDataRows classes

issueID:CARBONDATA-544
type:Bug
changed files:
texts:Delete core/.TestFileFactory.carbondata.crc,core/Testdb.carbon
Use mvn package to compile and running test cases, always generating the needless test files.Need to delete the test files under core/.TestFileFactory.carbondata.crc,core/Testdb.carbon.
issueID:CARBONDATA-545
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/datastore/chunk/impl/FixedLengthDimensionDataChunk.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/store/impl/safe/SafeVariableLengthDimensionDataChunkStore.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/store/DimensionChunkStoreFactory.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/store/impl/safe/SafeFixedLengthDimensionDataChunkStore.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/store/impl/unsafe/UnsafeFixedLengthDimensionDataChunkStore.java
core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelRangeGrtrThanEquaToFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/store/impl/safe/SafeAbsractDimensionDataChunkStore.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/store/DimensionDataChunkStore.java
core/src/main/java/org/apache/carbondata/core/memory/CarbonUnsafe.java
core/src/main/java/org/apache/carbondata/core/scan/scanner/impl/FilterScanner.java
core/src/main/java/org/apache/carbondata/core/scan/result/AbstractScannedResult.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/DimensionColumnDataChunk.java
core/src/main/java/org/apache/carbondata/core/datastore/compression/CompressorFactory.java
core/src/main/java/org/apache/carbondata/core/scan/collector/impl/AbstractScannedResultCollector.java
processing/src/main/java/org/apache/carbondata/processing/store/writer/AbstractFactDataWriter.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/impl/AbstractDimensionDataChunk.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/impl/VariableLengthDimensionDataChunk.java
core/src/main/java/org/apache/carbondata/core/scan/complextypes/PrimitiveQueryType.java
core/src/main/java/org/apache/carbondata/core/scan/complextypes/ComplexQueryType.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/ExcludeFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelRangeLessThanEqualFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/datastore/compression/Compressor.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
core/src/main/java/org/apache/carbondata/core/util/CarbonMetadataUtil.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelRangeLessThanFiterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/store/impl/unsafe/UnsafeAbstractDimensionDataChunkStore.java
core/src/main/java/org/apache/carbondata/core/datastore/compression/SnappyCompressor.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/IncludeFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelRangeGrtThanFiterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/store/impl/unsafe/UnsafeVariableLengthDimesionDataChunkStore.java
texts:Carbon Query GC Problem
ProblemThere are lots of gc when carbon is processing more number of records during query, which is impacting carbon query performance.To solve this gc problem happening when query output is too huge or when more number of records are processed.SolutionCurrently we are storing all the data which is read during query from carbon data file in heap, when number of query output is huge it is causing more gc. Instead of storing in heap we can store this data in offheap and will clear when scanning is finished for that query.
issueID:CARBONDATA-546
type:Improvement
changed files:processing/src/main/java/org/apache/carbondata/processing/util/CarbonLoaderUtil.java
core/src/main/java/org/apache/carbondata/core/util/path/CarbonTablePath.java
core/src/main/java/org/apache/carbondata/core/metadata/AbsoluteTableIdentifier.java
processing/src/main/java/org/apache/carbondata/processing/util/DeleteLoadFolders.java
texts:Extract data management command to carbon-spark-common module
Currently there are duplicated code for data management command in carbon-spark and carbon-spark2 module. In this PR, following commands are removed from carbonTableSchema.scala and extracted to carbon-spark-common: ShowLoads DeleteLoadById DeleteLoadByDate CleanFiles
issueID:CARBONDATA-547
type:Improvement
changed files:
texts:Add CarbonSession and enabled parser to use all carbon commands
Currently no DDL commands like CREATE,LOAD,ALTER,DROP,DESCRIBE, SHOW LOADS, DELETE SEGMENTS etc are not working in Spark 2.0 integration.So please add CarbonSession and overwrite the SQL parser to make all this commands work.
issueID:CARBONDATA-549
type:Sub-task
changed files:
texts:code improvement for bigint compression
code cleanup for big int/ decimal compression.
issueID:CARBONDATA-55
type:Bug
changed files:
texts:Pushdown greaterthan and lessthan filters to Carbon
Currently GreaterThan and LessThan filters are handled in spark layer. But these could be pushed down to Carbon layer to improve performance
issueID:CARBONDATA-551
type:Test
changed files:
texts:Implement unit test cases for classes in processing package

issueID:CARBONDATA-552
type:Bug
changed files:
texts:Unthrown FilterUnsupportedException in catch block
new FilterUnsupportedException(e) is not thrown in core/src/main/java/org/apache/carbondata/scan/filter/resolver/RowLevelRangeFilterResolverImpl.java (found in the current github snapshot, ddeb00425537ff59bdfba76779c5d96287e07d2e)230       }231     } catch (FilterIllegalMemberException e) {232       new FilterUnsupportedException(e);233     }234     return filterValuesList;
issueID:CARBONDATA-553
type:Test
changed files:
texts:Create integration test-case for dataframe API

issueID:CARBONDATA-554
type:Bug
changed files:
texts:Maven build failed when run command "mvn clean install -DskipTests"
Maven build failed  because of checkstyle
issueID:CARBONDATA-555
type:Improvement
changed files:
texts:Configure Integration testcases to be run on hadoop cluster

issueID:CARBONDATA-556
type:Improvement
changed files:
texts:when loading, it may happen that multiple tasks in one node, while other node has no task.
the algorithm of carbondata loading is aim to ensure one task in one node , but in cluster, it may happen that mutilple tasks in one node, while other node has no tasks, which cost more time because of out of balance load.
issueID:CARBONDATA-557
type:Bug
changed files:
texts:Option use_kettle is not work when use spark-1.5
when run command: LOAD DATA LOCAL INPATH '/path/data.csv' into table t3 OPTIONS('USE_KETTLE'='false');load data still using the orginal flow instead of the new flow
issueID:CARBONDATA-558
type:Bug
changed files:processing/src/main/java/org/apache/carbondata/processing/loading/converter/impl/MeasureFieldConverterImpl.java
texts:Load performance bad when use_kettle=false
When you import a data file, the measure column contains many empty strings， if use_kettle=false, the load performance has a sharp declineI checked the logs of executor, many warnnings printed like below: 16/12/22 07:03:12 WARN MeasureFieldConverterImpl: pool-22-thread-6 Cant not convert :  to Numeric type value. Value considered as null.16/12/22 07:03:12 WARN MeasureFieldConverterImpl: pool-22-thread-1 Cant not convert :  to Numeric type value. Value considered as null.16/12/22 07:03:12 WARN MeasureFieldConverterImpl: pool-22-thread-6 Cant not convert :  to Numeric type value. Value considered as null.16/12/22 07:03:12 WARN MeasureFieldConverterImpl: pool-22-thread-1 Cant not convert :  to Numeric type value. Value considered as null.16/12/22 07:03:12 WARN MeasureFieldConverterImpl: pool-22-thread-2 Cant not convert :  to Numeric type value. Value considered as null.16/12/22 07:03:12 WARN MeasureFieldConverterImpl: pool-22-thread-3 Cant not convert :  to Numeric type value. Value considered as null.
issueID:CARBONDATA-559
type:Bug
changed files:
texts:Job failed at last step
Hi team,My job alwasy failed at last step:it said 'yarn' user don't have write access to target data path(storeLocation).But I tested twice with 10000 rows data, both successed. could you help look into the log? Please refer to the attachment. Search 'access=WRITE' you can see the exception.Search 'Exception' for other exceptions.thanks,Lionel
issueID:CARBONDATA-56
type:Bug
changed files:
texts:Exception thrown when aggregation on dimension which has decimal datatype
In case of aggregation on dimension which has decimal datatype, the data from utility class is returned as spark sql decimal type. Because of this while aggregating this data data with java decimal type class cast exception is thrown.
issueID:CARBONDATA-560
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/scan/executor/impl/AbstractQueryExecutor.java
texts:In QueryExecutionException, can not use executorService.shutdownNow() to shut down immediately.
In QueryExecutionException, can not use executorService.shutdownNow() to shut down immediately.
issueID:CARBONDATA-561
type:Improvement
changed files:
texts:Merge the two CarbonOption.scala into one under spark-common
Merge the below two CarbonOption.scala into one under spark-common:/integration/spark-common/src/main/scala/org/apache/carbondata/spark/CarbonOption.scala/integration/spark2/src/main/scala/org/apache/carbondata/spark/CarbonOption.scala
issueID:CARBONDATA-562
type:New Feature
changed files:
texts:Supporting Spark 1.6.3 integration in CarbonData
Carbon is compiled with below command mvn clean -P spark-1.6,provided -Dspark.version=1.6.3 -DskipTests packagestart carbon context in yarn client mode bin/spark-shell --master yarn-client --executor-memory 2G  --executor-cores 2 --driver-memory 2G  --num-executors 3  ____              __     / _/_  ___ ____/ /_    \ \/ _ \/ _ `/ __/  '/   /__/ ./&#95;,// //&#95;\   version 1.6.3      /_/Using Scala version 2.10.5 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_111)Type in expressions to have them evaluated.scala> import org.apache.spark.sql.CarbonContext;import org.apache.spark.sql.CarbonContextscala> val cc=new CarbonContext(sc,"hdfs://hacluser/opt/CarbonStore");e of dependencies)scala.tools.reflect.ToolBoxError: reflective compilation has failed:not enough arguments for constructor Optimizer: (conf: org.apache.spark.sql.catalyst.CatalystConf)org.apache.spark.sql.catalyst.optimizer.Optimizer.Unspecified value parameter conf.        at scala.tools.reflect.ToolBoxFactory$ToolBoxImpl$ToolBoxGlobal.throwIfErrors(ToolBoxFactory.scala:314)at org.apache.spark.sql.CodeGenerateFactory.<init>(CodeGenerateFactory.scala:32)        at org.apache.spark.sql.CodeGenerateFactory$.init(CodeGenerateFactory.scala:67)        at org.apache.spark.sql.CarbonContext.<init>(CarbonContext.scala:58)        at org.apache.spark.sql.CarbonContext.<init>(CarbonContext.scala:52)
issueID:CARBONDATA-563
type:Bug
changed files:
texts:Select Queries are  not working with spark 1.6.2.
Create carbon table create table x (a int ,b string) stored by 'carbondata'Load data to carbon table run query  select count from x;  Java.lang.ClassCastException:[Ljava.lang.Object;can not be cast to org.apache.sql.catalyst.InternalRowLog snap shot in attached.
issueID:CARBONDATA-564
type:Improvement
changed files:processing/src/main/java/org/apache/carbondata/processing/loading/model/CarbonLoadModel.java
processing/src/main/java/org/apache/carbondata/processing/util/CarbonLoaderUtil.java
texts:long time ago, carbon may use dimension table csv file to make dictionary, but now unsed, so remove
long time ago, carbon may use dimension table csv file to make dictionary, but now with coldict, allDictionary and so on , there is no need for dimesion table file to make dictionary, and to make carbondata code easy to read, these unused part should be removed.
issueID:CARBONDATA-565
type:Task
changed files:
texts:Clean up code suggested by IDE analyzer

issueID:CARBONDATA-566
type:Sub-task
changed files:integration/spark-datasource/src/main/scala/org/apache/carbondata/spark/vectorreader/VectorizedCarbonRecordReader.java
integration/spark-datasource/src/main/scala/org/apache/carbondata/spark/vectorreader/ColumnarVectorWrapper.java
processing/src/main/java/org/apache/carbondata/processing/util/CarbonQueryUtil.java
texts:clean up code for carbon-spark2 module

issueID:CARBONDATA-568
type:Sub-task
changed files:core/src/main/java/org/apache/carbondata/core/metadata/schema/table/TableInfo.java
processing/src/main/java/org/apache/carbondata/processing/merger/CarbonCompactionUtil.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/column/ColumnUniqueIdGenerator.java
processing/src/main/java/org/apache/carbondata/processing/loading/steps/DataWriterProcessorStepImpl.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/resolverinfo/DimColumnResolvedFilterInfo.java
core/src/main/java/org/apache/carbondata/core/util/CarbonProperties.java
core/src/main/java/org/apache/carbondata/core/scan/collector/impl/RawBasedResultCollector.java
core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelRangeGrtrThanEquaToFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/FilterResolverIntf.java
core/src/main/java/org/apache/carbondata/core/datastore/FileHolder.java
core/src/main/java/org/apache/carbondata/core/datastore/block/SegmentProperties.java
core/src/main/java/org/apache/carbondata/core/scan/expression/exception/FilterUnsupportedException.java
core/src/main/java/org/apache/carbondata/core/reader/ThriftReader.java
core/src/main/java/org/apache/carbondata/core/datastore/columnar/ColumnarKeyStoreMetadata.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/TableSchema.java
core/src/main/java/org/apache/carbondata/core/service/ColumnUniqueIdService.java
core/src/main/java/org/apache/carbondata/core/scan/executor/impl/AbstractQueryExecutor.java
processing/src/main/java/org/apache/carbondata/processing/util/CarbonLoaderUtil.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/ExcludeFilterExecuterImpl.java
processing/src/main/java/org/apache/carbondata/processing/loading/steps/DataConverterProcessorStepImpl.java
hadoop/src/main/java/org/apache/carbondata/hadoop/readsupport/CarbonReadSupport.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/reader/DimensionColumnChunkReader.java
processing/src/main/java/org/apache/carbondata/processing/util/CarbonDataProcessorUtil.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/FilterExecuter.java
core/src/main/java/org/apache/carbondata/core/scan/expression/logical/BinaryLogicalExpression.java
core/src/main/java/org/apache/carbondata/core/scan/expression/exception/FilterIllegalMemberException.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/OrFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/scan/complextypes/StructQueryType.java
core/src/main/java/org/apache/carbondata/core/scan/result/vector/MeasureDataVectorProcessor.java
core/src/main/java/org/apache/carbondata/core/keygenerator/columnar/ColumnarSplitter.java
core/src/main/java/org/apache/carbondata/core/writer/CarbonIndexFileWriter.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/AndFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/util/DataTypeUtil.java
core/src/main/java/org/apache/carbondata/core/scan/scanner/BlockletScanner.java
core/src/main/java/org/apache/carbondata/core/datastore/impl/FileHolderImpl.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/reader/MeasureColumnChunkReader.java
core/src/main/java/org/apache/carbondata/core/scan/result/iterator/DetailQueryResultIterator.java
integration/spark2/src/main/java/org/apache/carbondata/spark/readsupport/SparkRowReadSupportImpl.java
core/src/main/java/org/apache/carbondata/core/scan/expression/conditional/ConditionalExpression.java
core/src/main/java/org/apache/carbondata/core/memory/HeapMemoryAllocator.java
core/src/main/java/org/apache/carbondata/core/scan/filter/FilterExpressionProcessor.java
core/src/main/java/org/apache/carbondata/core/metadata/blocklet/datachunk/DataChunk.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/reader/measure/AbstractMeasureChunkReader.java
core/src/main/java/org/apache/carbondata/core/scan/filter/GenericQueryType.java
core/src/main/java/org/apache/carbondata/core/scan/scanner/impl/FilterScanner.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/RowLevelRangeFilterResolverImpl.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/DimensionColumnDataChunk.java
processing/src/main/java/org/apache/carbondata/processing/datatypes/PrimitiveDataType.java
core/src/main/java/org/apache/carbondata/core/scan/executor/impl/QueryExecutorProperties.java
core/src/main/java/org/apache/carbondata/core/scan/executor/infos/AggregatorInfo.java
core/src/main/java/org/apache/carbondata/core/scan/collector/impl/DictionaryBasedResultCollector.java
processing/src/main/java/org/apache/carbondata/processing/loading/AbstractDataLoadProcessorStep.java
processing/src/main/java/org/apache/carbondata/processing/loading/converter/impl/RowConverterImpl.java
core/src/main/java/org/apache/carbondata/core/scan/executor/exception/QueryExecutionException.java
core/src/main/java/org/apache/carbondata/core/stats/QueryStatisticsRecorderDummy.java
core/src/main/java/org/apache/carbondata/core/datastore/impl/FileFactory.java
core/src/main/java/org/apache/carbondata/core/scan/scanner/AbstractBlockletScanner.java
core/src/main/java/org/apache/carbondata/core/scan/expression/Expression.java
hadoop/src/main/java/org/apache/carbondata/hadoop/util/CarbonInputFormatUtil.java
core/src/main/java/org/apache/carbondata/core/util/CarbonTimeStatisticsFactory.java
core/src/main/java/org/apache/carbondata/core/scan/collector/impl/DictionaryBasedVectorResultCollector.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelRangeGrtThanFiterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/metadata/converter/ThriftWrapperSchemaConverterImpl.java
core/src/main/java/org/apache/carbondata/core/scan/executor/infos/BlockExecutionInfo.java
core/src/main/java/org/apache/carbondata/core/datastore/impl/DFSFileHolderImpl.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/merger/UnsafeIntermediateFileMerger.java
hadoop/src/main/java/org/apache/carbondata/hadoop/CacheAccessClient.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/LogicalFilterResolverImpl.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelRangeTypeExecuterFacory.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/RowLevelFilterResolverImpl.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/column/CarbonMeasure.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/resolverinfo/visitor/NoDictionaryTypeVisitor.java
core/src/main/java/org/apache/carbondata/core/scan/expression/BinaryExpression.java
core/src/main/java/org/apache/carbondata/core/scan/result/iterator/RawResultIterator.java
core/src/main/java/org/apache/carbondata/core/datastore/columnar/ColumnarKeyStoreDataHolder.java
core/src/main/java/org/apache/carbondata/core/util/path/CarbonTablePath.java
core/src/main/java/org/apache/carbondata/core/keygenerator/directdictionary/timestamp/TimeStampGranularityTypeValue.java
core/src/main/java/org/apache/carbondata/core/reader/CarbonFooterReader.java
core/src/main/java/org/apache/carbondata/core/datastore/block/TableTaskInfo.java
core/src/main/java/org/apache/carbondata/core/scan/model/QueryModel.java
core/src/main/java/org/apache/carbondata/core/memory/CarbonUnsafe.java
core/src/main/java/org/apache/carbondata/core/util/AbstractDataFileFooterConverter.java
processing/src/main/java/org/apache/carbondata/processing/merger/CarbonCompactionExecutor.java
core/src/main/java/org/apache/carbondata/core/scan/model/QueryColumn.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/column/ColumnSchema.java
core/src/main/java/org/apache/carbondata/core/scan/complextypes/ComplexQueryType.java
core/src/main/java/org/apache/carbondata/core/keygenerator/mdkey/AbstractKeyGenerator.java
processing/src/main/java/org/apache/carbondata/processing/loading/steps/SortProcessorStepImpl.java
core/src/main/java/org/apache/carbondata/core/keygenerator/KeyGenerator.java
core/src/main/java/org/apache/carbondata/core/util/CarbonMetadataUtil.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/IncludeFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/scan/executor/QueryExecutor.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/resolverinfo/visitor/ResolvedFilterInfoVisitorIntf.java
core/src/main/java/org/apache/carbondata/core/scan/complextypes/PrimitiveQueryType.java
processing/src/main/java/org/apache/carbondata/processing/loading/converter/impl/FieldEncoderFactory.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/resolverinfo/MeasureColumnResolvedFilterInfo.java
core/src/main/java/org/apache/carbondata/core/scan/result/iterator/AbstractDetailQueryResultIterator.java
core/src/main/java/org/apache/carbondata/core/metadata/encoder/Encoding.java
core/src/main/java/org/apache/carbondata/core/scan/executor/util/QueryUtil.java
core/src/main/java/org/apache/carbondata/core/scan/executor/impl/VectorDetailQueryExecutor.java
core/src/main/java/org/apache/carbondata/core/datastore/DataRefNode.java
core/src/main/java/org/apache/carbondata/core/keygenerator/directdictionary/timestamp/TimeStampGranularityConstants.java
core/src/main/java/org/apache/carbondata/core/cache/Cache.java
core/src/main/java/org/apache/carbondata/core/keygenerator/mdkey/MultiDimKeyVarLengthGenerator.java
core/src/main/java/org/apache/carbondata/core/keygenerator/columnar/impl/MultiDimKeyVarLengthVariableSplitGenerator.java
core/src/main/java/org/apache/carbondata/core/scan/complextypes/ArrayQueryType.java
core/src/main/java/org/apache/carbondata/core/scan/filter/FilterUtil.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/UnsafeSortDataRows.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/ConditionalFilterResolverImpl.java
hadoop/src/main/java/org/apache/carbondata/hadoop/readsupport/impl/DictionaryDecodeReadSupport.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/CarbonTable.java
core/src/main/java/org/apache/carbondata/core/datastore/block/AbstractIndex.java
core/src/main/java/org/apache/carbondata/core/cache/dictionary/AbstractColumnDictionaryInfo.java
processing/src/main/java/org/apache/carbondata/processing/loading/converter/RowConverter.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelRangeLessThanEqualFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/resolverinfo/visitor/DictionaryColumnVisitor.java
core/src/main/java/org/apache/carbondata/core/keygenerator/columnar/impl/MultiDimKeyVarLengthEquiSplitGenerator.java
processing/src/main/java/org/apache/carbondata/processing/model/CarbonDataLoadSchema.java
core/src/main/java/org/apache/carbondata/core/scan/filter/FilterProcessor.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/resolverinfo/visitor/CustomTypeDictionaryVisitor.java
core/src/main/java/org/apache/carbondata/core/scan/result/vector/CarbonColumnVector.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelRangeLessThanFiterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/scan/executor/impl/DetailQueryExecutor.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/impl/UnsafeParallelReadMergeSorterImpl.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/IntermediateFileMerger.java
core/src/main/java/org/apache/carbondata/core/datastore/exception/IndexBuilderException.java
texts:clean up code for carbon-core module

issueID:CARBONDATA-569
type:Sub-task
changed files:processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/SortParameters.java
processing/src/main/java/org/apache/carbondata/processing/store/writer/CarbonFactDataWriter.java
processing/src/main/java/org/apache/carbondata/processing/datatypes/PrimitiveDataType.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/UnsafeCarbonRowPage.java
processing/src/main/java/org/apache/carbondata/processing/store/writer/AbstractFactDataWriter.java
processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactDataHandlerColumnar.java
processing/src/main/java/org/apache/carbondata/processing/util/CarbonLoaderUtil.java
processing/src/main/java/org/apache/carbondata/processing/datatypes/StructDataType.java
processing/src/main/java/org/apache/carbondata/processing/datatypes/GenericDataType.java
processing/src/main/java/org/apache/carbondata/processing/loading/steps/InputProcessorStepImpl.java
processing/src/main/java/org/apache/carbondata/processing/util/NonDictionaryUtil.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/holder/UnsafeInmemoryMergeHolder.java
processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactDataHandlerModel.java
processing/src/main/java/org/apache/carbondata/processing/datatypes/ArrayDataType.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/impl/ParallelReadMergeSorterImpl.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/merger/UnsafeInMemoryIntermediateDataMerger.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/impl/UnsafeParallelReadMergeSorterImpl.java
processing/src/main/java/org/apache/carbondata/processing/loading/CarbonDataLoadConfiguration.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/UnsafeSortDataRows.java
processing/src/main/java/org/apache/carbondata/processing/loading/DataField.java
processing/src/main/java/org/apache/carbondata/processing/loading/model/CarbonLoadModel.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/merger/UnsafeSingleThreadFinalSortFilesMerger.java
texts:clean up code for carbon-processing module

issueID:CARBONDATA-57
type:Bug
changed files:
texts:BLOCK distribution un wanted wait for the executor node even though the sufficient nodes are available

issueID:CARBONDATA-570
type:Sub-task
changed files:processing/src/main/java/org/apache/carbondata/processing/csvload/CSVRecordReaderIterator.java
hadoop/src/main/java/org/apache/carbondata/hadoop/util/CarbonInputFormatUtil.java
hadoop/src/main/java/org/apache/carbondata/hadoop/CacheAccessClient.java
core/src/main/java/org/apache/carbondata/hadoop/CarbonInputSplit.java
texts:clean up code for carbon-hadoop module

issueID:CARBONDATA-571
type:Sub-task
changed files:core/src/main/java/org/apache/carbondata/core/scan/executor/impl/VectorDetailQueryExecutor.java
core/src/main/java/org/apache/carbondata/core/scan/result/iterator/DetailQueryResultIterator.java
processing/src/main/java/org/apache/carbondata/processing/merger/CarbonCompactionExecutor.java
processing/src/main/java/org/apache/carbondata/processing/merger/CarbonDataMergerUtil.java
processing/src/main/java/org/apache/carbondata/processing/merger/RowResultMergerProcessor.java
common/src/main/java/org/apache/carbondata/common/exceptions/sql/MalformedCarbonCommandException.java
core/src/main/java/org/apache/carbondata/core/scan/executor/impl/DetailQueryExecutor.java
core/src/main/java/org/apache/carbondata/core/scan/result/iterator/AbstractDetailQueryResultIterator.java
processing/src/main/java/org/apache/carbondata/processing/util/CarbonLoaderUtil.java
processing/src/main/java/org/apache/carbondata/processing/util/DeleteLoadFolders.java
core/src/main/java/org/apache/carbondata/core/scan/filter/FilterUtil.java
core/src/main/java/org/apache/carbondata/core/scan/executor/util/QueryUtil.java
processing/src/main/java/org/apache/carbondata/processing/util/CarbonQueryUtil.java
core/src/main/java/org/apache/carbondata/core/scan/result/iterator/VectorDetailQueryResultIterator.java
texts:clean up code for carbon-spark module

issueID:CARBONDATA-572
type:Sub-task
changed files:core/src/main/java/org/apache/carbondata/core/scan/executor/impl/VectorDetailQueryExecutor.java
core/src/main/java/org/apache/carbondata/core/scan/result/iterator/DetailQueryResultIterator.java
processing/src/main/java/org/apache/carbondata/processing/merger/CarbonCompactionExecutor.java
processing/src/main/java/org/apache/carbondata/processing/merger/CarbonDataMergerUtil.java
processing/src/main/java/org/apache/carbondata/processing/merger/RowResultMergerProcessor.java
common/src/main/java/org/apache/carbondata/common/exceptions/sql/MalformedCarbonCommandException.java
core/src/main/java/org/apache/carbondata/core/scan/executor/impl/DetailQueryExecutor.java
core/src/main/java/org/apache/carbondata/core/scan/result/iterator/AbstractDetailQueryResultIterator.java
processing/src/main/java/org/apache/carbondata/processing/util/CarbonLoaderUtil.java
processing/src/main/java/org/apache/carbondata/processing/util/DeleteLoadFolders.java
core/src/main/java/org/apache/carbondata/core/scan/filter/FilterUtil.java
core/src/main/java/org/apache/carbondata/core/scan/executor/util/QueryUtil.java
processing/src/main/java/org/apache/carbondata/processing/util/CarbonQueryUtil.java
core/src/main/java/org/apache/carbondata/core/scan/result/iterator/VectorDetailQueryResultIterator.java
texts:clean up code for carbon-spark-common module

issueID:CARBONDATA-573
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/stats/QueryStatisticsRecorderImpl.java
core/src/main/java/org/apache/carbondata/core/scan/scanner/AbstractBlockletScanner.java
texts:To fix query statistic issue
when enable query Statistics,the statistics print empty result except executor time cost.and if for non filter query, the number of blocklet are no caculate.
issueID:CARBONDATA-574
type:Bug
changed files:
texts:Add thrift server support to Spark 2.0 carbon integration
Add thrift server support to Spark 2.0 carbon integration
issueID:CARBONDATA-575
type:Test
changed files:
texts:Remove integration-testcases module
Remove integration-testcases module.
issueID:CARBONDATA-576
type:Improvement
changed files:
texts:Add mvn build guide
Add mvn build guide to github
issueID:CARBONDATA-577
type:Bug
changed files:
texts:Carbon session is not working in spark shell.
Currently user cannot create CarbonSession from spark shell , it always creates SparkSession only so carbon queries cannot be executed on spark shell.
issueID:CARBONDATA-578
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/metadata/ColumnarFormatVersion.java
core/src/main/java/org/apache/carbondata/core/util/DataFileFooterConverterFactory.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/reader/CarbonDataReaderFactory.java
processing/src/main/java/org/apache/carbondata/processing/store/CarbonDataWriterFactory.java
texts:CarbonData V2 Format Default Behavior
In the CarbonCommonConstants.java it has been specified : public static final String CARBON_DATA_FILE_DEFAULT_VERSION = "V2"; as default. Then why in CarbonDataReaderFactory.java  public DimensionColumnChunkReader getDimensionColumnChunkReader(ColumnarFormatVersion version,      BlockletInfo blockletInfo, int[] eachColumnValueSize, String filePath) {    switch (version) {      case V2:        return new CompressedDimensionChunkFileBasedReaderV2(blockletInfo, eachColumnValueSize,            filePath);      case V1:        return new CompressedDimensionChunkFileBasedReaderV1(blockletInfo, eachColumnValueSize,            filePath);      default:        throw new IllegalArgumentException("invalid format version: " + version);    }  }throw an exception for Invalid format, when default Format Version is V2.By default it should take V2 as default format.
issueID:CARBONDATA-579
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/scan/executor/util/QueryUtil.java
core/src/main/java/org/apache/carbondata/core/metadata/ColumnIdentifier.java
core/src/main/java/org/apache/carbondata/core/stats/DriverQueryStatisticsRecorderImpl.java
texts:Handle Fortify issues
Handle Fortify issues
issueID:CARBONDATA-58
type:Bug
changed files:integration/spark-common/src/main/java/org/apache/carbondata/spark/load/CarbonLoaderUtil.java
texts:dataloading is launched with wrong number of task
Cluster info:10 node cluster data replication : 3start thrift server with three executors.start dataload:The number of task should be equal to the number of executors.
issueID:CARBONDATA-580
type:New Feature
changed files:integration/spark2/src/main/java/org/apache/carbondata/spark/readsupport/SparkRowReadSupportImpl.java
texts:Support Spark 2.1 in Carbon
Support latest Spark 2.1 in Carbon.
issueID:CARBONDATA-581
type:Bug
changed files:
texts:Node locality cannot be obtained in group by queries
Node locality cannot be obtained in group by queries due to previous RDD not able to return as data is shuffled.In this case, we are not able to distribute blocks and request for executors
issueID:CARBONDATA-582
type:Bug
changed files:
texts:Able to create table When Number Of Buckets is Given in negative
In carbon data i create a table with number of buckets specified as a negative numberhere are the logsjdbc:hive2://localhost:10000> CREATE TABLE BucketTest1111352983(name int,ID string)USING org.apache.spark.sql.CarbonSource OPTIONS("bucketnumber"="-1", "bucketcolumns"="name1");---------+ Result  ---------+---------+No rows selected (0.277 seconds)this is because there is no check whether the bucket number given is positive or not in hive it gives a errorhere are the logs in hivehive> CREATE TABLE BucketTest1111352983(name int,ID string)USING org.apache.spark.sql.CarbonSource OPTIONS("bucketnumber"="-1", "bucketcolumns"="name1");---------+ Result  ---------+---------+No rows selected (0.277 seconds) in carbon data as well it should not allow this
issueID:CARBONDATA-583
type:Bug
changed files:
texts:Replace Function is not working  for string/char
I am running "replace" function but it is giving error : "undefined function replace".Query : select replace('aaabbccaabb', 'aaa', 't');Expected Result : "tbbccaabb"Result : Error: org.apache.spark.sql.AnalysisException: undefined function replace; line 1 pos 30 (state=,code=0)
issueID:CARBONDATA-584
type:Bug
changed files:
texts:no exception when table name is empty when using bucketing in table using carbon source
creating a table when table name is null does not give any exception with carbon source formathere are the logs0: jdbc:hive2://localhost:10000>  CREATE TABLE testing2(String string)USING org.apache.spark.sql.CarbonSource OPTIONS("bucketnumber"="1", "bucketcolumns"="String",tableName=" ");---------+ Result  ---------+---------+no table is get  created in hdfs it should throw a exception in this case
issueID:CARBONDATA-585
type:Bug
changed files:
texts:Dictionary file is locked for Updation
On the execution of the following query :LOAD DATA inpath 'hdfs://localhost:54310/csv/test.csv' INTO table employee options('DELIMITER'=',', 'FILEHEADER'='id, firstname');the table schema is a following :-----------------------------+ col_name   data_type   comment  -----------------------------+ id         bigint       firstname       string      -----------------------------+The load gets successful at times but we also end up often with the following error :Dictionary file is locked for Updation.Following below are the logs :AUDIT 02-01 18:17:07,009 - &#91;knoldus&#93;&#91;pallavi&#93;&#91;Thread-110&#93;Dataload failure for default.employee. Please check the logsINFO  02-01 18:17:07,020 - pool-30-thread-1 Successfully deleted the lock file /tmp/default/employee/meta.lockINFO  02-01 18:17:07,022 - Table MetaData Unlocked Successfully after data loadERROR 02-01 18:17:07,022 - Error executing query, currentState RUNNING, org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 4 times, most recent failure: Lost task 0.3 in stage 2.0 (TID 5, 192.168.2.188): java.lang.RuntimeException: Dictionary file firstname is locked for updation. Please try after some time at scala.sys.package$.error(package.scala:27) at org.apache.carbondata.spark.rdd.CarbonGlobalDictionaryGenerateRDD$$anon$1.<init>(CarbonGlobalDictionaryRDD.scala:364) at org.apache.carbondata.spark.rdd.CarbonGlobalDictionaryGenerateRDD.compute(CarbonGlobalDictionaryRDD.scala:302) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306) at org.apache.spark.rdd.RDD.iterator(RDD.scala:270) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66) at org.apache.spark.scheduler.Task.run(Task.scala:89) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745)Driver stacktrace: at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431) at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419) at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418) at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59) at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47) at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418) at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799) at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799) at scala.Option.foreach(Option.scala:236) at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799) at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640) at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599) at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588) at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48) at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620) at org.apache.spark.SparkContext.runJob(SparkContext.scala:1832) at org.apache.spark.SparkContext.runJob(SparkContext.scala:1845) at org.apache.spark.SparkContext.runJob(SparkContext.scala:1858) at org.apache.spark.SparkContext.runJob(SparkContext.scala:1929) at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:927) at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150) at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111) at org.apache.spark.rdd.RDD.withScope(RDD.scala:316) at org.apache.spark.rdd.RDD.collect(RDD.scala:926) at org.apache.carbondata.spark.util.GlobalDictionaryUtil$.generateGlobalDictionary(GlobalDictionaryUtil.scala:769)
issueID:CARBONDATA-586
type:Improvement
changed files:
texts:Create table with &#39;Char&#39; data type but it workes as &#39;String&#39; data type
I am trying to use Char data type with Carbon Data latest version and it created successfully. When I started loading data in this that time I found that it is taking data more then its size. I have checked it with hive and there it is working fine.EX :- 1. Carbon Data : 1.1 create table test_carbon (name char(10)) stored by 'org.apache.carbondata.format';1.2 desc test_carbon;Output : ---------------------------------+ col_name  data_type   comment   ------------------------------------- name         string          -------------------------------------1.3 LOAD DATA INPATH 'hdfs://localhost:54310/test.csv' into table test_carbon OPTIONS ('FILEHEADER'='name');1.4 select * from test_carbon;Output : ------------------------        name               ------------------------ Anurag Srivasrata   Robert                     james james           ------------------------2. Hive : 2.1 create table test_hive (name char(10));2.2 desc test_hive;Output : ------------------------------------ col_name  data_type   comment  ------------------------------------ name         char(10)     NULL       ------------------------------------2.3 LOAD DATA INPATH 'hdfs://localhost:54310/test.csv' into table test_hive;2.4 select * from test_hive;Output : ----------------    name         ---------------- james jame    Anurag Sri     Robert          ----------------So as hive truncate remaining string with Char data type in carbon data it should work like hive.
issueID:CARBONDATA-587
type:Bug
changed files:
texts:for any SQL syntax reserved keywords should be avoided.
I Am Able to use reserved keyword in Carbon Table CommandsCREATE TABLE Bug221755915(int int)USING org.apache.spark.sql.CarbonSource;---------+ Result  ---------+---------+jdbc:hive2://localhost:10000>  CREATE TABLE null(int int)USING org.apache.spark.sql.CarbonSource;---------+ Result  ---------+---------+No rows selected (0.267 seconds)there is no check on identifiers in carbon dataanother example create database join;Error: org.apache.spark.sql.AnalysisException: cannot recognize input near 'join' '<EOF>' '<EOF>' in create database statement; line 1 pos 16 (state=,code=0)
issueID:CARBONDATA-588
type:Sub-task
changed files:
texts:cleanup WriterCompressModel
a separate compression type field is unnecessary and error-prone as it has been captured in compressionFinder abstraction.
issueID:CARBONDATA-589
type:Bug
changed files:
texts:carbon spark shell is not working with spark 2.0
carbon shell is not working with spark 2.0 version here are the logs./carknoldus@knoldus:~/Desktop/open source/incubator-carbondata/bin$ ./carbon-spark-shelljava.lang.ClassNotFoundException: org.apache.spark.repl.carbon.Main at java.net.URLClassLoader.findClass(URLClassLoader.java:381) at java.lang.ClassLoader.loadClass(ClassLoader.java:424) at java.lang.ClassLoader.loadClass(ClassLoader.java:357) at java.lang.Class.forName0(Native Method) at java.lang.Class.forName(Class.java:348) at org.apache.spark.util.Utils$.classForName(Utils.scala:225) at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:686) at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185) at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210) at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124) at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
issueID:CARBONDATA-59
type:Bug
changed files:
texts:Filter queries on columns other than string datatype cannot get the correct result when included as dictionary column
Problem: when a column with datatype other than string is added to dictionary include property, then filters queries on that column cannot get the correct resultAnalysis: In Sort index file always the numeric values comes first and then the string values. When we perform binary search then when a string value is encountered then parsing operation to correspond datatype throws an exception and -1 value is returned because of which we move further towards last index but actually we should move towards lower index as all numeric values are above the string values.
issueID:CARBONDATA-590
type:Bug
changed files:
texts:unusual behaviour of using carbonthrift server with spark 2.0
have a look at these logsstarted thrift server ./bin/spark-submit --conf spark.sql.hive.thriftServer.singleSession=true --class org.apache.carbondata.spark.thriftserver.CarbonThriftServer $SPARK_HOME/carbonlib/carbondata_2.11-1.0.0-incubating-SNAPSHOT-shade-hadoop2.2.0.jar hdfs://localhost:54310/opt/carbonStorelogsINFO  04-01 12:46:55,421 - Service:ThriftBinaryCLIService is started.INFO  04-01 12:46:55,421 - Service:HiveServer2 is started.started beelineconnected to thrift server !connect jdbc:hive2://localhost:10000executed the queryCREATE TABLE Bug212(int string)USING org.apache.spark.sql.CarbonSourceOPTIONS("bucketnumber"="1", "bucketcolumns"="String","tableName"="t100");Error:org.apache.carbondata.spark.exception.MalformedCarbonCommandException: Tabledefault.t 100 can not be created without key columns. Please useDICTIONARY_INCLUDE or DICTIONARY_EXCLUDE to set at least one key column ifall specified columns are numeric types (state=,code=0)2 minutes later CREATE TABLE Bug211(int int)USING org.apache.spark.sql.CarbonSourceOPTIONS("bucketnumber"="1", "bucketcolumns"="String","tableName"="t 100");---------+ Result  ---------+---------+No rows selected (0.212 seconds)
issueID:CARBONDATA-591
type:Improvement
changed files:
texts:Remove unused code for spark 2.0 datatype utils
Remove unused code for data type utils for spark 2.0. I look the below code snippet and debug that there is code for spark 2.x datatype conversion in  DataTypeConverterUtil.scala.DataTypeConverterUtil.scala  def convertToCarbonTypeForSpark2(dataType: String): DataType = {    dataType.toLowerCase match {      case "stringtype" => DataType.STRING      case "inttype" => DataType.INT      case "integertype" => DataType.INT      case "tinyinttype" => DataType.SHORT      case "shorttype" => DataType.SHORT      case "longtype" => DataType.LONG      case "biginttype" => DataType.LONG      case "numerictype" => DataType.DOUBLE      case "doubletype" => DataType.DOUBLE      case "decimaltype" => DataType.DECIMAL      case "timestamptype" => DataType.TIMESTAMP      case "datetype" => DataType.DATE      case "arraytype" => DataType.ARRAY      case "structtype" => DataType.STRUCT      case _ => sys.error(s"Unsupported data type: $dataType")    }}In spark 2.x there is types stringtype and inttype etc as a API not in the query itself.
issueID:CARBONDATA-592
type:Bug
changed files:
texts:In Single pass loading ,when we not setting  ALL_DICTIONARY_PATH still it is showing error for that in logs
In Single Pass loading , When we Execute the load query without ALL_DICTIONARY_PATH with single_pass true it is showing the ERROR &#8212; main Can't use single_pass, because SINGLE_PASS and ALL_DICTIONARY_PATHcan not be used together, and USE_KETTLE must be set as falseFor Example: /CREATE TABLECREATE TABLE uniqdata_INCLUDEDICTIONARY (CUST_ID int,CUST_NAME String,ACTIVE_EMUI_VERSION string, DOB timestamp, DOJ timestamp, BIGINT_COLUMN1 bigint,BIGINT_COLUMN2 bigint,DECIMAL_COLUMN1 decimal(30,10), DECIMAL_COLUMN2 decimal(36,10),Double_COLUMN1 double, Double_COLUMN2 double,INTEGER_COLUMN1 int) STORED BY 'org.apache.carbondata.format' TBLPROPERTIES('DICTIONARY_INCLUDE'='CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1,Double_COLUMN2,INTEGER_COLUMN1');//LOAD DATALOAD DATA INPATH 'hdfs://hadoop-master:54311/data/7000_UniqData.csv' into table uniqdata_INCLUDEDICTIONARY OPTIONS('DELIMITER'=',' , 'QUOTECHAR'='"','BAD_RECORDS_LOGGER_ENABLE'='TRUE', 'BAD_RECORDS_ACTION'='FORCE','FILEHEADER'='CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1,Double_COLUMN2,INTEGER_COLUMN1','SINGLE_PASS'='true','USE_KETTLE' ='false');Logs as attached in Screenshot.
issueID:CARBONDATA-593
type:Bug
changed files:
texts:Select command seems to be not working on carbon-spark-shell . It throws a runtime error on select query after show method is invoked
Select command seems to be not working on carbon-spark-shell . It throws a runtime error on select query after show method is invoked. It says java.io.FileNotFoundException: File does not exist: /home/hduser/software/spark-1.6.2-bin-hadoop2.6/carbon.store/mydb/demo/Fact/Part0/Segment_0Query Executed :scala> cc.sql("create table demo(id int,name string,age int,city string) stored by 'carbondata'").showscala> cc.sql("LOAD DATA inpath 'hdfs://hadoop-master:54311/data/employee.csv' INTO table demo"); cc.sql("select * from demo").showResult :It's throwing java.lang.RuntimeException : File Not Foundscala> cc.sql("select * from mydb.demo").showINFO  04-01 16:29:22,157 - main Query &#91;SELECT * FROM MYDB.DEMO&#93;INFO  04-01 16:29:22,160 - Parsing command: select * from mydb.demoINFO  04-01 16:29:22,161 - Parse CompletedINFO  04-01 16:29:22,162 - Parsing command: select * from mydb.demoINFO  04-01 16:29:22,163 - Parse CompletedINFO  04-01 16:29:22,163 - 0: get_table : db=mydb tbl=demoINFO  04-01 16:29:22,163 - ugi=hduser ip=unknown-ip-addr cmd=get_table : db=mydb tbl=demo INFO  04-01 16:29:22,195 - main Starting to optimize planjava.io.FileNotFoundException: File does not exist: /home/hduser/software/spark-1.6.2-bin-hadoop2.6/carbon.store/mydb/demo/Fact/Part0/Segment_0 at org.apache.hadoop.hdfs.DistributedFileSystem$17.doCall(DistributedFileSystem.java:1110) at org.apache.hadoop.hdfs.DistributedFileSystem$17.doCall(DistributedFileSystem.java:1102) at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81) at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1102) at org.apache.hadoop.fs.FileSystem.resolvePath(FileSystem.java:747) at org.apache.hadoop.hdfs.DistributedFileSystem$15.<init>(DistributedFileSystem.java:726) at org.apache.hadoop.hdfs.DistributedFileSystem.listLocatedStatus(DistributedFileSystem.java:717) at org.apache.hadoop.fs.FileSystem.listLocatedStatus(FileSystem.java:1780) at org.apache.carbondata.hadoop.CarbonInputFormat.getFileStatusOfSegments(CarbonInputFormat.java:559) at org.apache.carbondata.hadoop.CarbonInputFormat.listStatus(CarbonInputFormat.java:519) at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.getSplits(FileInputFormat.java:340) at org.apache.carbondata.hadoop.CarbonInputFormat.getSplitsInternal(CarbonInputFormat.java:251) at org.apache.carbondata.hadoop.CarbonInputFormat.getTableBlockInfo(CarbonInputFormat.java:372) at org.apache.carbondata.hadoop.CarbonInputFormat.getSegmentAbstractIndexs(CarbonInputFormat.java:402) at org.apache.carbondata.hadoop.CarbonInputFormat.getDataBlocksOfSegment(CarbonInputFormat.java:325) at org.apache.carbondata.hadoop.CarbonInputFormat.getSplits(CarbonInputFormat.java:288) at org.apache.carbondata.hadoop.CarbonInputFormat.getSplits(CarbonInputFormat.java:237) at org.apache.carbondata.spark.rdd.CarbonScanRDD.getPartitions(CarbonScanRDD.scala:82) at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239) at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237) at scala.Option.getOrElse(Option.scala:120) at org.apache.spark.rdd.RDD.partitions(RDD.scala:237) at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35) at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239) at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237) at scala.Option.getOrElse(Option.scala:120) at org.apache.spark.rdd.RDD.partitions(RDD.scala:237) at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35) at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239) at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237) at scala.Option.getOrElse(Option.scala:120) at org.apache.spark.rdd.RDD.partitions(RDD.scala:237) at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35) at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239) at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237) at scala.Option.getOrElse(Option.scala:120) at org.apache.spark.rdd.RDD.partitions(RDD.scala:237) at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35) at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239) at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237) at scala.Option.getOrElse(Option.scala:120) at org.apache.spark.rdd.RDD.partitions(RDD.scala:237) at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:190) at org.apache.spark.sql.execution.Limit.executeCollect(basicOperators.scala:165) at org.apache.spark.sql.execution.SparkPlan.executeCollectPublic(SparkPlan.scala:174) at org.apache.spark.sql.DataFrame$$anonfun$org$apache$spark$sql$DataFrame$$execute$1$1.apply(DataFrame.scala:1499) at org.apache.spark.sql.DataFrame$$anonfun$org$apache$spark$sql$DataFrame$$execute$1$1.apply(DataFrame.scala:1499) at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:56) at org.apache.spark.sql.DataFrame.withNewExecutionId(DataFrame.scala:2086) at org.apache.spark.sql.DataFrame.org$apache$spark$sql$DataFrame$$execute$1(DataFrame.scala:1498) at org.apache.spark.sql.DataFrame.org$apache$spark$sql$DataFrame$$collect(DataFrame.scala:1505) at org.apache.spark.sql.DataFrame$$anonfun$head$1.apply(DataFrame.scala:1375) at org.apache.spark.sql.DataFrame$$anonfun$head$1.apply(DataFrame.scala:1374) at org.apache.spark.sql.DataFrame.withCallback(DataFrame.scala:2099) at org.apache.spark.sql.DataFrame.head(DataFrame.scala:1374) at org.apache.spark.sql.DataFrame.take(DataFrame.scala:1456) at org.apache.spark.sql.DataFrame.showString(DataFrame.scala:170) at org.apache.spark.sql.DataFrame.show(DataFrame.scala:350) at org.apache.spark.sql.DataFrame.show(DataFrame.scala:311) at org.apache.spark.sql.DataFrame.show(DataFrame.scala:319) at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:31) at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:36) at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:38) at $iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:40) at $iwC$$iwC$$iwC$$iwC.<init>(<console>:42) at $iwC$$iwC$$iwC.<init>(<console>:44) at $iwC$$iwC.<init>(<console>:46) at $iwC.<init>(<console>:48) at <init>(<console>:50) at .<init>(<console>:54) at .<clinit>(<console>) at .<init>(<console>:7) at .<clinit>(<console>) at $print(<console>) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:1065) at org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1346) at org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:840) at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:871) at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:819) at org.apache.spark.repl.SparkILoop.reallyInterpret$1(SparkILoop.scala:857) at org.apache.spark.repl.SparkILoop.interpretStartingWith(SparkILoop.scala:902) at org.apache.spark.repl.SparkILoop.command(SparkILoop.scala:814) at org.apache.spark.repl.SparkILoop.processLine$1(SparkILoop.scala:657) at org.apache.spark.repl.SparkILoop.innerLoop$1(SparkILoop.scala:665) at org.apache.spark.repl.SparkILoop.org$apache$spark$repl$SparkILoop$$loop(SparkILoop.scala:670) at org.apache.spark.repl.SparkILoop$$anonfun$org$apache$spark$repl$SparkILoop$$process$1.apply$mcZ$sp(SparkILoop.scala:997) at org.apache.spark.repl.SparkILoop$$anonfun$org$apache$spark$repl$SparkILoop$$process$1.apply(SparkILoop.scala:945) at org.apache.spark.repl.SparkILoop$$anonfun$org$apache$spark$repl$SparkILoop$$process$1.apply(SparkILoop.scala:945) at scala.tools.nsc.util.ScalaClassLoader$.savingContextLoader(ScalaClassLoader.scala:135) at org.apache.spark.repl.SparkILoop.org$apache$spark$repl$SparkILoop$$process(SparkILoop.scala:945) at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:1059) at org.apache.spark.repl.Main$.main(Main.scala:31) at org.apache.spark.repl.Main.main(Main.scala) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731) at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181) at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206) at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121) at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
issueID:CARBONDATA-594
type:Bug
changed files:
texts:Select command seems to be not working on carbon-spark-shell . It throws a runtime error on select query after show method is invoked
Select command seems to be not working on carbon-spark-shell . It throws a runtime error on select query after show method is invoked. It says java.io.FileNotFoundException: File does not exist: /home/hduser/software/spark-1.6.2-bin-hadoop2.6/carbon.store/mydb/demo/Fact/Part0/Segment_0
issueID:CARBONDATA-595
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/locks/HdfsFileLock.java
texts:Drop Table for carbon throws NPE with HDFS lock type.
Start version :- 1.6.2 Start carbon thrift serverset HDFS LOCK Typedrop table from beeline0: jdbc:hive2://hacluster> drop table oscon_new_1;Error: java.lang.NullPointerException (state=,code=0)Error in thrftserver 17/01/04 20:40:08 AUDIT DropTableCommand: &#91;hadoop-master&#93;&#91;anonymous&#93;&#91;Thread-182&#93;Deleted table &#91;oscon_new_1&#93; under database &#91;default&#93;17/01/04 20:40:08 ERROR AbstractDFSCarbonFile: pool-25-thread-12 Exception occured:File does not exist: hdfs://hacluster/opt/CarbonStore/default/oscon_new_1/droptable.lock17/01/04 20:40:08 ERROR SparkExecuteStatementOperation: Error executing query, currentState RUNNING,java.lang.NullPointerException        at org.apache.carbondata.core.datastorage.store.filesystem.AbstractDFSCarbonFile.delete(AbstractDFSCarbonFile.java:128)        at org.apache.carbondata.lcm.locks.HdfsFileLock.unlock(HdfsFileLock.java:110)        at org.apache.spark.sql.execution.command.DropTableCommand.run(carbonTableSchema.scala:613)        at org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult$lzycompute(commands.scala:58)        at org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult(commands.scala:56)Note :- lock file and data are deleted successfully but in beeline it throws ERROR message instead of success.
issueID:CARBONDATA-596
type:Bug
changed files:
texts:unable to crete filter resolver tree exception when using equals expression with carbon input format api
unable to crete filter resolver tree exception when using equals expression Expression expression = new EqualToExpression(new ColumnExpression("c1", DataType.STRING),                new LiteralExpression("a", DataType.STRING));        CarbonInputFormat.setFilterPredicates(job.getConfiguration(), expression);        List splits = carbonInputFormat.getSplits(job);it throws java.io.IOException: Error while resolving filter expression
issueID:CARBONDATA-597
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
texts:Unable to fetch data with "select" query
I am running Carbon Data with thrift server and I am able to Create Table and Load Data but as I run select * from table_name;, Its giving me error : Block B-tree loading failed.Create Table :  CREATE TABLE uniqdata (CUST_ID int,CUST_NAME char(30),ACTIVE_EMUI_VERSION string, DOB timestamp, DOJ timestamp, BIGINT_COLUMN1 bigint,BIGINT_COLUMN2 bigint,DECIMAL_COLUMN1 decimal(30,10), DECIMAL_COLUMN2 decimal(36,10),Double_COLUMN1 double, Double_COLUMN2 double, INTEGER_COLUMN1 int) STORED BY 'org.apache.carbondata.format' TBLPROPERTIES ("TABLE_BLOCKSIZE"= "256 MB");Load Data : LOAD DATA INPATH 'hdfs://localhost:54310/2000_UniqData.csv' into table uniqdata OPTIONS ('DELIMITER'=',' ,'QUOTECHAR'='"','BAD_RECORDS_ACTION'='FORCE','FILEHEADER'='CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1,Double_COLUMN2,INTEGER_COLUMN1');Select Query : select * from uniqdata;PFA for stack Trace.
issueID:CARBONDATA-599
type:Bug
changed files:
texts:should not be able to create table when number of bucket is precedded with arthematic operators
when i created a table in carbon data it works even if arthematic number precedded the bucketnumber here are the logs spark.sql("""CREATE TABLE bugs(ID string)USING org.apache.spark.sql.CarbonSource OPTIONS("bucketnumber"="+1","bucketcolumns"="ID","tableName"="bugs")""");WARN  05-01 17:40:31,912 - Couldn't find corresponding Hive SerDe for data source provider org.apache.spark.sql.CarbonSource. Persisting data source table `default`.`bugs5` into Hive metastore in Spark SQL specific format, which is NOT compatible with Hive.res0: org.apache.spark.sql.DataFrame = []but in hive it gives exceptionhere are logs ====hive> CREATE TABLE test888(user_id BIGINT, firstname STRING, lastname STRING)    > CLUSTERED BY(user_id) INTO +1 BUCKETS;FAILED: ParseException line 2:27 extraneous input '+' expecting Number near '<EOF>'
issueID:CARBONDATA-6
type:Bug
changed files:
texts:data mismatch between the carbon Table and Hive Table for columns having empty values for non numeric data type
csv data: 1,2015-7-23 00:00:00,china,aaa1,phone197,ASD69643,15000.435252,,,,,,
issueID:CARBONDATA-60
type:Bug
changed files:
texts:wrong result when using union all
the issue can be reproduced by following code:the expected result is 1 row, but actual result is 2 rows.----+ c1_c1----+200  1279  1----+    import cc.implicits._    val df=sc.parallelize(1 to 1000).map(x => (x+"", (x+100)+"")).toDF("c1", "c2")    import org.carbondata.spark._    df.saveAsCarbonFile(Map("tableName" -> "carbon1"))    cc.sql("""    select c1,count from(      select c1 as c1,c2 as c2 from carbon1      union all      select c2 as c1,c1 as c2 from carbon1     )t      where c1='200'      group by c1    """).show()
issueID:CARBONDATA-600
type:Bug
changed files:
texts:Should reuse unit test case for integration module

issueID:CARBONDATA-601
type:Test
changed files:core/src/main/java/org/apache/carbondata/core/scan/expression/ExpressionResult.java
texts:Should reuse unit test case for integration module

issueID:CARBONDATA-602
type:Bug
changed files:
texts:When we are  loading data 3 or 4 time using &#39;USE_KETTLE&#39; =&#39;false&#39; with &#39;SINGLE_PASS&#39;=&#39;true&#39;, It is throwing an error
When we are Loading  data  using 'USE_KETTLE' ='false' with 'SINGLE_PASS'='true' ,It is Throwing an error &#8211; Error: java.lang.Exception: Data load failed due to error while write dictionary file! (state=,code=0) and without  'USE_KETTLE' ='false' Data load is successfulFor Example:CREATE TABLE uniqdata_INCLUDEDICTIONARY (CUST_ID int,CUST_NAME String,ACTIVE_EMUI_VERSION string, DOB timestamp, DOJ timestamp, BIGINT_COLUMN1 bigint,BIGINT_COLUMN2 bigint,DECIMAL_COLUMN1 decimal(30,10), DECIMAL_COLUMN2 decimal(36,10),Double_COLUMN1 double, Double_COLUMN2 double,INTEGER_COLUMN1 int) STORED BY 'org.apache.carbondata.format' TBLPROPERTIES('DICTIONARY_INCLUDE'='CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1,Double_COLUMN2,INTEGER_COLUMN1');0: jdbc:hive2://192.168.2.126:10000> LOAD DATA INPATH 'hdfs://localhost:54311/payal/7000_UniqData.csv' into table uniqdata_INCLUDEDICTIONARY OPTIONS('DELIMITER'=',' , 'QUOTECHAR'='"','BAD_RECORDS_LOGGER_ENABLE'='TRUE', 'BAD_RECORDS_ACTION'='FORCE','FILEHEADER'='CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1,Double_COLUMN2,INTEGER_COLUMN1','SINGLE_PASS'='true','USE_KETTLE' ='fail');Error: java.lang.IllegalArgumentException: For input string: "fail" (state=,code=0)LOAD DATA INPATH 'hdfs://hadoop-master:54311/data/uniqdata/7000_UniqData.csv' into table uniqdata_INCLUDEDICTIONARY OPTIONS('DELIMITER'=',' , 'QUOTECHAR'='"','BAD_RECORDS_LOGGER_ENABLE'='TRUE', 'BAD_RECORDS_ACTION'='FORCE','FILEHEADER'='CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1,Double_COLUMN2,INTEGER_COLUMN1','SINGLE_PASS'='true');---------+ Result  ---------+---------+LOGS.INFO  06-01 13:31:54,820 - Running query 'LOAD DATA INPATH 'hdfs://hadoop-master:54311/data/uniqdata/7000_UniqData.csv' into table uniqdata_INCLUDEDICTIONARY OPTIONS('DELIMITER'=',' , 'QUOTECHAR'='"','BAD_RECORDS_LOGGER_ENABLE'='TRUE', 'BAD_RECORDS_ACTION'='FORCE','FILEHEADER'='CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1,Double_COLUMN2,INTEGER_COLUMN1','SINGLE_PASS'='true','USE_KETTLE' ='false')' with 2e6007f7-946d-4071-a73f-30d90538ebd6INFO  06-01 13:31:54,820 - pool-26-thread-58 Query &#91;LOAD DATA INPATH &#39;HDFS://HADOOP-MASTER:54311/DATA/UNIQDATA/7000_UNIQDATA.CSV&#39; INTO TABLE UNIQDATA_INCLUDEDICTIONARY OPTIONS(&#39;DELIMITER&#39;=&#39;,&#39; , &#39;QUOTECHAR&#39;=&#39;"&#39;,&#39;BAD_RECORDS_LOGGER_ENABLE&#39;=&#39;TRUE&#39;, &#39;BAD_RECORDS_ACTION&#39;=&#39;FORCE&#39;,&#39;FILEHEADER&#39;=&#39;CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,DOUBLE_COLUMN1,DOUBLE_COLUMN2,INTEGER_COLUMN1&#39;,&#39;SINGLE_PASS&#39;=&#39;TRUE&#39;,&#39;USE_KETTLE&#39; =&#39;FALSE&#39;)&#93;INFO  06-01 13:31:54,831 - Successfully able to get the table metadata file lockINFO  06-01 13:31:54,834 - pool-26-thread-58 Initiating Direct Load for the Table : (meradb.uniqdata_includedictionary)AUDIT 06-01 13:31:54,838 - &#91;deepak-Vostro-3546&#93;&#91;hduser&#93;&#91;Thread-494&#93;Data load request has been received for table meradb.uniqdata_includedictionaryAUDIT 06-01 13:31:54,838 - &#91;deepak-Vostro-3546&#93;&#91;hduser&#93;&#91;Thread-494&#93;Data is loading with New Data Flow for table meradb.uniqdata_includedictionaryINFO  06-01 13:31:54,891 - pool-26-thread-58 &#91;Block Distribution&#93;INFO  06-01 13:31:54,891 - pool-26-thread-58 totalInputSpaceConsumed: 1505367 , defaultParallelism: 8INFO  06-01 13:31:54,891 - pool-26-thread-58 mapreduce.input.fileinputformat.split.maxsize: 16777216INFO  06-01 13:31:54,891 - Total input paths to process : 1INFO  06-01 13:31:54,892 - pool-26-thread-58 Executors configured : 1INFO  06-01 13:31:54,893 - pool-26-thread-58 Requesting total executors: 1INFO  06-01 13:31:54,897 - pool-26-thread-58 Total Time taken to ensure the required executors : 3INFO  06-01 13:31:54,897 - pool-26-thread-58 Time elapsed to allocate the required executors: 0INFO  06-01 13:31:54,898 - pool-26-thread-58 Total Time taken in block allocation: 6INFO  06-01 13:31:54,898 - pool-26-thread-58 Total no of blocks: 1, No.of Nodes: 1INFO  06-01 13:31:54,898 - pool-26-thread-58 #Node: hadoop-slave-1 no.of.blocks: 1 , mismatch locations: ,knoldusINFO  06-01 13:31:55,057 - Block broadcast_62 stored as values in memory (estimated size 150.4 MB, free 300.0 MB)INFO  06-01 13:31:55,064 - Block broadcast_62_piece0 stored as bytes in memory (estimated size 19.7 KB, free 300.0 MB)INFO  06-01 13:31:55,064 - Added broadcast_62_piece0 in memory on 192.168.2.174:32778 (size: 19.7 KB, free: 511.0 MB)INFO  06-01 13:31:55,064 - Created broadcast 62 from broadcast at NewCarbonDataLoadRDD.scala:109INFO  06-01 13:31:55,067 - Starting job: collect at CarbonDataRDDFactory.scala:632INFO  06-01 13:31:55,067 - Got job 31 (collect at CarbonDataRDDFactory.scala:632) with 1 output partitionsINFO  06-01 13:31:55,067 - Final stage: ResultStage 38 (collect at CarbonDataRDDFactory.scala:632)INFO  06-01 13:31:55,067 - Parents of final stage: List()INFO  06-01 13:31:55,067 - Missing parents: List()INFO  06-01 13:31:55,068 - Submitting ResultStage 38 (NewCarbonDataLoadRDD&#91;150&#93; at RDD at NewCarbonDataLoadRDD.scala:91), which has no missing parentsINFO  06-01 13:31:55,068 - Preferred Location for split : hadoop-slave-1INFO  06-01 13:31:55,069 - Block broadcast_63 stored as values in memory (estimated size 12.0 KB, free 300.0 MB)INFO  06-01 13:31:55,070 - Block broadcast_63_piece0 stored as bytes in memory (estimated size 5.8 KB, free 300.0 MB)INFO  06-01 13:31:55,070 - Added broadcast_63_piece0 in memory on 192.168.2.174:32778 (size: 5.8 KB, free: 511.0 MB)INFO  06-01 13:31:55,071 - Created broadcast 63 from broadcast at DAGScheduler.scala:1006INFO  06-01 13:31:55,071 - Submitting 1 missing tasks from ResultStage 38 (NewCarbonDataLoadRDD&#91;150&#93; at RDD at NewCarbonDataLoadRDD.scala:91)INFO  06-01 13:31:55,071 - Adding task set 38.0 with 1 tasksINFO  06-01 13:31:55,072 - Starting task 0.0 in stage 38.0 (TID 92, hadoop-slave-1, partition 0,NODE_LOCAL, 2498 bytes)INFO  06-01 13:31:55,083 - Added broadcast_63_piece0 in memory on hadoop-slave-1:34995 (size: 5.8 KB, free: 511.0 MB)INFO  06-01 13:31:55,096 - Added broadcast_62_piece0 in memory on hadoop-slave-1:34995 (size: 19.7 KB, free: 511.0 MB)AUDIT 06-01 13:31:55,120 - &#91;deepak-Vostro-3546&#93;&#91;hduser&#93;&#91;Thread-428&#93;Connected org.apache.carbondata.core.dictionary.server.DictionaryServerHandler@7c9223efINFO  06-01 13:31:56,510 - Finished task 0.0 in stage 38.0 (TID 92) in 1439 ms on hadoop-slave-1 (1/1)INFO  06-01 13:31:56,510 - Removed TaskSet 38.0, whose tasks have all completed, from poolINFO  06-01 13:31:56,510 - ResultStage 38 (collect at CarbonDataRDDFactory.scala:632) finished in 1.439 sINFO  06-01 13:31:56,510 - Job 31 finished: collect at CarbonDataRDDFactory.scala:632, took 1.443490 sINFO  06-01 13:31:56,511 - pool-26-thread-58 Acquired lock for tablemeradb.uniqdata_includedictionary for table status updationINFO  06-01 13:31:56,595 - pool-26-thread-58 Successfully deleted the lock file /tmp/meradb/uniqdata_includedictionary/tablestatus.lockINFO  06-01 13:31:56,595 - pool-26-thread-58 Table unlocked successfully after table status updationmeradb.uniqdata_includedictionaryERROR 06-01 13:31:56,595 - pool-26-thread-58 Error while close dictionary server and write dictionary file for meradb.uniqdata_includedictionaryERROR 06-01 13:31:56,595 - pool-26-thread-58java.lang.Exception: Dataload failed due to error while write dictionary file!    at org.apache.carbondata.spark.rdd.CarbonDataRDDFactory$.loadCarbonData(CarbonDataRDDFactory.scala:773)    at org.apache.spark.sql.execution.command.LoadTable.run(carbonTableSchema.scala:470)    at org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult$lzycompute(commands.scala:58)    at org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult(commands.scala:56)    at org.apache.spark.sql.execution.ExecutedCommand.doExecute(commands.scala:70)    at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$5.apply(SparkPlan.scala:132)    at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$5.apply(SparkPlan.scala:130)    at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)    at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:130)    at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:55)    at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:55)    at org.apache.spark.sql.DataFrame.<init>(DataFrame.scala:145)    at org.apache.spark.sql.DataFrame.<init>(DataFrame.scala:130)    at org.apache.spark.sql.CarbonContext.sql(CarbonContext.scala:137)    at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:211)    at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1$$anon$2.run(SparkExecuteStatementOperation.scala:154)    at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1$$anon$2.run(SparkExecuteStatementOperation.scala:151)    at java.security.AccessController.doPrivileged(Native Method)    at javax.security.auth.Subject.doAs(Subject.java:422)    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)    at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1.run(SparkExecuteStatementOperation.scala:164)    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)    at java.util.concurrent.FutureTask.run(FutureTask.java:266)    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)    at java.lang.Thread.run(Thread.java:745)AUDIT 06-01 13:31:56,596 - &#91;deepak-Vostro-3546&#93;&#91;hduser&#93;&#91;Thread-494&#93;Dataload failure for meradb.uniqdata_includedictionary. Please check the logsINFO  06-01 13:31:56,596 - pool-26-thread-58 Successfully deleted the lock file /tmp/meradb/uniqdata_includedictionary/meta.lockINFO  06-01 13:31:56,596 - Table MetaData Unlocked Successfully after data loadERROR 06-01 13:31:56,597 - Error executing query, currentState RUNNING,java.lang.Exception: Dataload failed due to error while write dictionary file!    at org.apache.carbondata.spark.rdd.CarbonDataRDDFactory$.loadCarbonData(CarbonDataRDDFactory.scala:773)    at org.apache.spark.sql.execution.command.LoadTable.run(carbonTableSchema.scala:470)    at org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult$lzycompute(commands.scala:58)    at org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult(commands.scala:56)    at org.apache.spark.sql.execution.ExecutedCommand.doExecute(commands.scala:70)    at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$5.apply(SparkPlan.scala:132)    at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$5.apply(SparkPlan.scala:130)    at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)    at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:130)    at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:55)    at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:55)    at org.apache.spark.sql.DataFrame.<init>(DataFrame.scala:145)    at org.apache.spark.sql.DataFrame.<init>(DataFrame.scala:130)    at org.apache.spark.sql.CarbonContext.sql(CarbonContext.scala:137)    at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:211)    at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1$$anon$2.run(SparkExecuteStatementOperation.scala:154)    at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1$$anon$2.run(SparkExecuteStatementOperation.scala:151)    at java.security.AccessController.doPrivileged(Native Method)    at javax.security.auth.Subject.doAs(Subject.java:422)    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)    at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1.run(SparkExecuteStatementOperation.scala:164)    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)    at java.util.concurrent.FutureTask.run(FutureTask.java:266)    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)    at java.lang.Thread.run(Thread.java:745)ERROR 06-01 13:31:56,597 - Error running hive query:org.apache.hive.service.cli.HiveSQLException: java.lang.Exception: Dataload failed due to error while write dictionary file!    at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:246)    at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1$$anon$2.run(SparkExecuteStatementOperation.scala:154)    at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1$$anon$2.run(SparkExecuteStatementOperation.scala:151)    at java.security.AccessController.doPrivileged(Native Method)    at javax.security.auth.Subject.doAs(Subject.java:422)    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)    at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1.run(SparkExecuteStatementOperation.scala:164)    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)    at java.util.concurrent.FutureTask.run(FutureTask.java:266)    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)    at java.lang.Thread.run(Thread.java:745)driver logsNFO  16-01 17:23:22,573 - Running query 'LOAD DATA INPATH 'hdfs://localhost:54311/payal/7000_UniqData.csv' into table uniqdata_INCLUDEDICTIONARY OPTIONS('DELIMITER'=',' , 'QUOTECHAR'='"','BAD_RECORDS_LOGGER_ENABLE'='TRUE', 'BAD_RECORDS_ACTION'='FORCE','FILEHEADER'='CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1,Double_COLUMN2,INTEGER_COLUMN1','SINGLE_PASS'='true','USE_KETTLE' ='fail')' with 0b9d16f0-90c7-4908-ace4-6314f1f4e4a5INFO  16-01 17:23:22,575 - pool-26-thread-49 Query &#91;LOAD DATA INPATH &#39;HDFS://LOCALHOST:54311/PAYAL/7000_UNIQDATA.CSV&#39; INTO TABLE UNIQDATA_INCLUDEDICTIONARY OPTIONS(&#39;DELIMITER&#39;=&#39;,&#39; , &#39;QUOTECHAR&#39;=&#39;"&#39;,&#39;BAD_RECORDS_LOGGER_ENABLE&#39;=&#39;TRUE&#39;, &#39;BAD_RECORDS_ACTION&#39;=&#39;FORCE&#39;,&#39;FILEHEADER&#39;=&#39;CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,DOUBLE_COLUMN1,DOUBLE_COLUMN2,INTEGER_COLUMN1&#39;,&#39;SINGLE_PASS&#39;=&#39;TRUE&#39;,&#39;USE_KETTLE&#39; =&#39;FAIL&#39;)&#93;INFO  16-01 17:23:22,595 - Successfully able to get the table metadata file lockINFO  16-01 17:23:22,597 - pool-26-thread-49 Successfully deleted the lock file /tmp/default/uniqdata_includedictionary/meta.lockINFO  16-01 17:23:22,601 - Table MetaData Unlocked Successfully after data loadERROR 16-01 17:23:22,601 - Error executing query, currentState RUNNING, java.lang.IllegalArgumentException: For input string: "fail"    at scala.collection.immutable.StringLike$class.parseBoolean(StringLike.scala:238)    at scala.collection.immutable.StringLike$class.toBoolean(StringLike.scala:226)    at scala.collection.immutable.StringOps.toBoolean(StringOps.scala:31)    at org.apache.spark.sql.execution.command.LoadTable.run(carbonTableSchema.scala:394)    at org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult$lzycompute(commands.scala:58)    at org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult(commands.scala:56)    at org.apache.spark.sql.execution.ExecutedCommand.doExecute(commands.scala:70)    at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$5.apply(SparkPlan.scala:132)    at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$5.apply(SparkPlan.scala:130)    at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)    at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:130)    at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:55)    at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:55)    at org.apache.spark.sql.DataFrame.<init>(DataFrame.scala:145)    at org.apache.spark.sql.DataFrame.<init>(DataFrame.scala:130)    at org.apache.spark.sql.CarbonContext.sql(CarbonContext.scala:139)    at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:211)    at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1$$anon$2.run(SparkExecuteStatementOperation.scala:154)    at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1$$anon$2.run(SparkExecuteStatementOperation.scala:151)    at java.security.AccessController.doPrivileged(Native Method)    at javax.security.auth.Subject.doAs(Subject.java:422)    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)    at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1.run(SparkExecuteStatementOperation.scala:164)    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)    at java.util.concurrent.FutureTask.run(FutureTask.java:266)    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)    at java.lang.Thread.run(Thread.java:745)ERROR 16-01 17:23:22,604 - Error running hive query: org.apache.hive.service.cli.HiveSQLException: java.lang.IllegalArgumentException: For input string: "fail"    at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:246)    at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1$$anon$2.run(SparkExecuteStatementOperation.scala:154)    at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1$$anon$2.run(SparkExecuteStatementOperation.scala:151)    at java.security.AccessController.doPrivileged(Native Method)    at javax.security.auth.Subject.doAs(Subject.java:422)    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)    at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1.run(SparkExecuteStatementOperation.scala:164)    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)    at java.util.concurrent.FutureTask.run(FutureTask.java:266)    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)    at java.lang.Thread.run(Thread.java:745)&#8211;
issueID:CARBONDATA-603
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/resolverinfo/visitor/CustomTypeDictionaryVisitor.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
core/src/main/java/org/apache/carbondata/core/cache/dictionary/ColumnDictionaryInfo.java
core/src/main/java/org/apache/carbondata/core/util/DataTypeUtil.java
core/src/main/java/org/apache/carbondata/core/keygenerator/directdictionary/DirectDictionaryKeyGeneratorFactory.java
core/src/main/java/org/apache/carbondata/core/keygenerator/directdictionary/timestamp/DateDirectDictionaryGenerator.java
core/src/main/java/org/apache/carbondata/core/keygenerator/directdictionary/DirectDictionaryGenerator.java
core/src/main/java/org/apache/carbondata/core/scan/expression/ExpressionResult.java
processing/src/main/java/org/apache/carbondata/processing/loading/model/CarbonLoadModel.java
core/src/main/java/org/apache/carbondata/core/scan/filter/FilterUtil.java
core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
texts:Unable to use filter with Date Data Type
I am creating table with DATE Data Type and loading data with CSV into the table.After that as I run the select query with WHERE clause, it converted value as NULL and provide me result with Null Value.Create Table :   CREATE TABLE uniqdata (CUST_ID int,CUST_NAME char(30),ACTIVE_EMUI_VERSION string, DOB date, DOJ date, BIGINT_COLUMN1 bigint,BIGINT_COLUMN2 bigint,DECIMAL_COLUMN1 decimal(30,10), DECIMAL_COLUMN2 decimal(36,10),Double_COLUMN1 double, Double_COLUMN2 double,INTEGER_COLUMN1 int) STORED BY 'org.apache.carbondata.format' TBLPROPERTIES ("TABLE_BLOCKSIZE"= "256 MB");Load Data : LOAD DATA INPATH 'hdfs://localhost:54310/2000_UniqData.csv' into table uniqdata OPTIONS('DELIMITER'=',' ,'QUOTECHAR'='"','BAD_RECORDS_ACTION'='FORCE','FILEHEADER'='CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1,Double_COLUMN2,INTEGER_COLUMN1');Select Query : Select cust_id, cust_name, dob from uniqdata where dob='1975-06-22';It is working fine on hive. I am attaching CSV with this.
issueID:CARBONDATA-604
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
texts:Use Kettle wrong option in Thrift server give wrong error message
I am using Thrift Server with following configuration: -Duse.kettle=defaultHere kettle value is wrong and when I run the thrift server, the thrift server is running successfully. After that, I am going to start beeline. In beeline, I am executing load command without any kettle option as below:  LOAD DATA INPATH 'hdfs://localhost:54310/csvs/3000_UniqData.csv' into table uniqdata OPTIONS('DELIMITER'=',' , 'QUOTECHAR'='"','BAD_RECORDS_ACTION'='FORCE','FILEHEADER'='CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1,Double_COLUMN2,INTEGER_COLUMN1');After executing above command, I am getting Error: java.lang.IllegalArgumentException: For input string: "default" (state=,code=0)This error, seems to be wrong, because I am not using any default value in load query, and default is used with thrift.So I have following Suggestions: 1. Change the error message, while load query is executing.2. While starting thrift server with wrong value, thrift give us an error and not start.
issueID:CARBONDATA-605
type:Sub-task
changed files:
texts:Add Update-delete related documentation

issueID:CARBONDATA-606
type:Improvement
changed files:
texts:Add a Flink example to read CarbonData files
Add a Flink example to read CarbonData files written by Spark
issueID:CARBONDATA-607
type:Sub-task
changed files:
texts:Cleanup ValueCompressionHolder class and all sub-classes
Rewrite ValueCompressionHolder class as a base class for compressing or uncompressing numeric data for measurement column chunk. refactor all sub-classes underorg.apache.carbondata.core.datastorage.store.compression.decimal.*org.apache.carbondata.core.datastorage.store.compression.nonDecimal.*org.apache.carbondata.core.datastorage.store.compression.none.*org.apache.carbondata.core.datastorage.store.compression.type.*as part of the work, also fix a performance bug to avoid creating unnecessary compression/uncompression value holder during compression or decompression.
issueID:CARBONDATA-608
type:Bug
changed files:
texts:Compliation Error with spark 1.6 profile

issueID:CARBONDATA-609
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/util/CarbonProperties.java
texts:CarbonDataFileVersionIssue
Problem: Data file version is always taking latest version even if version is set to 1, This is because in carbon.properties we are calling valueof with string parameter and it is throwing illegalArugumentExcemption Solution: Need to parse the string value to short and then call valueof method
issueID:CARBONDATA-61
type:Bug
changed files:integration/spark-common/src/main/java/org/apache/carbondata/spark/merger/CarbonCompactionExecutor.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/SortDataRows.java
processing/src/main/java/org/apache/carbondata/processing/model/CarbonDataLoadSchema.java
processing/src/main/java/org/apache/carbondata/processing/surrogatekeysgenerator/csvbased/BadRecordsLogger.java
core/src/main/java/org/apache/carbondata/core/cache/CarbonLRUCache.java
integration/spark-common/src/main/java/org/apache/carbondata/spark/merger/RowResultMerger.java
integration/spark-common/src/main/java/org/apache/carbondata/spark/util/CarbonQueryUtil.java
core/src/main/java/org/apache/carbondata/core/scan/filter/FilterUtil.java
integration/spark-common/src/main/java/org/apache/carbondata/spark/load/CarbonLoaderUtil.java
core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
texts:Change Cube to Table
Due to history reason, CarbonData used cube concept, but it is not doing pre-aggregation, so it is better to change to table.
issueID:CARBONDATA-610
type:Bug
changed files:
texts:Update is working with String Datatype now but Date Datatype still not working
Update query is not working with spark 1.6 and String and dateCreate Table Query = CREATE TABLE uniqdata (CUST_ID int,CUST_NAME String,ACTIVE_EMUI_VERSION string, DOB date, DOJ date, BIGINT_COLUMN1 bigint,BIGINT_COLUMN2 bigint,DECIMAL_COLUMN1 decimal(30,10), DECIMAL_COLUMN2 decimal(36,10),Double_COLUMN1 double, Double_COLUMN2 double,INTEGER_COLUMN1 int) STORED BY 'org.apache.carbondata.format' TBLPROPERTIES ("TABLE_BLOCKSIZE"= "256 MB");Load data query = LOAD DATA INPATH 'hdfs://localhost:54310/tmp/testData/3000_UniqData.csv' into table uniqdata OPTIONS('FILEHEADER'='CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1,Double_COLUMN2,INTEGER_COLUMN1');Query for String = update uniqdata set (CUST_NAME)=('tony stark');Query for Date = update uniqdata set (dob)=('2016-12-11') where cust_name = 'CUST_NAME_01999';After update NULL is being inserted as shown below: ----------------------------------------------------------------------------------------------------- cust_id   cust_name      active_emui_version               dob                     doj            bigint_column1   bigint_column2       decimal_column1          decimal_column2         double_column1        double_column2      integer_column1  ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- 10987     NULL        ACTIVE_EMUI_VERSION_01987   1975-06-11 01:00:03.0   1975-06-11 02:00:03.0   123372038841     -223372034867    12345.6808882656         22345.6808882656         1.12345674897976E10   -1.12345674897976E10   1988              10988     NULL        ACTIVE_EMUI_VERSION_01988   1975-06-12 01:00:03.0   1975-06-12 02:00:03.0   123372038842     -223372034866    12345.6808892656         22345.6808892656         1.12345674897976E10   -1.12345674897976E10   1989              10989     NULL        ACTIVE_EMUI_VERSION_01989   1975-06-13 01:00:03.0   1975-06-13 02:00:03.0   123372038843     -223372034865    12345.6808902656         22345.6808902656         1.12345674897976E10   -1.12345674897976E10   1990              10990     NULL        ACTIVE_EMUI_VERSION_01990   1975-06-14 01:00:03.0   1975-06-14 02:00:03.0   123372038844     -223372034864    12345.6808912656         22345.6808912656         1.12345674897976E10   -1.12345674897976E10   1991              10991     NULL        ACTIVE_EMUI_VERSION_01991   1975-06-15 01:00:03.0   1975-06-15 02:00:03.0   123372038845     -223372034863    12345.6808922656         22345.6808922656         1.12345674897976E10   -1.12345674897976E10   1992              10992     NULL        ACTIVE_EMUI_VERSION_01992   1975-06-16 01:00:03.0   1975-06-16 02:00:03.0   123372038846     -223372034862    12345.6808932656         22345.6808932656         1.12345674897976E10   -1.12345674897976E10   1993              10993     NULL        ACTIVE_EMUI_VERSION_01993   1975-06-17 01:00:03.0   1975-06-17 02:00:03.0   123372038847     -223372034861    12345.6808942656         22345.6808942656         1.12345674897976E10   -1.12345674897976E10   1994              10994     NULL        ACTIVE_EMUI_VERSION_01994   1975-06-18 01:00:03.0   1975-06-18 02:00:03.0   123372038848     -223372034860    12345.6808952656         22345.6808952656         1.12345674897976E10   -1.12345674897976E10   1995              10995     NULL        ACTIVE_EMUI_VERSION_01995   1975-06-19 01:00:03.0   1975-06-19 02:00:03.0   123372038849     -223372034859    12345.6808962656         22345.6808962656         1.12345674897976E10   -1.12345674897976E10   1996              10996     NULL        ACTIVE_EMUI_VERSION_01996   1975-06-20 01:00:03.0   1975-06-20 02:00:03.0   123372038850     -223372034858    12345.6808972656         22345.6808972656         1.12345674897976E10   -1.12345674897976E10   1997              10997     NULL        ACTIVE_EMUI_VERSION_01997   1975-06-21 01:00:03.0   1975-06-21 02:00:03.0   123372038851     -223372034857    12345.6808982656         22345.6808982656         1.12345674897976E10   -1.12345674897976E10   1998              10998     NULL        ACTIVE_EMUI_VERSION_01998   1975-06-22 01:00:03.0   1975-06-22 02:00:03.0   123372038852     -223372034856    12345.6808992656         22345.6808992656         1.12345674897976E10   -1.12345674897976E10   1999             -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------For Date snapshot is attached
issueID:CARBONDATA-611
type:Bug
changed files:
texts:mvn clean -Pbuild-with-format package does not work
mvn clean -Pbuild-with-format package does not work
issueID:CARBONDATA-613
type:Bug
changed files:
texts:Data Load is not working for Decimal(38,0) datatype
I tried to load data into a table having datatype as Decimal(38,0) but it is showing DataLoad Failure.Create table command:CREATE TABLE uniqdata2 (CUST_ID int,CUST_NAME String,ACTIVE_EMUI_VERSION string, DOB timestamp, DOJ timestamp, BIGINT_COLUMN1 bigint,BIGINT_COLUMN2 bigint,DECIMAL_COLUMN1 decimal(38,0), DECIMAL_COLUMN2 decimal(38,0),Double_COLUMN1 double, Double_COLUMN2 double,INTEGER_COLUMN1 int) STORED BY 'org.apache.carbondata.format' TBLPROPERTIES ("TABLE_BLOCKSIZE"= "256 MB")Data Load command:LOAD DATA INPATH 'hdfs://localhost:54311/testFiles/2000_UniqDataWithMaxNoOfDigitsBeforeDecimal.csv' into table uniqdata2 OPTIONS('DELIMITER'=',' , 'QUOTECHAR'='"','BAD_RECORDS_ACTION'='FORCE','FILEHEADER'='CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1,Double_COLUMN2,INTEGER_COLUMN1');I tried the same queries in hive and it works successfully.Here are the logs for the exception:ERROR 09-01 16:09:11,855 - pool-185-thread-1 Problem while writing the carbon data filejava.lang.InterruptedException at java.lang.Object.wait(Native Method) at java.lang.Object.wait(Object.java:502) at org.apache.carbondata.processing.store.CarbonFactDataHandlerColumnar$BlockletDataHolder.get(CarbonFactDataHandlerColumnar.java:1539) at org.apache.carbondata.processing.store.CarbonFactDataHandlerColumnar$Consumer.call(CarbonFactDataHandlerColumnar.java:1629) at org.apache.carbondata.processing.store.CarbonFactDataHandlerColumnar$Consumer.call(CarbonFactDataHandlerColumnar.java:1611) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745)ERROR 09-01 16:09:11,856 - &#91;uniqdata2: Graph - MDKeyGenuniqdata2&#93;&#91;partitionID:0&#93; org.apache.carbondata.processing.store.writer.exception.CarbonDataWriterException: For input string: "12345678901234567890123456789012345678"java.util.concurrent.ExecutionException: org.apache.carbondata.processing.store.writer.exception.CarbonDataWriterException: For input string: "12345678901234567890123456789012345678" at java.util.concurrent.FutureTask.report(FutureTask.java:122) at java.util.concurrent.FutureTask.get(FutureTask.java:192) at org.apache.carbondata.processing.store.CarbonFactDataHandlerColumnar.processWriteTaskSubmitList(CarbonFactDataHandlerColumnar.java:1117) at org.apache.carbondata.processing.store.CarbonFactDataHandlerColumnar.finish(CarbonFactDataHandlerColumnar.java:1084) at org.apache.carbondata.processing.mdkeygen.MDKeyGenStep.processRow(MDKeyGenStep.java:222) at org.pentaho.di.trans.step.RunThread.run(RunThread.java:50) at java.lang.Thread.run(Thread.java:745)Caused by: org.apache.carbondata.processing.store.writer.exception.CarbonDataWriterException: For input string: "12345678901234567890123456789012345678" at org.apache.carbondata.processing.store.CarbonFactDataHandlerColumnar$Producer.call(CarbonFactDataHandlerColumnar.java:1603) at org.apache.carbondata.processing.store.CarbonFactDataHandlerColumnar$Producer.call(CarbonFactDataHandlerColumnar.java:1569) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) ... 1 moreCaused by: java.lang.NumberFormatException: For input string: "12345678901234567890123456789012345678" at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65) at java.lang.Long.parseLong(Long.java:592) at java.lang.Long.parseLong(Long.java:631) at org.apache.carbondata.processing.store.CarbonFactDataHandlerColumnar.processDataRows(CarbonFactDataHandlerColumnar.java:595) at org.apache.carbondata.processing.store.CarbonFactDataHandlerColumnar.access$700(CarbonFactDataHandlerColumnar.java:85) at org.apache.carbondata.processing.store.CarbonFactDataHandlerColumnar$Producer.call(CarbonFactDataHandlerColumnar.java:1592) ... 5 moreERROR 09-01 16:09:11,856 - &#91;uniqdata2: Graph - MDKeyGenuniqdata2&#93;&#91;partitionID:0&#93; Failed for table: uniqdata2 in  finishing data handlerorg.apache.carbondata.processing.store.writer.exception.CarbonDataWriterException: org.apache.carbondata.processing.store.writer.exception.CarbonDataWriterException: For input string: "12345678901234567890123456789012345678" at org.apache.carbondata.processing.store.CarbonFactDataHandlerColumnar.processWriteTaskSubmitList(CarbonFactDataHandlerColumnar.java:1123) at org.apache.carbondata.processing.store.CarbonFactDataHandlerColumnar.finish(CarbonFactDataHandlerColumnar.java:1084) at org.apache.carbondata.processing.mdkeygen.MDKeyGenStep.processRow(MDKeyGenStep.java:222) at org.pentaho.di.trans.step.RunThread.run(RunThread.java:50) at java.lang.Thread.run(Thread.java:745)Caused by: java.util.concurrent.ExecutionException: org.apache.carbondata.processing.store.writer.exception.CarbonDataWriterException: For input string: "12345678901234567890123456789012345678" at java.util.concurrent.FutureTask.report(FutureTask.java:122) at java.util.concurrent.FutureTask.get(FutureTask.java:192) at org.apache.carbondata.processing.store.CarbonFactDataHandlerColumnar.processWriteTaskSubmitList(CarbonFactDataHandlerColumnar.java:1117) ... 4 moreCaused by: org.apache.carbondata.processing.store.writer.exception.CarbonDataWriterException: For input string: "12345678901234567890123456789012345678" at org.apache.carbondata.processing.store.CarbonFactDataHandlerColumnar$Producer.call(CarbonFactDataHandlerColumnar.java:1603) at org.apache.carbondata.processing.store.CarbonFactDataHandlerColumnar$Producer.call(CarbonFactDataHandlerColumnar.java:1569) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) ... 1 moreCaused by: java.lang.NumberFormatException: For input string: "12345678901234567890123456789012345678" at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65) at java.lang.Long.parseLong(Long.java:592) at java.lang.Long.parseLong(Long.java:631) at org.apache.carbondata.processing.store.CarbonFactDataHandlerColumnar.processDataRows(CarbonFactDataHandlerColumnar.java:595) at org.apache.carbondata.processing.store.CarbonFactDataHandlerColumnar.access$700(CarbonFactDataHandlerColumnar.java:85) at org.apache.carbondata.processing.store.CarbonFactDataHandlerColumnar$Producer.call(CarbonFactDataHandlerColumnar.java:1592) ... 5 more
issueID:CARBONDATA-614
type:Bug
changed files:processing/src/main/java/org/apache/carbondata/processing/loading/model/CarbonLoadModel.java
texts:Fix dictionary locked issue
When carbon.properties.filepath is configured exactly,  still show the following exception.Error: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 4 times, most recent failure: Lost task 0.3 in stage 2.0 (TID 8, hadoop-slave-2): java.lang.RuntimeException: Dictionary file name is locked for updation. Please try after some time at scala.sys.package$.error(package.scala:27) at org.apache.carbondata.spark.rdd.CarbonGlobalDictionaryGenerateRDD$$anon$1.<init>(CarbonGlobalDictionaryRDD.scala:364) at org.apache.carbondata.spark.rdd.CarbonGlobalDictionaryGenerateRDD.compute(CarbonGlobalDictionaryRDD.scala:302) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306) at org.apache.spark.rdd.RDD.iterator(RDD.scala:270) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66) at org.apache.spark.scheduler.Task.run(Task.scala:89) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745)
issueID:CARBONDATA-615
type:Bug
changed files:
texts:Update query store wrong value for Date data type
I am trying to update DOB column with Date Data Type. It is storing a day before date which I have mentioned for updating in DOB column.Create Table : CREATE TABLE uniqdata (CUST_ID int,CUST_NAME char(30),ACTIVE_EMUI_VERSION string, DOB Date, DOJ Date, BIGINT_COLUMN1 bigint,BIGINT_COLUMN2 bigint,DECIMAL_COLUMN1 decimal(30,10), DECIMAL_COLUMN2 decimal(36,10),Double_COLUMN1 double, Double_COLUMN2 double, INTEGER_COLUMN1 int) STORED BY 'org.apache.carbondata.format';Load Data : LOAD DATA INPATH 'hdfs://localhost:54310/2000_UniqData.csv' into table uniqdata OPTIONS ('DELIMITER'=',' ,'QUOTECHAR'='"','BAD_RECORDS_ACTION'='FORCE','FILEHEADER'='CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1,Double_COLUMN2,INTEGER_COLUMN1','SINGLE_PASS'='true');Update Query :  update uniqdata set (dob)=(to_date('2016-12-01')) where cust_name = 'CUST_NAME_01999';Expected Result : It should update DOB column with 2016-12-01.Actual Result : It is updating DOB column with 2016-11-30.
issueID:CARBONDATA-616
type:Improvement
changed files:
texts:Remove the duplicated class CarbonDataWriterException.java
Remove the duplicated class CarbonDataWriterException.java &#91;1&#93;&#91;1&#93;org.apache.carbondata.core.writer.exception.CarbonDataWriterException.java &#91;2&#93;org.apache.carbondata.processing.store.writer.exception.CarbonDataWriterException.java
issueID:CARBONDATA-617
type:Bug
changed files:
texts:Insert query not working with UNION
I created 3 table all having same schemaCreate table commands:CREATE TABLE uniqdata (CUST_ID int,CUST_NAME String,ACTIVE_EMUI_VERSION string, DOB timestamp, DOJ timestamp, BIGINT_COLUMN1 bigint,BIGINT_COLUMN2 bigint,DECIMAL_COLUMN1 decimal(30,10), DECIMAL_COLUMN2 decimal(36,10),Double_COLUMN1 double, Double_COLUMN2 double,INTEGER_COLUMN1 int) STORED BY 'org.apache.carbondata.format';CREATE TABLE student (CUST_ID int,CUST_NAME String,ACTIVE_EMUI_VERSION string, DOB timestamp, DOJ timestamp, BIGINT_COLUMN1 bigint,BIGINT_COLUMN2 bigint,DECIMAL_COLUMN1 decimal(30,10), DECIMAL_COLUMN2 decimal(36,10),Double_COLUMN1 double, Double_COLUMN2 double,INTEGER_COLUMN1 int) STORED BY 'org.apache.carbondata.format';CREATE TABLE department (CUST_ID int,CUST_NAME String,ACTIVE_EMUI_VERSION string, DOB timestamp, DOJ timestamp, BIGINT_COLUMN1 bigint,BIGINT_COLUMN2 bigint,DECIMAL_COLUMN1 decimal(30,10), DECIMAL_COLUMN2 decimal(36,10),Double_COLUMN1 double, Double_COLUMN2 double,INTEGER_COLUMN1 int) STORED BY 'org.apache.carbondata.format';and I loaded the uniqdata and department table with the attached CSV(2000_UniqData.csv)and the insert query used to load data in student table was:insert into student select CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1,Double_COLUMN2,INTEGER_COLUMN1 from uniqdata UNION select CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1,Double_COLUMN2,INTEGER_COLUMN1 from department;When I try to insert data into student with union operation, it gives java.lang.Exception: DataLoad failure.(attached below)The Union query works well when used alone but when insert is used with Union it fails.Also, if I used hive tables instead of carbon tables insert does not work.
issueID:CARBONDATA-618
type:Bug
changed files:
texts:Add new profile to build all modules for release purpose
Add new profile to build all modules for release purpose
issueID:CARBONDATA-619
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
texts:Compaction API for Spark 2.1 : Issue in compaction type
Compaction API for Spark 2.1 : Compaction is successful on the segment ids irrespective of the compaction types which are( Minor and Major) i.e The compaction was successful for type other than Minor/Major I created a table with name carbon_table and made 5 loads into the table with the threshold of (4,3)cc.sql("create table carbon_table(id int,name string) stored by 'carbondata'")cc.sql("LOAD DATA LOCAL INPATH 'hdfs://hadoop-master:54311/data/employee.csv'INTO TABLE carbon_table options('FILEHEADER'='id,name')")For instance Compaction.compaction(spark,"default","carbon_table","xyz")here default is the database namecarbon_table is the table name xyz is the compaction typeI invoked the above method to compact the segments and it was executed successfully and compacted the segment ids. On show segment It produced the result with success for compaction . I have attached the screen shot for the same.
issueID:CARBONDATA-62
type:Bug
changed files:processing/src/main/java/org/apache/carbondata/processing/datatypes/PrimitiveDataType.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
core/src/main/java/org/apache/carbondata/core/util/DataTypeUtil.java
texts:Values not valid for column datatype are not getting discarded while generating global dictionary
If value read from raw data is not valid for its datatype then discard that value at the time of data loading and insert null at its place while storing data in carbon format. If not done so, filter queries other than isNull condition will display all the null values in the result
issueID:CARBONDATA-620
type:Bug
changed files:
texts:Compaction is failing in case of multiple blocklet
Problem: Compaction is failing in case of multiple blocklet and when each segment dictionary column size is changingReason: this is because during compaction we need to update the dictionary byte value with last segment key generator, in carbon merge rdd we are passing the oldest segment cardinality value we need to pass latest segment cardinalitySolution: Pass the latest segment cardinality.
issueID:CARBONDATA-621
type:Bug
changed files:
texts:Compaction is failing in case of multiple blocklet
Problem: Compaction is failing in case of multiple blocklet and when each segment dictionary column size is changingReason: this is because during compaction we need to update the dictionary byte value with last segment key generator, in carbon merge rdd we are passing the oldest segment cardinality value we need to pass latest segment cardinalitySolution: Pass the latest segment cardinality.
issueID:CARBONDATA-622
type:Bug
changed files:processing/src/main/java/org/apache/carbondata/processing/util/CarbonDataProcessorUtil.java
processing/src/main/java/org/apache/carbondata/processing/loading/DataLoadProcessBuilder.java
processing/src/main/java/org/apache/carbondata/processing/loading/model/CarbonLoadModel.java
texts:Should use the same fileheader reader for dict generation and data loading
We can get file header from DDL command and CSV file. 1. If the file header comes from DDL command, separate this file header by comma ","2. if the file header comes from CSV file, sparate this file header by specify delimiter in DDL command.
issueID:CARBONDATA-623
type:Bug
changed files:
texts:If we drop table after this condition ---(Firstly we load data in table with single pass true and use kettle false and then in same table load data 2nd time with single pass true and use kettle false ), it is throwing Error: java.lang.NullPointerException
1.Firstly we load data in table with single pass true and use kettle false data load successfully and  we are getting result set properly.2.then in same table load data in table with single pass true and use kettle false data load successfully and  we are getting result set properly.3.But after that if we drop the table ,its is throwing null pointer exception.Queries0: jdbc:hive2://hadoop-master:10000> CREATE TABLE uniqdata_INCLUDEDICTIONARY (CUST_ID int,CUST_NAME String,ACTIVE_EMUI_VERSION string, DOB timestamp, DOJ timestamp, BIGINT_COLUMN1 bigint,BIGINT_COLUMN2 bigint,DECIMAL_COLUMN1 decimal(30,10), DECIMAL_COLUMN2 decimal(36,10),Double_COLUMN1 double, Double_COLUMN2 double,INTEGER_COLUMN1 int) STORED BY 'org.apache.carbondata.format' TBLPROPERTIES('DICTIONARY_INCLUDE'='CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1,Double_COLUMN2,INTEGER_COLUMN1');---------+ Result  ---------+---------+No rows selected (1.13 seconds)0: jdbc:hive2://hadoop-master:10000> LOAD DATA INPATH 'hdfs://hadoop-master:54311/data/uniqdata/7000_UniqData.csv' into table uniqdata_INCLUDEDICTIONARY OPTIONS('DELIMITER'=',' , 'QUOTECHAR'='"','BAD_RECORDS_LOGGER_ENABLE'='TRUE', 'BAD_RECORDS_ACTION'='FORCE','FILEHEADER'='CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1,Double_COLUMN2,INTEGER_COLUMN1','SINGLE_PASS'='false','USE_KETTLE' ='false');---------+ Result  ---------+---------+No rows selected (22.814 seconds)0: jdbc:hive2://hadoop-master:10000> 0: jdbc:hive2://hadoop-master:10000> select count (distinct CUST_NAME) from uniqdata_INCLUDEDICTIONARY ;-------+  _c0  -------+ 7002  -------+1 row selected (3.055 seconds)0: jdbc:hive2://hadoop-master:10000> select  count(CUST_NAME) from uniqdata_INCLUDEDICTIONARY ;-------+  _c0  -------+ 7013  -------+1 row selected (0.366 seconds)0: jdbc:hive2://hadoop-master:10000> LOAD DATA INPATH 'hdfs://hadoop-master:54311/data/uniqdata/7000_UniqData.csv' into table uniqdata_INCLUDEDICTIONARY OPTIONS('DELIMITER'=',' , 'QUOTECHAR'='"','BAD_RECORDS_LOGGER_ENABLE'='TRUE', 'BAD_RECORDS_ACTION'='FORCE','FILEHEADER'='CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1,Double_COLUMN2,INTEGER_COLUMN1','SINGLE_PASS'='true','USE_KETTLE' ='false');---------+ Result  ---------+---------+No rows selected (4.837 seconds)0: jdbc:hive2://hadoop-master:10000> select  count(CUST_NAME) from uniqdata_INCLUDEDICTIONARY ;--------+  _c0   --------+ 14026  --------+1 row selected (0.458 seconds)0: jdbc:hive2://hadoop-master:10000> select count (distinct CUST_NAME) from uniqdata_INCLUDEDICTIONARY ;-------+  _c0  -------+ 7002  -------+1 row selected (3.173 seconds)0: jdbc:hive2://hadoop-master:10000> drop table uniqdata_includedictionary;Error: java.lang.NullPointerException (state=,code=0)Logs WARN  11-01 12:56:52,722 - Lost task 0.0 in stage 61.0 (TID 1740, hadoop-slave-2): FetchFailed(BlockManagerId(0, hadoop-slave-3, 45331), shuffleId=22, mapId=0, reduceId=0, message=org.apache.spark.shuffle.FetchFailedException: Failed to connect to hadoop-slave-3:45331 at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:323) at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:300) at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:51) at scala.collection.Iterator$$anon$11.next(Iterator.scala:328) at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371) at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327) at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32) at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39) at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327) at org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.processInputs(TungstenAggregationIterator.scala:504) at org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.<init>(TungstenAggregationIterator.scala:686) at org.apache.spark.sql.execution.aggregate.TungstenAggregate$$anonfun$doExecute$1$$anonfun$2.apply(TungstenAggregate.scala:95) at org.apache.spark.sql.execution.aggregate.TungstenAggregate$$anonfun$doExecute$1$$anonfun$2.apply(TungstenAggregate.scala:86) at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710) at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306) at org.apache.spark.rdd.RDD.iterator(RDD.scala:270) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306) at org.apache.spark.rdd.RDD.iterator(RDD.scala:270) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66) at org.apache.spark.scheduler.Task.run(Task.scala:89) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745)Caused by: java.io.IOException: Failed to connect to hadoop-slave-3:45331 at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:216) at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:167) at org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:90) at org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:140) at org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43) at org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:170) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.run(FutureTask.java:266) ... 3 moreCaused by: java.nio.channels.UnresolvedAddressException at sun.nio.ch.Net.checkAddress(Net.java:101) at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:622) at io.netty.channel.socket.nio.NioSocketChannel.doConnect(NioSocketChannel.java:209) at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.connect(AbstractNioChannel.java:207) at io.netty.channel.DefaultChannelPipeline$HeadContext.connect(DefaultChannelPipeline.java:1097) at io.netty.channel.AbstractChannelHandlerContext.invokeConnect(AbstractChannelHandlerContext.java:471) at io.netty.channel.AbstractChannelHandlerContext.connect(AbstractChannelHandlerContext.java:456) at io.netty.channel.ChannelOutboundHandlerAdapter.connect(ChannelOutboundHandlerAdapter.java:47) at io.netty.channel.AbstractChannelHandlerContext.invokeConnect(AbstractChannelHandlerContext.java:471) at io.netty.channel.AbstractChannelHandlerContext.connect(AbstractChannelHandlerContext.java:456) at io.netty.channel.ChannelDuplexHandler.connect(ChannelDuplexHandler.java:50) at io.netty.channel.AbstractChannelHandlerContext.invokeConnect(AbstractChannelHandlerContext.java:471) at io.netty.channel.AbstractChannelHandlerContext.connect(AbstractChannelHandlerContext.java:456) at io.netty.channel.AbstractChannelHandlerContext.connect(AbstractChannelHandlerContext.java:438) at io.netty.channel.DefaultChannelPipeline.connect(DefaultChannelPipeline.java:908) at io.netty.channel.AbstractChannel.connect(AbstractChannel.java:203) at io.netty.bootstrap.Bootstrap$2.run(Bootstrap.java:166) at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:357) at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:357) at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111) ... 1 moreAUDIT 11-01 12:59:57,699 - &#91;deepak-Vostro-3546&#93;&#91;hduser&#93;&#91;Thread-631&#93;Deleted table &#91;uniqdata_includedictionary&#93; under database &#91;default&#93;ERROR 11-01 12:59:57,702 - pool-26-thread-55 Exception occured:File does not exist: hdfs://hadoop-master:54311/opt/carbonStore/default/uniqdata_includedictionary/droptable.lockERROR 11-01 12:59:57,702 - Error executing query, currentState RUNNING, java.lang.NullPointerException at org.apache.carbondata.core.datastorage.store.filesystem.AbstractDFSCarbonFile.delete(AbstractDFSCarbonFile.java:128) at org.apache.carbondata.lcm.locks.HdfsFileLock.unlock(HdfsFileLock.java:110) at org.apache.spark.sql.execution.command.DropTableCommand.run(carbonTableSchema.scala:613) at org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult$lzycompute(commands.scala:58) at org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult(commands.scala:56) at org.apache.spark.sql.execution.ExecutedCommand.doExecute(commands.scala:70) at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$5.apply(SparkPlan.scala:132) at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$5.apply(SparkPlan.scala:130) at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150) at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:130) at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:55) at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:55) at org.apache.spark.sql.DataFrame.<init>(DataFrame.scala:145) at org.apache.spark.sql.DataFrame.<init>(DataFrame.scala:130) at org.apache.spark.sql.CarbonContext.sql(CarbonContext.scala:137) at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:211) at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1$$anon$2.run(SparkExecuteStatementOperation.scala:154) at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1$$anon$2.run(SparkExecuteStatementOperation.scala:151) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628) at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1.run(SparkExecuteStatementOperation.scala:164) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745)ERROR 11-01 12:59:57,702 - Error running hive query: org.apache.hive.service.cli.HiveSQLException: java.lang.NullPointerException at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:246) at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1$$anon$2.run(SparkExecuteStatementOperation.scala:154) at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1$$anon$2.run(SparkExecuteStatementOperation.scala:151) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628) at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1.run(SparkExecuteStatementOperation.scala:164) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745)
issueID:CARBONDATA-624
type:Improvement
changed files:
texts:Complete CarbonData document to be present in git and the same needs to sync with the carbondata.apace.org and for further updates.
The information about CarbonData is there is git and cwiki. So we have to merge all the information and create the markdown files for each topic about CarbonData.This markdown files will be having the complete information about CarbonData like Overview, Installation, Configuration, DDL, DML, Use case and so on.Also these markdown information will be sync to the website documentation - carbondata.apace.org
issueID:CARBONDATA-625
type:Bug
changed files:
texts:Abnormal behaviour of Int datatype
I was trying to create a table having int as a column and loaded data into the table. Data loading was performed successfully but when I viewed the data of the table, there was some wrong data present in the table. I was trying to load BigInt data to an int column. All the data in int column is loaded with the first value of the csv. Below are the details for the queries:create table xyz(a int, b string)stored by 'carbondata';Data load query:LOAD DATA INPATH 'hdfs://localhost:54311/testFiles/testMaxValueForBigInt.csv' into table xyz OPTIONS('DELIMITER'=',' , 'QUOTECHAR'='"','FILEHEADER'='a,b');select query:select * from xyz;PFA the screenshot of the output and the csv file.
issueID:CARBONDATA-626
type:Bug
changed files:
texts:[Dataload] Dataloading is not working with delimiter set as "|"
Description : Data loading fail with delimiter as "|" .Steps:> 1. Create table> 2. Load data into tableLog :---------- create table DIM_TERMINAL(ID int,TAC String,TER_BRAND_NAME String,TER_MODEL_NAME String,TER_MODENAME String,TER_TYPE_ID String,TER_TYPE_NAME_EN String,TER_TYPE_NAME_CHN String,TER_OSTYPE String,TER_OS_TYPE_NAME String,HSPASPEED String,LTESPEED String,VOLTE_FLAG String,flag String) stored by 'org.apache.carbondata.format' TBLPROPERTIES ('DICTIONARY_INCLUDE'='TAC,TER_BRAND_NAME,TER_MODEL_NAME,TER_MODENAME,TER_TYPE_ID,TER_TYPE_NAME_EN,TER_TYPE_NAME_CHN,TER_OSTYPE,TER_OS_TYPE_NAME,HSPASPEED,LTESPEED,VOLTE_FLAG,flag'); jdbc:hive2://172.168.100.212:23040> LOAD DATA inpath 'hdfs://hacluster/SEQIQ/IQ_DIM_TERMINAL.csv' INTO table DIM_TERMINAL1 OPTIONS('DELIMITER'='|','USE_KETTLE'='false','QUOTECHAR'='','FILEHEADER'= 'ID,TAC,TER_BRAND_NAME,TER_MODEL_NAME,TER_MODENAME,TER_TYPE_ID,TER_TYPE_NAME_EN,TER_TYPE_NAME_CHN,TER_OSTYPE,TER_OS_TYPE_NAME,HSPASPEED,LTESPEED,VOLTE_FLAG,flag');Error: java.lang.RuntimeException: Data loading failed. table not found: default.dim_terminal1 (state=,code=0)0: jdbc:hive2://172.168.100.212:23040> LOAD DATA inpath 'hdfs://hacluster/SEQIQ/IQ_DIM_TERMINAL1.csv' INTO table DIM_TERMINAL OPTIONS('DELIMITER'='|','USE_KETTLE'='false','QUOTECHAR'='','FILEHEADER'= 'ID,TAC,TER_BRAND_NAME,TER_MODEL_NAME,TER_MODENAME,TER_TYPE_ID,TER_TYPE_NAME_EN,TER_TYPE_NAME_CHN,TER_OSTYPE,TER_OS_TYPE_NAME,HSPASPEED,LTESPEED,VOLTE_FLAG,flag');Error: org.apache.spark.sql.AnalysisException: Reference 'D' is ambiguous, could be: D#4893, D#4907, D#4920, D#4935, D#4952, D#5025, D#5034.; (state=,code=0) csv raw details :103880|99000537|MI|2S H1SC 3C|2G/3G|0|SmartPhone|SmartPhone|4|Android|||1|
issueID:CARBONDATA-627
type:Bug
changed files:
texts:Fix Union unit test case for spark2
UnionTestCase failed in spark2, We should fix it.
issueID:CARBONDATA-628
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/scan/collector/impl/DictionaryBasedVectorResultCollector.java
texts:Issue when measure selection with out table order gives wrong result with vectorized reader enabled
If the table is created with measure order like m1, m2 and user selects the measures m2, m1 then it returns wrong result with vectorized reader enabled
issueID:CARBONDATA-629
type:Bug
changed files:
texts:Issue with database name case sensitivity
When database name is provided in any DDL/DML command, the database name is interpreted and used in the same case as provided by the user. This leads to different behavior in windows and unix systems as windows is case sensitive and linux systems are case insensitive.Consider a case for create database. Lets say database name is "Carbon". While executing database name is provided as Carbon but while deleting or using or creating table the case is changed to "CARbOn". In these cases system will not behave correctly and if HDFS UI is checked the database Carbon will still exist even after dropping database as the case for database name was different in the 2 commands execution.
issueID:CARBONDATA-63
type:Improvement
changed files:
texts:provide recommend values for different scenarios
For carbon load part, there are some parameters like carbon.sort.size, carbon.sort.intermediate.files.limit and so on ,which will impact performance.Based on user scenario such as file size, raw data format and so on, we may can give recommend values. so any suggestions?
issueID:CARBONDATA-630
type:Bug
changed files:
texts:Unable to use string function on string/char data type column
I am trying to execute string function like: reverse, concat, lower, upper with the string/char column but it is giving error and when I am giving direct string value to it, it is working.Create Table : CREATE TABLE uniqdata_char (CUST_ID int,CUST_NAME char(30),ACTIVE_EMUI_VERSION char(30), DOB timestamp, DOJ timestamp, BIGINT_COLUMN1 bigint,BIGINT_COLUMN2 bigint,DECIMAL_COLUMN1 decimal(30,10), DECIMAL_COLUMN2 decimal(36,10),Double_COLUMN1 double, Double_COLUMN2 double,INTEGER_COLUMN1 int) STORED BY 'org.apache.carbondata.format' TBLPROPERTIES ('TABLE_BLOCKSIZE'= '256 MB');Load Data : LOAD DATA INPATH 'hdfs://localhost:54310/2000_UniqData.csv' into table uniqdata_char OPTIONS ('DELIMITER'=',' ,'QUOTECHAR'='""','BAD_RECORDS_ACTION'='FORCE','FILEHEADER'='CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1,Double_COLUMN2,INTEGER_COLUMN1','MAXCOLUMNS'='12');Query :  select Lower(cust_name) from uniqdata_char;After running the query I am getting error.But when I am running :select Lower('TESTING') from uniqdata_char;It is working fine.I have attached CSV and Executor log with it.
issueID:CARBONDATA-632
type:Bug
changed files:
texts:Fix wrong comments of load data  in CarbonDataRDDFactory.scala
Fix wrong comments of load data  in CarbonDataRDDFactory.scala:1.Currently, not use NewHadoopRDD and DummyLoadRDD2.Clean invalid code:     inputFormat match {            case configurable: Configurable =>              configurable.setConf(hadoopConfiguration)            case _ =>          }
issueID:CARBONDATA-633
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/scan/result/AbstractScannedResult.java
core/src/main/java/org/apache/carbondata/core/scan/executor/impl/VectorDetailQueryExecutor.java
core/src/main/java/org/apache/carbondata/core/scan/executor/impl/AbstractQueryExecutor.java
core/src/main/java/org/apache/carbondata/core/scan/executor/impl/DetailQueryExecutor.java
core/src/main/java/org/apache/carbondata/core/scan/result/iterator/AbstractDetailQueryResultIterator.java
core/src/main/java/org/apache/carbondata/core/scan/scanner/impl/FilterScanner.java
texts:Query Crash issue in case of offheap
ProblemQuery is failing sometime when number of 1 block contains more than one blockletReasonOffheap memory is getting clear before all the records are accessedSolutiionOnce all the records are accessed then clear the memory
issueID:CARBONDATA-634
type:Bug
changed files:
texts:Load Query options invalid values are not consistent behaviour.
If we pass invalid keyword in ('BAD_RECORDS_ACTION'='FAIL'), its behaves like default('BAD_RECORDS_ACTION'='FORCE') , here we require some error message instead of default behavior.  QueryCREATE TABLE Dataload_uniqdata1 (CUST_ID int,CUST_NAME String,ACTIVE_EMUI_VERSION string, DOB timestamp, DOJ timestamp, BIGINT_COLUMN1 bigint,BIGINT_COLUMN2 bigint,DECIMAL_COLUMN1 decimal(30,10), DECIMAL_COLUMN2 decimal(36,10),Double_COLUMN1 double, Double_COLUMN2 double,INTEGER_COLUMN1 int) STORED BY 'org.apache.carbondata.format'; LOAD DATA INPATH 'hdfs://hadoop-master:54311/data/uniqdata/2000_UniqData.csv' into table Dataload_uniqdata OPTIONS('DELIMITER'=',' , 'QUOTECHAR'='"','BAD_RECORDS_LOGGER_ENABLE'='TRUE', 'BAD_RECORDS_ACTION'='FAIL','FILEHEADER'='CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1,Double_COLUMN2,INTEGER_COLUMN1'); select *   from Dataload_uniqdata limit 10 ;
issueID:CARBONDATA-635
type:Bug
changed files:
texts:ClassCastException in Spark 2.1 Cluster mode in insert query when name of column is changed and When the orders of columns are changed in the tables
:::::::::  SCENARIO 1 :::::::CREATE TABLE uniqdata (CUST_ID int,CUST_NAME String,ACTIVE_EMUI_VERSION string, DOB timestamp, DOJ timestamp, BIGINT_COLUMN1 bigint,BIGINT_COLUMN2 bigint,DECIMAL_COLUMN1 decimal(30,10), DECIMAL_COLUMN2 decimal(36,10),Double_COLUMN1 double, Double_COLUMN2 double,INTEGER_COLUMN1 int) STORED BY 'org.apache.carbondata.format' TBLPROPERTIES ("TABLE_BLOCKSIZE"= "256 MB");CREATE TABLE student (CUST_ID2 int,CUST_ADDR String,ACTIVE_EMUI_VERSION string, DOB timestamp, DOJ timestamp, BIGINT_COLUMN1 bigint,BIGINT_COLUMN2 bigint,DECIMAL_COLUMN1 decimal(30,10), DECIMAL_COLUMN2 decimal(36,10),Double_COLUMN1 double, Double_COLUMN2 double,INTEGER_COLUMN1 int) STORED BY 'org.apache.carbondata.format' TBLPROPERTIES ("TABLE_BLOCKSIZE"= "256 MB");LOAD DATA inpath 'hdfs://hadoop-master:54311/data/2000_UniqData.csv' INTO table uniqdata options('DELIMITER'=',', 'FILEHEADER'='CUST_ID, CUST_NAME, ACTIVE_EMUI_VERSION, DOB, DOJ, BIGINT_COLUMN1, BIGINT_COLUMN2, DECIMAL_COLUMN1, DECIMAL_COLUMN2, Double_COLUMN1, Double_COLUMN2, INTEGER_COLUMN1');insert into student select * from uniqdata;:::::::::  SCENARIO 2 :::::::CREATE TABLE uniqdata (CUST_ID int,CUST_NAME String,ACTIVE_EMUI_VERSION string, DOB timestamp, DOJ timestamp, BIGINT_COLUMN1 bigint,BIGINT_COLUMN2 bigint,DECIMAL_COLUMN1 decimal(30,10), DECIMAL_COLUMN2 decimal(36,10),Double_COLUMN1 double, Double_COLUMN2 double) STORED BY 'org.apache.carbondata.format' TBLPROPERTIES ("TABLE_BLOCKSIZE"= "256 MB");CREATE TABLE student (ACTIVE_EMUI_VERSION string, DOB timestamp, CUST_ID int,CUST_NAME String, DOJ timestamp, BIGINT_COLUMN1 bigint,BIGINT_COLUMN2 bigint,DECIMAL_COLUMN1 decimal(30,10), DECIMAL_COLUMN2 decimal(36,10),Double_COLUMN1 double, Double_COLUMN2 double) STORED BY 'org.apache.carbondata.format' TBLPROPERTIES ("TABLE_BLOCKSIZE"= "256 MB");LOAD DATA inpath 'hdfs://hadoop-master:54311/data/2000_UniqData.csv' INTO table uniqdata options('DELIMITER'=',', 'FILEHEADER'='CUST_ID, CUST_NAME, ACTIVE_EMUI_VERSION, DOB, DOJ, BIGINT_COLUMN1, BIGINT_COLUMN2, DECIMAL_COLUMN1, DECIMAL_COLUMN2, Double_COLUMN1, Double_COLUMN2, INTEGER_COLUMN1');Above two scenarios have the same result and exception as below,0: jdbc:hive2://hadoop-master:10000> insert into student select * from uniqdata;Error: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 26.0 failed 4 times, most recent failure: Lost task 0.3 in stage 26.0 (TID 38, 192.168.2.176, executor 0): java.lang.ClassCastException: org.apache.spark.unsafe.types.UTF8String cannot be cast to java.lang.Integer at scala.runtime.BoxesRunTime.unboxToInt(BoxesRunTime.java:101) at org.apache.spark.sql.CarbonDictionaryDecoder$$anonfun$doExecute$1$$anonfun$7$$anon$1$$anonfun$next$1.apply$mcVI$sp(CarbonDictionaryDecoder.scala:186) at org.apache.spark.sql.CarbonDictionaryDecoder$$anonfun$doExecute$1$$anonfun$7$$anon$1$$anonfun$next$1.apply(CarbonDictionaryDecoder.scala:183) at org.apache.spark.sql.CarbonDictionaryDecoder$$anonfun$doExecute$1$$anonfun$7$$anon$1$$anonfun$next$1.apply(CarbonDictionaryDecoder.scala:183) at scala.collection.mutable.ArraySeq.foreach(ArraySeq.scala:74) at org.apache.spark.sql.CarbonDictionaryDecoder$$anonfun$doExecute$1$$anonfun$7$$anon$1.next(CarbonDictionaryDecoder.scala:183) at org.apache.spark.sql.CarbonDictionaryDecoder$$anonfun$doExecute$1$$anonfun$7$$anon$1.next(CarbonDictionaryDecoder.scala:174) at scala.collection.Iterator$$anon$11.next(Iterator.scala:409) at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source) at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43) at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377) at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408) at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408) at org.apache.carbondata.spark.rdd.CarbonBlockDistinctValuesCombineRDD.compute(CarbonGlobalDictionaryRDD.scala:293) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) at org.apache.spark.rdd.RDD.iterator(RDD.scala:287) at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96) at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53) at org.apache.spark.scheduler.Task.run(Task.scala:99) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745)Driver stacktrace: (state=,code=0)
issueID:CARBONDATA-636
type:Bug
changed files:processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/SortDataRows.java
processing/src/main/java/org/apache/carbondata/processing/util/CarbonDataProcessorUtil.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/impl/ParallelReadMergeSorterImpl.java
processing/src/main/java/org/apache/carbondata/processing/loading/DataLoadProcessBuilder.java
processing/src/main/java/org/apache/carbondata/processing/loading/CarbonDataLoadConfiguration.java
processing/src/main/java/org/apache/carbondata/processing/loading/steps/InputProcessorStepImpl.java
processing/src/main/java/org/apache/carbondata/processing/loading/model/CarbonLoadModel.java
texts:Testcases are failing in spark 1.6 and 2.1 with no kettle flow.
Testcases are failing in spark 1.6 and 2.1 with no kettle flow.
issueID:CARBONDATA-637
type:Improvement
changed files:
texts:Remove table_status file

issueID:CARBONDATA-638
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/metadata/blocklet/BlockletInfo.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/TableInfo.java
core/src/main/java/org/apache/carbondata/core/datastore/columnar/UnBlockIndexer.java
core/src/main/java/org/apache/carbondata/core/util/DataFileFooterConverter.java
processing/src/main/java/org/apache/carbondata/processing/loading/DataLoadExecutor.java
core/src/main/java/org/apache/carbondata/core/scan/collector/impl/RawBasedResultCollector.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelRangeGrtrThanEquaToFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/FilterResolverIntf.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/SchemaEvolutionEntry.java
core/src/main/java/org/apache/carbondata/core/scan/expression/exception/FilterUnsupportedException.java
integration/spark-datasource/src/main/scala/org/apache/carbondata/spark/vectorreader/VectorizedCarbonRecordReader.java
core/src/main/java/org/apache/carbondata/core/datastore/columnar/ColumnWithRowIdForHighCard.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/BucketingInfo.java
core/src/main/java/org/apache/carbondata/core/datastore/TableSegmentUniqueIdentifier.java
core/src/main/java/org/apache/carbondata/core/datastore/block/BlockInfo.java
core/src/main/java/org/apache/carbondata/core/datastore/block/Distributable.java
core/src/main/java/org/apache/carbondata/core/locks/HdfsFileLock.java
core/src/main/java/org/apache/carbondata/core/cache/dictionary/ColumnDictionaryInfo.java
core/src/main/java/org/apache/carbondata/core/scan/executor/impl/AbstractQueryExecutor.java
processing/src/main/java/org/apache/carbondata/processing/util/CarbonLoaderUtil.java
core/src/main/java/org/apache/carbondata/core/stats/DriverQueryStatisticsRecorderDummy.java
processing/src/main/java/org/apache/carbondata/processing/loading/steps/DataConverterProcessorStepImpl.java
core/src/main/java/org/apache/carbondata/core/scan/expression/LiteralExpression.java
processing/src/main/java/org/apache/carbondata/processing/util/CarbonDataProcessorUtil.java
core/src/main/java/org/apache/carbondata/core/datastore/IndexKey.java
core/src/main/java/org/apache/carbondata/core/scan/processor/BlockletIterator.java
core/src/main/java/org/apache/carbondata/core/scan/filter/ColumnFilterInfo.java
core/src/main/java/org/apache/carbondata/core/cache/dictionary/DictionaryColumnUniqueIdentifier.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/AndFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/scan/model/ProjectionColumn.java
core/src/main/java/org/apache/carbondata/core/util/DataTypeUtil.java
core/src/main/java/org/apache/carbondata/core/locks/ICarbonLock.java
core/src/main/java/org/apache/carbondata/core/scan/expression/ExpressionResult.java
core/src/main/java/org/apache/carbondata/core/keygenerator/directdictionary/DirectDictionaryGenerator.java
core/src/main/java/org/apache/carbondata/core/datastore/block/BlockletInfos.java
core/src/main/java/org/apache/carbondata/core/datastore/filesystem/AbstractDFSCarbonFile.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/DimensionColumnPage.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/reader/MeasureColumnChunkReader.java
core/src/main/java/org/apache/carbondata/core/scan/result/iterator/DetailQueryResultIterator.java
processing/src/main/java/org/apache/carbondata/processing/merger/CarbonDataMergerUtilResult.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/comparator/UnsafeRowComparatorForNormalDIms.java
integration/spark2/src/main/java/org/apache/carbondata/spark/readsupport/SparkRowReadSupportImpl.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/impl/ParallelReadMergeSorterWithBucketingImpl.java
core/src/main/java/org/apache/carbondata/core/memory/HeapMemoryAllocator.java
core/src/main/java/org/apache/carbondata/core/scan/filter/FilterExpressionProcessor.java
core/src/main/java/org/apache/carbondata/core/metadata/converter/SchemaConverter.java
core/src/main/java/org/apache/carbondata/core/scan/expression/conditional/LessThanEqualToExpression.java
processing/src/main/java/org/apache/carbondata/processing/loading/model/CarbonLoadModel.java
core/src/main/java/org/apache/carbondata/core/scan/result/vector/CarbonColumnarBatch.java
core/src/main/java/org/apache/carbondata/core/scan/result/vector/impl/CarbonColumnVectorImpl.java
core/src/main/java/org/apache/carbondata/core/scan/executor/infos/MeasureInfo.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/impl/FixedLengthDimensionColumnPage.java
core/src/main/java/org/apache/carbondata/core/scan/collector/impl/AbstractScannedResultCollector.java
core/src/main/java/org/apache/carbondata/core/scan/executor/exception/QueryExecutionException.java
core/src/main/java/org/apache/carbondata/core/datastore/impl/FileFactory.java
core/src/main/java/org/apache/carbondata/core/datastore/filesystem/LocalCarbonFile.java
core/src/main/java/org/apache/carbondata/core/mutate/data/RowCountDetailsVO.java
core/src/main/java/org/apache/carbondata/core/datastore/compression/Compressor.java
core/src/main/java/org/apache/carbondata/core/scan/expression/conditional/GreaterThanEqualToExpression.java
core/src/main/java/org/apache/carbondata/core/scan/collector/impl/DictionaryBasedVectorResultCollector.java
core/src/main/java/org/apache/carbondata/core/writer/CarbonDeleteDeltaWriter.java
core/src/main/java/org/apache/carbondata/core/metadata/converter/ThriftWrapperSchemaConverterImpl.java
core/src/main/java/org/apache/carbondata/core/stats/QueryStatisticsConstants.java
core/src/main/java/org/apache/carbondata/core/stats/QueryStatisticsRecorder.java
core/src/main/java/org/apache/carbondata/core/scan/expression/conditional/EqualToExpression.java
core/src/main/java/org/apache/carbondata/core/scan/scanner/impl/BlockletFilterScanner.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/RowLevelFilterResolverImpl.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/store/impl/safe/SafeFixedLengthDimensionDataChunkStore.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/column/CarbonMeasure.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/resolverinfo/visitor/NoDictionaryTypeVisitor.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/store/impl/unsafe/UnsafeFixedLengthDimensionDataChunkStore.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/store/impl/safe/SafeAbsractDimensionDataChunkStore.java
core/src/main/java/org/apache/carbondata/core/util/path/CarbonTablePath.java
core/src/main/java/org/apache/carbondata/core/metadata/blocklet/index/BlockletMinMaxIndex.java
core/src/main/java/org/apache/carbondata/core/scan/expression/LeafExpression.java
core/src/main/java/org/apache/carbondata/core/datastore/block/TableTaskInfo.java
core/src/main/java/org/apache/carbondata/core/scan/model/QueryModel.java
core/src/main/java/org/apache/carbondata/core/datastore/filesystem/HDFSCarbonFile.java
core/src/main/java/org/apache/carbondata/core/scan/collector/ScannedResultCollector.java
core/src/main/java/org/apache/carbondata/core/scan/expression/conditional/BinaryConditionalExpression.java
core/src/main/java/org/apache/carbondata/core/scan/wrappers/ByteArrayWrapper.java
core/src/main/java/org/apache/carbondata/core/stats/QueryStatisticsRecorderImpl.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelRangeTypeExecuterFactory.java
core/src/main/java/org/apache/carbondata/core/scan/expression/logical/FalseExpression.java
core/src/main/java/org/apache/carbondata/core/scan/scanner/impl/BlockletFullScanner.java
core/src/main/java/org/apache/carbondata/core/datastore/compression/CompressorFactory.java
core/src/main/java/org/apache/carbondata/core/metadata/ColumnIdentifier.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/DimColumnExecuterFilterInfo.java
processing/src/main/java/org/apache/carbondata/processing/loading/partition/impl/HashPartitionerImpl.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/column/ColumnSchema.java
core/src/main/java/org/apache/carbondata/core/memory/UnsafeMemoryAllocator.java
core/src/main/java/org/apache/carbondata/core/scan/filter/intf/ExpressionType.java
core/src/main/java/org/apache/carbondata/core/reader/CarbonDeleteDeltaFileReader.java
core/src/main/java/org/apache/carbondata/core/scan/result/iterator/VectorDetailQueryResultIterator.java
processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactDataHandlerModel.java
core/src/main/java/org/apache/carbondata/core/stats/QueryStatistic.java
core/src/main/java/org/apache/carbondata/core/mutate/TupleIdEnum.java
core/src/main/java/org/apache/carbondata/core/datastore/FileReader.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/IncludeFilterExecuterImpl.java
processing/src/main/java/org/apache/carbondata/processing/loading/converter/impl/MeasureFieldConverterImpl.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/impl/AbstractDimensionColumnPage.java
core/src/main/java/org/apache/carbondata/core/metadata/index/BlockIndexInfo.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/resolverinfo/MeasureColumnResolvedFilterInfo.java
core/src/main/java/org/apache/carbondata/core/metadata/datatype/DataType.java
core/src/main/java/org/apache/carbondata/core/scan/result/iterator/AbstractDetailQueryResultIterator.java
core/src/main/java/org/apache/carbondata/core/scan/executor/util/QueryUtil.java
core/src/main/java/org/apache/carbondata/core/scan/expression/logical/OrExpression.java
processing/src/main/java/org/apache/carbondata/processing/loading/parser/CarbonParserFactory.java
core/src/main/java/org/apache/carbondata/core/datastore/DataRefNode.java
core/src/main/java/org/apache/carbondata/core/scan/result/BlockletScannedResult.java
core/src/main/java/org/apache/carbondata/core/locks/CarbonLockFactory.java
processing/src/main/java/org/apache/carbondata/processing/merger/NodeMultiBlockRelation.java
processing/src/main/java/org/apache/carbondata/processing/util/DeleteLoadFolders.java
core/src/main/java/org/apache/carbondata/core/scan/filter/FilterUtil.java
processing/src/main/java/org/apache/carbondata/processing/loading/converter/impl/NonDictionaryFieldConverterImpl.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/column/CarbonColumn.java
core/src/main/java/org/apache/carbondata/core/metadata/ColumnarFormatVersion.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/UnsafeCarbonRowPage.java
core/src/main/java/org/apache/carbondata/core/memory/MemoryException.java
core/src/main/java/org/apache/carbondata/core/cache/CacheProvider.java
core/src/main/java/org/apache/carbondata/core/datastore/block/AbstractIndex.java
core/src/main/java/org/apache/carbondata/core/datastore/block/TaskBlockInfo.java
processing/src/main/java/org/apache/carbondata/processing/util/CarbonQueryUtil.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/column/CarbonImplicitDimension.java
core/src/main/java/org/apache/carbondata/core/mutate/UpdateVO.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/resolverinfo/visitor/DictionaryColumnVisitor.java
core/src/main/java/org/apache/carbondata/core/scan/filter/FilterProcessor.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/resolverinfo/visitor/CustomTypeDictionaryVisitor.java
core/src/main/java/org/apache/carbondata/core/scan/result/vector/CarbonColumnVector.java
core/src/main/java/org/apache/carbondata/core/datastore/compression/SnappyCompressor.java
core/src/main/java/org/apache/carbondata/core/scan/executor/impl/DetailQueryExecutor.java
core/src/main/java/org/apache/carbondata/core/scan/expression/UnknownExpression.java
processing/src/main/java/org/apache/carbondata/processing/csvload/BlockDetails.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/SortParameters.java
processing/src/main/java/org/apache/carbondata/processing/merger/CarbonCompactionUtil.java
processing/src/main/java/org/apache/carbondata/processing/loading/steps/DataWriterProcessorStepImpl.java
core/src/main/java/org/apache/carbondata/core/cache/CacheType.java
core/src/main/java/org/apache/carbondata/core/scan/result/iterator/ChunkRowIterator.java
core/src/main/java/org/apache/carbondata/core/datastore/filesystem/CarbonFile.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/resolverinfo/DimColumnResolvedFilterInfo.java
core/src/main/java/org/apache/carbondata/core/util/CarbonProperties.java
core/src/main/java/org/apache/carbondata/core/writer/CarbonDeleteDeltaWriterImpl.java
core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
core/src/main/java/org/apache/carbondata/core/service/impl/ColumnUniqueIdGenerator.java
core/src/main/java/org/apache/carbondata/core/scan/filter/intf/RowImpl.java
core/src/main/java/org/apache/carbondata/core/scan/model/ProjectionDimension.java
core/src/main/java/org/apache/carbondata/core/datastore/block/SegmentProperties.java
core/src/main/java/org/apache/carbondata/core/metadata/blocklet/SegmentInfo.java
core/src/main/java/org/apache/carbondata/core/reader/CarbonDeleteFilesDataReader.java
core/src/main/java/org/apache/carbondata/core/mutate/SegmentUpdateDetails.java
processing/src/main/java/org/apache/carbondata/processing/loading/DataLoadProcessBuilder.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelRangeLessThanFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/reader/ThriftReader.java
core/src/main/java/org/apache/carbondata/core/datastore/columnar/ColumnarKeyStoreMetadata.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/TableSchema.java
core/src/main/java/org/apache/carbondata/core/reader/CarbonDeleteDeltaFileReaderImpl.java
core/src/main/java/org/apache/carbondata/core/service/ColumnUniqueIdService.java
core/src/main/java/org/apache/carbondata/core/stats/DriverQueryStatisticsRecorderImpl.java
processing/src/main/java/org/apache/carbondata/processing/merger/RowResultMergerProcessor.java
core/src/main/java/org/apache/carbondata/core/mutate/DeleteDeltaBlockletDetails.java
core/src/main/java/org/apache/carbondata/core/util/DataFileFooterConverterFactory.java
core/src/main/java/org/apache/carbondata/core/writer/ThriftWriter.java
core/src/main/java/org/apache/carbondata/core/locks/ZooKeeperLocking.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/ExcludeFilterExecuterImpl.java
hadoop/src/main/java/org/apache/carbondata/hadoop/readsupport/CarbonReadSupport.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/reader/DimensionColumnChunkReader.java
core/src/main/java/org/apache/carbondata/core/datastore/filesystem/CarbonFileFilter.java
core/src/main/java/org/apache/carbondata/core/scan/expression/logical/BinaryLogicalExpression.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/FilterExecuter.java
core/src/main/java/org/apache/carbondata/core/scan/expression/exception/FilterIllegalMemberException.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/reader/CarbonDataReaderFactory.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/OrFilterExecuterImpl.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonTableInputFormat.java
processing/src/main/java/org/apache/carbondata/processing/loading/model/CarbonDataLoadSchema.java
core/src/main/java/org/apache/carbondata/core/metadata/blocklet/DataFileFooter.java
core/src/main/java/org/apache/carbondata/hadoop/CarbonInputSplit.java
core/src/main/java/org/apache/carbondata/core/scan/complextypes/StructQueryType.java
core/src/main/java/org/apache/carbondata/core/scan/result/vector/MeasureDataVectorProcessor.java
core/src/main/java/org/apache/carbondata/core/scan/result/RowBatch.java
core/src/main/java/org/apache/carbondata/core/metadata/blocklet/index/BlockletBTreeIndex.java
core/src/main/java/org/apache/carbondata/core/scan/result/impl/NonFilterQueryScannedResult.java
core/src/main/java/org/apache/carbondata/core/scan/executor/QueryExecutorFactory.java
processing/src/main/java/org/apache/carbondata/processing/store/CarbonDataWriterFactory.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/store/impl/safe/SafeVariableLengthDimensionDataChunkStore.java
core/src/main/java/org/apache/carbondata/core/fileoperations/AtomicFileOperationsImpl.java
core/src/main/java/org/apache/carbondata/core/scan/executor/util/RestructureUtil.java
core/src/main/java/org/apache/carbondata/core/datastore/block/TableBlockInfo.java
core/src/main/java/org/apache/carbondata/core/scan/expression/conditional/InExpression.java
core/src/main/java/org/apache/carbondata/core/scan/expression/conditional/ListExpression.java
core/src/main/java/org/apache/carbondata/core/scan/result/vector/ColumnVectorInfo.java
core/src/main/java/org/apache/carbondata/core/scan/scanner/BlockletScanner.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/store/impl/unsafe/UnsafeVariableLengthDimensionDataChunkStore.java
core/src/main/java/org/apache/carbondata/core/datastore/impl/FileReaderImpl.java
core/src/main/java/org/apache/carbondata/core/scan/expression/conditional/ConditionalExpression.java
core/src/main/java/org/apache/carbondata/core/datastore/filesystem/AlluxioCarbonFile.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/reader/measure/AbstractMeasureChunkReader.java
core/src/main/java/org/apache/carbondata/core/metadata/blocklet/datachunk/DataChunk.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/SchemaEvolution.java
core/src/main/java/org/apache/carbondata/core/scan/filter/GenericQueryType.java
core/src/main/java/org/apache/carbondata/core/datastore/filesystem/ViewFSCarbonFile.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/RowLevelRangeFilterResolverImpl.java
integration/spark-datasource/src/main/scala/org/apache/carbondata/spark/vectorreader/ColumnarVectorWrapper.java
core/src/main/java/org/apache/carbondata/core/util/DataFileFooterConverter2.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/impl/VariableLengthDimensionColumnPage.java
core/src/main/java/org/apache/carbondata/core/mutate/DeleteDeltaBlockDetails.java
processing/src/main/java/org/apache/carbondata/processing/datatypes/PrimitiveDataType.java
core/src/main/java/org/apache/carbondata/core/scan/executor/impl/QueryExecutorProperties.java
core/src/main/java/org/apache/carbondata/core/scan/collector/impl/DictionaryBasedResultCollector.java
core/src/main/java/org/apache/carbondata/core/metadata/blocklet/index/BlockletIndex.java
core/src/main/java/org/apache/carbondata/core/keygenerator/directdictionary/DirectDictionaryKeyGeneratorFactory.java
core/src/main/java/org/apache/carbondata/core/metadata/CarbonMetadata.java
core/src/main/java/org/apache/carbondata/core/stats/QueryStatisticsRecorderDummy.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/SchemaReader.java
core/src/main/java/org/apache/carbondata/core/scan/expression/Expression.java
hadoop/src/main/java/org/apache/carbondata/hadoop/util/CarbonInputFormatUtil.java
core/src/main/java/org/apache/carbondata/core/util/CarbonTimeStatisticsFactory.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/column/CarbonDimension.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelRangeGrtThanFiterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/scan/expression/conditional/NotEqualsExpression.java
core/src/main/java/org/apache/carbondata/core/scan/executor/infos/BlockExecutionInfo.java
core/src/main/java/org/apache/carbondata/core/fileoperations/FileWriteOperation.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/comparator/UnsafeRowComparator.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/LogicalFilterResolverImpl.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/store/DimensionChunkStoreFactory.java
core/src/main/java/org/apache/carbondata/core/scan/expression/conditional/LessThanExpression.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/mutate/data/BlockMappingVO.java
core/src/main/java/org/apache/carbondata/core/locks/AbstractCarbonLock.java
core/src/main/java/org/apache/carbondata/core/scan/expression/BinaryExpression.java
core/src/main/java/org/apache/carbondata/core/datastore/columnar/ColumnarKeyStoreDataHolder.java
core/src/main/java/org/apache/carbondata/core/datastore/impl/DFSFileReaderImpl.java
core/src/main/java/org/apache/carbondata/core/scan/result/iterator/RawResultIterator.java
core/src/main/java/org/apache/carbondata/core/locks/ZookeeperInit.java
core/src/main/java/org/apache/carbondata/core/metadata/CarbonTableIdentifier.java
processing/src/main/java/org/apache/carbondata/processing/loading/BadRecordsLogger.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/resolverinfo/visitor/FilterInfoTypeVisitorFactory.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/store/DimensionDataChunkStore.java
core/src/main/java/org/apache/carbondata/core/memory/CarbonUnsafe.java
processing/src/main/java/org/apache/carbondata/processing/loading/CarbonDataLoadConfiguration.java
core/src/main/java/org/apache/carbondata/core/util/AbstractDataFileFooterConverter.java
core/src/main/java/org/apache/carbondata/core/stats/QueryStatisticsModel.java
core/src/main/java/org/apache/carbondata/core/scan/expression/conditional/NotInExpression.java
processing/src/main/java/org/apache/carbondata/processing/merger/CarbonCompactionExecutor.java
core/src/main/java/org/apache/carbondata/core/mutate/CarbonUpdateUtil.java
processing/src/main/java/org/apache/carbondata/processing/store/writer/AbstractFactDataWriter.java
core/src/main/java/org/apache/carbondata/core/scan/complextypes/ComplexQueryType.java
processing/src/main/java/org/apache/carbondata/processing/merger/CarbonDataMergerUtil.java
core/src/main/java/org/apache/carbondata/core/keygenerator/directdictionary/timestamp/TimeStampDirectDictionaryGenerator.java
core/src/main/java/org/apache/carbondata/core/util/CarbonMetadataUtil.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/store/impl/unsafe/UnsafeAbstractDimensionDataChunkStore.java
core/src/main/java/org/apache/carbondata/core/keygenerator/directdictionary/timestamp/DateDirectDictionaryGenerator.java
core/src/main/java/org/apache/carbondata/core/scan/executor/QueryExecutor.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/resolverinfo/visitor/ResolvedFilterInfoVisitorIntf.java
processing/src/main/java/org/apache/carbondata/processing/loading/DataField.java
core/src/main/java/org/apache/carbondata/core/locks/LocalFileLock.java
core/src/main/java/org/apache/carbondata/core/scan/complextypes/PrimitiveQueryType.java
processing/src/main/java/org/apache/carbondata/processing/loading/converter/impl/FieldEncoderFactory.java
processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactDataHandlerColumnar.java
processing/src/main/java/org/apache/carbondata/processing/loading/partition/Partitioner.java
core/src/main/java/org/apache/carbondata/core/metadata/encoder/Encoding.java
core/src/main/java/org/apache/carbondata/core/scan/executor/impl/VectorDetailQueryExecutor.java
core/src/main/java/org/apache/carbondata/core/scan/expression/conditional/GreaterThanExpression.java
core/src/main/java/org/apache/carbondata/core/scan/filter/intf/FilterExecuterType.java
core/src/main/java/org/apache/carbondata/core/statusmanager/SegmentStatusManager.java
core/src/main/java/org/apache/carbondata/core/fileoperations/AtomicFileOperations.java
core/src/main/java/org/apache/carbondata/core/scan/complextypes/ArrayQueryType.java
core/src/main/java/org/apache/carbondata/core/locks/CarbonLockUtil.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/ConditionalFilterResolverImpl.java
core/src/main/java/org/apache/carbondata/core/statusmanager/LoadMetadataDetails.java
processing/src/main/java/org/apache/carbondata/processing/merger/NodeBlockRelation.java
core/src/main/java/org/apache/carbondata/core/locks/LockUsage.java
hadoop/src/main/java/org/apache/carbondata/hadoop/CarbonMultiBlockSplit.java
core/src/main/java/org/apache/carbondata/core/scan/expression/ColumnExpression.java
processing/src/main/java/org/apache/carbondata/processing/store/writer/CarbonFactDataWriter.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/CarbonTable.java
core/src/main/java/org/apache/carbondata/core/scan/filter/intf/RowIntf.java
hadoop/src/main/java/org/apache/carbondata/hadoop/readsupport/impl/DictionaryDecodeReadSupport.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/metadata/FilterResolverMetadata.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelRangeLessThanEqualFilterExecuterImpl.java
processing/src/main/java/org/apache/carbondata/processing/loading/converter/impl/DirectDictionaryFieldConverterImpl.java
core/src/main/java/org/apache/carbondata/core/scan/result/impl/FilterQueryScannedResult.java
core/src/main/java/org/apache/carbondata/core/scan/expression/logical/AndExpression.java
core/src/main/java/org/apache/carbondata/core/datastore/block/TableBlockUniqueIdentifier.java
hadoop/src/main/java/org/apache/carbondata/hadoop/CarbonRecordReader.java
core/src/main/java/org/apache/carbondata/core/scan/model/ProjectionMeasure.java
core/src/main/java/org/apache/carbondata/core/metadata/AbsoluteTableIdentifier.java
core/src/main/java/org/apache/carbondata/core/datastore/exception/IndexBuilderException.java
texts:Move and refine package in carbon-core module
As a step to refactor code to make it more modular, this PR move following package in carbon-core modulemove org.apache.carbondata.core.carbon to org.apache.carbondata.coremove org.apache.carbondata.common.ext to org.apache.carbondata.core.servicerename org.apache.carbondata.core.update to org.apache.carbondata.core.mutate and move org.apache.carbondata.common.iudprocessor.iuddata to org.apache.carbondata.core.mutate.datamove org.apache.carbondata.core.partition to org.apache.carbondata.processingmove org.apache.carbondata.fileoperation to org.apache.carbondata.core.atomicmove org.apache.carbondata.locks to org.apache.carbondata.core.updatestatus.locksmove CarbonDataLoadSchema to carbon-processingmove all Ideintifier class to org.apaceh.carbondata.core.metadatamove org.apache.carbondata.core.datastorage to org.apache.carbondata.core.datastore
issueID:CARBONDATA-639
type:Bug
changed files:
texts:"Delete data" feature doesn&#39;t work
1.Version:CarbonData is the latest master , Spark 1.6.22.Test Environment: spark-shell with command of ./bin/spark-shell --master local --jars ${carbondata_jar},${mysql_jar}3.Reproduce steps:1.Finish load data , query as below : cc.sql("select * from connectdemo1 where age > 40").show() :----------------------  namegenderprovincesinglerage----------------------AbbottFemale      AB  false 54AbbottFemale      AB  false 51AbbottFemale      AB  false 65AbbottFemale      AB  false 47AbbottFemale      AB  false 56AbbottFemale      AB  false 54AbbottFemale      AB  false 61AbbottFemale      AB  false 58AbbottFemale      AB  false 62AbbottFemale      AB  false 47AbbottFemale      AB  false 49AbbottFemale      AB  false 53AbbottFemale      AB  false 63AbbottFemale      AB  false 48AbbottFemale      AB  false 52AbbottFemale      AB  false 52AbbottFemale      AB  false 62AbbottFemale      AB  false 61AbbottFemale      AB  false 63AbbottFemale      AB  false 49----------------------cc.sql("select count from connectdemo1").show()-------    _c0-------9000000-------2.cc.sql("delete from connectdemo1 where age > 60").show3.query again with cc.sql("select * from connectdemo1 where age > 40").show() and cc.sql("select count from connectdemo1").show()  : same result with before deletion , so the delete operation is failed.----------------------  namegenderprovincesinglerage----------------------AbbottFemale      AB  false 54AbbottFemale      AB  false 51AbbottFemale      AB  false 65AbbottFemale      AB  false 47AbbottFemale      AB  false 56AbbottFemale      AB  false 54AbbottFemale      AB  false 61AbbottFemale      AB  false 58AbbottFemale      AB  false 62AbbottFemale      AB  false 47AbbottFemale      AB  false 49AbbottFemale      AB  false 53AbbottFemale      AB  false 63AbbottFemale      AB  false 48AbbottFemale      AB  false 52AbbottFemale      AB  false 52AbbottFemale      AB  false 62AbbottFemale      AB  false 61AbbottFemale      AB  false 63AbbottFemale      AB  false 49-----------------------------    _c0-------9000000-------
issueID:CARBONDATA-64
type:Bug
changed files:
texts:data mismatch between the carbon Table and Hive Table for data having empty lines
data mismatch between the carbon Table and Hive Table for data having empty linescreate table if not exists emptyRowCarbonTable (eid string,ename String,sal decimal,presal " +        "decimal,comm decimal" +        "(37,37),deptno decimal(18,2),Desc String) STORED BY 'org.apache.carbondata.format'LOAD DATA INPATH '$csvFilePath' INTO table emptyRowCarbonTable OPTIONS('DELIMITER'=',','QUOTECHAR'='"','FILEHEADER'='eid,ename,sal,presal,comm,deptno,Desc')1,2015-17-23 00:00:00,china,aaa1,phone197,ASD69643,15000.435252,\N,\N,\N,\N,\N,\N4,,d,,d,d,
issueID:CARBONDATA-640
type:Bug
changed files:
texts:Insert Query with Hardcoded values is not working
1)Creating table employees,Managerscreate table employees(name string, empid string, mgrid string, mobileno bigint) stored by 'carbondata';2)create table managers(name string, empid string, mgrid string, mobileno bigint) stored by 'carbondata';Insert into Select Queriesinsert into managers select 'harry','h2399','v788232',99823230205;Error Description:Error: org.apache.spark.sql.AnalysisException: Failed to recognize predicate '<EOF>'. Failed rule: 'regularBody' in statement; line 1 pos 65 (state=,code=0)
issueID:CARBONDATA-641
type:Bug
changed files:
texts:DICTIONARY_EXCLUDE is not working with &#39;DATE&#39; column
I am trying to create a table with "DICTIONARY_EXCLUDE" and this property is not working for "DATE" Data Type.Query :  CREATE TABLE uniqdata_date_dictionary (CUST_ID int,CUST_NAME string,ACTIVE_EMUI_VERSION string, DOB date, DOJ date, BIGINT_COLUMN1 bigint,BIGINT_COLUMN2 bigint,DECIMAL_COLUMN1 decimal(30,10), DECIMAL_COLUMN2 decimal(36,10),Double_COLUMN1 double, Double_COLUMN2 double,INTEGER_COLUMN1 int) STORED BY 'org.apache.carbondata.format' TBLPROPERTIES ("TABLE_BLOCKSIZE"= "256 MB","DICTIONARY_EXCLUDE"="DOB,DOJ");Expected Result : Table created.Actual Result : Error: org.apache.carbondata.spark.exception.MalformedCarbonCommandException: DICTIONARY_EXCLUDE is unsupported for date data type column: dob (state=,code=0)But is is working fine, If I use 'TIMESTAMP' in place of 'DATE'.
issueID:CARBONDATA-642
type:Bug
changed files:
texts:Delete Subquery is not working while creating and loading 2 tables
Create table uiqdataCREATE TABLE uniqdata (CUST_ID int,CUST_NAME String,ACTIVE_EMUI_VERSION string, DOB timestamp, DOJ timestamp, BIGINT_COLUMN1 bigint,BIGINT_COLUMN2 bigint,DECIMAL_COLUMN1 decimal(30,10), DECIMAL_COLUMN2 decimal(36,10),Double_COLUMN1 double, Double_COLUMN2 double,INTEGER_COLUMN1 int) STORED BY 'org.apache.carbondata.format' TBLPROPERTIES('DICTIONARY_INCLUDE'='BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,INTEGER_COLUMN1,CUST_ID');Create table uniqdata1CREATE TABLE uniqdata1 (CUST_ID int,CUST_NAME String,ACTIVE_EMUI_VERSION string, DOB timestamp, DOJ timestamp, BIGINT_COLUMN1 bigint,BIGINT_COLUMN2 bigint,DECIMAL_COLUMN1 decimal(30,10), DECIMAL_COLUMN2 decimal(36,10),Double_COLUMN1 double, Double_COLUMN2 double,INTEGER_COLUMN1 int) STORED BY 'org.apache.carbondata.format' TBLPROPERTIES('DICTIONARY_INCLUDE'='BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,INTEGER_COLUMN1,CUST_ID');Load the data into uniqdataLOAD DATA INPATH 'hdfs://hacluster/vyom/2000_UniqData.csv' into table uniqdata OPTIONS('DELIMITER'=',' , 'QUOTECHAR'='"','BAD_RECORDS_LOGGER_ENABLE'='TRUE', 'BAD_RECORDS_ACTION'='REDIRECT','FILEHEADER'='CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1,Double_COLUMN2,INTEGER_COLUMN1');Load the data into uniqdata1LOAD DATA INPATH 'hdfs://hacluster/vyom/2000_UniqData.csv' into table uniqdata1 OPTIONS('DELIMITER'=',' , 'QUOTECHAR'='"','BAD_RECORDS_LOGGER_ENABLE'='TRUE', 'BAD_RECORDS_ACTION'='REDIRECT','FILEHEADER'='CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1,Double_COLUMN2,INTEGER_COLUMN1');Delete Subquery with below errorError: org.apache.spark.sql.AnalysisException:Unsupported language features in query: select tupleId from uniqdata1 a where  a.CUST_ID in (Select b.CUST_ID from (Select c.CUST_ID from  uniqdata c ) b)TOK_QUERY 1, 0,45, 20  TOK_FROM 1, 4,8, 20    TOK_TABREF 1, 6,8, 20      TOK_TABNAME 1, 6,6, 20        uniqdata1 1, 6,6, 20      a 1, 8,8, 30  TOK_INSERT 0, -1,45, 0    TOK_DESTINATION 0, -1,-1, 0      TOK_DIR 0, -1,-1, 0        TOK_TMP_FILE 0, -1,-1, 0    TOK_SELECT 1, 0,2, 7      TOK_SELEXPR 1, 2,2, 7        TOK_TABLE_OR_COL 1, 2,2, 7          tupleId 1, 2,2, 7    TOK_WHERE 1, 10,45, 49      TOK_SUBQUERY_EXPR 1, 13,45, 49        TOK_SUBQUERY_OP 1, 17,17, 49          in 1, 17,17, 49        TOK_QUERY 1, 19,45, 99          TOK_FROM 1, 26,44, 99            TOK_SUBQUERY 1, 28,44, 99              TOK_QUERY 1, 29,40, 99                TOK_FROM 1, 35,40, 99                  TOK_TABREF 1, 38,40, 99                    TOK_TABNAME 1, 38,38, 99                      uniqdata 1, 38,38, 99                    c 1, 40,40, 108                TOK_INSERT 0, -1,33, 0                  TOK_DESTINATION 0, -1,-1, 0                    TOK_DIR 0, -1,-1, 0                      TOK_TMP_FILE 0, -1,-1, 0                  TOK_SELECT 1, 29,33, 84                    TOK_SELEXPR 1, 31,33, 84                      . 1, 31,33, 84                        TOK_TABLE_OR_COL 1, 31,31, 83                          c 1, 31,31, 83                        CUST_ID 1, 33,33, 85              b 1, 44,44, 112          TOK_INSERT 0, -1,24, 0            TOK_DESTINATION 0, -1,-1, 0              TOK_DIR 0, -1,-1, 0                TOK_TMP_FILE 0, -1,-1, 0            TOK_SELECT 1, 20,24, 61              TOK_SELEXPR 1, 22,24, 61Csv: 2000_UniqData.csv
issueID:CARBONDATA-643
type:Bug
changed files:
texts:When we are passing ALL_DICTIONARY_PATH&#39; in load query ,it is throwing null pointer exception.
When we are passing ALL_DICTIONARY_PATH' in load query ,it is throwing null pointer exception.//CREATE TABLECREATE TABLE uniq_include_dictionary (CUST_ID int,CUST_NAME String,ACTIVE_EMUI_VERSION string, DOB timestamp, DOJ timestamp, BIGINT_COLUMN1 bigint,BIGINT_COLUMN2 bigint,DECIMAL_COLUMN1 decimal(30,10), DECIMAL_COLUMN2 decimal(36,10),Double_COLUMN1 double, Double_COLUMN2 double,INTEGER_COLUMN1 int) STORED BY 'org.apache.carbondata.format' TBLPROPERTIES('DICTIONARY_INCLUDE'='CUST_ID,Double_COLUMN2,DECIMAL_COLUMN2');//LOAD QUERY LOAD DATA INPATH 'hdfs://localhost:54311/payal/7000_UniqData.csv' into table uniq_include_dictionary OPTIONS('DELIMITER'=',' , 'QUOTECHAR'='""','BAD_RECORDS_LOGGER_ENABLE'='TRUE', 'BAD_RECORDS_ACTION'='FORCE','FILEHEADER'='CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1,Double_COLUMN2,INTEGER_COLUMN1','SINGLE_PASS'='false','ALL_DICTIONARY_PATH'='hdfs://localhost:54311/opt/alldictionary/data.dictionary');Error: java.lang.NullPointerException (state=,code=0)LOGSINFO  16-01 16:56:37,624 - Running query 'LOAD DATA INPATH 'hdfs://localhost:54311/payal/7000_UniqData.csv' into table uniq_include_dictionary OPTIONS('DELIMITER'=',' , 'QUOTECHAR'='"','BAD_RECORDS_LOGGER_ENABLE'='TRUE', 'BAD_RECORDS_ACTION'='FORCE','FILEHEADER'='CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1,Double_COLUMN2,INTEGER_COLUMN1','SINGLE_PASS'='false','MULTILINE'='true','ALL_DICTIONARY_PATH'='hdfs://localhost:54311/opt/alldictionary/data.dictionary')' with 17ec3816-91e3-462e-aa9b-a7ae76405564INFO  16-01 16:56:37,625 - pool-26-thread-34 Query &#91;LOAD DATA INPATH &#39;HDFS://LOCALHOST:54311/PAYAL/7000_UNIQDATA.CSV&#39; INTO TABLE UNIQ_INCLUDE_DICTIONARY OPTIONS(&#39;DELIMITER&#39;=&#39;,&#39; , &#39;QUOTECHAR&#39;=&#39;"&#39;,&#39;BAD_RECORDS_LOGGER_ENABLE&#39;=&#39;TRUE&#39;, &#39;BAD_RECORDS_ACTION&#39;=&#39;FORCE&#39;,&#39;FILEHEADER&#39;=&#39;CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,DOUBLE_COLUMN1,DOUBLE_COLUMN2,INTEGER_COLUMN1&#39;,&#39;SINGLE_PASS&#39;=&#39;FALSE&#39;,&#39;MULTILINE&#39;=&#39;TRUE&#39;,&#39;ALL_DICTIONARY_PATH&#39;=&#39;HDFS://LOCALHOST:54311/OPT/ALLDICTIONARY/DATA.DICTIONARY&#39;)&#93;INFO  16-01 16:56:37,641 - Successfully able to get the table metadata file lockINFO  16-01 16:56:37,644 - pool-26-thread-34 Initiating Direct Load for the Table : (default.uniq_include_dictionary)INFO  16-01 16:56:37,644 - pool-26-thread-34 Generate global dictionary from dictionary files!ERROR 16-01 16:56:37,645 - pool-26-thread-34 Exception occured:File does not exist: hdfs://localhost:54311/opt/alldictionary/data.dictionaryERROR 16-01 16:56:37,645 - pool-26-thread-34 generate global dictionary failedjava.lang.NullPointerException    at org.apache.carbondata.core.datastorage.store.filesystem.AbstractDFSCarbonFile.getName(AbstractDFSCarbonFile.java:83)    at org.apache.carbondata.spark.util.GlobalDictionaryUtil$.validateAllDictionaryPath(GlobalDictionaryUtil.scala:649)    at org.apache.carbondata.spark.util.GlobalDictionaryUtil$.generateGlobalDictionary(GlobalDictionaryUtil.scala:743)    at org.apache.spark.sql.execution.command.LoadTable.run(carbonTableSchema.scala:569)    at org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult$lzycompute(commands.scala:58)    at org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult(commands.scala:56)    at org.apache.spark.sql.execution.ExecutedCommand.doExecute(commands.scala:70)    at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$5.apply(SparkPlan.scala:132)    at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$5.apply(SparkPlan.scala:130)    at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)    at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:130)    at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:55)    at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:55)    at org.apache.spark.sql.DataFrame.<init>(DataFrame.scala:145)    at org.apache.spark.sql.DataFrame.<init>(DataFrame.scala:130)    at org.apache.spark.sql.CarbonContext.sql(CarbonContext.scala:139)    at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:211)    at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1$$anon$2.run(SparkExecuteStatementOperation.scala:154)    at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1$$anon$2.run(SparkExecuteStatementOperation.scala:151)    at java.security.AccessController.doPrivileged(Native Method)    at javax.security.auth.Subject.doAs(Subject.java:422)    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)    at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1.run(SparkExecuteStatementOperation.scala:164)    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)    at java.util.concurrent.FutureTask.run(FutureTask.java:266)    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)    at java.lang.Thread.run(Thread.java:745)ERROR 16-01 16:56:37,645 - pool-26-thread-34 java.lang.NullPointerException    at org.apache.carbondata.core.datastorage.store.filesystem.AbstractDFSCarbonFile.getName(AbstractDFSCarbonFile.java:83)    at org.apache.carbondata.spark.util.GlobalDictionaryUtil$.validateAllDictionaryPath(GlobalDictionaryUtil.scala:649)    at org.apache.carbondata.spark.util.GlobalDictionaryUtil$.generateGlobalDictionary(GlobalDictionaryUtil.scala:743)    at org.apache.spark.sql.execution.command.LoadTable.run(carbonTableSchema.scala:569)    at org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult$lzycompute(commands.scala:58)    at org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult(commands.scala:56)    at org.apache.spark.sql.execution.ExecutedCommand.doExecute(commands.scala:70)    at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$5.apply(SparkPlan.scala:132)    at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$5.apply(SparkPlan.scala:130)    at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)    at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:130)    at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:55)    at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:55)    at org.apache.spark.sql.DataFrame.<init>(DataFrame.scala:145)    at org.apache.spark.sql.DataFrame.<init>(DataFrame.scala:130)    at org.apache.spark.sql.CarbonContext.sql(CarbonContext.scala:139)    at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:211)    at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1$$anon$2.run(SparkExecuteStatementOperation.scala:154)    at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1$$anon$2.run(SparkExecuteStatementOperation.scala:151)    at java.security.AccessController.doPrivileged(Native Method)    at javax.security.auth.Subject.doAs(Subject.java:422)    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)    at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1.run(SparkExecuteStatementOperation.scala:164)    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)    at java.util.concurrent.FutureTask.run(FutureTask.java:266)    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)    at java.lang.Thread.run(Thread.java:745)AUDIT 16-01 16:56:37,645 - &#91;sid-vostro-3546&#93;&#91;hduser&#93;&#91;Thread-263&#93;Dataload failure for default.uniq_include_dictionary. Please check the logsINFO  16-01 16:56:37,646 - pool-26-thread-34 Successfully deleted the lock file /tmp/default/uniq_include_dictionary/meta.lockINFO  16-01 16:56:37,646 - Table MetaData Unlocked Successfully after data loadERROR 16-01 16:56:37,646 - Error executing query, currentState RUNNING, java.lang.NullPointerException    at org.apache.carbondata.core.datastorage.store.filesystem.AbstractDFSCarbonFile.getName(AbstractDFSCarbonFile.java:83)    at org.apache.carbondata.spark.util.GlobalDictionaryUtil$.validateAllDictionaryPath(GlobalDictionaryUtil.scala:649)    at org.apache.carbondata.spark.util.GlobalDictionaryUtil$.generateGlobalDictionary(GlobalDictionaryUtil.scala:743)    at org.apache.spark.sql.execution.command.LoadTable.run(carbonTableSchema.scala:569)    at org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult$lzycompute(commands.scala:58)    at org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult(commands.scala:56)    at org.apache.spark.sql.execution.ExecutedCommand.doExecute(commands.scala:70)    at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$5.apply(SparkPlan.scala:132)    at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$5.apply(SparkPlan.scala:130)    at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)    at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:130)    at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:55)    at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:55)    at org.apache.spark.sql.DataFrame.<init>(DataFrame.scala:145)    at org.apache.spark.sql.DataFrame.<init>(DataFrame.scala:130)    at org.apache.spark.sql.CarbonContext.sql(CarbonContext.scala:139)    at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:211)    at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1$$anon$2.run(SparkExecuteStatementOperation.scala:154)    at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1$$anon$2.run(SparkExecuteStatementOperation.scala:151)    at java.security.AccessController.doPrivileged(Native Method)    at javax.security.auth.Subject.doAs(Subject.java:422)    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)    at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1.run(SparkExecuteStatementOperation.scala:164)    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)    at java.util.concurrent.FutureTask.run(FutureTask.java:266)    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)    at java.lang.Thread.run(Thread.java:745)ERROR 16-01 16:56:37,646 - Error running hive query: org.apache.hive.service.cli.HiveSQLException: java.lang.NullPointerException    at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:246)    at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1$$anon$2.run(SparkExecuteStatementOperation.scala:154)    at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1$$anon$2.run(SparkExecuteStatementOperation.scala:151)    at java.security.AccessController.doPrivileged(Native Method)    at javax.security.auth.Subject.doAs(Subject.java:422)    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)    at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1.run(SparkExecuteStatementOperation.scala:164)    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)    at java.util.concurrent.FutureTask.run(FutureTask.java:266)    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)    at java.lang.Thread.run(Thread.java:745)
issueID:CARBONDATA-644
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/datastore/impl/FileFactory.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
core/src/main/java/org/apache/carbondata/core/datastore/impl/DFSFileReaderImpl.java
core/src/main/java/org/apache/carbondata/core/metadata/AbsoluteTableIdentifier.java
texts:Select query fails randomly on spark shell
I created a carbon table and loaded it with data(csv attached) via spark shell by running the below command from $SPARK_HOME/bin :./spark-shelland then to get the carbon context :import org.apache.spark.sql.CarbonContext; val cc = new CarbonContext(sc); CREATE AND LOAD  COMMANDS:scala>cc.sql("CREATE TABLE connectdemo1 (name String, gender String, province String , singler String, age Int) STORED BY 'org.apache.carbondata.format'");scala>cc.sql("LOAD DATA inpath 'hdfs://localhost:54310/BabuStore/Data/uniqdata/connectdemo1.csv' INTO table connectdemo1 options('DELIMITER'=',','FILEHEADER'='name, gender, province, singler, age')");The above two operations were successful but when I tried to run the select query, it failed giving me the java.io.FileNotFoundException(see the log attached)scala> cc.sql("select * from connectdemo1").show();
issueID:CARBONDATA-645
type:Bug
changed files:
texts:Query Analysis exception while running Delete segment
Delete segment query does not work0: jdbc:hive2://172.168.100.212:23040> delete from table t_carbn01 where segment.id in (3.1);Error: org.apache.spark.sql.AnalysisException: Table not found: table; line 1 pos 26 (state=,code=0)0: jdbc:hive2://172.168.100.212:23040> show tables;------------------------ tableName   isTemporary  ------------------------ t_carbn01   false
issueID:CARBONDATA-646
type:Bug
changed files:
texts:Bad record handling is not correct for Int data type
With Bad record handling as default,If Char value is given for Int data type, that is handled properly(moving NULL).If Decimal values is given for Int Data type, it is stripping the decimal, where it should consider this as bad record and move NULL.Bad record csv:TRUE,2.7,423.0,A,200000000003454300, 121.5,4.99,2.44,SE3423ee,asfdsffdfg,EtryTRWT,2012-01-12 03:14:05.123456729,2012-01-200: jdbc:hive2://172.168.100.212:23040> select * from t_carbn01 where qty_total is NULL;---------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ active_status   item_type_cd   qty_day_avg   qty_total       sell_price       sell_pricep   discount_price   profit   item_code    item_name   outlet_name        update_time        create_date  ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ TRUE            2              423           NULL        200000000003454304   121.5         4.99             2.44     SE3423ee    asfdsffdfg   EtryTRWT      2012-01-12 03:14:05.0   2012-01-20   --------------------------------------------------------------------------------------------------------------------------------------------------------------+---------0: jdbc:hive2://172.168.100.212:23040> desc t_carbn01;--------------------------------------+    col_name        data_type    comment  --------------------------------------+ active_status    string          item_type_cd     bigint          qty_day_avg      bigint          qty_total        bigint          sell_price       bigint          sell_pricep      double          discount_price   double          profit           decimal(3,2)    item_code        string          item_name        string          outlet_name      string          update_time      timestamp       create_date      string         --------------------------------------+
issueID:CARBONDATA-647
type:Bug
changed files:
texts:Dictionary Lock  issue with load data through beeline.
I am executing below-mentioned query on beeline and it throws dictionary file is locked for updation. Dictionary file is random every time I create a new table followed by load. ExampleCREATE TABLE uni (CUST_ID int,CUST_NAME String,ACTIVE_EMUI_VERSION string, DOB timestamp, DOJ timestamp, BIGINT_COLUMN1 bigint,BIGINT_COLUMN2 bigint,DECIMAL_COLUMN1 decimal(30,10), DECIMAL_COLUMN2 decimal(36,10),Double_COLUMN1 double, Double_COLUMN2 double,INTEGER_COLUMN1 int) STORED BY 'org.apache.carbondata.format' ;LOAD DATA INPATH 'hdfs://hadoop-master:54311/payal/2000_UniqData.csv' into table uni OPTIONS('DELIMITER'=',' , 'QUOTECHAR'='"','BAD_RECORDS_ACTION'='FORCE','FILEHEADER'='CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1,Double_COLUMN2,INTEGER_COLUMN1') ;Error: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 7.0 failed 4 times, most recent failure: Lost task 0.3 in stage 7.0 (TID 47, hadoop-slave-3): java.lang.RuntimeException: Dictionary file cust_name is locked for updation. Please try after some time at scala.sys.package$.error(package.scala:27) at org.apache.carbondata.spark.rdd.CarbonGlobalDictionaryGenerateRDD$$anon$1.<init>(CarbonGlobalDictionaryRDD.scala:396) at org.apache.carbondata.spark.rdd.CarbonGlobalDictionaryGenerateRDD.compute(CarbonGlobalDictionaryRDD.scala:334) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306) at org.apache.spark.rdd.RDD.iterator(RDD.scala:270) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66) at org.apache.spark.scheduler.Task.run(Task.scala:89) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745)Driver stacktrace: (state=,code=0)
issueID:CARBONDATA-648
type:Bug
changed files:common/src/main/java/org/apache/carbondata/common/logging/impl/ExtendedRollingFileAppender.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/sort/TimSort.java
processing/src/main/java/org/apache/carbondata/processing/util/DeleteLoadFolders.java
core/src/main/java/org/apache/carbondata/core/scan/executor/exception/QueryExecutionException.java
core/src/main/java/org/apache/carbondata/core/datastore/exception/IndexBuilderException.java
texts:Code Clean Up
Code clean up :Remove 1. Author names and Ids2. DTS issue ids
issueID:CARBONDATA-649
type:Bug
changed files:
texts:Rand() function is not working while updating data
I am using update functionality with the rand(1) and rand() which return deterministic value or random value.But as I run query it gives error.Create Table : CREATE TABLE uniqdata (CUST_ID int,CUST_NAME String,ACTIVE_EMUI_VERSION string, DOB timestamp, DOJ timestamp, BIGINT_COLUMN1 bigint,BIGINT_COLUMN2 bigint,DECIMAL_COLUMN1 decimal(30,10), DECIMAL_COLUMN2 decimal(36,10),Double_COLUMN1 double, Double_COLUMN2 double,INTEGER_COLUMN1 int) STORED BY 'org.apache.carbondata.format' TBLPROPERTIES ("TABLE_BLOCKSIZE"= "256 MB");Load Data : LOAD DATA INPATH 'hdfs://localhost:54310/2000_UniqData.csv' into table uniqdata OPTIONS ('DELIMITER'=',' ,'QUOTECHAR'='""','BAD_RECORDS_ACTION'='FORCE','FILEHEADER'='CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1,Double_COLUMN2,INTEGER_COLUMN1','MAXCOLUMNS'='12');Query-1 : Update uniqdata  set (decimal_column1) = (rand());Query-2 : Update uniqdata  set (decimal_column1) = (rand(1));Expected Result : Update column with random value.Actual Result :Error: java.lang.RuntimeException: Update operation failed. Job aborted due to stage failure: Task 0 in stage 3.0 failed 4 times, most recent failure: Lost task 0.3 in stage 3.0 (TID 205, 192.168.2.140): java.lang.ArrayIndexOutOfBoundsException: 1I have attached screen shot of log, executor log and CSV with this.
issueID:CARBONDATA-65
type:Bug
changed files:
texts:Data load fails if there is space in the header names provided in FILEHEADER option in load command
Problem:1. When header is specified in the FILEHEADER option in data load command and if it contains spaces between header names separated by comma then data load fails.2. unwanted checks for check file header
issueID:CARBONDATA-650
type:Bug
changed files:
texts:Columns switching error in performing the string functions
We tried to execute the string function ASCII(COLUMN_NAME) on the Carbon Table in insert query with the following scenario, Create carbon tables as :CREATE TABLE target_uniqdata (CUST_ID int,CUST_NAME String,ACTIVE_EMUI_VERSION string, DOB timestamp, DOJ timestamp, BIGINT_COLUMN1 bigint,BIGINT_COLUMN2 bigint,DECIMAL_COLUMN1 decimal(30,10), DECIMAL_COLUMN2 decimal(36,10),Double_COLUMN1 double, Double_COLUMN2 double,INTEGER_COLUMN1 int) STORED BY 'org.apache.carbondata.format' TBLPROPERTIES ("TABLE_BLOCKSIZE"= "256 MB");CREATE TABLE source_uniqdata (CUST_ID int,CUST_NAME String,ACTIVE_EMUI_VERSION string, DOB timestamp, DOJ timestamp, BIGINT_COLUMN1 bigint,BIGINT_COLUMN2 bigint,DECIMAL_COLUMN1 decimal(30,10), DECIMAL_COLUMN2 decimal(36,10),Double_COLUMN1 double, Double_COLUMN2 double,INTEGER_COLUMN1 int) STORED BY 'org.apache.carbondata.format' TBLPROPERTIES ("TABLE_BLOCKSIZE"= "256 MB"); Load records into source carbon table as :LOAD DATA inpath 'hdfs://hadoop-master:54311/data/uniqdata/2000_UniqData.csv' INTO table source_uniqdata options('DELIMITER'=',', 'FILEHEADER'='CUST_ID, CUST_NAME, ACTIVE_EMUI_VERSION, DOB, DOJ, BIGINT_COLUMN1, BIGINT_COLUMN2, DECIMAL_COLUMN1, DECIMAL_COLUMN2, Double_COLUMN1, Double_COLUMN2, INTEGER_COLUMN1'); Insert the records of source table into target table and apply the string functioninsert into table target_uniqdata select CUST_ID, ASCII(CUST_NAME), ACTIVE_EMUI_VERSION, DOB, DOJ, BIGINT_COLUMN1, BIGINT_COLUMN2, DECIMAL_COLUMN1, DECIMAL_COLUMN2, Double_COLUMN1, Double_COLUMN2, INTEGER_COLUMN1 from source_uniqdata;Below is the order of columns in the source_uniqdata table,---------------------------------------------------------------------------------------------------------+    CUST_NAME         ACTIVE_EMUI_VERSION               DOB                     DOJ            CUST_ID   BIGINT_COLUMN1   BIGINT_COLUMN2       DECIMAL_COLUMN1          DECIMAL_COLUMN2         Double_COLUMN1        Double_COLUMN2      INTEGER_COLUMN1  ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- CUST_NAME_00987   ACTIVE_EMUI_VERSION_00987   1972-09-14 01:00:03.0   1972-09-14 02:00:03.0   9987      123372037841     -223372035867    12345679888.1234000000   22345679888.1234000000   1.12345674897976E10   -1.12345674897976E10   988               CUST_NAME_00988   ACTIVE_EMUI_VERSION_00988   1972-09-15 01:00:03.0   1972-09-15 02:00:03.0   9988      123372037842     -223372035866    12345679889.1234000000   22345679889.1234000000   1.12345674897976E10   -1.12345674897976E10   989               CUST_NAME_00989   ACTIVE_EMUI_VERSION_00989   1972-09-16 01:00:03.0   1972-09-16 02:00:03.0   9989      123372037843     -223372035865    12345679890.1234000000   22345679890.1234000000   1.12345674897976E10   -1.12345674897976E10   990               CUST_NAME_00990   ACTIVE_EMUI_VERSION_00990   1972-09-17 01:00:03.0   1972-09-17 02:00:03.0   9990      123372037844     -223372035864    12345679891.1234000000   22345679891.1234000000   1.12345674897976E10   -1.12345674897976E10   991               CUST_NAME_00991   ACTIVE_EMUI_VERSION_00991   1972-09-18 01:00:03.0   1972-09-18 02:00:03.0   9991      123372037845     -223372035863    12345679892.1234000000   22345679892.1234000000   1.12345674897976E10   -1.12345674897976E10   992               CUST_NAME_00992   ACTIVE_EMUI_VERSION_00992   1972-09-19 01:00:03.0   1972-09-19 02:00:03.0   9992      123372037846     -223372035862    12345679893.1234000000   22345679893.1234000000   1.12345674897976E10   -1.12345674897976E10   993               CUST_NAME_00993   ACTIVE_EMUI_VERSION_00993   1972-09-20 01:00:03.0   1972-09-20 02:00:03.0   9993      123372037847     -223372035861    12345679894.1234000000   22345679894.1234000000   1.12345674897976E10   -1.12345674897976E10   994               CUST_NAME_00994   ACTIVE_EMUI_VERSION_00994   1972-09-21 01:00:03.0   1972-09-21 02:00:03.0   9994      123372037848     -223372035860    12345679895.1234000000   22345679895.1234000000   1.12345674897976E10   -1.12345674897976E10   995               CUST_NAME_00995   ACTIVE_EMUI_VERSION_00995   1972-09-22 01:00:03.0   1972-09-22 02:00:03.0   9995      123372037849     -223372035859    12345679896.1234000000   22345679896.1234000000   1.12345674897976E10   -1.12345674897976E10   996               CUST_NAME_00996   ACTIVE_EMUI_VERSION_00996   1972-09-23 01:00:03.0   1972-09-23 02:00:03.0   9996      123372037850     -223372035858    12345679897.1234000000   22345679897.1234000000   1.12345674897976E10   -1.12345674897976E10   997               CUST_NAME_00997   ACTIVE_EMUI_VERSION_00997   1972-09-24 01:00:03.0   1972-09-24 02:00:03.0   9997      123372037851     -223372035857    12345679898.1234000000   22345679898.1234000000   1.12345674897976E10   -1.12345674897976E10   998               CUST_NAME_00998   ACTIVE_EMUI_VERSION_00998   1972-09-25 01:00:03.0   1972-09-25 02:00:03.0   9998      123372037852     -223372035856    12345679899.1234000000   22345679899.1234000000   1.12345674897976E10   -1.12345674897976E10   999               CUST_NAME_00999   ACTIVE_EMUI_VERSION_00999   1972-09-26 01:00:03.0   1972-09-26 02:00:03.0   9999      123372037853     -223372035855    12345679900.1234000000   22345679900.1234000000   1.12345674897976E10   -1.12345674897976E10   1000             ::::: AFTER ASCII OPERATION ::::SPARK 2.1::::::--------------------------------------------------------------------------------- CUST_NAME   ACTIVE_EMUI_VERSION    DOB            DOJ             CUST_ID    BIGINT_COLUMN1   BIGINT_COLUMN2       DECIMAL_COLUMN1          DECIMAL_COLUMN2         Double_COLUMN1        Double_COLUMN2      INTEGER_COLUMN1  ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- 9987        67                    NULL   1972-09-14 01:00:03.0   85264203    123372037841     -223372035867    12345679888.1234000000   22345679888.1234000000   1.12345674897976E10   -1.12345674897976E10   988               9988        67                    NULL   1972-09-15 01:00:03.0   85350603    123372037842     -223372035866    12345679889.1234000000   22345679889.1234000000   1.12345674897976E10   -1.12345674897976E10   989               9989        67                    NULL   1972-09-16 01:00:03.0   85437003    123372037843     -223372035865    12345679890.1234000000   22345679890.1234000000   1.12345674897976E10   -1.12345674897976E10   990               9990        67                    NULL   1972-09-17 01:00:03.0   85523403    123372037844     -223372035864    12345679891.1234000000   22345679891.1234000000   1.12345674897976E10   -1.12345674897976E10   991               9991        67                    NULL   1972-09-18 01:00:03.0   85609803    123372037845     -223372035863    12345679892.1234000000   22345679892.1234000000   1.12345674897976E10   -1.12345674897976E10   992               9992        67                    NULL   1972-09-19 01:00:03.0   85696203    123372037846     -223372035862    12345679893.1234000000   22345679893.1234000000   1.12345674897976E10   -1.12345674897976E10   993               9993        67                    NULL   1972-09-20 01:00:03.0   85782603    123372037847     -223372035861    12345679894.1234000000   22345679894.1234000000   1.12345674897976E10   -1.12345674897976E10   994               9994        67                    NULL   1972-09-21 01:00:03.0   85869003    123372037848     -223372035860    12345679895.1234000000   22345679895.1234000000   1.12345674897976E10   -1.12345674897976E10   995               9995        67                    NULL   1972-09-22 01:00:03.0   85955403    123372037849     -223372035859    12345679896.1234000000   22345679896.1234000000   1.12345674897976E10   -1.12345674897976E10   996               9996        67                    NULL   1972-09-23 01:00:03.0   86041803    123372037850     -223372035858    12345679897.1234000000   22345679897.1234000000   1.12345674897976E10   -1.12345674897976E10   997               9997        67                    NULL   1972-09-24 01:00:03.0   86128203    123372037851     -223372035857    12345679898.1234000000   22345679898.1234000000   1.12345674897976E10   -1.12345674897976E10   998               9998        67                    NULL   1972-09-25 01:00:03.0   86214603    123372037852     -223372035856    12345679899.1234000000   22345679899.1234000000   1.12345674897976E10   -1.12345674897976E10   999               9999        67                    NULL   1972-09-26 01:00:03.0   86301003    123372037853     -223372035855    12345679900.1234000000   22345679900.1234000000   1.12345674897976E10   -1.12345674897976E10   1000             ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------As you can see the values of the columns have been switched.::::: AFTER ASCII OPERATION ::::SPARK 1.6::::::------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- cust_id   cust_name      active_emui_version               dob                     doj            bigint_column1   bigint_column2       decimal_column1          decimal_column2         double_column1        double_column2      integer_column1  ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- 10801     67          ACTIVE_EMUI_VERSION_01801   1974-12-07 01:00:03.0   1974-12-07 02:00:03.0   123372038655     -223372035053    12345680702.1234000000   22345680702.1234000000   1.12345674897976E10   -1.12345674897976E10   1802              10802     67          ACTIVE_EMUI_VERSION_01802   1974-12-08 01:00:03.0   1974-12-08 02:00:03.0   123372038656     -223372035052    12345680703.1234000000   22345680703.1234000000   1.12345674897976E10   -1.12345674897976E10   1803              10803     67          ACTIVE_EMUI_VERSION_01803   1974-12-09 01:00:03.0   1974-12-09 02:00:03.0   123372038657     -223372035051    12345680704.1234000000   22345680704.1234000000   1.12345674897976E10   -1.12345674897976E10   1804              10804     67          ACTIVE_EMUI_VERSION_01804   1974-12-10 01:00:03.0   1974-12-10 02:00:03.0   123372038658     -223372035050    12345680705.1234000000   22345680705.1234000000   1.12345674897976E10   -1.12345674897976E10   1805              10805     67          ACTIVE_EMUI_VERSION_01805   1974-12-11 01:00:03.0   1974-12-11 02:00:03.0   123372038659     -223372035049    12345680706.1234000000   22345680706.1234000000   1.12345674897976E10   -1.12345674897976E10   1806              10806     67          ACTIVE_EMUI_VERSION_01806   1974-12-12 01:00:03.0   1974-12-12 02:00:03.0   123372038660     -223372035048    12345680707.1234000000   22345680707.1234000000   1.12345674897976E10   -1.12345674897976E10   1807              10807     67          ACTIVE_EMUI_VERSION_01807   1974-12-13 01:00:03.0   1974-12-13 02:00:03.0   123372038661     -223372035047    12345680708.1234000000   22345680708.1234000000   1.12345674897976E10   -1.12345674897976E10   1808              10808     67          ACTIVE_EMUI_VERSION_01808   1974-12-14 01:00:03.0   1974-12-14 02:00:03.0   123372038662     -223372035046    12345680709.1234000000   22345680709.1234000000   1.12345674897976E10   -1.12345674897976E10   1809              10809     67          ACTIVE_EMUI_VERSION_01809   1974-12-15 01:00:03.0   1974-12-15 02:00:03.0   123372038663     -223372035045    12345680710.1234000000   22345680710.1234000000   1.12345674897976E10   -1.12345674897976E10   1810              10810     67          ACTIVE_EMUI_VERSION_01810   1974-12-16 01:00:03.0   1974-12-16 02:00:03.0   123372038664     -223372035044    12345680711.1234000000   22345680711.1234000000   1.12345674897976E10   -1.12345674897976E10   1811              10811     67          ACTIVE_EMUI_VERSION_01811   1974-12-17 01:00:03.0   1974-12-17 02:00:03.0   123372038665     -223372035043    12345680712.1234000000   22345680712.1234000000   1.12345674897976E10   -1.12345674897976E10   1812              10812     67          ACTIVE_EMUI_VERSION_01812   1974-12-18 01:00:03.0   1974-12-18 02:00:03.0   123372038666     -223372035042    12345680713.1234000000   22345680713.1234000000   1.12345674897976E10   -1.12345674897976E10   1813              10813     67          ACTIVE_EMUI_VERSION_01813   1974-12-19 01:00:03.0   1974-12-19 02:00:03.0   123372038667     -223372035041    12345680714.1234000000   22345680714.1234000000   1.12345674897976E10   -1.12345674897976E10   1814             Here, its working in Spark 1.6
issueID:CARBONDATA-651
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/metadata/blocklet/BlockletInfo.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/TableInfo.java
core/src/main/java/org/apache/carbondata/core/datastore/columnar/UnBlockIndexer.java
core/src/main/java/org/apache/carbondata/core/util/DataFileFooterConverter.java
processing/src/main/java/org/apache/carbondata/processing/exception/MultipleMatchingException.java
processing/src/main/java/org/apache/carbondata/processing/loading/DataLoadExecutor.java
core/src/main/java/org/apache/carbondata/core/scan/collector/impl/RawBasedResultCollector.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelRangeGrtrThanEquaToFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/FilterResolverIntf.java
core/src/main/java/org/apache/carbondata/hadoop/internal/index/Block.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/SchemaEvolutionEntry.java
core/src/main/java/org/apache/carbondata/core/scan/expression/exception/FilterUnsupportedException.java
core/src/main/java/org/apache/carbondata/core/datastore/columnar/ColumnWithRowIdForHighCard.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/holder/UnsafeFinalMergePageHolder.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/BucketingInfo.java
core/src/main/java/org/apache/carbondata/core/datastore/TableSegmentUniqueIdentifier.java
core/src/main/java/org/apache/carbondata/core/datastore/block/BlockInfo.java
core/src/main/java/org/apache/carbondata/core/datastore/block/Distributable.java
core/src/main/java/org/apache/carbondata/core/locks/HdfsFileLock.java
core/src/main/java/org/apache/carbondata/core/util/ByteUtil.java
core/src/main/java/org/apache/carbondata/core/datastore/page/KeyColumnPage.java
core/src/main/java/org/apache/carbondata/core/devapi/BiDictionary.java
core/src/main/java/org/apache/carbondata/core/cache/dictionary/ColumnDictionaryInfo.java
core/src/main/java/org/apache/carbondata/core/scan/executor/impl/AbstractQueryExecutor.java
core/src/main/java/org/apache/carbondata/core/datastore/row/CarbonRow.java
processing/src/main/java/org/apache/carbondata/processing/util/CarbonLoaderUtil.java
processing/src/main/java/org/apache/carbondata/processing/loading/steps/InputProcessorStepImpl.java
core/src/main/java/org/apache/carbondata/core/stats/DriverQueryStatisticsRecorderDummy.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonTableOutputFormat.java
processing/src/main/java/org/apache/carbondata/processing/loading/steps/DataConverterProcessorStepImpl.java
core/src/main/java/org/apache/carbondata/core/scan/expression/LiteralExpression.java
processing/src/main/java/org/apache/carbondata/processing/util/CarbonDataProcessorUtil.java
core/src/main/java/org/apache/carbondata/core/scan/processor/BlockletIterator.java
common/src/main/java/org/apache/carbondata/common/CarbonIterator.java
core/src/main/java/org/apache/carbondata/core/datastore/IndexKey.java
core/src/main/java/org/apache/carbondata/core/scan/filter/ColumnFilterInfo.java
core/src/main/java/org/apache/carbondata/core/cache/dictionary/DictionaryColumnUniqueIdentifier.java
core/src/main/java/org/apache/carbondata/core/keygenerator/columnar/ColumnarSplitter.java
core/src/main/java/org/apache/carbondata/core/cache/dictionary/Dictionary.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/AndFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/scan/model/ProjectionColumn.java
core/src/main/java/org/apache/carbondata/core/util/DataTypeUtil.java
core/src/main/java/org/apache/carbondata/core/locks/ICarbonLock.java
core/src/main/java/org/apache/carbondata/core/scan/expression/ExpressionResult.java
core/src/main/java/org/apache/carbondata/core/keygenerator/directdictionary/DirectDictionaryGenerator.java
core/src/main/java/org/apache/carbondata/core/cache/dictionary/DictionaryInfo.java
core/src/main/java/org/apache/carbondata/core/datastore/block/BlockletInfos.java
core/src/main/java/org/apache/carbondata/core/datastore/filesystem/AbstractDFSCarbonFile.java
common/src/main/java/org/apache/carbondata/common/constants/LoggerAction.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/DimensionColumnPage.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/reader/MeasureColumnChunkReader.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/SortObserver.java
core/src/main/java/org/apache/carbondata/core/scan/result/iterator/DetailQueryResultIterator.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/comparator/UnsafeRowComparatorForNormalDIms.java
integration/spark2/src/main/java/org/apache/carbondata/spark/readsupport/SparkRowReadSupportImpl.java
processing/src/main/java/org/apache/carbondata/processing/sort/exception/CarbonSortKeyAndGroupByException.java
processing/src/main/java/org/apache/carbondata/processing/loading/dictionary/DirectDictionary.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/impl/ParallelReadMergeSorterImpl.java
processing/src/main/java/org/apache/carbondata/processing/csvload/BoundedInputStream.java
core/src/main/java/org/apache/carbondata/core/devapi/DictionaryGenerationException.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/impl/ParallelReadMergeSorterWithBucketingImpl.java
core/src/main/java/org/apache/carbondata/core/scan/filter/FilterExpressionProcessor.java
core/src/main/java/org/apache/carbondata/core/metadata/converter/SchemaConverter.java
core/src/main/java/org/apache/carbondata/core/scan/expression/conditional/LessThanEqualToExpression.java
processing/src/main/java/org/apache/carbondata/processing/loading/model/CarbonLoadModel.java
core/src/main/java/org/apache/carbondata/core/scan/result/vector/impl/CarbonColumnVectorImpl.java
processing/src/main/java/org/apache/carbondata/processing/loading/parser/RowParser.java
core/src/main/java/org/apache/carbondata/core/scan/executor/infos/MeasureInfo.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/impl/FixedLengthDimensionColumnPage.java
core/src/main/java/org/apache/carbondata/core/scan/collector/impl/AbstractScannedResultCollector.java
processing/src/main/java/org/apache/carbondata/processing/loading/AbstractDataLoadProcessorStep.java
core/src/main/java/org/apache/carbondata/core/keygenerator/mdkey/Bits.java
processing/src/main/java/org/apache/carbondata/processing/loading/parser/impl/RowParserImpl.java
core/src/main/java/org/apache/carbondata/core/scan/executor/exception/QueryExecutionException.java
core/src/main/java/org/apache/carbondata/core/datastore/impl/FileFactory.java
core/src/main/java/org/apache/carbondata/core/datastore/filesystem/LocalCarbonFile.java
core/src/main/java/org/apache/carbondata/core/datastore/compression/Compressor.java
core/src/main/java/org/apache/carbondata/core/scan/expression/conditional/GreaterThanEqualToExpression.java
core/src/main/java/org/apache/carbondata/core/scan/collector/impl/DictionaryBasedVectorResultCollector.java
core/src/main/java/org/apache/carbondata/core/writer/CarbonDeleteDeltaWriter.java
processing/src/main/java/org/apache/carbondata/processing/loading/converter/impl/ComplexFieldConverterImpl.java
core/src/main/java/org/apache/carbondata/core/metadata/converter/ThriftWrapperSchemaConverterImpl.java
core/src/main/java/org/apache/carbondata/core/stats/QueryStatisticsConstants.java
processing/src/main/java/org/apache/carbondata/processing/loading/converter/impl/AbstractDictionaryFieldConverterImpl.java
core/src/main/java/org/apache/carbondata/core/stats/QueryStatisticsRecorder.java
core/src/main/java/org/apache/carbondata/core/scan/expression/conditional/EqualToExpression.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/holder/SortTempChunkHolder.java
core/src/main/java/org/apache/carbondata/core/scan/scanner/impl/BlockletFilterScanner.java
hadoop/src/main/java/org/apache/carbondata/hadoop/CacheAccessClient.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/RowLevelFilterResolverImpl.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/store/impl/safe/SafeFixedLengthDimensionDataChunkStore.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/column/CarbonMeasure.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/resolverinfo/visitor/NoDictionaryTypeVisitor.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/store/impl/unsafe/UnsafeFixedLengthDimensionDataChunkStore.java
core/src/main/java/org/apache/carbondata/core/datastore/exception/CarbonDataWriterException.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/store/impl/safe/SafeAbsractDimensionDataChunkStore.java
core/src/main/java/org/apache/carbondata/core/util/path/CarbonTablePath.java
core/src/main/java/org/apache/carbondata/core/metadata/blocklet/index/BlockletMinMaxIndex.java
core/src/main/java/org/apache/carbondata/core/reader/CarbonFooterReader.java
core/src/main/java/org/apache/carbondata/core/scan/expression/LeafExpression.java
core/src/main/java/org/apache/carbondata/core/datastore/block/TableTaskInfo.java
core/src/main/java/org/apache/carbondata/core/scan/model/QueryModel.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/merger/UnsafeInMemoryIntermediateDataMerger.java
core/src/main/java/org/apache/carbondata/core/datastore/filesystem/HDFSCarbonFile.java
core/src/main/java/org/apache/carbondata/core/scan/collector/ScannedResultCollector.java
core/src/main/java/org/apache/carbondata/core/scan/expression/conditional/BinaryConditionalExpression.java
core/src/main/java/org/apache/carbondata/core/scan/wrappers/ByteArrayWrapper.java
core/src/main/java/org/apache/carbondata/core/stats/QueryStatisticsRecorderImpl.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelRangeTypeExecuterFactory.java
common/src/main/java/org/apache/carbondata/common/logging/LogService.java
core/src/main/java/org/apache/carbondata/core/scan/scanner/impl/BlockletFullScanner.java
core/src/main/java/org/apache/carbondata/core/datastore/compression/CompressorFactory.java
core/src/main/java/org/apache/carbondata/core/metadata/ColumnIdentifier.java
core/src/main/java/org/apache/carbondata/core/scan/expression/logical/FalseExpression.java
processing/src/main/java/org/apache/carbondata/processing/loading/parser/impl/StructParserImpl.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/DimColumnExecuterFilterInfo.java
processing/src/main/java/org/apache/carbondata/processing/loading/partition/impl/HashPartitionerImpl.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/column/ColumnSchema.java
core/src/main/java/org/apache/carbondata/core/keygenerator/mdkey/AbstractKeyGenerator.java
core/src/main/java/org/apache/carbondata/core/metadata/ValueEncoderMeta.java
core/src/main/java/org/apache/carbondata/core/scan/filter/intf/ExpressionType.java
core/src/main/java/org/apache/carbondata/core/reader/CarbonDeleteDeltaFileReader.java
core/src/main/java/org/apache/carbondata/core/scan/result/iterator/VectorDetailQueryResultIterator.java
processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactDataHandlerModel.java
core/src/main/java/org/apache/carbondata/core/stats/QueryStatistic.java
core/src/main/java/org/apache/carbondata/core/mutate/TupleIdEnum.java
core/src/main/java/org/apache/carbondata/core/datastore/FileReader.java
hadoop/src/main/java/org/apache/carbondata/hadoop/CarbonProjection.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/IncludeFilterExecuterImpl.java
processing/src/main/java/org/apache/carbondata/processing/loading/row/CarbonRowBatch.java
processing/src/main/java/org/apache/carbondata/processing/csvload/StringArrayWritable.java
processing/src/main/java/org/apache/carbondata/processing/loading/converter/impl/MeasureFieldConverterImpl.java
processing/src/main/java/org/apache/carbondata/processing/loading/converter/BadRecordLogHolder.java
core/src/main/java/org/apache/carbondata/core/cache/dictionary/DictionaryByteArrayWrapper.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/impl/AbstractDimensionColumnPage.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/SortDataRows.java
core/src/main/java/org/apache/carbondata/core/util/LoadStatistics.java
core/src/main/java/org/apache/carbondata/core/metadata/index/BlockIndexInfo.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/resolverinfo/MeasureColumnResolvedFilterInfo.java
core/src/main/java/org/apache/carbondata/core/metadata/datatype/DataType.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/SingleThreadFinalSortFilesMerger.java
core/src/main/java/org/apache/carbondata/core/scan/result/iterator/AbstractDetailQueryResultIterator.java
processing/src/main/java/org/apache/carbondata/processing/datatypes/GenericDataType.java
common/src/main/java/org/apache/carbondata/common/logging/impl/AuditLevel.java
core/src/main/java/org/apache/carbondata/core/scan/executor/util/QueryUtil.java
core/src/main/java/org/apache/carbondata/core/scan/expression/logical/OrExpression.java
processing/src/main/java/org/apache/carbondata/processing/loading/parser/CarbonParserFactory.java
core/src/main/java/org/apache/carbondata/core/keygenerator/factory/KeyGeneratorFactory.java
core/src/main/java/org/apache/carbondata/core/datastore/DataRefNode.java
core/src/main/java/org/apache/carbondata/core/scan/result/BlockletScannedResult.java
processing/src/main/java/org/apache/carbondata/processing/loading/FailureCauses.java
processing/src/main/java/org/apache/carbondata/processing/loading/parser/impl/ArrayParserImpl.java
core/src/main/java/org/apache/carbondata/core/util/ObjectSerializationUtil.java
core/src/main/java/org/apache/carbondata/core/keygenerator/mdkey/MultiDimKeyVarLengthGenerator.java
core/src/main/java/org/apache/carbondata/core/reader/CarbonIndexFileReader.java
core/src/main/java/org/apache/carbondata/core/cache/dictionary/ForwardDictionary.java
core/src/main/java/org/apache/carbondata/core/keygenerator/columnar/impl/MultiDimKeyVarLengthVariableSplitGenerator.java
core/src/main/java/org/apache/carbondata/core/locks/CarbonLockFactory.java
processing/src/main/java/org/apache/carbondata/processing/merger/NodeMultiBlockRelation.java
processing/src/main/java/org/apache/carbondata/processing/util/DeleteLoadFolders.java
core/src/main/java/org/apache/carbondata/core/scan/filter/FilterUtil.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/UnsafeSortDataRows.java
processing/src/main/java/org/apache/carbondata/processing/loading/converter/impl/NonDictionaryFieldConverterImpl.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/column/CarbonColumn.java
core/src/main/java/org/apache/carbondata/core/metadata/ColumnarFormatVersion.java
processing/src/main/java/org/apache/carbondata/processing/loading/parser/impl/PrimitiveParserImpl.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/UnsafeCarbonRowPage.java
core/src/main/java/org/apache/carbondata/core/memory/MemoryException.java
core/src/main/java/org/apache/carbondata/core/cache/CacheProvider.java
core/src/main/java/org/apache/carbondata/core/cache/dictionary/AbstractColumnDictionaryInfo.java
core/src/main/java/org/apache/carbondata/core/datastore/block/AbstractIndex.java
core/src/main/java/org/apache/carbondata/core/keygenerator/KeyGenException.java
core/src/main/java/org/apache/carbondata/core/datastore/block/TaskBlockInfo.java
common/src/main/java/org/apache/carbondata/common/logging/impl/StatisticLevel.java
processing/src/main/java/org/apache/carbondata/processing/util/CarbonQueryUtil.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/column/CarbonImplicitDimension.java
core/src/main/java/org/apache/carbondata/core/mutate/UpdateVO.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/resolverinfo/visitor/DictionaryColumnVisitor.java
core/src/main/java/org/apache/carbondata/core/scan/filter/FilterProcessor.java
processing/src/main/java/org/apache/carbondata/processing/datatypes/ArrayDataType.java
core/src/main/java/org/apache/carbondata/core/scan/result/vector/CarbonColumnVector.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/resolverinfo/visitor/CustomTypeDictionaryVisitor.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
core/src/main/java/org/apache/carbondata/core/datastore/compression/SnappyCompressor.java
core/src/main/java/org/apache/carbondata/core/reader/CarbonDictionaryMetadataReader.java
core/src/main/java/org/apache/carbondata/core/scan/executor/impl/DetailQueryExecutor.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/impl/UnsafeParallelReadMergeSorterImpl.java
core/src/main/java/org/apache/carbondata/core/scan/expression/UnknownExpression.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/IntermediateFileMerger.java
processing/src/main/java/org/apache/carbondata/processing/csvload/BlockDetails.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/SortParameters.java
processing/src/main/java/org/apache/carbondata/processing/merger/CarbonCompactionUtil.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/column/ColumnUniqueIdGenerator.java
processing/src/main/java/org/apache/carbondata/processing/loading/steps/DataWriterProcessorStepImpl.java
core/src/main/java/org/apache/carbondata/core/cache/CacheType.java
core/src/main/java/org/apache/carbondata/core/scan/result/iterator/ChunkRowIterator.java
core/src/main/java/org/apache/carbondata/core/cache/dictionary/DictionaryChunksWrapper.java
core/src/main/java/org/apache/carbondata/core/datastore/filesystem/CarbonFile.java
core/src/main/java/org/apache/carbondata/core/writer/CarbonDeleteDeltaWriterImpl.java
processing/src/main/java/org/apache/carbondata/processing/datatypes/StructDataType.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/resolverinfo/DimColumnResolvedFilterInfo.java
core/src/main/java/org/apache/carbondata/core/util/CarbonProperties.java
core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
core/src/main/java/org/apache/carbondata/core/service/impl/ColumnUniqueIdGenerator.java
core/src/main/java/org/apache/carbondata/core/scan/filter/intf/RowImpl.java
core/src/main/java/org/apache/carbondata/core/scan/model/ProjectionDimension.java
core/src/main/java/org/apache/carbondata/core/datastore/block/SegmentProperties.java
core/src/main/java/org/apache/carbondata/core/metadata/blocklet/SegmentInfo.java
core/src/main/java/org/apache/carbondata/core/reader/CarbonDeleteFilesDataReader.java
core/src/main/java/org/apache/carbondata/core/mutate/SegmentUpdateDetails.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/holder/UnsafeSortTempFileChunkHolder.java
processing/src/main/java/org/apache/carbondata/processing/loading/DataLoadProcessBuilder.java
core/src/main/java/org/apache/carbondata/core/datastore/exception/IndexBuilderException.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelRangeLessThanFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/reader/ThriftReader.java
core/src/main/java/org/apache/carbondata/core/datastore/columnar/ColumnarKeyStoreMetadata.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/TableSchema.java
core/src/main/java/org/apache/carbondata/core/reader/CarbonDeleteDeltaFileReaderImpl.java
processing/src/main/java/org/apache/carbondata/processing/loading/parser/ComplexParser.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/holder/UnsafeInmemoryHolder.java
core/src/main/java/org/apache/carbondata/core/stats/DriverQueryStatisticsRecorderImpl.java
core/src/main/java/org/apache/carbondata/core/service/ColumnUniqueIdService.java
processing/src/main/java/org/apache/carbondata/processing/merger/RowResultMergerProcessor.java
core/src/main/java/org/apache/carbondata/core/memory/IntPointerBuffer.java
core/src/main/java/org/apache/carbondata/core/mutate/DeleteDeltaBlockletDetails.java
core/src/main/java/org/apache/carbondata/core/util/DataFileFooterConverterFactory.java
processing/src/main/java/org/apache/carbondata/processing/loading/exception/BadRecordFoundException.java
core/src/main/java/org/apache/carbondata/core/writer/ThriftWriter.java
core/src/main/java/org/apache/carbondata/core/locks/ZooKeeperLocking.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/ExcludeFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/reader/CarbonDictionaryColumnMetaChunk.java
hadoop/src/main/java/org/apache/carbondata/hadoop/readsupport/CarbonReadSupport.java
processing/src/main/java/org/apache/carbondata/processing/csvload/CSVInputFormat.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/reader/DimensionColumnChunkReader.java
core/src/main/java/org/apache/carbondata/core/datastore/filesystem/CarbonFileFilter.java
core/src/main/java/org/apache/carbondata/core/scan/expression/logical/BinaryLogicalExpression.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/FilterExecuter.java
core/src/main/java/org/apache/carbondata/core/scan/expression/exception/FilterIllegalMemberException.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/reader/CarbonDataReaderFactory.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/OrFilterExecuterImpl.java
processing/src/main/java/org/apache/carbondata/processing/loading/model/CarbonDataLoadSchema.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonTableInputFormat.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/SortIntermediateFileMerger.java
core/src/main/java/org/apache/carbondata/core/metadata/blocklet/DataFileFooter.java
core/src/main/java/org/apache/carbondata/hadoop/CarbonInputSplit.java
core/src/main/java/org/apache/carbondata/core/scan/complextypes/StructQueryType.java
processing/src/main/java/org/apache/carbondata/processing/loading/converter/FieldConverter.java
core/src/main/java/org/apache/carbondata/core/scan/result/vector/MeasureDataVectorProcessor.java
core/src/main/java/org/apache/carbondata/core/scan/result/RowBatch.java
core/src/main/java/org/apache/carbondata/core/metadata/blocklet/index/BlockletBTreeIndex.java
core/src/main/java/org/apache/carbondata/core/scan/result/impl/NonFilterQueryScannedResult.java
core/src/main/java/org/apache/carbondata/core/scan/executor/QueryExecutorFactory.java
core/src/main/java/org/apache/carbondata/core/devapi/DictionaryGenerator.java
core/src/main/java/org/apache/carbondata/core/writer/CarbonIndexFileWriter.java
common/src/main/java/org/apache/carbondata/common/logging/impl/ExtendedRollingFileAppender.java
core/src/main/java/org/apache/carbondata/core/fileoperations/AtomicFileOperationsImpl.java
processing/src/main/java/org/apache/carbondata/processing/store/CarbonDataWriterFactory.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/store/impl/safe/SafeVariableLengthDimensionDataChunkStore.java
core/src/main/java/org/apache/carbondata/core/datastore/block/TableBlockInfo.java
core/src/main/java/org/apache/carbondata/core/scan/executor/util/RestructureUtil.java
core/src/main/java/org/apache/carbondata/core/scan/expression/conditional/ListExpression.java
core/src/main/java/org/apache/carbondata/core/scan/expression/conditional/InExpression.java
core/src/main/java/org/apache/carbondata/core/scan/result/vector/ColumnVectorInfo.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/NewRowComparatorForNormalDims.java
core/src/main/java/org/apache/carbondata/core/scan/scanner/BlockletScanner.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/store/impl/unsafe/UnsafeVariableLengthDimensionDataChunkStore.java
core/src/main/java/org/apache/carbondata/core/datastore/impl/FileReaderImpl.java
processing/src/main/java/org/apache/carbondata/processing/util/TableOptionConstant.java
core/src/main/java/org/apache/carbondata/core/scan/expression/conditional/ConditionalExpression.java
core/src/main/java/org/apache/carbondata/core/cache/CarbonLRUCache.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/sort/TimSort.java
core/src/main/java/org/apache/carbondata/core/datastore/filesystem/AlluxioCarbonFile.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/reader/measure/AbstractMeasureChunkReader.java
core/src/main/java/org/apache/carbondata/core/metadata/blocklet/datachunk/DataChunk.java
core/src/main/java/org/apache/carbondata/core/scan/filter/GenericQueryType.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/SchemaEvolution.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/SortTempFileChunkHolder.java
core/src/main/java/org/apache/carbondata/core/datastore/filesystem/ViewFSCarbonFile.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/RowLevelRangeFilterResolverImpl.java
integration/spark-datasource/src/main/scala/org/apache/carbondata/spark/vectorreader/ColumnarVectorWrapper.java
core/src/main/java/org/apache/carbondata/core/util/DataFileFooterConverter2.java
core/src/main/java/org/apache/carbondata/core/cache/Cacheable.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/impl/VariableLengthDimensionColumnPage.java
core/src/main/java/org/apache/carbondata/core/mutate/DeleteDeltaBlockDetails.java
processing/src/main/java/org/apache/carbondata/processing/datatypes/PrimitiveDataType.java
core/src/main/java/org/apache/carbondata/core/scan/executor/impl/QueryExecutorProperties.java
core/src/main/java/org/apache/carbondata/core/scan/collector/impl/DictionaryBasedResultCollector.java
core/src/main/java/org/apache/carbondata/core/metadata/blocklet/index/BlockletIndex.java
core/src/main/java/org/apache/carbondata/core/keygenerator/directdictionary/DirectDictionaryKeyGeneratorFactory.java
core/src/main/java/org/apache/carbondata/core/metadata/CarbonMetadata.java
processing/src/main/java/org/apache/carbondata/processing/loading/converter/impl/RowConverterImpl.java
processing/src/main/java/org/apache/carbondata/processing/loading/parser/GenericParser.java
core/src/main/java/org/apache/carbondata/core/stats/QueryStatisticsRecorderDummy.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/sort/UnsafeIntSortDataFormat.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/SchemaReader.java
core/src/main/java/org/apache/carbondata/core/scan/expression/Expression.java
hadoop/src/main/java/org/apache/carbondata/hadoop/util/CarbonInputFormatUtil.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/holder/UnsafeCarbonRowForMerge.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/column/CarbonDimension.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelRangeGrtThanFiterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/scan/expression/conditional/NotEqualsExpression.java
core/src/main/java/org/apache/carbondata/core/scan/executor/infos/BlockExecutionInfo.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/Sorter.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/merger/UnsafeIntermediateFileMerger.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/merger/UnsafeIntermediateMerger.java
core/src/main/java/org/apache/carbondata/core/fileoperations/FileWriteOperation.java
processing/src/main/java/org/apache/carbondata/processing/exception/DataLoadingException.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/comparator/UnsafeRowComparator.java
processing/src/main/java/org/apache/carbondata/processing/loading/complexobjects/ArrayObject.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/LogicalFilterResolverImpl.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/store/DimensionChunkStoreFactory.java
core/src/main/java/org/apache/carbondata/core/scan/expression/conditional/LessThanExpression.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelFilterExecuterImpl.java
processing/src/main/java/org/apache/carbondata/processing/util/NonDictionaryUtil.java
core/src/main/java/org/apache/carbondata/core/scan/expression/BinaryExpression.java
core/src/main/java/org/apache/carbondata/core/datastore/columnar/ColumnarKeyStoreDataHolder.java
core/src/main/java/org/apache/carbondata/core/datastore/impl/DFSFileReaderImpl.java
processing/src/main/java/org/apache/carbondata/processing/loading/constants/DataLoadProcessorConstants.java
core/src/main/java/org/apache/carbondata/core/scan/result/iterator/RawResultIterator.java
core/src/main/java/org/apache/carbondata/core/locks/ZookeeperInit.java
core/src/main/java/org/apache/carbondata/core/metadata/CarbonTableIdentifier.java
processing/src/main/java/org/apache/carbondata/processing/loading/BadRecordsLogger.java
core/src/main/java/org/apache/carbondata/core/keygenerator/directdictionary/timestamp/TimeStampGranularityTypeValue.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/resolverinfo/visitor/FilterInfoTypeVisitorFactory.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/store/DimensionDataChunkStore.java
core/src/main/java/org/apache/carbondata/core/memory/CarbonUnsafe.java
processing/src/main/java/org/apache/carbondata/processing/loading/CarbonDataLoadConfiguration.java
core/src/main/java/org/apache/carbondata/core/util/AbstractDataFileFooterConverter.java
common/src/main/java/org/apache/carbondata/common/logging/LogServiceFactory.java
core/src/main/java/org/apache/carbondata/core/stats/QueryStatisticsModel.java
core/src/main/java/org/apache/carbondata/core/scan/expression/conditional/NotInExpression.java
processing/src/main/java/org/apache/carbondata/processing/merger/CarbonCompactionExecutor.java
processing/src/main/java/org/apache/carbondata/processing/store/writer/AbstractFactDataWriter.java
processing/src/main/java/org/apache/carbondata/processing/csvload/CSVRecordReaderIterator.java
core/src/main/java/org/apache/carbondata/core/scan/complextypes/ComplexQueryType.java
processing/src/main/java/org/apache/carbondata/processing/loading/steps/SortProcessorStepImpl.java
core/src/main/java/org/apache/carbondata/core/keygenerator/KeyGenerator.java
processing/src/main/java/org/apache/carbondata/processing/exception/SliceMergerException.java
processing/src/main/java/org/apache/carbondata/processing/merger/CarbonDataMergerUtil.java
core/src/main/java/org/apache/carbondata/core/keygenerator/directdictionary/timestamp/TimeStampDirectDictionaryGenerator.java
core/src/main/java/org/apache/carbondata/core/util/CarbonMetadataUtil.java
processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactHandlerFactory.java
core/src/main/java/org/apache/carbondata/core/memory/UnsafeMemoryManager.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/store/impl/unsafe/UnsafeAbstractDimensionDataChunkStore.java
core/src/main/java/org/apache/carbondata/core/keygenerator/directdictionary/timestamp/DateDirectDictionaryGenerator.java
core/src/main/java/org/apache/carbondata/core/scan/executor/QueryExecutor.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/resolverinfo/visitor/ResolvedFilterInfoVisitorIntf.java
processing/src/main/java/org/apache/carbondata/processing/loading/DataField.java
core/src/main/java/org/apache/carbondata/core/locks/LocalFileLock.java
core/src/main/java/org/apache/carbondata/core/scan/complextypes/PrimitiveQueryType.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/merger/UnsafeSingleThreadFinalSortFilesMerger.java
processing/src/main/java/org/apache/carbondata/processing/loading/converter/impl/FieldEncoderFactory.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/NewRowComparator.java
common/src/main/java/org/apache/carbondata/common/exceptions/sql/MalformedCarbonCommandException.java
processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactDataHandlerColumnar.java
processing/src/main/java/org/apache/carbondata/processing/loading/partition/Partitioner.java
core/src/main/java/org/apache/carbondata/core/metadata/encoder/Encoding.java
core/src/main/java/org/apache/carbondata/core/scan/executor/impl/VectorDetailQueryExecutor.java
core/src/main/java/org/apache/carbondata/core/scan/expression/conditional/GreaterThanExpression.java
common/src/main/java/org/apache/carbondata/common/logging/impl/AuditExtendedRollingFileAppender.java
processing/src/main/java/org/apache/carbondata/processing/loading/exception/CarbonDataLoadingException.java
core/src/main/java/org/apache/carbondata/core/scan/filter/intf/FilterExecuterType.java
core/src/main/java/org/apache/carbondata/core/keygenerator/directdictionary/timestamp/TimeStampGranularityConstants.java
core/src/main/java/org/apache/carbondata/core/cache/Cache.java
core/src/main/java/org/apache/carbondata/core/statusmanager/SegmentStatusManager.java
core/src/main/java/org/apache/carbondata/core/fileoperations/AtomicFileOperations.java
core/src/main/java/org/apache/carbondata/core/scan/complextypes/ArrayQueryType.java
core/src/main/java/org/apache/carbondata/core/locks/CarbonLockUtil.java
core/src/main/java/org/apache/carbondata/core/statusmanager/LoadMetadataDetails.java
processing/src/main/java/org/apache/carbondata/processing/merger/NodeBlockRelation.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/ConditionalFilterResolverImpl.java
core/src/main/java/org/apache/carbondata/core/locks/LockUsage.java
processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactHandler.java
hadoop/src/main/java/org/apache/carbondata/hadoop/CarbonMultiBlockSplit.java
processing/src/main/java/org/apache/carbondata/processing/loading/complexobjects/StructObject.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/holder/UnsafeCarbonRow.java
processing/src/main/java/org/apache/carbondata/processing/store/writer/CarbonFactDataWriter.java
core/src/main/java/org/apache/carbondata/core/scan/expression/ColumnExpression.java
core/src/main/java/org/apache/carbondata/core/scan/filter/intf/RowIntf.java
hadoop/src/main/java/org/apache/carbondata/hadoop/readsupport/impl/DictionaryDecodeReadSupport.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/CarbonTable.java
processing/src/main/java/org/apache/carbondata/processing/loading/converter/RowConverter.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/metadata/FilterResolverMetadata.java
core/src/main/java/org/apache/carbondata/core/reader/CarbonDictionaryReader.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelRangeLessThanEqualFilterExecuterImpl.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/holder/UnsafeInmemoryMergeHolder.java
processing/src/main/java/org/apache/carbondata/processing/loading/converter/impl/DirectDictionaryFieldConverterImpl.java
core/src/main/java/org/apache/carbondata/core/keygenerator/columnar/impl/MultiDimKeyVarLengthEquiSplitGenerator.java
core/src/main/java/org/apache/carbondata/core/scan/result/impl/FilterQueryScannedResult.java
core/src/main/java/org/apache/carbondata/core/scan/expression/logical/AndExpression.java
processing/src/main/java/org/apache/carbondata/processing/merger/CompactionType.java
core/src/main/java/org/apache/carbondata/core/datastore/block/TableBlockUniqueIdentifier.java
hadoop/src/main/java/org/apache/carbondata/hadoop/CarbonRecordReader.java
core/src/main/java/org/apache/carbondata/core/scan/model/ProjectionMeasure.java
core/src/main/java/org/apache/carbondata/core/metadata/AbsoluteTableIdentifier.java
core/src/main/java/org/apache/carbondata/core/locks/AbstractCarbonLock.java
texts:Fix the license header of java file to be same with scala&#39;s

issueID:CARBONDATA-653
type:Bug
changed files:
texts:Select query displays wrong data for Decimal(38,38)
I tried to load data into a table having decimal(38,38) as a column. The data-load was successful, but when I displayed the data, there was some wrong data present in the table.Below are the queries:create table testDecimal(a decimal(38,38), b String) stored by 'carbondata';LOAD DATA INPATH 'hdfs://localhost:54311/testFiles/testMaxDigitsAfterDecimal.csv' into table testDecimal OPTIONS('DELIMITER'=',' , 'QUOTECHAR'='"','FILEHEADER'='a,b');select * from testDecimal;
issueID:CARBONDATA-654
type:Bug
changed files:
texts:Add data update and deletion example
Add data update and deletion example
issueID:CARBONDATA-655
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
texts:Make nokettle dataload flow as default in carbon
Make nokettle dataload flow as default in carbon
issueID:CARBONDATA-656
type:Improvement
changed files:
texts:Simplify the carbon session creation
Now it is cumbersome to create CarbonSession through spark shell. We should minimize steps to give more usability for first time users.
issueID:CARBONDATA-657
type:Bug
changed files:
texts:We are not able to create table with shared dictionary columns in spark 2.1
We are not able to create table with shared dictionary columns not working with spark-2.1 but  it is working fine with spark 1.6 spark 1.6 logs 0: jdbc:hive2://localhost:10000> CREATE TABLE uniq_shared_dictionary (CUST_ID int,CUST_NAME String,ACTIVE_EMUI_VERSION string, DOB timestamp, DOJ timestamp, BIGINT_COLUMN1 bigint,BIGINT_COLUMN2 bigint,DECIMAL_COLUMN1 decimal(30,10), DECIMAL_COLUMN2 decimal(36,10),Double_COLUMN1 double, Double_COLUMN2 double,INTEGER_COLUMN1 int) STORED BY 'org.apache.carbondata.format' TBLPROPERTIES('DICTIONARY_INCLUDE'='CUST_ID,Double_COLUMN2,DECIMAL_COLUMN2','columnproperties.CUST_ID.shared_column'='shared.CUST_ID','columnproperties.decimal_column2.shared_column'='shared.decimal_column2');---------+ Result  ---------+---------+in spark 2.1 logs -------0: jdbc:hive2://hadoop-master:10000> CREATE TABLE uniq_shared_dictionary (CUST_ID int,CUST_NAME String,ACTIVE_EMUI_VERSION string, DOB timestamp, DOJ timestamp, BIGINT_COLUMN1 bigint,BIGINT_COLUMN2 bigint,DECIMAL_COLUMN1 decimal(30,10), DECIMAL_COLUMN2 decimal(36,10),Double_COLUMN1 double, Double_COLUMN2 double,INTEGER_COLUMN1 int) STORED BY 'org.apache.carbondata.format' TBLPROPERTIES('DICTIONARY_INCLUDE'='CUST_ID,Double_COLUMN2,DECIMAL_COLUMN2','columnproperties.CUST_ID.shared_column'='shared.CUST_ID','columnproperties.decimal_column2.shared_column'='shared.decimal_column2');Error: org.apache.carbondata.spark.exception.MalformedCarbonCommandException: Invalid table properties columnproperties.cust_id.shared_column (state=,code=0)LOGSERROR 18-01 13:31:18,147 - Error executing query, currentState RUNNING, org.apache.carbondata.spark.exception.MalformedCarbonCommandException: Invalid table properties columnproperties.cust_id.shared_columnat org.apache.carbondata.spark.util.CommonUtil$$anonfun$validateTblProperties$1.apply(CommonUtil.scala:141)at org.apache.carbondata.spark.util.CommonUtil$$anonfun$validateTblProperties$1.apply(CommonUtil.scala:137)at scala.collection.Iterator$class.foreach(Iterator.scala:893)at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)at scala.con llection.AbstractIterable.foreach(Iterable.scala:54)at org.apache.carbondata.spark.util.CommonUtil$.validateTblProperties(CommonUtil.scala:137)at org.apache.spark.sql.parser.CarbonSqlAstBuilder.visitCreateTable(CarbonSparkSqlParser.scala:135)at org.apache.spark.sql.parser.CarbonSqlAstBuilder.visitCreateTable(CarbonSparkSqlParser.scala:60)at org.apache.spark.sql.catalyst.parser.SqlBaseParser$CreateTableContext.accept(SqlBaseParser.java:503)at org.antlr.v4.runtime.tree.AbstractParseTreeVisitor.visit(AbstractParseTreeVisitor.java:42)at org.apache.spark.sql.catalyst.parser.AstBuilder$$anonfun$visitSingleStatement$1.apply(AstBuilder.scala:66)at org.apache.spark.sql.catalyst.parser.AstBuilder$$anonfun$visitSingleStatement$1.apply(AstBuilder.scala:66)at org.apache.spark.sql.catalyst.parser.ParserUtils$.withOrigin(ParserUtils.scala:93)at org.apache.spark.sql.catalyst.parser.AstBuilder.visitSingleStatement(AstBuilder.scala:65)at org.apache.spark.sql.catalyst.parser.AbstractSqlParser$$anonfun$parsePlan$1.apply(ParseDriver.scala:54)at org.apache.spark.sql.catalyst.parser.AbstractSqlParser$$anonfun$parsePlan$1.apply(ParseDriver.scala:53)at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:82)at org.apache.spark.sql.parser.CarbonSparkSqlParser.parse(CarbonSparkSqlParser.scala:45)but if we give column name in lower case in spark 2.1 it works finespark 2.1CREATE TABLE uniq_shared_dictionary (cust_id int,CUST_NAME String,ACTIVE_EMUI_VERSION string, DOB timestamp, DOJ timestamp, BIGINT_COLUMN1 bigint,BIGINT_COLUMN2 bigint,DECIMAL_COLUMN1 decimal(30,10), decimal_column2 decimal(36,10),Double_COLUMN1 double, Double_COLUMN2 double,INTEGER_COLUMN1 int) STORED BY 'org.apache.carbondata.format' TBLPROPERTIES('DICTIONARY_INCLUDE'='CUST_ID,Double_COLUMN2,DECIMAL_COLUMN2','columnproperties.cust_id.shared_column'='shared.cust_id','columnproperties.decimal_column2.shared_column'='shared.decimal_column2');---------+ Result  ---------+---------+No rows selected (2.644 seconds)
issueID:CARBONDATA-658
type:Bug
changed files:
texts:Compression is not working for BigInt and Int datatype
I tried to load data into a table having bigInt as a column. Firstly I loaded small bigint values to the table and noted down the carbondata file size then I loaded max bigint values to the table and again noted the carbondata file size.For large bigint values the carbondata file size was 684.25 Kb and for small bigint values it was 684.26 Kb. So I could not figure out whether compression is performed or not.I tried the same scenario with int datatype as well. For large int values the carbondata file size was 684.24 Kb and for small int values it was 684.26 Kb.Below are the queries:For BigInt table:Create table test(a BigInt, b String) stored by 'carbondata';LOAD DATA INPATH 'hdfs://localhost:54311/testFiles/100000_LargeBigInt.csv' into table test OPTIONS('DELIMITER'=',' , 'QUOTECHAR'='"','FILEHEADER'='b,a');LOAD DATA INPATH 'hdfs://localhost:54311/testFiles/100000_SmallBigInt.csv' into table test OPTIONS('DELIMITER'=',' , 'QUOTECHAR'='"','FILEHEADER'='b,a');For Int table:Create table test(a Int, b String) stored by 'carbondata';LOAD DATA INPATH 'hdfs://localhost:54311/testFiles/100000_LargeInt.csv' into table test OPTIONS('DELIMITER'=',' , 'QUOTECHAR'='"','FILEHEADER'='b,a');LOAD DATA INPATH 'hdfs://localhost:54311/testFiles/100000_SmallInt.csv' into table test OPTIONS('DELIMITER'=',' , 'QUOTECHAR'='"','FILEHEADER'='b,a');
issueID:CARBONDATA-659
type:Task
changed files:processing/src/main/java/org/apache/carbondata/processing/merger/CarbonCompactionUtil.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/merger/UnsafeIntermediateFileMerger.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/IntermediateFileMerger.java
core/src/main/java/org/apache/carbondata/core/util/DataTypeUtil.java
core/src/main/java/org/apache/carbondata/core/scan/expression/ExpressionResult.java
processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactDataHandlerColumnar.java
core/src/main/java/org/apache/carbondata/core/datastore/block/BlockletInfos.java
processing/src/main/java/org/apache/carbondata/processing/loading/DataLoadExecutor.java
core/src/main/java/org/apache/carbondata/core/scan/result/iterator/RawResultIterator.java
core/src/main/java/org/apache/carbondata/core/util/path/CarbonTablePath.java
processing/src/main/java/org/apache/carbondata/processing/merger/CarbonDataMergerUtilResult.java
core/src/main/java/org/apache/carbondata/core/reader/CarbonDeleteFilesDataReader.java
integration/spark2/src/main/java/org/apache/carbondata/spark/readsupport/SparkRowReadSupportImpl.java
core/src/main/java/org/apache/carbondata/core/datastore/block/TableTaskInfo.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/impl/ParallelReadMergeSorterWithBucketingImpl.java
core/src/main/java/org/apache/carbondata/core/mutate/SegmentUpdateDetails.java
core/src/main/java/org/apache/carbondata/core/statusmanager/SegmentStatusManager.java
core/src/main/java/org/apache/carbondata/core/cache/CarbonLRUCache.java
core/src/main/java/org/apache/carbondata/core/keygenerator/columnar/impl/MultiDimKeyVarLengthVariableSplitGenerator.java
core/src/main/java/org/apache/carbondata/core/scan/filter/FilterExpressionProcessor.java
core/src/main/java/org/apache/carbondata/core/datastore/columnar/ColumnWithRowIdForHighCard.java
processing/src/main/java/org/apache/carbondata/processing/util/DeleteLoadFolders.java
processing/src/main/java/org/apache/carbondata/processing/loading/DataLoadProcessBuilder.java
core/src/main/java/org/apache/carbondata/core/scan/filter/FilterUtil.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelRangeLessThanFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/statusmanager/LoadMetadataDetails.java
core/src/main/java/org/apache/carbondata/core/datastore/TableSegmentUniqueIdentifier.java
core/src/main/java/org/apache/carbondata/core/locks/HdfsFileLock.java
core/src/main/java/org/apache/carbondata/core/stats/DriverQueryStatisticsRecorderImpl.java
processing/src/main/java/org/apache/carbondata/processing/merger/RowResultMergerProcessor.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/CarbonTable.java
core/src/main/java/org/apache/carbondata/core/mutate/CarbonUpdateUtil.java
core/src/main/java/org/apache/carbondata/core/scan/collector/impl/DictionaryBasedResultCollector.java
core/src/main/java/org/apache/carbondata/core/cache/CacheProvider.java
core/src/main/java/org/apache/carbondata/core/scan/executor/impl/AbstractQueryExecutor.java
processing/src/main/java/org/apache/carbondata/processing/loading/partition/impl/HashPartitionerImpl.java
processing/src/main/java/org/apache/carbondata/processing/util/CarbonLoaderUtil.java
core/src/main/java/org/apache/carbondata/core/datastore/block/TaskBlockInfo.java
processing/src/main/java/org/apache/carbondata/processing/loading/converter/impl/RowConverterImpl.java
core/src/main/java/org/apache/carbondata/core/stats/DriverQueryStatisticsRecorderDummy.java
core/src/main/java/org/apache/carbondata/core/keygenerator/columnar/impl/MultiDimKeyVarLengthEquiSplitGenerator.java
core/src/main/java/org/apache/carbondata/core/mutate/TupleIdEnum.java
processing/src/main/java/org/apache/carbondata/processing/merger/CarbonDataMergerUtil.java
processing/src/main/java/org/apache/carbondata/processing/util/CarbonDataProcessorUtil.java
core/src/main/java/org/apache/carbondata/core/keygenerator/directdictionary/timestamp/TimeStampDirectDictionaryGenerator.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonTableInputFormat.java
core/src/main/java/org/apache/carbondata/core/memory/UnsafeMemoryManager.java
hadoop/src/main/java/org/apache/carbondata/hadoop/CarbonRecordReader.java
core/src/main/java/org/apache/carbondata/core/scan/complextypes/StructQueryType.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelRangeGrtThanFiterExecuterImpl.java
processing/src/main/java/org/apache/carbondata/processing/loading/converter/impl/MeasureFieldConverterImpl.java
processing/src/main/java/org/apache/carbondata/processing/loading/converter/impl/ComplexFieldConverterImpl.java
texts:Should add WhitespaceAround and ParenPad to javastyle

issueID:CARBONDATA-66
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/scan/expression/conditional/NotInExpression.java
core/src/main/java/org/apache/carbondata/core/keygenerator/directdictionary/timestamp/TimeStampDirectDictionaryGenerator.java
core/src/main/java/org/carbondata/scan/filter/FilterUtil.java
core/src/main/java/org/apache/carbondata/core/scan/expression/conditional/InExpression.java
core/src/main/java/org/carbondata/scan/filter/FilterExpressionProcessor.java
core/src/main/java/org/carbondata/scan/expression/ExpressionResult.java
texts:Filter was failing when join condition is been applied between two tables
Filter was failing when join condition is been applied between two tables in case of timestamp,bigint and big decimal data type columns.Query Sample:select b.* from big_int_basicc a join big_int_basic11 b on a.productdate=b.productdate").show()
issueID:CARBONDATA-661
type:Sub-task
changed files:core/src/main/java/org/apache/carbondata/core/reader/CarbonDeleteDeltaFileReaderImpl.java
core/src/main/java/org/apache/carbondata/core/keygenerator/columnar/impl/MultiDimKeyVarLengthEquiSplitGenerator.java
core/src/main/java/org/apache/carbondata/core/scan/result/impl/NonFilterQueryScannedResult.java
core/src/main/java/org/apache/carbondata/core/scan/result/BlockletScannedResult.java
core/src/main/java/org/apache/carbondata/core/scan/result/impl/FilterQueryScannedResult.java
core/src/main/java/org/apache/carbondata/core/scan/expression/ExpressionResult.java
texts:misc cleanup in carbon core
cleanup un-exercised code/field/functions as well as minor code improvement.
issueID:CARBONDATA-662
type:Bug
changed files:
texts:Minor Compaction results of count of individual column is incorrect
Carbon_Properties:carbon.allowed.compaction.days = 2carbon.enable.auto.load.merge = truecarbon.compaction.level.threshold = 4,3carbon.numberof.preserve.segments = 2carbon.sort.file.buffer.size = 20carbon.leaf.node.size = 120000max.query.execution.time = 60carbon.number.of.cores.while.loading = 6carbon.number.of.cores = 4carbon.sort.size = 500000Steps for compaction :1)Create table uniqdata:CREATE TABLE uniqdata (CUST_ID int,CUST_NAME String,ACTIVE_EMUI_VERSION string, DOB timestamp, DOJ timestamp, BIGINT_COLUMN1 bigint,BIGINT_COLUMN2 bigint,DECIMAL_COLUMN1 decimal(30,10), DECIMAL_COLUMN2 decimal(36,10),Double_COLUMN1 double, Double_COLUMN2 double,INTEGER_COLUMN1 int) STORED BY 'org.apache.carbondata.format';2)load the Uniqdata:LOAD DATA INPATH 'hdfs://hacluster/vyom/2000_UniqData.csv' into table uniqdata OPTIONS('DELIMITER'=',' , 'QUOTECHAR'='"','BAD_RECORDS_ACTION'='FORCE','FILEHEADER'='CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1,Double_COLUMN2,INTEGER_COLUMN1');3)Select the count of cust_name by filtering Cust_id0: jdbc:hive2://172.168.100.212:23040> select count( (CUST_NAME)) from uniqdata where CUST_ID=9000 ;------+ _c0  ------+ 1    ------+4)load the Uniqdata:LOAD DATA INPATH 'hdfs://hacluster/vyom/2000_UniqData.csv' into table uniqdata OPTIONS('DELIMITER'=',' , 'QUOTECHAR'='"','BAD_RECORDS_ACTION'='FORCE','FILEHEADER'='CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1,Double_COLUMN2,INTEGER_COLUMN1');5)Select 0: jdbc:hive2://172.168.100.212:23040> select count( (CUST_NAME)) from uniqdata where CUST_ID=9000 ;------+ _c0  ------+ 2    ------+6)Load into uniqdata0: jdbc:hive2://172.168.100.212:23040> LOAD DATA INPATH 'hdfs://hacluster/vyom/2000_UniqData.csv' into table uniqdata OPTIONS('DELIMITER'=',' , 'QUOTECHAR'='"','BAD_RECORDS_ACTION'='FORCE','FILEHEADER'='CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1,Double_COLUMN2,INTEGER_COLUMN1');---------+ Result  ---------+---------+No rows selected (0.327 seconds)7)0: jdbc:hive2://172.168.100.212:23040> select count( (CUST_NAME)) from uniqdata where CUST_ID=9000 ;------+ _c0  ------+ 3    ------+8)0: jdbc:hive2://172.168.100.212:23040> LOAD DATA INPATH 'hdfs://hacluster/vyom/2000_UniqData.csv' into table uniqdata OPTIONS('DELIMITER'=',' , 'QUOTECHAR'='"','BAD_RECORDS_ACTION'='FORCE','FILEHEADER'='CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1,Double_COLUMN2,INTEGER_COLUMN1');---------+ Result  ---------+---------+9)Select the Count0: jdbc:hive2://172.168.100.212:23040> select count( (CUST_NAME)) from uniqdata where CUST_ID=9000 ;------+ _c0  ------+ 4    ------+1 row selected (5.194 seconds)10)Load the data0: jdbc:hive2://172.168.100.212:23040> LOAD DATA INPATH 'hdfs://hacluster/vyom/2000_UniqData.csv' into table uniqdata OPTIONS('DELIMITER'=',' , 'QUOTECHAR'='"','BAD_RECORDS_ACTION'='FORCE','FILEHEADER'='CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1,Double_COLUMN2,INTEGER_COLUMN1');---------+ Result  ---------+---------+No rows selected (0.342 seconds)11)Select into  cust_name0: jdbc:hive2://172.168.100.212:23040> select count( (CUST_NAME)) from uniqdata where CUST_ID=9000 ;------+ _c0  ------+ 5    ------+1 row selected (5.19 seconds)12)Show Segments for table uniqdata;------------------------------------------------------------------------------------ SegmentSequenceId        Status           Load Start Time            Load End Time       ------------------------------------------------------------------------------------ 5                   Partial Success   2017-01-19 16:33:14.006   2017-01-19 16:33:14.186   4                   Partial Success   2017-01-19 16:32:49.484   2017-01-19 16:32:49.691   3                   Compacted         2017-01-19 16:32:41.044   2017-01-19 16:32:41.252   2                   Compacted         2017-01-19 16:32:36.952   2017-01-19 16:32:37.159   1                   Compacted         2017-01-19 16:32:33.251   2017-01-19 16:32:33.447   0.1                 Success           2017-01-19 16:33:14.205   2017-01-19 16:33:14.531   0                   Compacted         2017-01-19 16:32:08.457   2017-01-19 16:32:08.657  ------------------------------------------------------------------------------------13)Select the cust_name by Cust_id ideally Count should be 6 but it was coming as 140: jdbc:hive2://172.168.100.212:23040> select count( (CUST_NAME)) from uniqdata where CUST_ID=9000 ;------+ _c0  ------+ 14   ------+
issueID:CARBONDATA-663
type:Bug
changed files:
texts:Major compaction is not working properly as per the configuration
I have set property carbon.major.compaction.size= 3 and load data which is the size of 5 MB and when I perform compaction it compacted, but initially it shouldn't be perform. Here is the queries :create table : create table test_major_compaction(id Int,name string)stored by 'carbondata';Load Data : Load two segments.LOAD DATA inpath 'hdfs://localhost:54310/sample_str_more1.csv' INTO table test_major_compaction options('DELIMITER'=',', 'FILEHEADER'='id, name','QUOTECHAR'='"');Show segments : show segments for table test_major_compaction;Alter Table : ALTER TABLE test_major_compaction COMPACT 'MAJOR';Show segments : Again see the segments :show segments for table test_major_compaction;I have attached all the data with the it.
issueID:CARBONDATA-664
type:Bug
changed files:
texts:Select queries fail when BAD_RECORDS_ACTION as FORCED is used in load query.
Below scenario is working on Spark 2.1, but not on Spark 1.6create table VMALL_DICTIONARY_INCLUDE (imei string,deviceInformationId int,MAC string,deviceColor string,device_backColor string,modelId string,marketName string,AMSize string,ROMSize string,CUPAudit string,CPIClocked string,series string,productionDate timestamp,bomCode string,internalModels string, deliveryTime string, channelsId string, channelsName string , deliveryAreaId string, deliveryCountry string, deliveryProvince string, deliveryCity string,deliveryDistrict string, deliveryStreet string, oxSingleNumber string, ActiveCheckTime string, ActiveAreaId string, ActiveCountry string, ActiveProvince string, Activecity string, ActiveDistrict string, ActiveStreet string, ActiveOperatorId string, Active_releaseId string, Active_EMUIVersion string, Active_operaSysVersion string, Active_BacVerNumber string, Active_BacFlashVer string, Active_webUIVersion string, Active_webUITypeCarrVer string,Active_webTypeDataVerNumber string, Active_operatorsVersion string, Active_phonePADPartitionedVersions string, Latest_YEAR int, Latest_MONTH int, Latest_DAY Decimal(30,10), Latest_HOUR string, Latest_areaId string, Latest_country string, Latest_province string, Latest_city string, Latest_district string, Latest_street string, Latest_releaseId string, Latest_EMUIVersion string, Latest_operaSysVersion string, Latest_BacVerNumber string, Latest_BacFlashVer string, Latest_webUIVersion string, Latest_webUITypeCarrVer string, Latest_webTypeDataVerNumber string, Latest_operatorsVersion string, Latest_phonePADPartitionedVersions string, Latest_operatorId string, gamePointDescription string,gamePointId double,contractNumber BigInt) STORED BY 'org.apache.carbondata.format' TBLPROPERTIES('DICTIONARY_INCLUDE'='imei,deviceInformationId,productionDate,gamePointId,Latest_DAY,contractNumber');LOAD DATA INPATH 'hdfs://hadoop-master:54311/data/100_olap_C20.csv' INTO table VMALL_DICTIONARY_INCLUDE options('DELIMITER'=',','QUOTECHAR'='"','BAD_RECORDS_ACTION'='FORCE','FILEHEADER'='imei,deviceInformationId,MAC,deviceColor,device_backColor,modelId,marketName,AMSize,ROMSize,CUPAudit,CPIClocked,series,productionDate,bomCode,internalModels,deliveryTime,channelsId,channelsName,deliveryAreaId,deliveryCountry,deliveryProvince,deliveryCity,deliveryDistrict,deliveryStreet,oxSingleNumber,contractNumber,ActiveCheckTime,ActiveAreaId,ActiveCountry,ActiveProvince,Activecity,ActiveDistrict,ActiveStreet,ActiveOperatorId,Active_releaseId,Active_EMUIVersion,Active_operaSysVersion,Active_BacVerNumber,Active_BacFlashVer,Active_webUIVersion,Active_webUITypeCarrVer,Active_webTypeDataVerNumber,Active_operatorsVersion,Active_phonePADPartitionedVersions,Latest_YEAR,Latest_MONTH,Latest_DAY,Latest_HOUR,Latest_areaId,Latest_country,Latest_province,Latest_city,Latest_district,Latest_street,Latest_releaseId,Latest_EMUIVersion,Latest_operaSysVersion,Latest_BacVerNumber,Latest_BacFlashVer,Latest_webUIVersion,Latest_webUITypeCarrVer,Latest_webTypeDataVerNumber,Latest_operatorsVersion,Latest_phonePADPartitionedVersions,Latest_operatorId,gamePointId,gamePointDescription');select sum(deviceinformationId) from VMALL_DICTIONARY_INCLUDE where deviceColor ='5Device Color' and modelId != '109' or Latest_DAY > '1234567890123540.0000000000' and contractNumber == '92233720368547800' or Active_operaSysVersion like 'Operating System Version' and gamePointId <=> '8.1366141918611E39' and deviceInformationId < '1000000' and productionDate not like '2016-07-01' and imei is null and Latest_HOUR is not null and channelsId <= '7' and Latest_releaseId >= '1' and Latest_MONTH between 6 and 8 and Latest_YEAR not between 2016 and 2017 and Latest_HOUR RLIKE '12' and gamePointDescription REGEXP 'Site' and imei in ('1AA1','1AA100','1AA10','1AA1000','1AA10000','1AA100000','1AA1000000','1AA100001','1AA100002','1AA100004','','NULL') and Active_BacVerNumber not in ('Background version number1','','null');This scenario results in the following exception,Error: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 48.0 failed 4 times, most recent failure: Lost task 0.3 in stage 48.0 (TID 152, hadoop-master): java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.NullPointerException at org.apache.carbondata.scan.result.iterator.DetailQueryResultIterator.next(DetailQueryResultIterator.java:65) at org.apache.carbondata.scan.result.iterator.DetailQueryResultIterator.next(DetailQueryResultIterator.java:35) at org.apache.carbondata.scan.result.iterator.ChunkRowIterator.<init>(ChunkRowIterator.java:43) at org.apache.carbondata.hadoop.CarbonRecordReader.initialize(CarbonRecordReader.java:81) at org.apache.carbondata.spark.rdd.CarbonScanRDD.compute(CarbonScanRDD.scala:194) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306) at org.apache.spark.rdd.RDD.iterator(RDD.scala:270) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306) at org.apache.spark.rdd.RDD.iterator(RDD.scala:270) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306) at org.apache.spark.rdd.RDD.iterator(RDD.scala:270) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306) at org.apache.spark.rdd.RDD.iterator(RDD.scala:270) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306) at org.apache.spark.rdd.RDD.iterator(RDD.scala:270) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306) at org.apache.spark.rdd.RDD.iterator(RDD.scala:270) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306) at org.apache.spark.rdd.RDD.iterator(RDD.scala:270) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306) at org.apache.spark.rdd.RDD.iterator(RDD.scala:270) at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73) at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41) at org.apache.spark.scheduler.Task.run(Task.scala:89) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745)Caused by: java.util.concurrent.ExecutionException: java.lang.NullPointerException at java.util.concurrent.FutureTask.report(FutureTask.java:122) at java.util.concurrent.FutureTask.get(FutureTask.java:192) at org.apache.carbondata.scan.result.iterator.DetailQueryResultIterator.next(DetailQueryResultIterator.java:52) ... 34 moreCaused by: java.lang.NullPointerException at org.apache.carbondata.scan.result.AbstractScannedResult.getDictionaryKeyIntegerArray(AbstractScannedResult.java:187) at org.apache.carbondata.scan.result.impl.FilterQueryScannedResult.getDictionaryKeyIntegerArray(FilterQueryScannedResult.java:53) at org.apache.carbondata.scan.collector.impl.DictionaryBasedResultCollector.collectData(DictionaryBasedResultCollector.java:111) at org.apache.carbondata.scan.processor.impl.DataBlockIteratorImpl.next(DataBlockIteratorImpl.java:52) at org.apache.carbondata.scan.processor.impl.DataBlockIteratorImpl.next(DataBlockIteratorImpl.java:33) at org.apache.carbondata.scan.result.iterator.DetailQueryResultIterator$1.call(DetailQueryResultIterator.java:78) at org.apache.carbondata.scan.result.iterator.DetailQueryResultIterator$1.call(DetailQueryResultIterator.java:72) at java.util.concurrent.FutureTask.run(FutureTask.java:266) ... 3 moreDriver stacktrace: (state=,code=0)
issueID:CARBONDATA-665
type:Bug
changed files:
texts:Comparision Failure occurs when we execute the same query in hive and Carbondata
Orderby is not working , so records are not coming in sequence as well there is data difference and some values being stored as nullData itself is stored incorrectly and is different from HiveSpark version :1.6.2Create 1 query : create table Test_Boundary (c1_int int,c2_Bigint Bigint,c3_Decimal Decimal(38,30),c4_double double,c5_string string,c6_Timestamp Timestamp,c7_Datatype_Desc string) STORED BY 'org.apache.carbondata.format'Load 1 Query : LOAD DATA INPATH 'HDFS_URL/BabuStore/Data/Test_Data1.csv' INTO table Test_Boundary OPTIONS('DELIMITER'=',','QUOTECHAR'='','BAD_RECORDS_ACTION'='FORCE','FILEHEADER'='')Create 2 query : create table Test_Boundary1 (c1_int int,c2_Bigint Bigint,c3_Decimal Decimal(38,30),c4_double double,c5_string string,c6_Timestamp Timestamp,c7_Datatype_Desc string) STORED BY 'org.apache.carbondata.format'Load 2 query:  LOAD DATA INPATH 'HDFS_URL/BabuStore/Data/Test_Data1.csv' INTO table Test_Boundary1 OPTIONS('DELIMITER'=',','QUOTECHAR'='','BAD_RECORDS_ACTION'='FORCE','FILEHEADER'='')Select Query : select c1_int,c2_Bigint,c3_Decimal,c4_double,c5_string,c6_Timestamp,c7_Datatype_Desc from Test_Boundary where c2_bigint=c2_bigint
issueID:CARBONDATA-666
type:Bug
changed files:
texts:Select Query fails with NullPoint exception if using 2 or more logical operators
select * from communication.flow_carbon where cus_ac = '6222621350672465397' and txn_bk IN ('00000000000', '00000000001','00000000002') OR own_bk IN ('00000000124','00000000175','00000000034','00000000231','00000000167','00000000182','00000000206') limit 10;Error: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 3, linux-79): java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.NullPointerExceptionat org.apache.carbondata.scan.result.iterator.DetailQueryResultIterator.next(DetailQueryResultIterator.java:65)at org.apache.carbondata.scan.result.iterator.DetailQueryResultIterator.next(DetailQueryResultIterator.java:35)at org.apache.carbondata.scan.result.iterator.ChunkRowIterator.<init>(ChunkRowIterator.java:43)at org.apache.carbondata.hadoop.CarbonRecordReader.initialize(CarbonRecordReader.java:81)at org.apache.carbondata.spark.rdd.CarbonScanRDD.compute(CarbonScanRDD.scala:194)at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)at org.apache.spark.scheduler.Task.run(Task.scala:89)at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)at java.lang.Thread.run(Thread.java:745)Caused by: java.util.concurrent.ExecutionException: java.lang.NullPointerExceptionat java.util.concurrent.FutureTask.report(FutureTask.java:122)at java.util.concurrent.FutureTask.get(FutureTask.java:192)at org.apache.carbondata.scan.result.iterator.DetailQueryResultIterator.next(DetailQueryResultIterator.java:52)... 24 moreCaused by: java.lang.NullPointerExceptionat org.apache.carbondata.scan.result.AbstractScannedResult.getDictionaryKeyIntegerArray(AbstractScannedResult.java:186)at org.apache.carbondata.scan.result.impl.FilterQueryScannedResult.getDictionaryKeyIntegerArray(FilterQueryScannedResult.java:53)at org.apache.carbondata.scan.collector.impl.DictionaryBasedResultCollector.collectData(DictionaryBasedResultCollector.java:111)at org.apache.carbondata.scan.processor.impl.DataBlockIteratorImpl.next(DataBlockIteratorImpl.java:52)at org.apache.carbondata.scan.processor.impl.DataBlockIteratorImpl.next(DataBlockIteratorImpl.java:33)at org.apache.carbondata.scan.result.iterator.DetailQueryResultIterator$1.call(DetailQueryResultIterator.java:78)at org.apache.carbondata.scan.result.iterator.DetailQueryResultIterator$1.call(DetailQueryResultIterator.java:72)at java.util.concurrent.FutureTask.run(FutureTask.java:266)... 3 more
issueID:CARBONDATA-667
type:Bug
changed files:
texts:after setting carbon property carbon.kettle.home in carbon.properties , while loading data, it is not referring to the carbon.properties file in carbonlib directory
after setting carbon property carbon.kettle.home in carbon.properties , placed it in carbonlib directory. But while loading it is not referring to that path. and gives error as carboin.kettle.home is not set.when carbon.properties file is placed inside /conf/ directory , it works fine.
issueID:CARBONDATA-668
type:Bug
changed files:processing/src/main/java/org/apache/carbondata/processing/loading/parser/impl/RowParserImpl.java
texts:Dataloads  fails for this particular query
Dataloads  fail for this query Create Query: create table smart_500_DE (MSISDN string,IMSI string,IMEI string,INTERFACEID int,GROUPID int,GGSN_ID double,SGSN_ID double,SESSION_INDICATOR int,BEGIN_TIME double,BEGIN_TIME_MSEL int,END_TIME double,END_TIME_MSEL int,PROT_CATEGORY int,PROT_TYPE int,L7_CARRIER_PROT int,SUB_PROT_TYPE int,SID double,PROBEID double,ENCRYPT_VERSION int,ROAMING_TYPE int,ROAM_DIRECTION int,MS_IP string,SERVER_IP string,MS_PORT int,SERVER_DECIMAL Decimal,APN string,SGSN_SIG_IP string,GGSN_SIG_IP_BigInt_NEGATIVE bigint,SGSN_USER_IP string,GGSN_USER_IP string,MCC string,MNC string,RAT int,LAC string,RAC string,SAC string,CI string,BROWSER_TIMESTAMP timestamp,TCP_CONN_STATES int,TCP_STATES_BIGINTPOSITIVE int,TCP_WIN_SIZE int,TCP_MSS int,TCP_CONN_TIMES int,TCP_CONN_2_FAILED_TIMES int,TCP_CONN_3_FAILED_TIMES int,HOST string,STREAMING_URL string,GET_STREAMING_FAILED_CODE int,GET_STREAMING_FLAG int,GET_NUM int,GET_SUCCEED_NUM int,GET_RETRANS_NUM int,GET_TIMEOUT_NUM int,INTBUFFER_FST_FLAG int,INTBUFFER_FULL_FLAG int,STALL_NUM int,VIDEO_FRAME_RATE int,VIDEO_CODEC_ID string,VIDEO_WIDTH int,VIDEO_HEIGHT int,AUDIO_CODEC_ID string,MEDIA_FILE_TYPE int,PLAY_STATE int,STREAMING_FLAG int,TCP_STATUS_INDICATOR int,DISCONNECTION_FLAG int,FAILURE_CODE int,FLAG int,TAC string,ECI string,TCP_SYN_TIME_MSEL int,TCP_FST_SYN_DIRECTION int,RAN_NE_USER_IP string,HOMEMCC string,HOMEMNC string,CHARGE_FLAG int,PREPAID_FLAG int,USER_AGENT string,MS_WIN_STAT_TOTAL_NUM int,MS_WIN_STAT_SMALL_NUM int,MS_ACK_TO_1STGET_DELAY int,SERVER_ACK_TO_1STDATA_DELAY int,STREAMING_TYPE int,SOURCE_VIDEO_QUALITY int,TETHERING_FLAG int,CARRIER_ID double,LAYER1ID int,LAYER2ID int,LAYER3ID int,LAYER4ID int,LAYER5ID int,LAYER6ID int,CHARGING_RULE_BASE_NAME string,SP string,EXTENDED_URL string,SV string,FIRST_SAI_CGI_ECGI string,EXTENDED_URL_OTHER string,SIGNALING_USE_FLAG int,DNS_RETRANS_NUM int,DNS_FAIL_CODE int,FIRST_RAT int,MS_INDICATOR string,LAST_SAI_CGI_ECGI string,LAST_RAT int,FIRST_LONGITUDE double,FIRST_LATITUDE double,FIRST_ALTITUDE int,FIRST_RASTERLONGITUDE double,FIRST_RASTERLATITUDE double,FIRST_RASTERALTITUDE int,FIRST_FREQUENCYSPOT int,FIRST_CLUTTER int,FIRST_USERBEHAVIOR int,FIRST_SPEED int,FIRST_CREDIBILITY int,LAST_LONGITUDE double,LAST_LATITUDE double,LAST_ALTITUDE int,LAST_RASTERLONGITUDE double,LAST_RASTERLATITUDE double,LAST_RASTERALTITUDE int,LAST_FREQUENCYSPOT int,LAST_CLUTTER int,LAST_USERBEHAVIOR int,LAST_SPEED int,LAST_CREDIBILITY int,IMEI_CIPHERTEXT string,APP_ID int,DOMAIN_NAME string,STREAMING_CACHE_IP string,STOP_LONGER_THAN_MIN_THRESHOLD int,STOP_LONGER_THAN_MAX_THRESHOLD int,PLAY_END_STAT int,STOP_START_TIME1 double,STOP_END_TIME1 double,STOP_START_TIME2 double,STOP_END_TIME2 double,STOP_START_TIME3 double,STOP_END_TIME3 double,STOP_START_TIME4 double,STOP_END_TIME4 double,STOP_START_TIME5 double,STOP_END_TIME5 double,STOP_START_TIME6 double,STOP_END_TIME6 double,STOP_START_TIME7 double,STOP_END_TIME7 double,STOP_START_TIME8 double,STOP_END_TIME8 double,STOP_START_TIME9 double,STOP_END_TIME9 double,STOP_START_TIME10 double,STOP_END_TIME10 double,FAIL_CLASS double,RECORD_TYPE double,NODATA_COUNT double,VIDEO_NODATA_DURATION double,VIDEO_SMOOTH_DURATION double,VIDEO_SD_DURATION double,VIDEO_HD_DURATION double,VIDEO_UHD_DURATION double,VIDEO_FHD_DURATION double,FLUCTUATION double,START_DOWNLOAD_THROUGHPUT double,L7_UL_GOODPUT_FULL_MSS double,SESSIONKEY string,FIRST_UCELLID double,LAST_UCELLID double,UCELLID1 double,LONGITUDE1 double,LATITUDE1 double,UCELLID2 double,LONGITUDE2 double,LATITUDE2 double,UCELLID3 double,LONGITUDE3 double,LATITUDE3 double,UCELLID4 double,LONGITUDE4 double,LATITUDE4 double,UCELLID5 double,LONGITUDE5 double,LATITUDE5 double,UCELLID6 double,LONGITUDE6 double,LATITUDE6 double,UCELLID7 double,LONGITUDE7 double,LATITUDE7 double,UCELLID8 double,LONGITUDE8 double,LATITUDE8 double,UCELLID9 double,LONGITUDE9 double,LATITUDE9 double,UCELLID10 double,LONGITUDE10 double,LATITUDE10 double,INTBUFFER_FULL_DELAY double,STALL_DURATION double,STREAMING_DW_PACKETS double,STREAMING_DOWNLOAD_DELAY double,PLAY_DURATION double,STREAMING_QUALITY int,VIDEO_DATA_RATE double,AUDIO_DATA_RATE double,STREAMING_FILESIZE double,STREAMING_DURATIOIN double,TCP_SYN_TIME double,TCP_RTT_STEP1 double,CHARGE_ID double,UL_REVERSE_TO_DL_DELAY double,DL_REVERSE_TO_UL_DELAY double,DATATRANS_DW_GOODPUT double,DATATRANS_DW_TOTAL_DURATION double,SUM_FRAGMENT_INTERVAL double,TCP_FIN_TIMES double,TCP_RESET_TIMES double,URL_CLASSIFICATION double,STREAMING_LQ_DURATIOIN double,MAX_DNS_DELAY double,MAX_DNS2SYN double,MAX_LATANCY_OF_LINK_SETUP double,MAX_SYNACK2FIRSTACK double,MAX_SYNACK2LASTACK double,MAX_ACK2GET_DELAY double,MAX_FRAG_INTERVAL_PREDELAY double,SUM_FRAG_INTERVAL_PREDELAY double,SERVICE_DELAY_MSEC double,HOMEPROVINCE double,HOMECITY double,SERVICE_ID double,CHARGING_CLASS double,DATATRANS_UL_DURATION double,ASSOCIATED_ID double,PACKET_LOSS_NUM double,JITTER double,MS_DNS_DELAY_MSEL double,GET_STREAMING_DELAY double,TCP_UL_RETRANS_WITHOUTPL double,TCP_DW_RETRANS_WITHOUTPL double,GET_MAX_UL_SIZE double,GET_MIN_UL_SIZE double,GET_MAX_DL_SIZE double,GET_MIN_DL_SIZE double,L4_UL_THROUGHPUT double,L4_DW_THROUGHPUT double,L4_UL_GOODPUT double,L4_DW_GOODPUT double,NETWORK_UL_TRAFFIC double,NETWORK_DL_TRAFFIC double,L4_UL_PACKETS double,L4_DW_PACKETS double,TCP_RTT double,TCP_UL_OUTOFSEQU double,TCP_DW_OUTOFSEQU double,TCP_UL_RETRANS double,TCP_DW_RETRANS double,TCP_UL_RETRANS_WITHPL double,TCP_DW_RETRANS_WITHPL double,TCP_UL_PACKAGES_WITHPL double,TCP_DW_PACKAGES_WITHPL double,TCP_UL_PACKAGES_WITHOUTPL double,TCP_DW_PACKAGES_WITHOUTPL double,UPPERLAYER_IP_UL_PACKETS double,UPPERLAYER_IP_DL_PACKETS double,DOWNLAYER_IP_UL_PACKETS double,DOWNLAYER_IP_DL_PACKETS double,UPPERLAYER_IP_UL_FRAGMENTS double,UPPERLAYER_IP_DL_FRAGMENTS double,DOWNLAYER_IP_UL_FRAGMENTS double,DOWNLAYER_IP_DL_FRAGMENTS double,VALID_TRANS_DURATION double,AIR_PORT_DURATION double,RADIO_CONN_TIMES double,RAN_NE_ID double,AVG_UL_RTT double,AVG_DW_RTT double,UL_RTT_LONG_NUM int,DW_RTT_LONG_NUM int,UL_RTT_STAT_NUM int,DW_RTT_STAT_NUM int,USER_PROBE_UL_LOST_PKT int,SERVER_PROBE_UL_LOST_PKT int,SERVER_PROBE_DW_LOST_PKT int,USER_PROBE_DW_LOST_PKT int,CHARGING_CHARACTERISTICS double,DL_SERIOUS_OUT_OF_ORDER_NUM double,DL_SLIGHT_OUT_OF_ORDER_NUM double,DL_FLIGHT_TOTAL_SIZE double,DL_FLIGHT_TOTAL_NUM double,DL_MAX_FLIGHT_SIZE double,UL_SERIOUS_OUT_OF_ORDER_NUM double,UL_SLIGHT_OUT_OF_ORDER_NUM double,UL_FLIGHT_TOTAL_SIZE double,UL_FLIGHT_TOTAL_NUM double,UL_MAX_FLIGHT_SIZE double,USER_DL_SLIGHT_OUT_OF_ORDER_PACKETS double,SERVER_UL_SLIGHT_OUT_OF_ORDER_PACKETS double,DL_CONTINUOUS_RETRANSMISSION_DELAY double,USER_HUNGRY_DELAY double,SERVER_HUNGRY_DELAY double,AVG_DW_RTT_MICRO_SEC int,AVG_UL_RTT_MICRO_SEC int,FLOW_SAMPLE_RATIO int) STORED BY 'org.apache.carbondata.format' TBLPROPERTIES ( 'DICTIONARY_EXCLUDE'='MSISDN,IMSI,IMEI,MS_IP,SERVER_IP,HOST,SP,MS_INDICATOR,streaming_url','DICTIONARY_INCLUDE'='SESSION_INDICATOR,SERVER_DECIMAL,TCP_STATES_BIGINTPOSITIVE')Load Query: LOAD DATA INPATH 'HDFS_URL/BabuStore/Data/SEQ500/seq_500Records.csv' into table smart_500_DE options('DELIMITER'=',', 'QUOTECHAR'='"','BAD_RECORDS_ACTION'='FORCE','FILEHEADER'='SID,PROBEID,INTERFACEID,GROUPID,GGSN_ID,SGSN_ID,dummy,SESSION_INDICATOR,BEGIN_TIME,BEGIN_TIME_MSEL,END_TIME,END_TIME_MSEL,PROT_CATEGORY,PROT_TYPE,L7_CARRIER_PROT,SUB_PROT_TYPE,MSISDN,IMSI,IMEI,ENCRYPT_VERSION,ROAMING_TYPE,ROAM_DIRECTION,MS_IP,SERVER_IP,MS_PORT,APN,SGSN_SIG_IP,GGSN_USER_IP,SGSN_USER_IP,MCC,MNC,RAT,LAC,RAC,SAC,CI,SERVER_DECIMAL,BROWSER_TIMESTAMP,TCP_CONN_STATES,GGSN_SIG_IP_BigInt_NEGATIVE,TCP_STATES_BIGINTPOSITIVE,dummy,TCP_WIN_SIZE,dummy,TCP_MSS,dummy,TCP_CONN_TIMES,dummy,TCP_CONN_2_FAILED_TIMES,dummy,TCP_CONN_3_FAILED_TIMES,HOST,STREAMING_URL,dummy,GET_STREAMING_FAILED_CODE,dummy,GET_STREAMING_FLAG,dummy,GET_NUM,dummy,GET_SUCCEED_NUM,dummy,GET_RETRANS_NUM,dummy,GET_TIMEOUT_NUM,INTBUFFER_FST_FLAG,INTBUFFER_FULL_FLAG,STALL_NUM,dummy,VIDEO_FRAME_RATE,dummy,VIDEO_CODEC_ID,dummy,VIDEO_WIDTH,dummy,VIDEO_HEIGHT,dummy,AUDIO_CODEC_ID,dummy,MEDIA_FILE_TYPE,dummy,PLAY_STATE,dummy,PLAY_STATE,dummy,STREAMING_FLAG,dummy,TCP_STATUS_INDICATOR,dummy,DISCONNECTION_FLAG,dummy,FAILURE_CODE,FLAG,TAC,ECI,dummy,TCP_SYN_TIME_MSEL,dummy,TCP_FST_SYN_DIRECTION,RAN_NE_USER_IP,HOMEMCC,HOMEMNC,dummy,CHARGE_FLAG,dummy,PREPAID_FLAG,dummy,USER_AGENT,dummy,MS_WIN_STAT_TOTAL_NUM,dummy,MS_WIN_STAT_SMALL_NUM,dummy,MS_ACK_TO_1STGET_DELAY,dummy,SERVER_ACK_TO_1STDATA_DELAY,dummy,STREAMING_TYPE,dummy,SOURCE_VIDEO_QUALITY,TETHERING_FLAG,CARRIER_ID,LAYER1ID,LAYER2ID,dummy,LAYER3ID,dummy,LAYER4ID,dummy,LAYER5ID,dummy,LAYER6ID,CHARGING_RULE_BASE_NAME,SP,dummy,EXTENDED_URL,SV,FIRST_SAI_CGI_ECGI,dummy,EXTENDED_URL_OTHER,SIGNALING_USE_FLAG,dummy,DNS_RETRANS_NUM,dummy,DNS_FAIL_CODE,FIRST_RAT,FIRST_RAT,MS_INDICATOR,LAST_SAI_CGI_ECGI,LAST_RAT,dummy,FIRST_LONGITUDE,dummy,FIRST_LATITUDE,dummy,FIRST_ALTITUDE,dummy,FIRST_RASTERLONGITUDE,dummy,FIRST_RASTERLATITUDE,dummy,FIRST_RASTERALTITUDE,dummy,FIRST_FREQUENCYSPOT,dummy,FIRST_CLUTTER,dummy,FIRST_USERBEHAVIOR,dummy,FIRST_SPEED,dummy,FIRST_CREDIBILITY,dummy,LAST_LONGITUDE,dummy,LAST_LATITUDE,dummy,LAST_ALTITUDE,dummy,LAST_RASTERLONGITUDE,dummy,LAST_RASTERLATITUDE,dummy,LAST_RASTERALTITUDE,dummy,LAST_FREQUENCYSPOT,dummy,LAST_CLUTTER,dummy,LAST_USERBEHAVIOR,dummy,LAST_SPEED,dummy,LAST_CREDIBILITY,dummy,IMEI_CIPHERTEXT,APP_ID,dummy,DOMAIN_NAME,dummy,STREAMING_CACHE_IP,dummy,STOP_LONGER_THAN_MIN_THRESHOLD,dummy,STOP_LONGER_THAN_MAX_THRESHOLD,dummy,PLAY_END_STAT,dummy,STOP_START_TIME1,dummy,STOP_END_TIME1,dummy,STOP_START_TIME2,dummy,STOP_END_TIME2,dummy,STOP_START_TIME3,dummy,STOP_END_TIME3,dummy,STOP_START_TIME4,dummy,STOP_END_TIME4,dummy,STOP_START_TIME5,dummy,STOP_END_TIME5,dummy,STOP_START_TIME6,dummy,STOP_END_TIME6,dummy,STOP_START_TIME7,dummy,STOP_END_TIME7,dummy,STOP_START_TIME8,dummy,STOP_END_TIME8,dummy,STOP_START_TIME9,dummy,STOP_END_TIME9,dummy,STOP_START_TIME10,dummy,STOP_END_TIME10,dummy,FAIL_CLASS,RECORD_TYPE,dummy,NODATA_COUNT,dummy,VIDEO_NODATA_DURATION,dummy,VIDEO_SMOOTH_DURATION,dummy,VIDEO_SD_DURATION,dummy,VIDEO_HD_DURATION,dummy,VIDEO_UHD_DURATION,dummy,VIDEO_FHD_DURATION,dummy,FLUCTUATION,dummy,START_DOWNLOAD_THROUGHPUT,dummy,L7_UL_GOODPUT_FULL_MSS,dummy,SESSIONKEY,dummy,FIRST_UCELLID,dummy,LAST_UCELLID,dummy,UCELLID1,dummy,LONGITUDE1,dummy,LATITUDE1,dummy,UCELLID2,dummy,LONGITUDE2,dummy,LATITUDE2,dummy,UCELLID3,dummy,LONGITUDE3,dummy,LATITUDE3,dummy,UCELLID4,dummy,LONGITUDE4,dummy,LATITUDE4,dummy,UCELLID5,dummy,LONGITUDE5,dummy,LATITUDE5,dummy,UCELLID6,dummy,LONGITUDE6,dummy,LATITUDE6,dummy,UCELLID7,dummy,LONGITUDE7,dummy,LATITUDE7,dummy,UCELLID8,dummy,LONGITUDE8,dummy,LATITUDE8,dummy,UCELLID9,dummy,LONGITUDE9,dummy,LATITUDE9,dummy,UCELLID10,dummy,LONGITUDE10,dummy,LATITUDE10,dummy,INTBUFFER_FULL_DELAY,dummy,STALL_DURATION,dummy,STREAMING_DW_PACKETS,dummy,STREAMING_DOWNLOAD_DELAY,dummy,PLAY_DURATION,dummy,STREAMING_QUALITY,dummy,VIDEO_DATA_RATE,dummy,AUDIO_DATA_RATE,dummy,STREAMING_FILESIZE,dummy,STREAMING_DURATIOIN,dummy,TCP_SYN_TIME,dummy,TCP_RTT_STEP1,CHARGE_ID,dummy,UL_REVERSE_TO_DL_DELAY,dummy,DL_REVERSE_TO_UL_DELAY,dummy,DATATRANS_DW_GOODPUT,dummy,DATATRANS_DW_TOTAL_DURATION,dummy,SUM_FRAGMENT_INTERVAL,dummy,TCP_FIN_TIMES,dummy,TCP_RESET_TIMES,dummy,URL_CLASSIFICATION,dummy,STREAMING_LQ_DURATIOIN,dummy,MAX_DNS_DELAY,dummy,MAX_DNS2SYN,dummy,MAX_LATANCY_OF_LINK_SETUP,dummy,MAX_SYNACK2FIRSTACK,dummy,MAX_SYNACK2LASTACK,dummy,MAX_ACK2GET_DELAY,dummy,MAX_FRAG_INTERVAL_PREDELAY,dummy,SUM_FRAG_INTERVAL_PREDELAY,dummy,SERVICE_DELAY_MSEC,dummy,HOMEPROVINCE,dummy,HOMECITY,dummy,SERVICE_ID,dummy,CHARGING_CLASS,dummy,DATATRANS_UL_DURATION,dummy,ASSOCIATED_ID,dummy,PACKET_LOSS_NUM,dummy,JITTER,dummy,MS_DNS_DELAY_MSEL,dummy,GET_STREAMING_DELAY,dummy,TCP_UL_RETRANS_WITHOUTPL,dummy,TCP_DW_RETRANS_WITHOUTPL,dummy,GET_MAX_UL_SIZE,dummy,GET_MIN_UL_SIZE,dummy,GET_MAX_DL_SIZE,dummy,GET_MIN_DL_SIZE,dummy,FLOW_SAMPLE_RATIO,dummy,UL_RTT_LONG_NUM,dummy,DW_RTT_LONG_NUM,dummy,UL_RTT_STAT_NUM,dummy,DW_RTT_STAT_NUM,dummy,USER_PROBE_UL_LOST_PKT,dummy,SERVER_PROBE_UL_LOST_PKT,dummy,SERVER_PROBE_DW_LOST_PKT,dummy,USER_PROBE_DW_LOST_PKT,dummy,AVG_DW_RTT_MICRO_SEC,dummy,AVG_UL_RTT_MICRO_SEC,dummy,RAN_NE_ID,dummy,AVG_UL_RTT,dummy,AVG_DW_RTT,dummy,CHARGING_CHARACTERISTICS,dummy,DL_SERIOUS_OUT_OF_ORDER_NUM,dummy,DL_SLIGHT_OUT_OF_ORDER_NUM,dummy,DL_FLIGHT_TOTAL_SIZE,dummy,DL_FLIGHT_TOTAL_NUM,dummy,DL_MAX_FLIGHT_SIZE,dummy,VALID_TRANS_DURATION,dummy,AIR_PORT_DURATION,dummy,RADIO_CONN_TIMES,dummy,UL_SERIOUS_OUT_OF_ORDER_NUM,dummy,UL_SLIGHT_OUT_OF_ORDER_NUM,dummy,UL_FLIGHT_TOTAL_SIZE,dummy,UL_FLIGHT_TOTAL_NUM,dummy,UL_MAX_FLIGHT_SIZE,dummy,USER_DL_SLIGHT_OUT_OF_ORDER_PACKETS,dummy,SERVER_UL_SLIGHT_OUT_OF_ORDER_PACKETS,dummy,DL_CONTINUOUS_RETRANSMISSION_DELAY,dummy,USER_HUNGRY_DELAY,dummy,SERVER_HUNGRY_DELAY,dummy,UPPERLAYER_IP_UL_FRAGMENTS,dummy,UPPERLAYER_IP_DL_FRAGMENTS,dummy,DOWNLAYER_IP_UL_FRAGMENTS,dummy,DOWNLAYER_IP_DL_FRAGMENTS,dummy,UPPERLAYER_IP_UL_PACKETS,dummy,UPPERLAYER_IP_DL_PACKETS,dummy,DOWNLAYER_IP_UL_PACKETS,dummy,DOWNLAYER_IP_DL_PACKETS,dummy,TCP_UL_PACKAGES_WITHPL,dummy,TCP_DW_PACKAGES_WITHPL,dummy,TCP_UL_PACKAGES_WITHOUTPL,dummy,TCP_DW_PACKAGES_WITHOUTPL,dummy,TCP_UL_RETRANS_WITHPL,dummy,TCP_DW_RETRANS_WITHPL,L4_UL_THROUGHPUT,L4_DW_THROUGHPUT,L4_UL_GOODPUT,L4_DW_GOODPUT,NETWORK_UL_TRAFFIC,NETWORK_DL_TRAFFIC,L4_UL_PACKETS,L4_DW_PACKETS,TCP_RTT,TCP_UL_OUTOFSEQU,TCP_DW_OUTOFSEQU,TCP_UL_RETRANS,TCP_DW_RETRANS')Stack Trace :&#91;exec&#93; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 4 times, most recent failure: Lost task 0.3 in stage 2.0 (TID 35, hadoop-master): org.apache.carbondata.processing.newflow.exception.CarbonDataLoadingException: Data Loading failed for table smart_500_de     &#91;exec&#93;  at org.apache.carbondata.processing.newflow.DataLoadExecutor.execute(DataLoadExecutor.java:54)     &#91;exec&#93;  at org.apache.carbondata.spark.rdd.NewCarbonDataLoadRDD$$anon$1.<init>(NewCarbonDataLoadRDD.scala:166)     &#91;exec&#93;  at org.apache.carbondata.spark.rdd.NewCarbonDataLoadRDD.compute(NewCarbonDataLoadRDD.scala:142)     &#91;exec&#93;  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)     &#91;exec&#93;  at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)     &#91;exec&#93;  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)     &#91;exec&#93;  at org.apache.spark.scheduler.Task.run(Task.scala:89)     &#91;exec&#93;  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)     &#91;exec&#93;  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)     &#91;exec&#93;  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)     &#91;exec&#93;  at java.lang.Thread.run(Thread.java:745)     &#91;exec&#93; Caused by: java.lang.ArrayIndexOutOfBoundsException     &#91;exec&#93;      &#91;exec&#93; Driver stacktrace:     &#91;exec&#93;  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)     &#91;exec&#93;  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)     &#91;exec&#93;  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)     &#91;exec&#93;  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)     &#91;exec&#93;  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)     &#91;exec&#93;  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)     &#91;exec&#93;  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)     &#91;exec&#93;  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)     &#91;exec&#93;  at scala.Option.foreach(Option.scala:236)     &#91;exec&#93;  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)     &#91;exec&#93;  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)     &#91;exec&#93;  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)used csv is attached below.
issueID:CARBONDATA-669
type:Bug
changed files:
texts:InsertIntoCarbonTableTestCase.insert into carbon table from carbon table union query  random test failure
org.apache.carbondata.spark.testsuite.allqueries.InsertIntoCarbonTableTestCase.insert into carbon table from carbon table union query  random test failureERROR 19-01 19:49:53,151 - Exception in task 1.0 in stage 1634.0 (TID 7523)java.lang.NullPointerException at org.apache.carbondata.core.scan.executor.impl.AbstractQueryExecutor.initQuery(AbstractQueryExecutor.java:136) at org.apache.carbondata.core.scan.executor.impl.AbstractQueryExecutor.getBlockExecutionInfos(AbstractQueryExecutor.java:219) at org.apache.carbondata.core.scan.executor.impl.DetailQueryExecutor.execute(DetailQueryExecutor.java:39) at org.apache.carbondata.hadoop.CarbonRecordReader.initialize(CarbonRecordReader.java:79) at org.apache.carbondata.spark.rdd.CarbonScanRDD.compute(CarbonScanRDD.scala:192) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306) at org.apache.spark.rdd.RDD.iterator(RDD.scala:270) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306) at org.apache.spark.rdd.RDD.iterator(RDD.scala:270) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306) at org.apache.spark.rdd.RDD.iterator(RDD.scala:270) at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:87) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306) at org.apache.spark.rdd.RDD.iterator(RDD.scala:270) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306) at org.apache.spark.rdd.RDD.iterator(RDD.scala:270) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306) at org.apache.spark.rdd.RDD.iterator(RDD.scala:270) at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73) at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41) at org.apache.spark.scheduler.Task.run(Task.scala:89) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) at java.lang.Thread.run(Thread.java:745)ERROR 19-01 19:49:53,152 - Task 1 in stage 1634.0 failed 1 times; aborting job
issueID:CARBONDATA-67
type:Bug
changed files:
texts:keyword is not allowed as carbon table name
LOAD DATA LOCAL INPATH '$testData' into table datahere the table name is data. but data is a keyword in carbon.so parse exception will occur when executing the load DDL.
issueID:CARBONDATA-670
type:Improvement
changed files:
texts:Add new MD files for Data Types and File Structure.
Add MD files for  : Data Types and File StructureUpdate the Overview Section.
issueID:CARBONDATA-671
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/keygenerator/directdictionary/timestamp/DateDirectDictionaryGenerator.java
texts:Date data is coming as null when date data is before 1970
Problem:Date data is coming as null when date data is before 1970 as cutoff time got removedSolution: We need to add cutofftime
issueID:CARBONDATA-672
type:Bug
changed files:
texts:Complex data type is not working while fetching it from Database
I created a table with an complex data type and then load data into it. Data loaded successfully but when I am trying to fetch data it is giving me error.Create Table :create table ct10(id Int,data array<Int>)stored by 'carbondata';Load Data :LOAD DATA inpath 'hdfs://localhost:54310/ct4.csv' INTO table ct1 options('DELIMITER'=',', 'FILEHEADER'='id, data','QUOTECHAR'='"','COMPLEX_DELIMITER_LEVEL_1'='#');Data Query :Select * from ct10;Error :While when we use query select  id from ct10;, its working fine.I am attaching screen shot, CSV and log with it.
issueID:CARBONDATA-673
type:Bug
changed files:processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactDataHandlerColumnar.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/store/impl/unsafe/UnsafeVariableLengthDimensionDataChunkStore.java
core/src/main/java/org/apache/carbondata/core/util/DataTypeUtil.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/reader/measure/AbstractMeasureChunkReader.java
texts:Reverting big decimal compression as it has below issue
ProblemWe had added code for big decimal compression for tpc-h activity. It turns out there is data inaccuracy when big decimal scale value is more then 18 then result is not accurateSolutionRevert the code to old wayTO-DOFind another way to compress big decimal data type.
issueID:CARBONDATA-674
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/util/path/CarbonTablePath.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
core/src/main/java/org/apache/carbondata/core/util/CarbonMetadataUtil.java
core/src/main/java/org/apache/carbondata/core/metadata/ValueEncoderMeta.java
core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
texts:Store compatibility 0.2 to 1.0
Problem: Query is not working with old storeSolution: As big decimal compression is added in new flow we need to add old code to handle old store
issueID:CARBONDATA-675
type:Bug
changed files:
texts:DataLoad failure
With performance test Create and Load this issue is coming.This issue is with 300 column performance table.System configuration :- RAM-30G ,Cores-16Command to start thrift :- ./bin/spark-submit --master yarn-client --executor-memory 16g --executor-cores 8 --driver-memory 4g --num-executors 3 --class org.apachecarbondata.spark.thriftserver.CarbonThriftServer <carbon jar path> "hdfs://hacluster/Opt/CarbonStore"Size of data :- 34.59034071 GBCreate query :- create table oscon_new_1 (ACTIVE_AREA_ID String, ACTIVE_CHECK_DY String, ACTIVE_CHECK_HOUR String, ACTIVE_CHECK_MM String, ACTIVE_CHECK_TIME String, ACTIVE_CHECK_YR String, ACTIVE_CITY String, ACTIVE_COUNTRY String, ACTIVE_DISTRICT String, ACTIVE_EMUI_VERSION String, ACTIVE_FIRMWARE_VER String, ACTIVE_NETWORK String, ACTIVE_OS_VERSION String, ACTIVE_PROVINCE String, BOM String, CHECK_DATE String, CHECK_DY String, CHECK_HOUR String, CHECK_MM String, CHECK_YR String, CUST_ADDRESS_ID String, CUST_AGE String, CUST_BIRTH_COUNTRY String, CUST_BIRTH_DY String, CUST_BIRTH_MM String, CUST_BIRTH_YR String, CUST_BUY_POTENTIAL String, CUST_CITY String, CUST_STATE String, CUST_COUNTRY String, CUST_COUNTY String, CUST_EMAIL_ADDR String, CUST_LAST_RVW_DATE timestamp, CUST_FIRST_NAME String, CUST_ID String, CUST_JOB_TITLE String, CUST_LAST_NAME String, CUST_LOGIN String, CUST_NICK_NAME String, CUST_PRFRD_FLG String, CUST_SEX String, CUST_STREET_NAME String, CUST_STREET_NO String, CUST_SUITE_NO String, CUST_ZIP String, DELIVERY_CITY String, DELIVERY_STATE String, DELIVERY_COUNTRY String, DELIVERY_DISTRICT String, DELIVERY_PROVINCE String, DEVICE_NAME String, INSIDE_NAME String, ITM_BRAND String, ITM_BRAND_ID String, ITM_CATEGORY String, ITM_CATEGORY_ID String, ITM_CLASS String, ITM_CLASS_ID String, ITM_COLOR String, ITM_CONTAINER String, ITM_FORMULATION String, ITM_MANAGER_ID String, ITM_MANUFACT String, ITM_MANUFACT_ID String, ITM_ID String, ITM_NAME String, ITM_REC_END_DATE String, ITM_REC_START_DATE String, LATEST_AREAID String, LATEST_CHECK_DY String, LATEST_CHECK_HOUR String, LATEST_CHECK_MM String, LATEST_CHECK_TIME String, LATEST_CHECK_YR String, LATEST_CITY String, LATEST_COUNTRY String, LATEST_DISTRICT String, LATEST_EMUI_VERSION String, LATEST_FIRMWARE_VER String, LATEST_NETWORK String, LATEST_OS_VERSION String, LATEST_PROVINCE String, OL_ORDER_DATE String, OL_ORDER_NO int, OL_RET_ORDER_NO String, OL_RET_DATE String, OL_SITE String, OL_SITE_DESC String, PACKING_DATE String, PACKING_DY String, PACKING_HOUR String, PACKING_LIST_NO String, PACKING_MM String, PACKING_YR String, PRMTION_ID String, PRMTION_NAME String, PRM_CHANNEL_CAT String, PRM_CHANNEL_DEMO String, PRM_CHANNEL_DETAILS String, PRM_CHANNEL_DMAIL String, PRM_CHANNEL_EMAIL String, PRM_CHANNEL_EVENT String, PRM_CHANNEL_PRESS String, PRM_CHANNEL_RADIO String, PRM_CHANNEL_TV String, PRM_DSCNT_ACTIVE String, PRM_END_DATE String, PRM_PURPOSE String, PRM_START_DATE String, PRODUCT_ID String, PROD_BAR_CODE String, PROD_BRAND_NAME String, PRODUCT_NAME String, PRODUCT_MODEL String, PROD_MODEL_ID String, PROD_COLOR String, PROD_SHELL_COLOR String, PROD_CPU_CLOCK String, PROD_IMAGE String, PROD_LIVE String, PROD_LOC String, PROD_LONG_DESC String, PROD_RAM String, PROD_ROM String, PROD_SERIES String, PROD_SHORT_DESC String, PROD_THUMB String, PROD_UNQ_DEVICE_ADDR String, PROD_UNQ_MDL_ID String, PROD_UPDATE_DATE String, PROD_UQ_UUID String, SHP_CARRIER String, SHP_CODE String, SHP_CONTRACT String, SHP_MODE_ID String, SHP_MODE String, STR_ORDER_DATE String, STR_ORDER_NO String, TRACKING_NO String, WH_CITY String, WH_COUNTRY String, WH_COUNTY String, WH_ID String, WH_NAME String, WH_STATE String, WH_STREET_NAME String, WH_STREET_NO String, WH_STREET_TYPE String, WH_SUITE_NO String, WH_ZIP String, CUST_DEP_COUNT double, CUST_VEHICLE_COUNT double, CUST_ADDRESS_CNT double, CUST_CRNT_CDEMO_CNT double, CUST_CRNT_HDEMO_CNT double, CUST_CRNT_ADDR_DM double, CUST_FIRST_SHIPTO_CNT double, CUST_FIRST_SALES_CNT double, CUST_GMT_OFFSET double, CUST_DEMO_CNT double, CUST_INCOME double, PROD_UNLIMITED int, PROD_OFF_PRICE double, PROD_UNITS int, TOTAL_PRD_COST double, TOTAL_PRD_DISC double, PROD_WEIGHT double, REG_UNIT_PRICE double, EXTENDED_AMT double, UNIT_PRICE_DSCNT_PCT double, DSCNT_AMT double, PROD_STD_CST double, TOTAL_TX_AMT double, FREIGHT_CHRG double, WAITING_PERIOD double, DELIVERY_PERIOD double, ITM_CRNT_PRICE double, ITM_UNITS double, ITM_WSLE_CST double, ITM_SIZE double, PRM_CST double, PRM_RESPONSE_TARGET double, PRM_ITM_DM double, SHP_MODE_CNT double, WH_GMT_OFFSET double, WH_SQ_FT double, STR_ORD_QTY double, STR_WSLE_CST double, STR_LIST_PRICE double, STR_SALES_PRICE double, STR_EXT_DSCNT_AMT double, STR_EXT_SALES_PRICE double, STR_EXT_WSLE_CST double, STR_EXT_LIST_PRICE double, STR_EXT_TX double, STR_COUPON_AMT double, STR_NET_PAID double, STR_NET_PAID_INC_TX double, STR_NET_PRFT double, STR_SOLD_YR_CNT double, STR_SOLD_MM_CNT double, STR_SOLD_ITM_CNT double, STR_TOTAL_CUST_CNT double, STR_AREA_CNT double, STR_DEMO_CNT double, STR_OFFER_CNT double, STR_PRM_CNT double, STR_TICKET_CNT double, STR_NET_PRFT_DM_A double, STR_NET_PRFT_DM_B double, STR_NET_PRFT_DM_C double, STR_NET_PRFT_DM_D double, STR_NET_PRFT_DM_E double, STR_RET_STR_ID double, STR_RET_REASON_CNT double, STR_RET_TICKET_NO double, STR_RTRN_QTY double, STR_RTRN_AMT double, STR_RTRN_TX double, STR_RTRN_AMT_INC_TX double, STR_RET_FEE double, STR_RTRN_SHIP_CST double, STR_RFNDD_CSH double, STR_REVERSED_CHRG double, STR_STR_CREDIT double, STR_RET_NET_LOSS double, STR_RTRNED_YR_CNT double, STR_RTRN_MM_CNT double, STR_RET_ITM_CNT double, STR_RET_CUST_CNT double, STR_RET_AREA_CNT double, STR_RET_OFFER_CNT double, STR_RET_PRM_CNT double, STR_RET_NET_LOSS_DM_A double, STR_RET_NET_LOSS_DM_B double, STR_RET_NET_LOSS_DM_C double, STR_RET_NET_LOSS_DM_D double, OL_ORD_QTY double, OL_WSLE_CST double, OL_LIST_PRICE double, OL_SALES_PRICE double, OL_EXT_DSCNT_AMT double, OL_EXT_SALES_PRICE double, OL_EXT_WSLE_CST double, OL_EXT_LIST_PRICE double, OL_EXT_TX double, OL_COUPON_AMT double, OL_EXT_SHIP_CST double, OL_NET_PAID double, OL_NET_PAID_INC_TX double, OL_NET_PAID_INC_SHIP double, OL_NET_PAID_INC_SHIP_TX double, OL_NET_PRFT double, OL_SOLD_YR_CNT double, OL_SOLD_MM_CNT double, OL_SHIP_DATE_CNT double, OL_ITM_CNT double, OL_BILL_CUST_CNT double, OL_BILL_AREA_CNT double, OL_BILL_DEMO_CNT double, OL_BILL_OFFER_CNT double, OL_SHIP_CUST_CNT double, OL_SHIP_AREA_CNT double, OL_SHIP_DEMO_CNT double, OL_SHIP_OFFER_CNT double, OL_WEB_PAGE_CNT double, OL_WEB_SITE_CNT double, OL_SHIP_MODE_CNT double, OL_WH_CNT double, OL_PRM_CNT double, OL_NET_PRFT_DM_A double, OL_NET_PRFT_DM_B double, OL_NET_PRFT_DM_C double, OL_NET_PRFT_DM_D double, OL_RET_RTRN_QTY double, OL_RTRN_AMT double, OL_RTRN_TX double, OL_RTRN_AMT_INC_TX double, OL_RET_FEE double, OL_RTRN_SHIP_CST double, OL_RFNDD_CSH double, OL_REVERSED_CHRG double, OL_ACCOUNT_CREDIT double, OL_RTRNED_YR_CNT double, OL_RTRNED_MM_CNT double, OL_RTRITM_CNT double, OL_RFNDD_CUST_CNT double, OL_RFNDD_AREA_CNT double, OL_RFNDD_DEMO_CNT double, OL_RFNDD_OFFER_CNT double, OL_RTRNING_CUST_CNT double, OL_RTRNING_AREA_CNT double, OL_RTRNING_DEMO_CNT double, OL_RTRNING_OFFER_CNT double, OL_RTRWEB_PAGE_CNT double, OL_REASON_CNT double, OL_NET_LOSS double, OL_NET_LOSS_DM_A double, OL_NET_LOSS_DM_B double, OL_NET_LOSS_DM_C double) STORED BY 'org.apache.carbondata.format';Load Query :- load data inpath 'hdfs://hacluster/benchmarks/CarbonData/data/datafile_0.csv' into table oscon_VIP options('DELIMITER'=',', 'QUOTECHAR'='"','FILEHEADER'='ACTIVE_AREA_ID, ACTIVE_CHECK_DY, ACTIVE_CHECK_HOUR, ACTIVE_CHECK_MM, ACTIVE_CHECK_TIME, ACTIVE_CHECK_YR, ACTIVE_CITY, ACTIVE_COUNTRY, ACTIVE_DISTRICT, ACTIVE_EMUI_VERSION, ACTIVE_FIRMWARE_VER, ACTIVE_NETWORK, ACTIVE_OS_VERSION, ACTIVE_PROVINCE, BOM, CHECK_DATE, CHECK_DY, CHECK_HOUR, CHECK_MM, CHECK_YR, CUST_ADDRESS_ID, CUST_AGE, CUST_BIRTH_COUNTRY, CUST_BIRTH_DY, CUST_BIRTH_MM, CUST_BIRTH_YR, CUST_BUY_POTENTIAL, CUST_CITY, CUST_STATE, CUST_COUNTRY, CUST_COUNTY, CUST_EMAIL_ADDR, CUST_LAST_RVW_DATE, CUST_FIRST_NAME, CUST_ID, CUST_JOB_TITLE, CUST_LAST_NAME, CUST_LOGIN, CUST_NICK_NAME, CUST_PRFRD_FLG, CUST_SEX, CUST_STREET_NAME, CUST_STREET_NO, CUST_SUITE_NO, CUST_ZIP, DELIVERY_CITY, DELIVERY_STATE, DELIVERY_COUNTRY, DELIVERY_DISTRICT, DELIVERY_PROVINCE, DEVICE_NAME, INSIDE_NAME, ITM_BRAND, ITM_BRAND_ID, ITM_CATEGORY, ITM_CATEGORY_ID, ITM_CLASS, ITM_CLASS_ID, ITM_COLOR, ITM_CONTAINER, ITM_FORMULATION, ITM_MANAGER_ID, ITM_MANUFACT, ITM_MANUFACT_ID, ITM_ID, ITM_NAME, ITM_REC_END_DATE, ITM_REC_START_DATE, LATEST_AREAID, LATEST_CHECK_DY, LATEST_CHECK_HOUR, LATEST_CHECK_MM, LATEST_CHECK_TIME, LATEST_CHECK_YR, LATEST_CITY, LATEST_COUNTRY, LATEST_DISTRICT, LATEST_EMUI_VERSION, LATEST_FIRMWARE_VER, LATEST_NETWORK, LATEST_OS_VERSION, LATEST_PROVINCE, OL_ORDER_DATE, OL_ORDER_NO, OL_RET_ORDER_NO, OL_RET_DATE, OL_SITE, OL_SITE_DESC, PACKING_DATE, PACKING_DY, PACKING_HOUR, PACKING_LIST_NO, PACKING_MM, PACKING_YR, PRMTION_ID, PRMTION_NAME, PRM_CHANNEL_CAT, PRM_CHANNEL_DEMO, PRM_CHANNEL_DETAILS, PRM_CHANNEL_DMAIL, PRM_CHANNEL_EMAIL, PRM_CHANNEL_EVENT, PRM_CHANNEL_PRESS, PRM_CHANNEL_RADIO, PRM_CHANNEL_TV, PRM_DSCNT_ACTIVE, PRM_END_DATE, PRM_PURPOSE, PRM_START_DATE, PRODUCT_ID, PROD_BAR_CODE, PROD_BRAND_NAME, PRODUCT_NAME, PRODUCT_MODEL, PROD_MODEL_ID, PROD_COLOR, PROD_SHELL_COLOR, PROD_CPU_CLOCK, PROD_IMAGE, PROD_LIVE, PROD_LOC, PROD_LONG_DESC, PROD_RAM, PROD_ROM, PROD_SERIES, PROD_SHORT_DESC, PROD_THUMB, PROD_UNQ_DEVICE_ADDR, PROD_UNQ_MDL_ID, PROD_UPDATE_DATE, PROD_UQ_UUID, SHP_CARRIER, SHP_CODE, SHP_CONTRACT, SHP_MODE_ID, SHP_MODE, STR_ORDER_DATE, STR_ORDER_NO, TRACKING_NO, WH_CITY, WH_COUNTRY, WH_COUNTY, WH_ID, WH_NAME, WH_STATE, WH_STREET_NAME, WH_STREET_NO, WH_STREET_TYPE, WH_SUITE_NO, WH_ZIP, CUST_DEP_COUNT, CUST_VEHICLE_COUNT, CUST_ADDRESS_CNT, CUST_CRNT_CDEMO_CNT, CUST_CRNT_HDEMO_CNT, CUST_CRNT_ADDR_DM, CUST_FIRST_SHIPTO_CNT, CUST_FIRST_SALES_CNT, CUST_GMT_OFFSET, CUST_DEMO_CNT, CUST_INCOME, PROD_UNLIMITED, PROD_OFF_PRICE, PROD_UNITS, TOTAL_PRD_COST, TOTAL_PRD_DISC, PROD_WEIGHT, REG_UNIT_PRICE, EXTENDED_AMT, UNIT_PRICE_DSCNT_PCT, DSCNT_AMT, PROD_STD_CST, TOTAL_TX_AMT, FREIGHT_CHRG, WAITING_PERIOD, DELIVERY_PERIOD, ITM_CRNT_PRICE, ITM_UNITS, ITM_WSLE_CST, ITM_SIZE, PRM_CST, PRM_RESPONSE_TARGET, PRM_ITM_DM, SHP_MODE_CNT, WH_GMT_OFFSET, WH_SQ_FT, STR_ORD_QTY, STR_WSLE_CST, STR_LIST_PRICE, STR_SALES_PRICE, STR_EXT_DSCNT_AMT, STR_EXT_SALES_PRICE, STR_EXT_WSLE_CST, STR_EXT_LIST_PRICE, STR_EXT_TX, STR_COUPON_AMT, STR_NET_PAID, STR_NET_PAID_INC_TX, STR_NET_PRFT, STR_SOLD_YR_CNT, STR_SOLD_MM_CNT, STR_SOLD_ITM_CNT, STR_TOTAL_CUST_CNT, STR_AREA_CNT, STR_DEMO_CNT, STR_OFFER_CNT, STR_PRM_CNT, STR_TICKET_CNT, STR_NET_PRFT_DM_A, STR_NET_PRFT_DM_B, STR_NET_PRFT_DM_C, STR_NET_PRFT_DM_D, STR_NET_PRFT_DM_E, STR_RET_STR_ID, STR_RET_REASON_CNT, STR_RET_TICKET_NO, STR_RTRN_QTY, STR_RTRN_AMT, STR_RTRN_TX, STR_RTRN_AMT_INC_TX, STR_RET_FEE, STR_RTRN_SHIP_CST, STR_RFNDD_CSH, STR_REVERSED_CHRG, STR_STR_CREDIT, STR_RET_NET_LOSS, STR_RTRNED_YR_CNT, STR_RTRN_MM_CNT, STR_RET_ITM_CNT, STR_RET_CUST_CNT, STR_RET_AREA_CNT, STR_RET_OFFER_CNT, STR_RET_PRM_CNT, STR_RET_NET_LOSS_DM_A, STR_RET_NET_LOSS_DM_B, STR_RET_NET_LOSS_DM_C, STR_RET_NET_LOSS_DM_D, OL_ORD_QTY, OL_WSLE_CST, OL_LIST_PRICE, OL_SALES_PRICE, OL_EXT_DSCNT_AMT, OL_EXT_SALES_PRICE, OL_EXT_WSLE_CST, OL_EXT_LIST_PRICE, OL_EXT_TX, OL_COUPON_AMT, OL_EXT_SHIP_CST, OL_NET_PAID, OL_NET_PAID_INC_TX, OL_NET_PAID_INC_SHIP, OL_NET_PAID_INC_SHIP_TX, OL_NET_PRFT, OL_SOLD_YR_CNT, OL_SOLD_MM_CNT, OL_SHIP_DATE_CNT, OL_ITM_CNT, OL_BILL_CUST_CNT, OL_BILL_AREA_CNT, OL_BILL_DEMO_CNT, OL_BILL_OFFER_CNT, OL_SHIP_CUST_CNT, OL_SHIP_AREA_CNT, OL_SHIP_DEMO_CNT, OL_SHIP_OFFER_CNT, OL_WEB_PAGE_CNT, OL_WEB_SITE_CNT, OL_SHIP_MODE_CNT, OL_WH_CNT, OL_PRM_CNT, OL_NET_PRFT_DM_A, OL_NET_PRFT_DM_B, OL_NET_PRFT_DM_C, OL_NET_PRFT_DM_D, OL_RET_RTRN_QTY, OL_RTRN_AMT, OL_RTRN_TX, OL_RTRN_AMT_INC_TX, OL_RET_FEE, OL_RTRN_SHIP_CST, OL_RFNDD_CSH, OL_REVERSED_CHRG, OL_ACCOUNT_CREDIT, OL_RTRNED_YR_CNT, OL_RTRNED_MM_CNT, OL_RTRITM_CNT, OL_RFNDD_CUST_CNT, OL_RFNDD_AREA_CNT, OL_RFNDD_DEMO_CNT, OL_RFNDD_OFFER_CNT, OL_RTRNING_CUST_CNT, OL_RTRNING_AREA_CNT, OL_RTRNING_DEMO_CNT, OL_RTRNING_OFFER_CNT, OL_RTRWEB_PAGE_CNT, OL_REASON_CNT, OL_NET_LOSS, OL_NET_LOSS_DM_A, OL_NET_LOSS_DM_B, OL_NET_LOSS_DM_C','BAD_RECORDS_ACTION'='FORCE','BAD_RECORDS_LOGGER_ENABLE'='FALSE');
issueID:CARBONDATA-676
type:Task
changed files:core/src/main/java/org/apache/carbondata/core/statusmanager/SegmentStatusManager.java
core/src/main/java/org/apache/carbondata/core/scan/result/iterator/VectorDetailQueryResultIterator.java
texts:Correcting spelling mistakes and removed unnecessary methods
To clean some code：Correct the spelling mistakeRemove unused functionIterate the Array instead of transform it to List.
issueID:CARBONDATA-677
type:Bug
changed files:
texts:Fixed GC issue and Re-factor Dictionary based result collector
Problem:When time stamp or date type is getting selected in query projection we are creating direct dictionary result collector object every for each time this is causing lots of gc.Solution: Need to create only one object for time stamp and date type
issueID:CARBONDATA-678
type:Bug
changed files:
texts:Corr function is not working for double datatype.
We have created a table uniqdata_h and then we upload data in this table then we perform select query having data-type double then it displays null value but when we perform select query with integer or bigint data-type it displays a value."Double" data-type is not working.CREATE QUERY :0: jdbc:hive2://localhost:10000> CREATE TABLE uniqdata_h (CUST_ID int,CUST_NAME String,ACTIVE_EMUI_VERSION string, DOB timestamp, DOJ timestamp, BIGINT_COLUMN1 bigint,BIGINT_COLUMN2 bigint,DECIMAL_COLUMN1 decimal(30,10), DECIMAL_COLUMN2 decimal(36,10),Double_COLUMN1 double, Double_COLUMN2 double,INTEGER_COLUMN1 int) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',';---------+ result  ---------+---------+No rows selected (4.249 seconds)LOAD 1 :0: jdbc:hive2://localhost:10000> load data local inpath '/opt/Carbon/CarbonData/TestData/Data/uniqdata/2000_UniqData.csv' into table uniqdata_h;---------+ Result  ---------+---------+No rows selected (1.176 seconds)LOAD 2 :0: jdbc:hive2://localhost:10000> load data local inpath '/opt/Carbon/CarbonData/TestData/Data/uniqdata/4000_UniqData.csv' into table uniqdata_h;---------+ Result  ---------+---------+No rows selected (0.485 seconds)LOAD 3 :0: jdbc:hive2://localhost:10000> load data local inpath '/opt/Carbon/CarbonData/TestData/Data/uniqdata/5000_UniqData.csv' into table uniqdata_h;---------+ Result  ---------+---------+No rows selected (0.365 seconds)LOAD 4 :0: jdbc:hive2://localhost:10000> load data local inpath '/opt/Carbon/CarbonData/TestData/Data/uniqdata/6000_UniqData.csv' into table uniqdata_h;---------+ Result  ---------+---------+No rows selected (0.594 seconds)LOAD 5 :0: jdbc:hive2://localhost:10000> load data local inpath '/opt/Carbon/CarbonData/TestData/Data/uniqdata/7000_UniqData.csv' into table uniqdata_h;---------+ Result  ---------+---------+No rows selected (0.753 seconds)LOAD 6 :0: jdbc:hive2://localhost:10000> load data local inpath '/opt/Carbon/CarbonData/TestData/Data/uniqdata/3000_1_UniqData.csv' into table uniqdata_h;---------+ Result  ---------+---------+No rows selected (0.417 seconds)SELECT QUERY : double data-type displays NULL value0: jdbc:hive2://localhost:10000> select corr(Double_COLUMN1,Double_COLUMN1)  as a from uniqdata_h ;-------+   a   -------+ NULL  -------+whereas select query with integer & bigdata type displays some value.0: jdbc:hive2://localhost:10000> select corr(integer_COLUMN1,bigint_COLUMN1)  as a from uniqdata_h ;---------------------+          a          ---------------------+ 1.0000000000000002  ---------------------+1 row selected (0.54 seconds)
issueID:CARBONDATA-68
type:Improvement
changed files:core/src/main/java/org/carbondata/scan/executor/infos/BlockExecutionInfo.java
core/src/main/java/org/apache/carbondata/core/scan/result/iterator/DetailQueryResultIterator.java
common/src/main/java/org/apache/carbondata/common/logging/LogService.java
core/src/main/java/org/carbondata/scan/executor/impl/QueryExecutorProperties.java
core/src/main/java/org/carbondata/scan/model/QueryModel.java
core/src/main/java/org/carbondata/scan/executor/impl/AbstractQueryExecutor.java
core/src/main/java/org/apache/carbondata/core/scan/result/iterator/AbstractDetailQueryResultIterator.java
texts:Added Performance statistics for query execution in driver and executor
Added Performance statistics for query execution in driver and executor for each phase of query execution
issueID:CARBONDATA-680
type:Bug
changed files:processing/src/main/java/org/apache/carbondata/processing/loading/steps/DataConverterProcessorStepImpl.java
common/src/main/java/org/apache/carbondata/common/logging/LogService.java
processing/src/main/java/org/apache/carbondata/processing/loading/steps/DataWriterProcessorStepImpl.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/impl/ParallelReadMergeSorterImpl.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/impl/ParallelReadMergeSorterWithBucketingImpl.java
core/src/main/java/org/apache/carbondata/core/memory/UnsafeMemoryManager.java
processing/src/main/java/org/apache/carbondata/processing/loading/AbstractDataLoadProcessorStep.java
processing/src/main/java/org/apache/carbondata/processing/loading/row/CarbonRowBatch.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/impl/UnsafeParallelReadMergeSorterImpl.java
processing/src/main/java/org/apache/carbondata/processing/loading/steps/InputProcessorStepImpl.java
processing/src/main/java/org/apache/carbondata/processing/loading/steps/SortProcessorStepImpl.java
texts:Add stats like rows processed in each step. And also fix unsafe sort enable issue.
Currently stats like number of rows processed in each step is not added in no kettle flow. Please add the same.And also unsafe sort is not enabling even though user enable the sort in property file.
issueID:CARBONDATA-681
type:Sub-task
changed files:processing/src/main/java/org/apache/carbondata/processing/loading/csvinput/CSVRecordReaderIterator.java
hadoop/src/main/java/org/apache/carbondata/hadoop/readsupport/CarbonReadSupport.java
integration/spark2/src/main/java/org/apache/carbondata/spark/readsupport/SparkRowReadSupportImpl.java
processing/src/main/java/org/apache/carbondata/processing/loading/csvinput/CSVInputFormat.java
hadoop/src/main/java/org/apache/carbondata/hadoop/readsupport/impl/DictionaryDecodeReadSupport.java
processing/src/main/java/org/apache/carbondata/processing/loading/csvinput/StringArrayWritable.java
processing/src/main/java/org/apache/carbondata/processing/loading/csvinput/BoundedInputStream.java
texts:CSVReader related code improvement
refactoring csv reader support during data loading, as well as replacing relevant class out of Carbon Hadoop component into data loading component (processing)
issueID:CARBONDATA-682
type:Bug
changed files:
texts:Fix license header for FloatDataTypeTestCase.scala and DateTypeTest.scala
Fix license header for FloatDataTypeTestCase.scala and DateTypeTest.scala
issueID:CARBONDATA-683
type:Improvement
changed files:
texts:Reduce test time
Reduce test time by:1. remove all unnecessary print2. make sample csv file size smaller3. change logger.audit to initialize strings in constructor
issueID:CARBONDATA-684
type:Bug
changed files:
texts:Improve test sufficiency and code coverage of carbondata-core module
There are not tests for the Dictionary generators, dictionary client and server.Add tests to ensure test sufficiency and code coverage.
issueID:CARBONDATA-685
type:Bug
changed files:
texts:Able to create table with spaces using carbon source
when using carbon source i am able to create table with spaceslogs====0: jdbc:hive2://localhost:10000> CREATE TABLE table (ID Int, date Timestamp, country String, name String, phonetype String, serialname String,    salary Int) USING org.apache.spark.sql.CarbonSource OPTIONS("tableName"="t a b l e ");---------+ Result  ---------+---------+No rows selecthere table with empty spaces is created in hdfsit should not allow this
issueID:CARBONDATA-686
type:Improvement
changed files:
texts:Extend period coverage in NOTICE

issueID:CARBONDATA-687
type:Improvement
changed files:
texts:Updated Documentation for New Features in Release 1.0.0

issueID:CARBONDATA-688
type:Bug
changed files:
texts:Abnormal behaviour of double datatype when used in DICTIONARY_INCLUDE and filtering null values
I tried to create a table having double as a column and load null values into that table. When I performed the select query on the table, it is displaying wrong data.Below are the commands used:Create table :create table  Comp_VMALL_DICTIONARY_INCLUDE (imei string,deviceInformationId int,MAC string,deviceColor string,device_backColor string,modelId string,marketName string,AMSize string,ROMSize string,CUPAudit string,CPIClocked string,series string,productionDate timestamp,bomCode string,internalModels string, deliveryTime string, channelsId string, channelsName string , deliveryAreaId string, deliveryCountry string, deliveryProvince string, deliveryCity string,deliveryDistrict string, deliveryStreet string, oxSingleNumber string, ActiveCheckTime string, ActiveAreaId string, ActiveCountry string, ActiveProvince string, Activecity string, ActiveDistrict string, ActiveStreet string, ActiveOperatorId string, Active_releaseId string, Active_EMUIVersion string, Active_operaSysVersion string, Active_BacVerNumber string, Active_BacFlashVer string, Active_webUIVersion string, Active_webUITypeCarrVer string,Active_webTypeDataVerNumber string, Active_operatorsVersion string, Active_phonePADPartitionedVersions string, Latest_YEAR int, Latest_MONTH int, Latest_DAY Decimal(30,10), Latest_HOUR string, Latest_areaId string, Latest_country string, Latest_province string, Latest_city string, Latest_district string, Latest_street string, Latest_releaseId string, Latest_EMUIVersion string, Latest_operaSysVersion string, Latest_BacVerNumber string, Latest_BacFlashVer string, Latest_webUIVersion string, Latest_webUITypeCarrVer string, Latest_webTypeDataVerNumber string, Latest_operatorsVersion string, Latest_phonePADPartitionedVersions string, Latest_operatorId string, gamePointDescription string,gamePointId double,contractNumber BigInt)  STORED BY 'org.apache.carbondata.format' TBLPROPERTIES('DICTIONARY_INCLUDE'='imei,deviceInformationId,productionDate,gamePointId,Latest_DAY,contractNumber');Load command:LOAD DATA INPATH  'hdfs://localhost:54311/BabuStore/DATA/100_olap_C20.csv' INTO table Comp_VMALL_DICTIONARY_INCLUDE options ('DELIMITER'=',', 'QUOTECHAR'='"', 'BAD_RECORDS_ACTION'='FORCE','FILEHEADER'='imei,deviceInformationId,MAC,deviceColor,device_backColor,modelId,marketName,AMSize,ROMSize,CUPAudit,CPIClocked,series,productionDate,bomCode,internalModels,deliveryTime,channelsId,channelsName,deliveryAreaId,deliveryCountry,deliveryProvince,deliveryCity,deliveryDistrict,deliveryStreet,oxSingleNumber,contractNumber,ActiveCheckTime,ActiveAreaId,ActiveCountry,ActiveProvince,Activecity,ActiveDistrict,ActiveStreet,ActiveOperatorId,Active_releaseId,Active_EMUIVersion,Active_operaSysVersion,Active_BacVerNumber,Active_BacFlashVer,Active_webUIVersion,Active_webUITypeCarrVer,Active_webTypeDataVerNumber,Active_operatorsVersion,Active_phonePADPartitionedVersions,Latest_YEAR,Latest_MONTH,Latest_DAY,Latest_HOUR,Latest_areaId,Latest_country,Latest_province,Latest_city,Latest_district,Latest_street,Latest_releaseId,Latest_EMUIVersion,Latest_operaSysVersion,Latest_BacVerNumber,Latest_BacFlashVer,Latest_webUIVersion,Latest_webUITypeCarrVer,Latest_webTypeDataVerNumber,Latest_operatorsVersion,Latest_phonePADPartitionedVersions,Latest_operatorId,gamePointId,gamePointDescription');Select query:select gamePointId  from Comp_VMALL_DICTIONARY_INCLUDE where gamePointId IS NOT NULL order by gamePointId;select gamePointId from Comp_VMALL_DICTIONARY_INCLUDE where gamePointId is NULL;The first select command displays null values as well and the second command displays no values.
issueID:CARBONDATA-689
type:Bug
changed files:
texts:User unable to create table using another database with carbon source.
I try to create the table in default database it was successfully created successfully and showing same in HDFS but when I create a new database(vinod) and inside I try to create the same table then as per the log it's created successfully but the table does not exist in HDFS(under vinod database).Steps to Reproduces:1: User should be default database.2: Create table by using the following command: " CREATE TABLE table11 (ID Int, date Timestamp, country String, name String, phonetype String, serialname String, salary Int) USING org.apache.spark.sql.CarbonSource OPTIONS("tableName"="table11"); "3:  Table created successfully in the default database.4: Create database vinod;5: Use vinod;6: Create the same table:      " CREATE TABLE table11 (ID Int, date Timestamp, country String, name String, phonetype String, serialname String, salary Int) USING org.apache.spark.sql.CarbonSource OPTIONS("tableName"="table11"); "7: Tables shows created as per log." 0: jdbc:hive2://localhost:10000> CREATE TABLE table11 (ID Int, date Timestamp, country String, name String, phonetype String, serialname String, salary Int) USING org.apache.spark.sql.CarbonSource OPTIONS("tableName"="table11");---------+ Result  ---------+---------+No rows selected (0.095 seconds)0: jdbc:hive2://localhost:10000> use vinod;---------+ Result  ---------+---------+No rows selected (0.02 seconds)0: jdbc:hive2://localhost:10000> CREATE TABLE table11 (ID Int, date Timestamp, country String, name String, phonetype String, serialname String, salary Int) USING org.apache.spark.sql.CarbonSource OPTIONS("tableName"="table11");---------+ Result  ---------+---------+No rows selected (0.093 seconds)"8: Table does not create on HDFS.
issueID:CARBONDATA-69
type:Bug
changed files:
texts:Column Group Data loading is Failing
When column group defined is not in schema order then data loading is failing. Please refer below scenario.create table colgrp_disorder (column1 string,column2 string,column3 string,column4 string,column5 string,column6 string,column7 string,column8 string,column9 string,column10 string,measure1 int,measure2 int,measure3 int,measure4 int) STORED BY 'org.apache.carbondata.format' TBLPROPERTIES (\"COLUMN_GROUPS\"=\"(column7,column8),(column2,column3,column4)\")
issueID:CARBONDATA-690
type:Bug
changed files:processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/holder/UnsafeSortTempFileChunkHolder.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/merger/UnsafeIntermediateFileMerger.java
processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactDataHandlerColumnar.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/UnsafeCarbonRowPage.java
texts:Carbon data load fails with default option for USE_KETTLE(False)
When load query is run with default option for USE_KETTLE, it fails at mdkey generation.sample query and issue:LOAD DATA  inpath 'hdfs://hacluster/user/OSCON/sparkhive/warehouse/communication.db/flow_text_1/20140113_0_120.csv' into table flow_carbon options('USE_KETTLE'='FALSE', 'DELIMITER'=',', 'QUOTECHAR'='"','FILEHEADER'='aco_ac,ac_dte,txn_cnt,jrn_par,mfm_jrn_no,cbn_jrn_no,ibs_jrn_no,vch_no,vch_seq,srv_cde,cus_no,bus_cd_no,id_flg,cus_ac,bv_cde,bv_no,txn_dte,txn_time,txn_tlr,txn_bk,txn_br,ety_tlr,ety_bk,ety_br,bus_pss_no,chk_flg,chk_tlr,chk_jrn_no,bus_sys_no,bus_opr_cde,txn_sub_cde,fin_bus_cde,fin_bus_sub_cde,opt_prd_cde,chl,tml_id,sus_no,sus_seq,cho_seq,itm_itm,itm_sub,itm_sss,dc_flg,amt,bal,ccy,spv_flg,vch_vld_dte,pst_bk,pst_br,ec_flg,aco_tlr,opp_ac,opp_ac_nme,opp_bk,gen_flg,his_rec_sum_flg,his_flg,vch_typ,val_dte,opp_ac_flg,cmb_flg,ass_vch_flg,cus_pps_flg,bus_rmk_cde,vch_bus_rmk,tec_rmk_cde,vch_tec_rmk,rsv_ara,own_br,own_bk,gems_last_upd_d,gems_last_upd_d_bat,maps_date,maps_job,dt');Error: java.lang.Exception: DataLoad failure: There is an unexpected error: unable to generate the mdkey (state=,code=0)
issueID:CARBONDATA-691
type:Bug
changed files:hadoop/src/main/java/org/apache/carbondata/hadoop/util/CarbonInputSplitTaskInfo.java
texts:After Compaction records count are mismatched.
Spark version - Spark 1.6.2 and spark2.1 After Compaction data showing is wrong.create table and load 4 times s( compaction threshold is 4,3)Load 4 times same data .each load 105 records as attached in file . -- SegmentSequenceId     Status        Load Start Time            Load End Time       ------------------------------------------------------------------------------ 3                   Compacted   2017-02-01 14:07:51.922   2017-02-01 14:07:52.591   2                   Compacted   2017-02-01 14:07:33.481   2017-02-01 14:07:34.443   1                   Compacted   2017-02-01 14:07:23.495   2017-02-01 14:07:24.167   0.1                 Success     2017-02-01 14:07:52.815   2017-02-01 14:07:57.201   0                   Compacted   2017-02-01 14:07:07.541   2017-02-01 14:07:11.983  ------------------------------------------------------------------------------5 rows selected (0.021 seconds)0: jdbc:hive2://8.99.61.4:23040> select count from Comp_VMALL_DICTIONARY_INCLUDE_7;-----------+ count(1)  -----------+ 1680      -----------+1 row selected (4.468 seconds)0: jdbc:hive2://8.99.61.4:23040> select count(imei) from Comp_VMALL_DICTIONARY_INCLUDE_7;--------------+ count(imei)  --------------+ 1680         --------------+Expected :-  total records should be 420 .
issueID:CARBONDATA-692
type:Bug
changed files:
texts:Support scalar subquery in carbon
Carbon cannot run scalar sub queries like belowselect sum(salary) from scalarsubquery t1where ID < (select sum(ID) from scalarsubquery t2 where t1.name = t2.name
issueID:CARBONDATA-693
type:Bug
changed files:
texts:Issue in select query for function Ceiling & Floor
When executed in Hive  it is working Fine, But when we execute same query in carbondata it gives an error.PushUP_FILTER_Test_Boundary_TC0590: jdbc:hive2://hadoop-master:10000> create table Test_Boundary2 (c1_int int,c2_Bigint Bigint,c3_Decimal Decimal(38,30),c4_double double,c5_string string,c6_Timestamp Timestamp,c7_Datatype_Desc string) ;---------+ result  ---------+---------+No rows selected (0.101 seconds)0: jdbc:hive2://hadoop-master:10000> show tables ;-----------------------------    tableName     isTemporary  ----------------------------- test_boundary    false         test_boundary2   false        -----------------------------2 rows selected (0.02 seconds)0: jdbc:hive2://hadoop-master:10000> load data local inpath '/home/server/Desktop/Test_Data1.csv' into table test_boundary2 ;---------+ Result  ---------+---------+No rows selected (0.201 seconds)0: jdbc:hive2://hadoop-master:10000> select c3_Decimal from Test_Boundary where floor(c3_Decimal)=0.00 or floor(c3_Decimal) IS NULL ;-------------+ c3_Decimal  -------------+-------------+No rows selected (0.315 seconds)0: jdbc:hive2://hadoop-master:10000> select count from Test_Boundary ;------+ _c0  ------+ 0    ------+CarbonData 0: jdbc:hive2://hadoop-master:10000> create table Test_Boundary (c1_int int,c2_Bigint Bigint,c3_Decimal Decimal(38,30),c4_double double,c5_string string,c6_Timestamp Timestamp,c7_Datatype_Desc string) STORED BY 'org.apache.carbondata.format' ;---------+ Result  ---------+---------+No rows selected (0.535 seconds)0: jdbc:hive2://hadoop-master:10000> select count from Test_Boundary ;------+ _c0  ------+ 0    ------+1 row selected (1.883 seconds) hdfs://192.168.2.145:54310hdfs://192.168.2.145:54310/HDFS_URL/BabuStore/Data/Test_Data1.csv (state=,code=0)0: jdbc:hive2://hadoop-master:10000> LOAD DATA INPATH 'hdfs://192.168.2.145:54310/BabuStore/Data/Test_Data1.csv' INTO table Test_Boundary OPTIONS('DELIMITER'=',','QUOTECHAR'='','BAD_RECORDS_ACTION'='FORCE','FILEHEADER'='') ;---------+ Result  ---------+---------+No rows selected (2.436 seconds)0: jdbc:hive2://hadoop-master:10000> select c3_Decimal from Test_Boundary where floor(c3_Decimal)=0.00 or floor(c3_Decimal) IS NULL ;Error: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 7.0 failed 4 times, most recent failure: Lost task 0.3 in stage 7.0 (TID 16, hadoop-master): org.apache.spark.sql.AnalysisException: Decimal scale (0) cannot be greater than precision (-28).; at org.apache.spark.sql.types.PrecisionInfo.<init>(DecimalType.scala:32) at org.apache.spark.sql.types.DecimalType.<init>(DecimalType.scala:68) at org.apache.spark.sql.types.DecimalType$.bounded(DecimalType.scala:155) at org.apache.spark.sql.types.Decimal.floor(Decimal.scala:326) at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificPredicate.eval(Unknown Source) at org.apache.spark.sql.catalyst.expressions.codegen.GeneratePredicate$$anonfun$create$2.apply(GeneratePredicate.scala:68) at org.apache.spark.sql.catalyst.expressions.codegen.GeneratePredicate$$anonfun$create$2.apply(GeneratePredicate.scala:68) at org.apache.spark.sql.execution.Filter$$anonfun$2$$anonfun$apply$2.apply(basicOperators.scala:74) at org.apache.spark.sql.execution.Filter$$anonfun$2$$anonfun$apply$2.apply(basicOperators.scala:72) at scala.collection.Iterator$$anon$14.hasNext(Iterator.scala:390) at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327) at scala.collection.Iterator$class.foreach(Iterator.scala:727) at scala.collection.AbstractIterator.foreach(Iterator.scala:1157) at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48) at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103) at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47) at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273) at scala.collection.AbstractIterator.to(Iterator.scala:1157) at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265) at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157) at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252) at scala.collection.AbstractIterator.toArray(Iterator.scala:1157) at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:927) at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:927) at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1858) at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1858) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66) at org.apache.spark.scheduler.Task.run(Task.scala:89) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745)PushUP_FILTER_Test_Boundary_TC065 : select c3_Decimal from Test_Boundary where floor(c3_Decimal)<=0.0 or floor(c3_Decimal) IS NOT NULLselect c3_Decimal from Test_Boundary where ceil(c3_Decimal)=0.0 or ceiling(c3_Decimal) IS NULLPushUP_FILTER_Test_Boundary_TC085 select c3_Decimal from Test_Boundary where ceil(c3_Decimal)<=0.0 or ceiling(c3_Decimal) IS NOT NULLPushUP_FILTER_Test_Boundary_TC088:select min(c1_int),max(c1_int),sum(c1_int),avg(c1_int) , count(c1_int), variance(c1_int) from Test_Boundary where rand(c1_int)=0.6201007799387834 or rand(c1_int)=0.45540022789662593
issueID:CARBONDATA-694
type:Improvement
changed files:
texts:Optimize quick start document through adding hdfs as storepath
Optimize quick start document through adding hdfs as storepath, such as : val carbon = SparkSession.builder().config(sc.getConf).getOrCreateCarbonSession("hdfs://localhost:9000/carbon/store”)
issueID:CARBONDATA-695
type:Improvement
changed files:
texts:Create CarbonDataFrameExample in example/spark2
Create CarbonDataFrameExample in example/spark2:1.Writes dataframe to carbondata file2. Reads carbon data to dataframe.For spark2, need to define schema for read carbon data, it is different with spark1.x.
issueID:CARBONDATA-696
type:Bug
changed files:
texts:NPE  when select query run on measure having double data type without fraction.
Create table as belowcc.sql("create table oscon_carbon_old  (CUST_PRFRD_FLG String,PROD_BRAND_NAME String,PROD_COLOR String,CUST_LAST_RVW_DATE String,CUST_COUNTRY String,CUST_CITY String,PRODUCT_NAME String,CUST_JOB_TITLE String,CUST_STATE String,CUST_BUY_POTENTIAL String,PRODUCT_MODEL String,ITM_ID String,ITM_NAME String,PRMTION_ID String,PRMTION_NAME String,SHP_MODE_ID String,SHP_MODE String,DELIVERY_COUNTRY String,DELIVERY_STATE String,DELIVERY_CITY String,DELIVERY_DISTRICT String,ACTIVE_EMUI_VERSION String,WH_NAME String,STR_ORDER_DATE String,OL_ORDER_NO String,OL_ORDER_DATE String,OL_SITE String,CUST_FIRST_NAME String,CUST_LAST_NAME String,CUST_BIRTH_DY String,CUST_BIRTH_MM String,CUST_BIRTH_YR String,CUST_BIRTH_COUNTRY String,CUST_SEX String,CUST_ADDRESS_ID String,CUST_STREET_NO String,CUST_STREET_NAME String,CUST_AGE String,CUST_SUITE_NO String,CUST_ZIP String,CUST_COUNTY String,PRODUCT_ID String,PROD_SHELL_COLOR String,DEVICE_NAME String,PROD_SHORT_DESC String,PROD_LONG_DESC String,PROD_THUMB String,PROD_IMAGE String,PROD_UPDATE_DATE String,PROD_LIVE String,PROD_LOC String,PROD_RAM String,PROD_ROM String,PROD_CPU_CLOCK String,PROD_SERIES String,ITM_REC_START_DATE String,ITM_REC_END_DATE String,ITM_BRAND_ID String,ITM_BRAND String,ITM_CLASS_ID String,ITM_CLASS String,ITM_CATEGORY_ID String,ITM_CATEGORY String,ITM_MANUFACT_ID String,ITM_MANUFACT String,ITM_FORMULATION String,ITM_COLOR String,ITM_CONTAINER String,ITM_MANAGER_ID String,PRM_START_DATE String,PRM_END_DATE String,PRM_CHANNEL_DMAIL String,PRM_CHANNEL_EMAIL String,PRM_CHANNEL_CAT String,PRM_CHANNEL_TV String,PRM_CHANNEL_RADIO String,PRM_CHANNEL_PRESS String,PRM_CHANNEL_EVENT String,PRM_CHANNEL_DEMO String,PRM_CHANNEL_DETAILS String,PRM_PURPOSE String,PRM_DSCNT_ACTIVE String,SHP_CODE String,SHP_CARRIER String,SHP_CONTRACT String,CHECK_DATE String,CHECK_YR String,CHECK_MM String,CHECK_DY String,CHECK_HOUR String,BOM String,INSIDE_NAME String,PACKING_DATE String,PACKING_YR String,PACKING_MM String,PACKING_DY String,PACKING_HOUR String,DELIVERY_PROVINCE String,PACKING_LIST_NO String,ACTIVE_CHECK_TIME String,ACTIVE_CHECK_YR String,ACTIVE_CHECK_MM String,ACTIVE_CHECK_DY String,ACTIVE_CHECK_HOUR String,ACTIVE_AREA_ID String,ACTIVE_COUNTRY String,ACTIVE_PROVINCE String,ACTIVE_CITY String,ACTIVE_DISTRICT String,ACTIVE_NETWORK String,ACTIVE_FIRMWARE_VER String,ACTIVE_OS_VERSION String,LATEST_CHECK_TIME String,LATEST_CHECK_YR String,LATEST_CHECK_MM String,LATEST_CHECK_DY String,LATEST_CHECK_HOUR String,LATEST_AREAID String,LATEST_COUNTRY String,LATEST_PROVINCE String,LATEST_CITY String,LATEST_DISTRICT String,LATEST_FIRMWARE_VER String,LATEST_EMUI_VERSION String,LATEST_OS_VERSION String,LATEST_NETWORK String,WH_ID String,WH_STREET_NO String,WH_STREET_NAME String,WH_STREET_TYPE String,WH_SUITE_NO String,WH_CITY String,WH_COUNTY String,WH_STATE String,WH_ZIP String,WH_COUNTRY String,OL_SITE_DESC String,OL_RET_ORDER_NO String,OL_RET_DATE String,PROD_MODEL_ID String,CUST_ID String,PROD_UNQ_MDL_ID String,CUST_NICK_NAME String,CUST_LOGIN String,CUST_EMAIL_ADDR String,PROD_UNQ_DEVICE_ADDR String,PROD_UQ_UUID String,PROD_BAR_CODE String,TRACKING_NO String,STR_ORDER_NO String,CUST_DEP_COUNT double,CUST_VEHICLE_COUNT double,CUST_ADDRESS_CNT double,CUST_CRNT_CDEMO_CNT double,CUST_CRNT_HDEMO_CNT double,CUST_CRNT_ADDR_DM double,CUST_FIRST_SHIPTO_CNT double,CUST_FIRST_SALES_CNT double,CUST_GMT_OFFSET double,CUST_DEMO_CNT double,CUST_INCOME double,PROD_UNLIMITED double,PROD_OFF_PRICE double,PROD_UNITS double,TOTAL_PRD_COST double,TOTAL_PRD_DISC double,PROD_WEIGHT double,REG_UNIT_PRICE double,EXTENDED_AMT double,UNIT_PRICE_DSCNT_PCT double,DSCNT_AMT double,PROD_STD_CST double,TOTAL_TX_AMT double,FREIGHT_CHRG double,WAITING_PERIOD double,DELIVERY_PERIOD double,ITM_CRNT_PRICE double,ITM_UNITS double,ITM_WSLE_CST double,ITM_SIZE double,PRM_CST double,PRM_RESPONSE_TARGET double,PRM_ITM_DM double,SHP_MODE_CNT double,WH_GMT_OFFSET double,WH_SQ_FT double,STR_ORD_QTY double,STR_WSLE_CST double,STR_LIST_PRICE double,STR_SALES_PRICE double,STR_EXT_DSCNT_AMT double,STR_EXT_SALES_PRICE double,STR_EXT_WSLE_CST double,STR_EXT_LIST_PRICE double,STR_EXT_TX double,STR_COUPON_AMT double,STR_NET_PAID double,STR_NET_PAID_INC_TX double,STR_NET_PRFT double,STR_SOLD_YR_CNT double,STR_SOLD_MM_CNT double,STR_SOLD_ITM_CNT double,STR_TOTAL_CUST_CNT double,STR_AREA_CNT double,STR_DEMO_CNT double,STR_OFFER_CNT double,STR_PRM_CNT double,STR_TICKET_CNT double,STR_NET_PRFT_DM_A double,STR_NET_PRFT_DM_B double,STR_NET_PRFT_DM_C double,STR_NET_PRFT_DM_D double,STR_NET_PRFT_DM_E double,STR_RET_STR_ID double,STR_RET_REASON_CNT double,STR_RET_TICKET_NO double,STR_RTRN_QTY double,STR_RTRN_AMT double,STR_RTRN_TX double,STR_RTRN_AMT_INC_TX double,STR_RET_FEE double,STR_RTRN_SHIP_CST double,STR_RFNDD_CSH double,STR_REVERSED_CHRG double,STR_STR_CREDIT double,STR_RET_NET_LOSS double,STR_RTRNED_YR_CNT double,STR_RTRN_MM_CNT double,STR_RET_ITM_CNT double,STR_RET_CUST_CNT double,STR_RET_AREA_CNT double,STR_RET_OFFER_CNT double,STR_RET_PRM_CNT double,STR_RET_NET_LOSS_DM_A double,STR_RET_NET_LOSS_DM_B double,STR_RET_NET_LOSS_DM_C double,STR_RET_NET_LOSS_DM_D double,OL_ORD_QTY double,OL_WSLE_CST double,OL_LIST_PRICE double,OL_SALES_PRICE double,OL_EXT_DSCNT_AMT double,OL_EXT_SALES_PRICE double,OL_EXT_WSLE_CST double,OL_EXT_LIST_PRICE double,OL_EXT_TX double,OL_COUPON_AMT double,OL_EXT_SHIP_CST double,OL_NET_PAID double,OL_NET_PAID_INC_TX double,OL_NET_PAID_INC_SHIP double,OL_NET_PAID_INC_SHIP_TX double,OL_NET_PRFT double,OL_SOLD_YR_CNT double,OL_SOLD_MM_CNT double,OL_SHIP_DATE_CNT double,OL_ITM_CNT double,OL_BILL_CUST_CNT double,OL_BILL_AREA_CNT double,OL_BILL_DEMO_CNT double,OL_BILL_OFFER_CNT double,OL_SHIP_CUST_CNT double,OL_SHIP_AREA_CNT double,OL_SHIP_DEMO_CNT double,OL_SHIP_OFFER_CNT double,OL_WEB_PAGE_CNT double,OL_WEB_SITE_CNT double,OL_SHIP_MODE_CNT double,OL_WH_CNT double,OL_PRM_CNT double,OL_NET_PRFT_DM_A double,OL_NET_PRFT_DM_B double,OL_NET_PRFT_DM_C double,OL_NET_PRFT_DM_D double,OL_RET_RTRN_QTY double,OL_RTRN_AMT double,OL_RTRN_TX double,OL_RTRN_AMT_INC_TX double,OL_RET_FEE double,OL_RTRN_SHIP_CST double,OL_RFNDD_CSH double,OL_REVERSED_CHRG double,OL_ACCOUNT_CREDIT double,OL_RTRNED_YR_CNT double,OL_RTRNED_MM_CNT double,OL_RTRITM_CNT double,OL_RFNDD_CUST_CNT double,OL_RFNDD_AREA_CNT double,OL_RFNDD_DEMO_CNT double,OL_RFNDD_OFFER_CNT double,OL_RTRNING_CUST_CNT double,OL_RTRNING_AREA_CNT double,OL_RTRNING_DEMO_CNT double,OL_RTRNING_OFFER_CNT double,OL_RTRWEB_PAGE_CNT double,OL_REASON_CNT double,OL_NET_LOSS double,OL_NET_LOSS_DM_A double,OL_NET_LOSS_DM_B double,OL_NET_LOSS_DM_C double) STORED BY 'org.apache.carbondata.format' tblproperties('DICTIONARY_EXCLUDE'='CUST_ID,CUST_NICK_NAME,CUST_FIRST_NAME,CUST_LAST_NAME,CUST_LOGIN,CUST_EMAIL_ADDR,PROD_UNQ_MDL_ID,PROD_UNQ_DEVICE_ADDR,PROD_UQ_UUID,DEVICE_NAME,PROD_BAR_CODE,ITM_ID,ITM_NAME,ITM_BRAND_ID,ITM_BRAND,BOM,PACKING_LIST_NO,TRACKING_NO,ACTIVE_FIRMWARE_VER,LATEST_FIRMWARE_VER,LATEST_EMUI_VERSION,LATEST_NETWORK,STR_ORDER_NO','table_blocksize'='256')")    cc.sql("load data LOCAL inpath 'D:\\dataoscon_10.csv' into table oscon_carbon_old options('USE_KETTLE'='true','DELIMITER'=',', 'QUOTECHAR'='\"','FILEHEADER'='ACTIVE_AREA_ID, ACTIVE_CHECK_DY, ACTIVE_CHECK_HOUR, ACTIVE_CHECK_MM, ACTIVE_CHECK_TIME, ACTIVE_CHECK_YR, ACTIVE_CITY, ACTIVE_COUNTRY, ACTIVE_DISTRICT, ACTIVE_EMUI_VERSION, ACTIVE_FIRMWARE_VER, ACTIVE_NETWORK, ACTIVE_OS_VERSION, ACTIVE_PROVINCE, BOM, CHECK_DATE, CHECK_DY, CHECK_HOUR, CHECK_MM, CHECK_YR, CUST_ADDRESS_ID, CUST_AGE, CUST_BIRTH_COUNTRY, CUST_BIRTH_DY, CUST_BIRTH_MM, CUST_BIRTH_YR, CUST_BUY_POTENTIAL, CUST_CITY, CUST_STATE, CUST_COUNTRY, CUST_COUNTY, CUST_EMAIL_ADDR, CUST_LAST_RVW_DATE, CUST_FIRST_NAME, CUST_ID, CUST_JOB_TITLE, CUST_LAST_NAME, CUST_LOGIN, CUST_NICK_NAME, CUST_PRFRD_FLG, CUST_SEX, CUST_STREET_NAME, CUST_STREET_NO, CUST_SUITE_NO, CUST_ZIP, DELIVERY_CITY, DELIVERY_STATE, DELIVERY_COUNTRY, DELIVERY_DISTRICT, DELIVERY_PROVINCE, DEVICE_NAME, INSIDE_NAME, ITM_BRAND, ITM_BRAND_ID, ITM_CATEGORY, ITM_CATEGORY_ID, ITM_CLASS, ITM_CLASS_ID, ITM_COLOR, ITM_CONTAINER, ITM_FORMULATION, ITM_MANAGER_ID, ITM_MANUFACT, ITM_MANUFACT_ID, ITM_ID, ITM_NAME, ITM_REC_END_DATE, ITM_REC_START_DATE, LATEST_AREAID, LATEST_CHECK_DY, LATEST_CHECK_HOUR, LATEST_CHECK_MM, LATEST_CHECK_TIME, LATEST_CHECK_YR, LATEST_CITY, LATEST_COUNTRY, LATEST_DISTRICT, LATEST_EMUI_VERSION, LATEST_FIRMWARE_VER, LATEST_NETWORK, LATEST_OS_VERSION, LATEST_PROVINCE, OL_ORDER_DATE, OL_ORDER_NO, OL_RET_ORDER_NO, OL_RET_DATE, OL_SITE, OL_SITE_DESC, PACKING_DATE, PACKING_DY, PACKING_HOUR, PACKING_LIST_NO, PACKING_MM, PACKING_YR, PRMTION_ID, PRMTION_NAME, PRM_CHANNEL_CAT, PRM_CHANNEL_DEMO, PRM_CHANNEL_DETAILS, PRM_CHANNEL_DMAIL, PRM_CHANNEL_EMAIL, PRM_CHANNEL_EVENT, PRM_CHANNEL_PRESS, PRM_CHANNEL_RADIO, PRM_CHANNEL_TV, PRM_DSCNT_ACTIVE, PRM_END_DATE, PRM_PURPOSE, PRM_START_DATE, PRODUCT_ID, PROD_BAR_CODE, PROD_BRAND_NAME, PRODUCT_NAME, PRODUCT_MODEL, PROD_MODEL_ID, PROD_COLOR, PROD_SHELL_COLOR, PROD_CPU_CLOCK, PROD_IMAGE, PROD_LIVE, PROD_LOC, PROD_LONG_DESC, PROD_RAM, PROD_ROM, PROD_SERIES, PROD_SHORT_DESC, PROD_THUMB, PROD_UNQ_DEVICE_ADDR, PROD_UNQ_MDL_ID, PROD_UPDATE_DATE, PROD_UQ_UUID, SHP_CARRIER, SHP_CODE, SHP_CONTRACT, SHP_MODE_ID, SHP_MODE, STR_ORDER_DATE, STR_ORDER_NO, TRACKING_NO, WH_CITY, WH_COUNTRY, WH_COUNTY, WH_ID, WH_NAME, WH_STATE, WH_STREET_NAME, WH_STREET_NO, WH_STREET_TYPE, WH_SUITE_NO, WH_ZIP, CUST_DEP_COUNT, CUST_VEHICLE_COUNT, CUST_ADDRESS_CNT, CUST_CRNT_CDEMO_CNT, CUST_CRNT_HDEMO_CNT, CUST_CRNT_ADDR_DM, CUST_FIRST_SHIPTO_CNT, CUST_FIRST_SALES_CNT, CUST_GMT_OFFSET, CUST_DEMO_CNT, CUST_INCOME, PROD_UNLIMITED, PROD_OFF_PRICE, PROD_UNITS, TOTAL_PRD_COST, TOTAL_PRD_DISC, PROD_WEIGHT, REG_UNIT_PRICE, EXTENDED_AMT, UNIT_PRICE_DSCNT_PCT, DSCNT_AMT, PROD_STD_CST, TOTAL_TX_AMT, FREIGHT_CHRG, WAITING_PERIOD, DELIVERY_PERIOD, ITM_CRNT_PRICE, ITM_UNITS, ITM_WSLE_CST, ITM_SIZE, PRM_CST, PRM_RESPONSE_TARGET, PRM_ITM_DM, SHP_MODE_CNT, WH_GMT_OFFSET, WH_SQ_FT, STR_ORD_QTY, STR_WSLE_CST, STR_LIST_PRICE, STR_SALES_PRICE, STR_EXT_DSCNT_AMT, STR_EXT_SALES_PRICE, STR_EXT_WSLE_CST, STR_EXT_LIST_PRICE, STR_EXT_TX, STR_COUPON_AMT, STR_NET_PAID, STR_NET_PAID_INC_TX, STR_NET_PRFT, STR_SOLD_YR_CNT, STR_SOLD_MM_CNT, STR_SOLD_ITM_CNT, STR_TOTAL_CUST_CNT, STR_AREA_CNT, STR_DEMO_CNT, STR_OFFER_CNT, STR_PRM_CNT, STR_TICKET_CNT, STR_NET_PRFT_DM_A, STR_NET_PRFT_DM_B, STR_NET_PRFT_DM_C, STR_NET_PRFT_DM_D, STR_NET_PRFT_DM_E, STR_RET_STR_ID, STR_RET_REASON_CNT, STR_RET_TICKET_NO, STR_RTRN_QTY, STR_RTRN_AMT, STR_RTRN_TX, STR_RTRN_AMT_INC_TX, STR_RET_FEE, STR_RTRN_SHIP_CST, STR_RFNDD_CSH, STR_REVERSED_CHRG, STR_STR_CREDIT, STR_RET_NET_LOSS, STR_RTRNED_YR_CNT, STR_RTRN_MM_CNT, STR_RET_ITM_CNT, STR_RET_CUST_CNT, STR_RET_AREA_CNT, STR_RET_OFFER_CNT, STR_RET_PRM_CNT, STR_RET_NET_LOSS_DM_A, STR_RET_NET_LOSS_DM_B, STR_RET_NET_LOSS_DM_C, STR_RET_NET_LOSS_DM_D, OL_ORD_QTY, OL_WSLE_CST, OL_LIST_PRICE, OL_SALES_PRICE, OL_EXT_DSCNT_AMT, OL_EXT_SALES_PRICE, OL_EXT_WSLE_CST, OL_EXT_LIST_PRICE, OL_EXT_TX, OL_COUPON_AMT, OL_EXT_SHIP_CST, OL_NET_PAID, OL_NET_PAID_INC_TX, OL_NET_PAID_INC_SHIP, OL_NET_PAID_INC_SHIP_TX, OL_NET_PRFT, OL_SOLD_YR_CNT, OL_SOLD_MM_CNT, OL_SHIP_DATE_CNT, OL_ITM_CNT, OL_BILL_CUST_CNT, OL_BILL_AREA_CNT, OL_BILL_DEMO_CNT, OL_BILL_OFFER_CNT, OL_SHIP_CUST_CNT, OL_SHIP_AREA_CNT, OL_SHIP_DEMO_CNT, OL_SHIP_OFFER_CNT, OL_WEB_PAGE_CNT, OL_WEB_SITE_CNT, OL_SHIP_MODE_CNT, OL_WH_CNT, OL_PRM_CNT, OL_NET_PRFT_DM_A, OL_NET_PRFT_DM_B, OL_NET_PRFT_DM_C, OL_NET_PRFT_DM_D, OL_RET_RTRN_QTY, OL_RTRN_AMT, OL_RTRN_TX, OL_RTRN_AMT_INC_TX, OL_RET_FEE, OL_RTRN_SHIP_CST, OL_RFNDD_CSH, OL_REVERSED_CHRG, OL_ACCOUNT_CREDIT, OL_RTRNED_YR_CNT, OL_RTRNED_MM_CNT, OL_RTRITM_CNT, OL_RFNDD_CUST_CNT, OL_RFNDD_AREA_CNT, OL_RFNDD_DEMO_CNT, OL_RFNDD_OFFER_CNT, OL_RTRNING_CUST_CNT, OL_RTRNING_AREA_CNT, OL_RTRNING_DEMO_CNT, OL_RTRNING_OFFER_CNT, OL_RTRWEB_PAGE_CNT, OL_REASON_CNT, OL_NET_LOSS, OL_NET_LOSS_DM_A, OL_NET_LOSS_DM_B, OL_NET_LOSS_DM_C','BAD_RECORDS_ACTION'='FORCE','BAD_RECORDS_LOGGER_ENABLE'='FALSE')")    cc.sql("select count from oscon_carbon_old").show()    cc.sql("select OL_SALES_PRICE from oscon_carbon_old limit 10").show()ERROR Caused by: java.lang.NullPointerException at org.apache.carbondata.core.datastore.compression.nondecimal.CompressionNonDecimalMaxMinInt.setUncompressedValues(CompressionNonDecimalMaxMinInt.java:95) at org.apache.carbondata.core.datastore.compression.nondecimal.CompressionNonDecimalMaxMinInt.uncompress(CompressionNonDecimalMaxMinInt.java:75) at org.apache.carbondata.core.datastore.chunk.reader.measure.v2.CompressedMeasureChunkFileBasedReaderV2.readMeasureChunksInGroup(CompressedMeasureChunkFileBasedReaderV2.java:218) at org.apache.carbondata.core.datastore.chunk.reader.measure.v2.CompressedMeasureChunkFileBasedReaderV2.readMeasureChunks(CompressedMeasureChunkFileBasedReaderV2.java:114) at org.apache.carbondata.core.datastore.impl.btree.BlockletBTreeLeafNode.getMeasureChunks(BlockletBTreeLeafNode.java:113)OL_SALES_PRICE  has one value without fraction .
issueID:CARBONDATA-697
type:Bug
changed files:
texts:single_pass is not used while doing data load
1. 1st load with 'USE_KETTLE'='FALSE','SINGLE_PASS'='FALSE'2. 2nd load with 'USE_KETTLE'='FALSE','SINGLE_PASS'='TRUE'Expected: GlobalDictionaryUtil should not be triggered in the 2nd time loadActual :  GlobalDictionaryUtil was triggeredJob Id (Job Group)  ▾DescriptionSubmittedDurationStages: Succeeded/Total Tasks (for all stages): Succeeded/Total3 (891a750d-1e08-432f-bcf4-5501332d6d09) LOAD DATA inpath 'hdfs://hacluster/commbank/splits/FG2' into table flow_carbon options('USE_KETTLE'='FALSE','SINGLE_PASS'='TRUE','DELIMITER'=',', 'QUOTECHAR'='"','FILEHEADER'='aco_ac,ac_dte,txn_cnt,jrn_par,mfm_jrn_no,cbn_jrn_no,ibs_jrn_no,vch_no,vch_seq,srv_cde,cus_no,bus_cd_no,id_flg,cus_ac,bv_cde,bv_no,txn_dte,txn_time,txn_tlr,txn_bk,txn_br,ety_tlr,ety_bk,ety_br,bus_pss_no,chk_flg,chk_tlr,chk_jrn_no,bus_sys_no,bus_opr_cde,txn_sub_cde,fin_bus_cde,fin_bus_sub_cde,opt_prd_cde,chl,tml_id,sus_no,sus_seq,cho_seq,itm_itm,itm_sub,itm_sss,dc_flg,amt,bal,ccy,spv_flg,vch_vld_dte,pst_bk,pst_br,ec_flg,aco_tlr,opp_ac,opp_ac_nme,opp_bk,gen_flg,his_rec_sum_flg,his_flg,vch_typ,val_dte,opp_ac_flg,cmb_flg,ass_vch_flg,cus_pps_flg,bus_rmk_cde,vch_bus_rmk,tec_rmk_cde,vch_tec_rmk,rsv_ara,own_br,own_bk,gems_last_upd_d,gems_last_upd_d_bat,maps_date,maps_job,dt')collect at CarbonDataRDDFactory.scala:651 2017/02/06 15:23:05 41 s 1/1 3/32 (891a750d-1e08-432f-bcf4-5501332d6d09) LOAD DATA inpath 'hdfs://hacluster/commbank/splits/FG2' into table flow_carbon options('USE_KETTLE'='FALSE','SINGLE_PASS'='TRUE','DELIMITER'=',', 'QUOTECHAR'='"','FILEHEADER'='aco_ac,ac_dte,txn_cnt,jrn_par,mfm_jrn_no,cbn_jrn_no,ibs_jrn_no,vch_no,vch_seq,srv_cde,cus_no,bus_cd_no,id_flg,cus_ac,bv_cde,bv_no,txn_dte,txn_time,txn_tlr,txn_bk,txn_br,ety_tlr,ety_bk,ety_br,bus_pss_no,chk_flg,chk_tlr,chk_jrn_no,bus_sys_no,bus_opr_cde,txn_sub_cde,fin_bus_cde,fin_bus_sub_cde,opt_prd_cde,chl,tml_id,sus_no,sus_seq,cho_seq,itm_itm,itm_sub,itm_sss,dc_flg,amt,bal,ccy,spv_flg,vch_vld_dte,pst_bk,pst_br,ec_flg,aco_tlr,opp_ac,opp_ac_nme,opp_bk,gen_flg,his_rec_sum_flg,his_flg,vch_typ,val_dte,opp_ac_flg,cmb_flg,ass_vch_flg,cus_pps_flg,bus_rmk_cde,vch_bus_rmk,tec_rmk_cde,vch_tec_rmk,rsv_ara,own_br,own_bk,gems_last_upd_d,gems_last_upd_d_bat,maps_date,maps_job,dt')collect at GlobalDictionaryUtil.scala:742 2017/02/06 15:22:29 36 s 2/2 42/421 (c7706941-8184-40ab-9ceb-712af96e15e3) LOAD DATA inpath 'hdfs://hacluster/commbank/splits/FG2' into table flow_carbon options('USE_KETTLE'='FALSE','SINGLE_PASS'='FALSE','DELIMITER'=',', 'QUOTECHAR'='"','FILEHEADER'='aco_ac,ac_dte,txn_cnt,jrn_par,mfm_jrn_no,cbn_jrn_no,ibs_jrn_no,vch_no,vch_seq,srv_cde,cus_no,bus_cd_no,id_flg,cus_ac,bv_cde,bv_no,txn_dte,txn_time,txn_tlr,txn_bk,txn_br,ety_tlr,ety_bk,ety_br,bus_pss_no,chk_flg,chk_tlr,chk_jrn_no,bus_sys_no,bus_opr_cde,txn_sub_cde,fin_bus_cde,fin_bus_sub_cde,opt_prd_cde,chl,tml_id,sus_no,sus_seq,cho_seq,itm_itm,itm_sub,itm_sss,dc_flg,amt,bal,ccy,spv_flg,vch_vld_dte,pst_bk,pst_br,ec_flg,aco_tlr,opp_ac,opp_ac_nme,opp_bk,gen_flg,his_rec_sum_flg,his_flg,vch_typ,val_dte,opp_ac_flg,cmb_flg,ass_vch_flg,cus_pps_flg,bus_rmk_cde,vch_bus_rmk,tec_rmk_cde,vch_tec_rmk,rsv_ara,own_br,own_bk,gems_last_upd_d,gems_last_upd_d_bat,maps_date,maps_job,dt')collect at CarbonDataRDDFactory.scala:651 2017/02/06 15:21:42 47 s 1/1 3/30 (c7706941-8184-40ab-9ceb-712af96e15e3) LOAD DATA inpath 'hdfs://hacluster/commbank/splits/FG2' into table flow_carbon options('USE_KETTLE'='FALSE','SINGLE_PASS'='FALSE','DELIMITER'=',', 'QUOTECHAR'='"','FILEHEADER'='aco_ac,ac_dte,txn_cnt,jrn_par,mfm_jrn_no,cbn_jrn_no,ibs_jrn_no,vch_no,vch_seq,srv_cde,cus_no,bus_cd_no,id_flg,cus_ac,bv_cde,bv_no,txn_dte,txn_time,txn_tlr,txn_bk,txn_br,ety_tlr,ety_bk,ety_br,bus_pss_no,chk_flg,chk_tlr,chk_jrn_no,bus_sys_no,bus_opr_cde,txn_sub_cde,fin_bus_cde,fin_bus_sub_cde,opt_prd_cde,chl,tml_id,sus_no,sus_seq,cho_seq,itm_itm,itm_sub,itm_sss,dc_flg,amt,bal,ccy,spv_flg,vch_vld_dte,pst_bk,pst_br,ec_flg,aco_tlr,opp_ac,opp_ac_nme,opp_bk,gen_flg,his_rec_sum_flg,his_flg,vch_typ,val_dte,opp_ac_flg,cmb_flg,ass_vch_flg,cus_pps_flg,bus_rmk_cde,vch_bus_rmk,tec_rmk_cde,vch_tec_rmk,rsv_ara,own_br,own_bk,gems_last_upd_d,gems_last_upd_d_bat,maps_date,maps_job,dt')collect at GlobalDictionaryUtil.scala:742
issueID:CARBONDATA-698
type:Bug
changed files:
texts:no_inverted_index is not working
i am creating no_inverted_index with invalid values using both spark 1.6 and 2.1 and it works spark 2.1 logs0: jdbc:hive2://localhost:10000> DROP TABLE IF EXISTS  productSalesTable;---------+ Result  ---------+---------+No rows selected (3.621 seconds)0: jdbc:hive2://localhost:10000>   CREATE TABLE productSalesTable( productNumber Int, productName String, storeCity String, storeProvince String, productCategory String, productBatch String, saleQuantity Int, revenue Int) STORED BY 'carbondata' TBLPROPERTIES ('COLUMN_GROUPS'='( productName)','DICTIONARY_INCLUDE'='productName', 'NO_INVERTED_INDEX'='1');spark 1.6 logscc.sql("DROP TABLE IF EXISTS  productSalesTable").show();    cc.sql("""CREATE TABLE productSalesTable( productNumber Int, productName String, storeCity String, storeProvince String, productCategory String, productBatch String, saleQuantity Int, revenue Int) STORED BY 'carbondata' TBLPROPERTIES ('COLUMN_GROUPS'='( productName)','DICTIONARY_INCLUDE'='productName', 'NO_INVERTED_INDEX'='1')""").show()AUDIT 07-02 15:48:14,485 - &#91;knoldus&#93;&#91;knoldus&#93;&#91;Thread-1&#93;Creating Table with Database name &#91;default&#93; and Table name &#91;productsalestable&#93;AUDIT 07-02 15:48:14,868 - &#91;knoldus&#93;&#91;knoldus&#93;&#91;Thread-1&#93;Table created with Database name &#91;default&#93; and Table name &#91;productsalestable&#93;while debugging the code i found out in carbon ddl sql parser tablePropertiess map contain the properties in lower caseand we are checking tableProperties.get("NO_INVERTED_INDEX")so it is failing due to string mismatch
issueID:CARBONDATA-7
type:Bug
changed files:
texts:Fortify issue fixes
Fix Explicit null dereferenced ,Dereference null return value
issueID:CARBONDATA-700
type:Bug
changed files:
texts:invalid example of no_inverted_index in carbondata ddl docs
example given in carbon docs ddl is wrong0: jdbc:hive2://localhost:10000> CREATE TABLE IF NOT EXISTS productSchema.productSalesTable ( productNumber Int, productName String, storeCity String, storeProvince String, productCategory String, productBatch String, saleQuantity Int, revenue Int) STORED BY 'carbondata' TBLPROPERTIES ('COLUMN_GROUPS'='(productName,productCategory)', 'DICTIONARY_EXCLUDE'='productName', 'DICTIONARY_INCLUDE'='productNumber', 'NO_INVERTED_INDEX'='productBatch');Error: org.apache.carbondata.spark.exception.MalformedCarbonCommandException: Column group is not supported for no dictionary columns:productname (state=,code=0) and further more column group should be contigous as well,so there are two issues
issueID:CARBONDATA-701
type:Improvement
changed files:processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/SortDataRows.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/SortParameters.java
processing/src/main/java/org/apache/carbondata/processing/loading/steps/DataWriterProcessorStepImpl.java
processing/src/main/java/org/apache/carbondata/processing/loading/AbstractDataLoadProcessorStep.java
core/src/main/java/org/apache/carbondata/core/util/CarbonProperties.java
processing/src/main/java/org/apache/carbondata/processing/loading/steps/InputProcessorStepImpl.java
processing/src/main/java/org/apache/carbondata/processing/loading/steps/DataConverterProcessorStepImpl.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
processing/src/main/java/org/apache/carbondata/processing/loading/csvinput/CSVInputFormat.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/impl/ParallelReadMergeSorterImpl.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/impl/ParallelReadMergeSorterWithBucketingImpl.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/holder/UnsafeSortTempFileChunkHolder.java
processing/src/main/java/org/apache/carbondata/processing/loading/row/CarbonRowBatch.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/impl/UnsafeParallelReadMergeSorterImpl.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/SortTempFileChunkHolder.java
texts:There is a memory leak issue in no kettle loading flow
When loading more data, throw OOM exception.
issueID:CARBONDATA-702
type:Bug
changed files:
texts:Created carbondata repository with adding format jar for facilitating compile
Create carbondata repository to keep format jar for facilitating compile.
issueID:CARBONDATA-703
type:Bug
changed files:
texts:Update build command after optimizing thrift compile issues
Update build command after optimizing thrift compile issues
issueID:CARBONDATA-704
type:Bug
changed files:
texts:data mismatch between hive and carbondata after loading for bigint values
carbondata0: jdbc:hive2://localhost:10000> create table Test_Boundary (c1_int int,c2_Bigint Bigint,c3_Decimal Decimal(38,30),c4_double double,c5_string string,c6_Timestamp Timestamp,c7_Datatype_Desc string) STORED BY 'org.apache.carbondata.format' ;0: jdbc:hive2://localhost:10000>  LOAD DATA INPATH 'hdfs://localhost:54310/Test_Data1.csv' INTO table Test_Boundary OPTIONS                            ('DELIMITER'=',','QUOTECHAR'='','BAD_RECORDS_ACTION'='FORCE','FILEHEADER'='');0: jdbc:hive2://localhost:10000> select c2_Bigint from Test_Boundary;----------------------+      c2_Bigint       ----------------------+ NULL                  NULL                  NULL                  9223372036854775807   9223372036854775807   9223372036854775807   9223372036854775807   9223372036854775807   9223372036854775807   9223372036854775807   9223372036854775807   9223372036854775807   9223372036854775807   9223372036854775807   9223372036854775807   9223372036854775807  ----------------------+but in hivecreate table Test_Boundary_hive (c1_int int,c2_Bigint Bigint,c3_Decimal Decimal(38,30),c4_double double,c5_string string,c6_Timestamp Timestamp,c7_Datatype_Desc string)  ROW FORMAT DELIMITED FIELDS TERMINATED BY ",";LOAD DATA LOCAL INPATH 'Test_Data1.csv' into table Test_Boundary_hive;select c2_Bigint from Test_Boundary_hive;-----------------------+       c2_Bigint       -----------------------+ 1234                   2345                   3456                   4567                   9223372036854775807    -9223372036854775808   -9223372036854775807   -9223372036854775806   -9223372036854775805   0                      9223372036854775807    9223372036854775807    9223372036854775807    NULL                   NULL                   NULL                  -----------------------+
issueID:CARBONDATA-705
type:Bug
changed files:
texts:Make the partition distribution as configurable and keep spark distribution as default
Make the partition distribution as configurable and keep spark distribution as default.
issueID:CARBONDATA-706
type:Bug
changed files:
texts:Mulitiple OR operators does not work properly in carbondata
Incorrect result displays with multiple OR operators. Steps to Reproduces:1:Create table using following Command " create table Carbon_automation (imei string,deviceInformationId int,MAC string,deviceColor string,device_backColor string,modelId string,marketName string,AMSize string,ROMSize string,CUPAudit string,CPIClocked string,series string,productionDate timestamp,bomCode string,internalModels string, deliveryTime string, channelsId string, channelsName string , deliveryAreaId string, deliveryCountry string, deliveryProvince string, deliveryCity string,deliveryDistrict string, deliveryStreet string, oxSingleNumber string, ActiveCheckTime string, ActiveAreaId string, ActiveCountry string, ActiveProvince string, Activecity string, ActiveDistrict string, ActiveStreet string, ActiveOperatorId string, Active_releaseId string, Active_EMUIVersion string, Active_operaSysVersion string, Active_BacVerNumber string, Active_BacFlashVer string, Active_webUIVersion string, Active_webUITypeCarrVer string,Active_webTypeDataVerNumber string, Active_operatorsVersion string, Active_phonePADPartitionedVersions string, Latest_YEAR int, Latest_MONTH int, Latest_DAY int, Latest_HOUR string, Latest_areaId string, Latest_country string, Latest_province string, Latest_city string, Latest_district string, Latest_street string, Latest_releaseId string, Latest_EMUIVersion string, Latest_operaSysVersion string, Latest_BacVerNumber string, Latest_BacFlashVer string, Latest_webUIVersion string, Latest_webUITypeCarrVer string, Latest_webTypeDataVerNumber string, Latest_operatorsVersion string, Latest_phonePADPartitionedVersions string, Latest_operatorId string, gamePointDescription string,gamePointId double,contractNumber double,imei_count int) STORED BY 'org.apache.carbondata.format' TBLPROPERTIES ('DICTIONARY_INCLUDE'='deviceInformationId,Latest_YEAR,Latest_MONTH,Latest_DAY')"2:Load Data with following command " LOAD DATA INPATH 'HDFS_URL/BabuStore/Data/HiveData' INTO TABLE Carbon_automation OPTIONS('DELIMITER'=',','QUOTECHAR'='"','BAD_RECORDS_ACTION'='FORCE','FILEHEADER'='imei,deviceInformationId,MAC,deviceColor,device_backColor,modelId,marketName,AMSize,ROMSize,CUPAudit,CPIClocked,series,productionDate,bomCode,internalModels,deliveryTime,channelsId,channelsName,deliveryAreaId,deliveryCountry,deliveryProvince,deliveryCity,deliveryDistrict,deliveryStreet,oxSingleNumber,contractNumber,ActiveCheckTime,ActiveAreaId,ActiveCountry,ActiveProvince,Activecity,ActiveDistrict,ActiveStreet,ActiveOperatorId,Active_releaseId,Active_EMUIVersion,Active_operaSysVersion,Active_BacVerNumber,Active_BacFlashVer,Active_webUIVersion,Active_webUITypeCarrVer,Active_webTypeDataVerNumber,Active_operatorsVersion,Active_phonePADPartitionedVersions,Latest_YEAR,Latest_MONTH,Latest_DAY,Latest_HOUR,Latest_areaId,Latest_country,Latest_province,Latest_city,Latest_district,Latest_street,Latest_releaseId,Latest_EMUIVersion,Latest_operaSysVersion,Latest_BacVerNumber,Latest_BacFlashVer,Latest_webUIVersion,Latest_webUITypeCarrVer,Latest_webTypeDataVerNumber,Latest_operatorsVersion,Latest_phonePADPartitionedVersions,Latest_operatorId,gamePointId,gamePointDescription,imei_count')"3:Now run the Select Query:" select imei,gamePointId, channelsId,series  from Carbon_automation where channelsId >=10 OR channelsId <=1 or series='7Series' "4:Result Displays " 0: jdbc:hive2://localhost:10000> select imei,gamePointId, channelsId,series  from Carbon_automation where channelsId >=10 OR channelsId <=1 or series='7Series';--------------------------------------------    imei      gamePointId   channelsId    series  -------------------------------------------- 1AA1         2738.562      4            7Series   1AA10        1714.635      4            7Series   1AA100       1271.0        6            5Series   1AA1000      692.0         3            5Series   1AA10000     2175.0        1            7Series   1AA100000    136.0         6            9Series   1AA1000000   1600.0        6            7Series   1AA100001    505.0         7            0Series   1AA100002    1341.0        3            0Series   1AA100003    2239.0        3            5Series   1AA100004    2970.0        2            4Series   1AA100005    2593.0        1            1Series   1AA100006    2572.0        2            6Series   1AA100007    1991.0        3            9Series   1AA100008    1442.0        1            8Series   1AA100009    1841.0        3            0Series   1AA10001     298.0         5            2Series   1AA100010    79.0          6            3Series   1AA100011    202.0         1            0Series   1AA100012    568.0         2            4Series   1AA100013    355.0         6            1Series   1AA100014    151.0         3            5Series   1AA100015    2863.0        1            4Series   1AA100016    1873.0        3            3Series   1AA100017    2205.0        3            9Series   1AA100018    441.0         4            8Series   1AA100019    2194.0        3            5Series   1AA10002     2972.0        5            0Series   1AA100020    256.0         7            5Series   1AA100021    1778.0        6            0Series   1AA100022    1999.0        5            5Series   1AA100023    2194.0        3            5Series   1AA100024    2483.0        2            6Series   1AA100025    1724.0        1            0Series   1AA100026    1768.0        7            7Series   1AA100027    2436.0        4            0Series   1AA100028    2849.0        5            5Series   1AA100029    1691.0        5            2Series   1AA10003     2071.0        4            7Series   1AA100030    1333.0        3            7Series   1AA100031    1080.0        1            7Series   1AA100032    1053.0        7            1Series   1AA100033    760.0         6            8Series   1AA100034    2061.0        6            2Series   1AA100035    2142.0        7            5Series   1AA100036    2224.0        5            5Series   1AA100037    1015.0        6            7Series   1AA100038    1229.0        3            6Series   1AA100039    1750.0        1            8Series   1AA10004     1717.0        4            5Series   1AA100040    2078.0        7            8Series   1AA100041    2734.0        1            5Series   1AA100042    2745.0        5            3Series   1AA100043    571.0         6            9Series   1AA100044    1697.0        2            8Series   1AA100045    2553.0        6            2Series   1AA100046    1077.0        4            3Series   1AA100047    1823.0        1            9Series   1AA100048    2399.0        1            3Series   1AA100049    2890.0        6            0Series   1AA10005     1608.0        2            8Series   1AA100050    29.0          1            2Series   1AA100051    1407.0        7            2Series   1AA100052    845.0         7            6Series   1AA100053    1655.0        3            2Series   1AA100054    1368.0        2            7Series   1AA100055    1728.0        7            7Series   1AA100056    750.0         5            6Series   1AA100057    2288.0        6            9Series   1AA100058    2635.0        4            5Series   1AA100059    1337.0        7            4Series   1AA10006     2478.0        5            3Series   1AA100060    538.0         4            8Series   1AA100061    1407.0        6            6Series   1AA100062    2952.0        6            9Series   1AA100063    1226.0        3            2Series   1AA100064    865.0         7            6Series   1AA100065    901.0         7            0Series   1AA100066    1864.0        4            6Series   1AA100067    572.0         7            4Series   1AA100068    412.0         7            8Series   1AA100069    1491.0        5            8Series   1AA10007     1350.0        3            8Series   1AA100070    1567.0        3            0Series   1AA100071    1973.0        5            0Series   1AA100072    448.0         7            4Series   1AA100073    2488.0        2            4Series   1AA100074    907.0         7            6Series   1AA100075    2507.0        6            3Series   1AA100076    732.0         7            0Series   1AA100077    2077.0        6            3Series   1AA100078    1434.0        5            2Series   1AA100079    1098.0        6            4Series   1AA10008     813.0         4            5Series   1AA100080    954.0         6            9Series   1AA100081    613.0         2            5Series   1AA100082    2348.0        7            5Series   1AA100083    2192.0        2            0Series   1AA100084    2826.0        4            0Series  --------------------------------------------99 rows selected (0.221 seconds)"5:Csv attached  "100_hive_test.csv"
issueID:CARBONDATA-708
type:Bug
changed files:
texts:Between operator does not work properly in carbondata.
Incorrect result displays.Note:Issue exist when you take Latest_HOUR as a String but if you take Latest_HOUR as a int then it's working fine.Steps to reproduce:1:Create table using following Command" create table Carbon_automation (imei string,deviceInformationId int,MAC string,deviceColor string,device_backColor string,modelId string,marketName string,AMSize string,ROMSize string,CUPAudit string,CPIClocked string,series string,productionDate timestamp,bomCode string,internalModels string, deliveryTime string, channelsId string, channelsName string , deliveryAreaId string, deliveryCountry string, deliveryProvince string, deliveryCity string,deliveryDistrict string, deliveryStreet string, oxSingleNumber string, ActiveCheckTime string, ActiveAreaId string, ActiveCountry string, ActiveProvince string, Activecity string, ActiveDistrict string, ActiveStreet string, ActiveOperatorId string, Active_releaseId string, Active_EMUIVersion string, Active_operaSysVersion string, Active_BacVerNumber string, Active_BacFlashVer string, Active_webUIVersion string, Active_webUITypeCarrVer string,Active_webTypeDataVerNumber string, Active_operatorsVersion string, Active_phonePADPartitionedVersions string, Latest_YEAR int, Latest_MONTH int, Latest_DAY int, Latest_HOUR string, Latest_areaId string, Latest_country string, Latest_province string, Latest_city string, Latest_district string, Latest_street string, Latest_releaseId string, Latest_EMUIVersion string, Latest_operaSysVersion string, Latest_BacVerNumber string, Latest_BacFlashVer string, Latest_webUIVersion string, Latest_webUITypeCarrVer string, Latest_webTypeDataVerNumber string, Latest_operatorsVersion string, Latest_phonePADPartitionedVersions string, Latest_operatorId string, gamePointDescription string,gamePointId double,contractNumber double,imei_count int) STORED BY 'org.apache.carbondata.format' TBLPROPERTIES ('DICTIONARY_INCLUDE'='deviceInformationId,Latest_YEAR,Latest_MONTH,Latest_DAY')"2:Load Data with following command" LOAD DATA INPATH 'HDFS_URL/BabuStore/Data/HiveData' INTO TABLE Carbon_automation OPTIONS('DELIMITER'=',','QUOTECHAR'='"','BAD_RECORDS_ACTION'='FORCE','FILEHEADER'='imei,deviceInformationId,MAC,deviceColor,device_backColor,modelId,marketName,AMSize,ROMSize,CUPAudit,CPIClocked,series,productionDate,bomCode,internalModels,deliveryTime,channelsId,channelsName,deliveryAreaId,deliveryCountry,deliveryProvince,deliveryCity,deliveryDistrict,deliveryStreet,oxSingleNumber,contractNumber,ActiveCheckTime,ActiveAreaId,ActiveCountry,ActiveProvince,Activecity,ActiveDistrict,ActiveStreet,ActiveOperatorId,Active_releaseId,Active_EMUIVersion,Active_operaSysVersion,Active_BacVerNumber,Active_BacFlashVer,Active_webUIVersion,Active_webUITypeCarrVer,Active_webTypeDataVerNumber,Active_operatorsVersion,Active_phonePADPartitionedVersions,Latest_YEAR,Latest_MONTH,Latest_DAY,Latest_HOUR,Latest_areaId,Latest_country,Latest_province,Latest_city,Latest_district,Latest_street,Latest_releaseId,Latest_EMUIVersion,Latest_operaSysVersion,Latest_BacVerNumber,Latest_BacFlashVer,Latest_webUIVersion,Latest_webUITypeCarrVer,Latest_webTypeDataVerNumber,Latest_operatorsVersion,Latest_phonePADPartitionedVersions,Latest_operatorId,gamePointId,gamePointDescription,imei_count')"3:Run the Queryselect Latest_DAY,Latest_HOUR,count(distinct AMSize) as AMSize_number,sum(gamePointId+contractNumber) as total from Carbon_automation where Latest_HOUR between 12 and 15 group by Latest_DAY,Latest_HOUR order by total desc4:No Result display:--------------------------------------------- Latest_DAY   Latest_HOUR   AMSize_number   total  ------------------------------------------------------------------------------------------No rows selected (2.133 seconds).5:CSV Attached "100_hive_test.csv"Expected Result:Correct Result should be display.
issueID:CARBONDATA-709
type:Bug
changed files:
texts:Incorrect documentation for bucketing in ddl section
in docs ddl bucketing section Columns in the BUCKETCOLUMN parameter must be either a dimension or a measure but combination of both is not supported. this line is incorrect here is the example0: jdbc:hive2://localhost:10000> CREATE TABLE uniqData_t11(ID Int,name string)stored by 'carbondata' TBLPROPERTIES("DICTIONARY_EXCLUDE"="name","bucketnumber"="1", "bucketcolumns"="ID");Error: java.lang.RuntimeException: Bucket field must be dimension column and should not be measure or complex column: ID (state=,code=0)so bucketing coloumn should be dimension onlyplus in parameter description tableName is not required and the example that added for bucketing is also wrong0: jdbc:hive2://localhost:10000> CREATE TABLE IF NOT EXISTS productSchema.productSalesTable ( productNumber Int, productName String, storeCity String, storeProvince String, productCategory String, productBatch String, saleQuantity Int, revenue Int) STORED BY 'carbondata' TBLPROPERTIES ('COLUMN_GROUPS'='(productName,productCategory)', 'DICTIONARY_EXCLUDE'='productName', 'DICTIONARY_INCLUDE'='productNumber', 'NO_INVERTED_INDEX'='productBatch', 'BUCKETNUMBER'='4', 'BUCKETCOLUMNS'='productNumber,saleQuantity');Error: org.apache.carbondata.spark.exception.MalformedCarbonCommandException: Invalid column group,column in group should be contiguous as per schema. (state=,code=0)
issueID:CARBONDATA-71
type:Bug
changed files:
texts:Percentile Aggregate function is not working for carbon format
We convert integer to double while data loading. But percentile works only for int and long. Hence we need to change int to long while data loading.
issueID:CARBONDATA-710
type:Improvement
changed files:
texts:Add content to Faqs and Troubleshooting

issueID:CARBONDATA-711
type:Bug
changed files:
texts:Inconsistent data load when single_pass=&#39;true&#39;
When we perform dataload with Single_pass='true' , it repeats some of the values in the table whereas the csv contains empty value for that column. PFA csv which is used for dataloading. And below is the create , load . and select query.CREATE TABLE uniq_shared_dictionary (CUST_ID int,CUST_NAME String,ACTIVE_EMUI_VERSION string, DOB timestamp, DOJ timestamp, BIGINT_COLUMN1 bigint,BIGINT_COLUMN2 bigint,DECIMAL_COLUMN1 decimal(30,10), DECIMAL_COLUMN2 decimal(36,10),Double_COLUMN1 double, Double_COLUMN2 double,INTEGER_COLUMN1 int) STORED BY 'org.apache.carbondata.format' TBLPROPERTIES('DICTIONARY_INCLUDE'='CUST_ID,Double_COLUMN2,DECIMAL_COLUMN2') LOAD DATA INPATH 'hdfs://192.168.2.145:54310/BabuStore/Data/uniqdata/2000_UniqData.csv' into table uniq_shared_dictionary OPTIONS('DELIMITER'=',' , 'QUOTECHAR'='"','BAD_RECORDS_LOGGER_ENABLE'='TRUE','FILEHEADER'='CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1,Double_COLUMN2,INTEGER_COLUMN1','SINGLE_Pass'='true') ;Output: 0: jdbc:hive2://hadoop-master:10000> select CUST_ID from uniq_shared_dictionary ;----------+ Cust_Id  ----------+ 8999      8999      8999      8999      8999      8999      8999      8999      8999      8999      8999      8999      9000      9001      9002      9003      9004      9005      9006      9007      9008      9009      9010      9011      9012      9013      9014      9015      9016      9017      9018      9019      9020      9021      9022      9023      9024      9025      9026      9027      9028      9029      9030      9031      9032      9033      9034      9035      9036      9037      9038      9039      9040      9041      9042      9043      9044      9045      9046      9047      9048      9049      9050      9051      9052      9053      9054      9055      9056      9057      9058      9059      9060      9061      9062      9063      9064      9065      9066      9067      9068      9069      9070      9071      9072      9073      9074      9075      9076      9077      9078      9079      9080      9081      9082      9083      9084      9085      9086      9087     ----------+ Cust_Id  ----------+ 9088      9089      9090      9091      9092      9093      9094      9095      9096      9097      9098      9099      9100      9101      9102      9103      9104      9105      9106      9107      9108      9109      9110      9111      9112      9113      9114      9115      9116      9117      9118      9119      9120      9121      9122      9123      9124      9125      9126      9127      9128      9129      9130      9131      9132      9133      9134      9135      9136      9137      9138      9139      9140      9141      9142      9143      9144      9145      9146      9147      9148      9149      9150      9151      9152      9153      9154      9155      9156      9157      9158      9159      9160      9161      9162      9163      9164      9165      9166      9167      9168      9169      9170      9171      9172      9173      9174      9175      9176      9177      9178      9179      9180      9181      9182      9183      9184      9185      9186      9187     ----------+ Cust_Id  ----------+ 9188      9189      9190      9191      9192      9193      9194      9195      9196      9197      9198      9199      9200      9201      9202      9203      9204      9205      9206      9207      9208      9209      9210      9211      9212      9213      9214      9215      9216      9217      9218      9219      9220      9221      9222      9223      9224      9225      9226      9227      9228      9229      9230      9231      9232      9233      9234      9235      9236      9237      9238      9239      9240      9241      9242      9243      9244      9245      9246      9247      9248      9249      9250      9251      9252      9253      9254      9255      9256      9257      9258      9259      9260      9261      9262      9263      9264      9265      9266      9267      9268      9269      9270      9271      9272      9273      9274      9275      9276      9277      9278      9279      9280      9281      9282      9283      9284      9285      9286      9287     ----------+ Cust_Id  ----------+ 9288      9289      9290      9291      9292      9293      9294      9295      9296      9297      9298      9299      9300      9301      9302      9303      9304      9305      9306      9307      9308      9309      9310      9311      9312      9313      9314      9315      9316      9317      9318      9319      9320      9321      9322      9323      9324      9325      9326      9327      9328      9329      9330      9331      9332      9333      9334      9335      9336      9337      9338      9339      9340      9341      9342      9343      9344      9345      9346      9347      9348      9349      9350      9351      9352      9353      9354      9355      9356      9357      9358      9359      9360      9361      9362      9363      9364      9365      9366      9367      9368      9369      9370      9371      9372      9373      9374      9375      9376      9377      9378      9379      9380      9381      9382      9383      9384      9385      9386      9387     ----------+ Cust_Id  ----------+ 9388      9389      9390      9391      9392      9393      9394      9395      9396      9397      9398      9399      9400      9401      9402      9403      9404      9405      9406      9407      9408      9409      9410      9411      9412      9413      9414      9415      9416      9417      9418      9419      9420      9421      9422      9423      9424      9425      9426      9427      9428      9429      9430      9431      9432      9433      9434      9435      9436      9437      9438      9439      9440      9441      9442      9443      9444      9445      9446      9447      9448      9449      9450      9451      9452      9453      9454      9455      9456      9457      9458      9459      9460      9461      9462      9463      9464      9465      9466      9467      9468      9469      9470      9471      9472      9473      9474      9475      9476      9477      9478      9479      9480      9481      9482      9483      9484      9485      9486      9487     ----------+ Cust_Id  ----------+ 9488      9489      9490      9491      9492      9493      9494      9495      9496      9497      9498      9499      9500      9501      9502      9503      9504      9505      9506      9507      9508      9509      9510      9511      9512      9513      9514      9515      9516      9517      9518      9519      9520      9521      9522      9523      9524      9525      9526      9527      9528      9529      9530      9531      9532      9533      9534      9535      9536      9537      9538      9539      9540      9541      9542      9543      9544      9545      9546      9547      9548      9549      9550      9551      9552      9553      9554      9555      9556      9557      9558      9559      9560      9561      9562      9563      9564      9565      9566      9567      9568      9569      9570      9571      9572      9573      9574      9575      9576      9577      9578      9579      9580      9581      9582      9583      9584      9585      9586      9587     ----------+ Cust_Id  ----------+ 9588      9589      9590      9591      9592      9593      9594      9595      9596      9597      9598      9599      9600      9601      9602      9603      9604      9605      9606      9607      9608      9609      9610      9611      9612      9613      9614      9615      9616      9617      9618      9619      9620      9621      9622      9623      9624      9625      9626      9627      9628      9629      9630      9631      9632      9633      9634      9635      9636      9637      9638      9639      9640      9641      9642      9643      9644      9645      9646      9647      9648      9649      9650      9651      9652      9653      9654      9655      9656      9657      9658      9659      9660      9661      9662      9663      9664      9665      9666      9667      9668      9669      9670      9671      9672      9673      9674      9675      9676      9677      9678      9679      9680      9681      9682      9683      9684      9685      9686      9687     ----------+ Cust_Id  ----------+ 9688      9689      9690      9691      9692      9693      9694      9695      9696      9697      9698      9699      9700      9701      9702      9703      9704      9705      9706      9707      9708      9709      9710      9711      9712      9713      9714      9715      9716      9717      9718      9719      9720      9721      9722      9723      9724      9725      9726      9727      9728      9729      9730      9731      9732      9733      9734      9735      9736      9737      9738      9739      9740      9741      9742      9743      9744      9745      9746      9747      9748      9749      9750      9751      9752      9753      9754      9755      9756      9757      9758      9759      9760      9761      9762      9763      9764      9765      9766      9767      9768      9769      9770      9771      9772      9773      9774      9775      9776      9777      9778      9779      9780      9781      9782      9783      9784      9785      9786      9787     ----------+ Cust_Id  ----------+ 9788      9789      9790      9791      9792      9793      9794      9795      9796      9797      9798      9799      9800      9801      9802      9803      9804      9805      9806      9807      9808      9809      9810      9811      9812      9813      9814      9815      9816      9817      9818      9819      9820      9821      9822      9823      9824      9825      9826      9827      9828      9829      9830      9831      9832      9833      9834      9835      9836      9837      9838      9839      9840      9841      9842      9843      9844      9845      9846      9847      9848      9849      9850      9851      9852      9853      9854      9855      9856      9857      9858      9859      9860      9861      9862      9863      9864      9865      9866      9867      9868      9869      9870      9871      9872      9873      9874      9875      9876      9877      9878      9879      9880      9881      9882      9883      9884      9885      9886      9887     ----------+ Cust_Id  ----------+ 9888      9889      9890      9891      9892      9893      9894      9895      9896      9897      9898      9899      9900      9901      9902      9903      9904      9905      9906      9907      9908      9909      9910      9911      9912      9913      9914      9915      9916      9917      9918      9919      9920      9921      9922      9923      9924      9925      9926      9927      9928      9929      9930      9931      9932      9933      9934      9935      9936      9937      9938      9939      9940      9941      9942      9943      9944      9945      9946      9947      9948      9949      9950      9951      9952      9953      9954      9955      9956      9957      9958      9959      9960      9961      9962      9963      9964      9965      9966      9967      9968      9969      9970      9971      9972      9973      9974      9975      9976      9977      9978      9979      9980      9981      9982      9983      9984      9985      9986      9987     ----------+ Cust_Id  ----------+ 9988      9989      9990      9991      9992      9993      9994      9995      9996      9997      9998      9999      10000     10001     10002     10003     10004     10005     10006     10007     10008     10009     10010     10011     10012     10013     10014     10015     10016     10017     10018     10019     10020     10021     10022     10023     10024     10025     10026     10027     10028     10029     10030     10031     10032     10033     10034     10035     10036     10037     10038     10039     10040     10041     10042     10043     10044     10045     10046     10047     10048     10049     10050     10051     10052     10053     10054     10055     10056     10057     10058     10059     10060     10061     10062     10063     10064     10065     10066     10067     10068     10069     10070     10071     10072     10073     10074     10075     10076     10077     10078     10079     10080     10081     10082     10083     10084     10085     10086     10087    ----------+ Cust_Id  ----------+ 10088     10089     10090     10091     10092     10093     10094     10095     10096     10097     10098     10099     10100     10101     10102     10103     10104     10105     10106     10107     10108     10109     10110     10111     10112     10113     10114     10115     10116     10117     10118     10119     10120     10121     10122     10123     10124     10125     10126     10127     10128     10129     10130     10131     10132     10133     10134     10135     10136     10137     10138     10139     10140     10141     10142     10143     10144     10145     10146     10147     10148     10149     10150     10151     10152     10153     10154     10155     10156     10157     10158     10159     10160     10161     10162     10163     10164     10165     10166     10167     10168     10169     10170     10171     10172     10173     10174     10175     10176     10177     10178     10179     10180     10181     10182     10183     10184     10185     10186     10187    ----------+ Cust_Id  ----------+ 10188     10189     10190     10191     10192     10193     10194     10195     10196     10197     10198     10199     10200     10201     10202     10203     10204     10205     10206     10207     10208     10209     10210     10211     10212     10213     10214     10215     10216     10217     10218     10219     10220     10221     10222     10223     10224     10225     10226     10227     10228     10229     10230     10231     10232     10233     10234     10235     10236     10237     10238     10239     10240     10241     10242     10243     10244     10245     10246     10247     10248     10249     10250     10251     10252     10253     10254     10255     10256     10257     10258     10259     10260     10261     10262     10263     10264     10265     10266     10267     10268     10269     10270     10271     10272     10273     10274     10275     10276     10277     10278     10279     10280     10281     10282     10283     10284     10285     10286     10287    ----------+ Cust_Id  ----------+ 10288     10289     10290     10291     10292     10293     10294     10295     10296     10297     10298     10299     10300     10301     10302     10303     10304     10305     10306     10307     10308     10309     10310     10311     10312     10313     10314     10315     10316     10317     10318     10319     10320     10321     10322     10323     10324     10325     10326     10327     10328     10329     10330     10331     10332     10333     10334     10335     10336     10337     10338     10339     10340     10341     10342     10343     10344     10345     10346     10347     10348     10349     10350     10351     10352     10353     10354     10355     10356     10357     10358     10359     10360     10361     10362     10363     10364     10365     10366     10367     10368     10369     10370     10371     10372     10373     10374     10375     10376     10377     10378     10379     10380     10381     10382     10383     10384     10385     10386     10387    ----------+ Cust_Id  ----------+ 10388     10389     10390     10391     10392     10393     10394     10395     10396     10397     10398     10399     10400     10401     10402     10403     10404     10405     10406     10407     10408     10409     10410     10411     10412     10413     10414     10415     10416     10417     10418     10419     10420     10421     10422     10423     10424     10425     10426     10427     10428     10429     10430     10431     10432     10433     10434     10435     10436     10437     10438     10439     10440     10441     10442     10443     10444     10445     10446     10447     10448     10449     10450     10451     10452     10453     10454     10455     10456     10457     10458     10459     10460     10461     10462     10463     10464     10465     10466     10467     10468     10469     10470     10471     10472     10473     10474     10475     10476     10477     10478     10479     10480     10481     10482     10483     10484     10485     10486     10487    ----------+ Cust_Id  ----------+ 10488     10489     10490     10491     10492     10493     10494     10495     10496     10497     10498     10499     10500     10501     10502     10503     10504     10505     10506     10507     10508     10509     10510     10511     10512     10513     10514     10515     10516     10517     10518     10519     10520     10521     10522     10523     10524     10525     10526     10527     10528     10529     10530     10531     10532     10533     10534     10535     10536     10537     10538     10539     10540     10541     10542     10543     10544     10545     10546     10547     10548     10549     10550     10551     10552     10553     10554     10555     10556     10557     10558     10559     10560     10561     10562     10563     10564     10565     10566     10567     10568     10569     10570     10571     10572     10573     10574     10575     10576     10577     10578     10579     10580     10581     10582     10583     10584     10585     10586     10587    ----------+ Cust_Id  ----------+ 10588     10589     10590     10591     10592     10593     10594     10595     10596     10597     10598     10599     10600     10601     10602     10603     10604     10605     10606     10607     10608     10609     10610     10611     10612     10613     10614     10615     10616     10617     10618     10619     10620     10621     10622     10623     10624     10625     10626     10627     10628     10629     10630     10631     10632     10633     10634     10635     10636     10637     10638     10639     10640     10641     10642     10643     10644     10645     10646     10647     10648     10649     10650     10651     10652     10653     10654     10655     10656     10657     10658     10659     10660     10661     10662     10663     10664     10665     10666     10667     10668     10669     10670     10671     10672     10673     10674     10675     10676     10677     10678     10679     10680     10681     10682     10683     10684     10685     10686     10687    ----------+ Cust_Id  ----------+ 10688     10689     10690     10691     10692     10693     10694     10695     10696     10697     10698     10699     10700     10701     10702     10703     10704     10705     10706     10707     10708     10709     10710     10711     10712     10713     10714     10715     10716     10717     10718     10719     10720     10721     10722     10723     10724     10725     10726     10727     10728     10729     10730     10731     10732     10733     10734     10735     10736     10737     10738     10739     10740     10741     10742     10743     10744     10745     10746     10747     10748     10749     10750     10751     10752     10753     10754     10755     10756     10757     10758     10759     10760     10761     10762     10763     10764     10765     10766     10767     10768     10769     10770     10771     10772     10773     10774     10775     10776     10777     10778     10779     10780     10781     10782     10783     10784     10785     10786     10787    ----------+ Cust_Id  ----------+ 10788     10789     10790     10791     10792     10793     10794     10795     10796     10797     10798     10799     10800     10801     10802     10803     10804     10805     10806     10807     10808     10809     10810     10811     10812     10813     10814     10815     10816     10817     10818     10819     10820     10821     10822     10823     10824     10825     10826     10827     10828     10829     10830     10831     10832     10833     10834     10835     10836     10837     10838     10839     10840     10841     10842     10843     10844     10845     10846     10847     10848     10849     10850     10851     10852     10853     10854     10855     10856     10857     10858     10859     10860     10861     10862     10863     10864     10865     10866     10867     10868     10869     10870     10871     10872     10873     10874     10875     10876     10877     10878     10879     10880     10881     10882     10883     10884     10885     10886     10887    ----------+ Cust_Id  ----------+ 10888     10889     10890     10891     10892     10893     10894     10895     10896     10897     10898     10899     10900     10901     10902     10903     10904     10905     10906     10907     10908     10909     10910     10911     10912     10913     10914     10915     10916     10917     10918     10919     10920     10921     10922     10923     10924     10925     10926     10927     10928     10929     10930     10931     10932     10933     10934     10935     10936     10937     10938     10939     10940     10941     10942     10943     10944     10945     10946     10947     10948     10949     10950     10951     10952     10953     10954     10955     10956     10957     10958     10959     10960     10961     10962     10963     10964     10965     10966     10967     10968     10969     10970     10971     10972     10973     10974     10975     10976     10977     10978     10979     10980     10981     10982     10983     10984     10985     10986     10987    ----------+ Cust_Id  ----------+ 10988     10989     10990     10991     10992     10993     10994     10995     10996     10997     10998     10999     NULL     ----------+2,013 rows selected (0.231 seconds)
issueID:CARBONDATA-712
type:Bug
changed files:processing/src/main/java/org/apache/carbondata/processing/loading/steps/DataConverterProcessorStepImpl.java
texts:&#39;BAD_RECORDS_ACTION&#39;=&#39;REDIRECT&#39; is not working properly.
When we tried to load data using 'BAD_RECORDS_ACTION'='REDIRECT' the bad record were not written in the file. I have set the property as carbon.badRecords.location=/opt/Carbon/Spark/badrecords.PFA for bad_record files which is empty & query csv.CREATE TABLE uniq_shared_dictionary (CUST_ID int,CUST_NAME String,ACTIVE_EMUI_VERSION string, DOB timestamp, DOJ timestamp, BIGINT_COLUMN1 bigint,BIGINT_COLUMN2 bigint,DECIMAL_COLUMN1 decimal(30,10), DECIMAL_COLUMN2 decimal(36,10),Double_COLUMN1 double, Double_COLUMN2 double,INTEGER_COLUMN1 int) STORED BY 'org.apache.carbondata.format' TBLPROPERTIES('DICTIONARY_INCLUDE'='CUST_ID,Double_COLUMN2,DECIMAL_COLUMN2','columnproperties.CUST_ID.shared_column'='shared.CUST_ID','columnproperties.decimal_column2.shared_column'='shared.decimal_column2')LOAD DATA INPATH 'HDFS_URL/BabuStore/Data/uniqdata/2000_UniqData.csv' into table uniq_shared_dictionary OPTIONS('DELIMITER'=',' , 'QUOTECHAR'='"','BAD_RECORDS_LOGGER_ENABLE'='TRUE', 'BAD_RECORDS_ACTION'='REDIRECT','FILEHEADER'='CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1,Double_COLUMN2,INTEGER_COLUMN1','SINGLE_Pass'='true')select Cust_Id from uniq_shared_dictionary ;The bad_records are also not loaded in the table.
issueID:CARBONDATA-714
type:Improvement
changed files:
texts:DOCUMENTATION - How to handle the bad records
A TroubleShooting topic can be added on how to handle the bad records:Some of the solution which can be captures are:1. Writing to CSV. What are the properties user need to set2. Null3. Fail when there is a bad recordsEtc
issueID:CARBONDATA-715
type:Improvement
changed files:processing/src/main/java/org/apache/carbondata/processing/loading/converter/impl/FieldEncoderFactory.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/SortParameters.java
processing/src/main/java/org/apache/carbondata/processing/loading/steps/DataConverterProcessorStepImpl.java
processing/src/main/java/org/apache/carbondata/processing/datatypes/PrimitiveDataType.java
processing/src/main/java/org/apache/carbondata/processing/loading/converter/impl/RowConverterImpl.java
processing/src/main/java/org/apache/carbondata/processing/loading/steps/SortProcessorStepImpl.java
texts:Optimize Single pass data load
1. Upgrade to latest netty-4.1.8 2. Optimize the serialization of key for passing in network.3. Launch individual dictionary client for each loading thread.
issueID:CARBONDATA-716
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
core/src/main/java/org/apache/carbondata/core/locks/HdfsFileLock.java
texts:Invalid hdfs lock path when load data if config viewfs
it will generate invalid table metadata lock path when load data if run CarbonData on viewfs/federation as follow:viewfs://haclusterviewfs://hacluster/tmp/carbondata/carbon.store/test/sample/meta.lock since it just checks schema 'hdfs://' but not 'viewfs://' in org.apache.carbondata.core.locks.HdfsFileLock
issueID:CARBONDATA-717
type:Bug
changed files:
texts:AND operator does not work properly in carbondata
Incorrect result displays to a user while use AND operator.Note:Issue exist when you take ID as a String but if you take ID as a int then it's working fine.Steps to Reproduce:1: create a table: CREATE TABLE IF NOT EXISTS t4 (ID string, name string) STORED BY 'carbondata';2:Load the dataLOAD DATA LOCAL INPATH '/home/Desktop/sample.csv' into table t4;3:Total record in the table.0: jdbc:hive2://localhost:10000> select * from t4;----------- ID    name  ----------- 1    david   2    eason   3    jarry  -----------4: SELECT * FROM t3 where id>=1 and id <3;0: jdbc:hive2://localhost:10000> SELECT * FROM t4 where id>=1 and id <3;----------- ID    name  ----------- 2    eason   3    jarry  -----------Expected Result: It should show the below result:0: jdbc:hive2://localhost:10000> SELECT * FROM t4 where id>=1 and id <3;----------- ID    name  ----------- 1    david   2    eason  -----------
issueID:CARBONDATA-718
type:Bug
changed files:
texts:All files have to contain Apache license header
Executing rat plugin shows several files without ASF headers, especially integration/spark2 scala files, README.md file, ...I gonna fix that and include rat in our build (with some exclusion).
issueID:CARBONDATA-719
type:Task
changed files:
texts:Add a release guide in documentation
We have to provide a release guide explaining the steps to perform a CarbonData release.
issueID:CARBONDATA-72
type:Bug
changed files:
texts:Column group count query
create table colgrp (column1 string,column2 string,column3 string,column4 string,column5 string,column6 string,column7 string,column8 string,column9 string,column10 string,measure1 int,measure2 int,measure3 int,measure4 int) STORED BY 'org.apache.carbondata.format' TBLPROPERTIES (\"COLUMN_GROUPS\"=\"(column2,column3,column4),(column7,column8,column9)\")Count query on column group is not workingselect count(column2) from colgrp
issueID:CARBONDATA-720
type:Task
changed files:
texts:Add a link to Apache security in the website Apache dropdown menu
Per rule, the website has to provide a link to http://www.apache.org/security.It would be also interesting to have a Security Advisories page to store the CVE (even if we don't have any for now).
issueID:CARBONDATA-721
type:Task
changed files:
texts:Add a link to release notes for each release
A link to the release notes (Jira) for each release would be good to allow users to check release content and eventually breaking changes.
issueID:CARBONDATA-722
type:Task
changed files:
texts:Add direct links to contribution guide, mailing lists, source repositories and Jira
In order to facilitate contribution, we should provide direct links and visibility to the contribution guide, mailing lists, source repositories and Jira directly on the website.
issueID:CARBONDATA-723
type:Task
changed files:
texts:Update team page marking PPMC member
The team page should list all committers and clearly mark the ones who are PPMC members.
issueID:CARBONDATA-724
type:Task
changed files:
texts:Improve contribution guide
Some minor improvements can be added to the contribution guide, especially for the merge, ...
issueID:CARBONDATA-725
type:Task
changed files:
texts:Create user mailing list
In order to address user specific question, we should create an user mailing list. The dev mailing list should be used only for the development discussions.
issueID:CARBONDATA-726
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/metadata/blocklet/BlockletInfo.java
core/src/main/java/org/apache/carbondata/core/scan/executor/infos/BlockExecutionInfo.java
core/src/main/java/org/apache/carbondata/core/datastore/columnar/UnBlockIndexer.java
core/src/main/java/org/apache/carbondata/core/scan/scanner/impl/BlockletFilterScanner.java
core/src/main/java/org/apache/carbondata/core/datastore/columnar/ColumnWithRowId.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/store/impl/safe/SafeFixedLengthDimensionDataChunkStore.java
core/src/main/java/org/apache/carbondata/core/util/CarbonProperties.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/impl/DimensionRawColumnChunk.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelRangeGrtrThanEquaToFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/datastore/impl/DFSFileReaderImpl.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelRangeLessThanFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/impl/MeasureRawColumnChunk.java
core/src/main/java/org/apache/carbondata/core/scan/scanner/impl/BlockletFullScanner.java
processing/src/main/java/org/apache/carbondata/processing/store/writer/AbstractFactDataWriter.java
core/src/main/java/org/apache/carbondata/core/scan/executor/impl/AbstractQueryExecutor.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/reader/dimension/AbstractDimensionChunkReader.java
core/src/main/java/org/apache/carbondata/core/util/DataFileFooterConverterFactory.java
core/src/main/java/org/apache/carbondata/core/scan/complextypes/ComplexQueryType.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/ExcludeFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/reader/DimensionColumnChunkReader.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/FilterExecuter.java
core/src/main/java/org/apache/carbondata/core/util/CarbonMetadataUtil.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/OrFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/reader/CarbonDataReaderFactory.java
core/src/main/java/org/apache/carbondata/core/datastore/FileReader.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/IncludeFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/scan/complextypes/StructQueryType.java
core/src/main/java/org/apache/carbondata/core/scan/complextypes/PrimitiveQueryType.java
core/src/main/java/org/apache/carbondata/core/cache/dictionary/Dictionary.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/AndFilterExecuterImpl.java
processing/src/main/java/org/apache/carbondata/processing/store/writer/v3/BlockletDataHolder.java
core/src/main/java/org/apache/carbondata/core/util/DataTypeUtil.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/reader/dimension/v3/DimensionChunkReaderV3.java
processing/src/main/java/org/apache/carbondata/processing/store/CarbonDataWriterFactory.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/AbstractRawColumnChunk.java
processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactDataHandlerColumnar.java
core/src/main/java/org/apache/carbondata/core/scan/result/iterator/AbstractDetailQueryResultIterator.java
core/src/main/java/org/apache/carbondata/core/scan/executor/util/QueryUtil.java
core/src/main/java/org/apache/carbondata/core/scan/scanner/BlockletScanner.java
core/src/main/java/org/apache/carbondata/core/datastore/impl/FileReaderImpl.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/reader/MeasureColumnChunkReader.java
processing/src/main/java/org/apache/carbondata/processing/store/writer/v3/CarbonFactDataWriterImplV3.java
core/src/main/java/org/apache/carbondata/core/util/DataFileFooterConverterV3.java
core/src/main/java/org/apache/carbondata/core/datastore/DataRefNode.java
core/src/main/java/org/apache/carbondata/core/scan/result/iterator/DetailQueryResultIterator.java
core/src/main/java/org/apache/carbondata/core/scan/result/BlockletScannedResult.java
core/src/main/java/org/apache/carbondata/core/cache/dictionary/ForwardDictionary.java
core/src/main/java/org/apache/carbondata/core/scan/complextypes/ArrayQueryType.java
core/src/main/java/org/apache/carbondata/core/scan/filter/FilterUtil.java
core/src/main/java/org/apache/carbondata/core/scan/filter/GenericQueryType.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/reader/measure/v3/MeasureChunkReaderV3.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/RowLevelRangeFilterResolverImpl.java
core/src/main/java/org/apache/carbondata/core/datastore/columnar/BlockIndexerStorageForShort.java
core/src/main/java/org/apache/carbondata/core/util/DataFileFooterConverter2.java
processing/src/main/java/org/apache/carbondata/processing/store/writer/CarbonFactDataWriter.java
core/src/main/java/org/apache/carbondata/core/metadata/ColumnarFormatVersion.java
core/src/main/java/org/apache/carbondata/core/util/BitSetGroup.java
core/src/main/java/org/apache/carbondata/core/cache/dictionary/AbstractColumnDictionaryInfo.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelRangeLessThanEqualFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/scan/result/impl/FilterQueryScannedResult.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonV3DataFormatConstants.java
core/src/main/java/org/apache/carbondata/core/scan/collector/impl/DictionaryBasedVectorResultCollector.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelRangeGrtThanFiterExecuterImpl.java
texts:Update with V3 format for better IO and processing optimization.
Problems in current format.1. IO read is slower since it needs to go for multiple seeks on the file to read column blocklets. Current size of blocklet is 120000, so it needs to read multiple times from file to scan the data on that column. Alternatively we can increase the blocklet size but it suffers for filter queries as it gets big blocklet to filter.2. Decompression is slower in current format, we are using inverted index for faster filter queries and using NumberCompressor to compress the inverted index in bit wise packing. It becomes slower so we should avoid number compressor. One alternative is to keep blocklet size with in 32000 so that inverted index can be written with short, but IO read suffers a lot.To overcome from above 2 issues we are introducing new format V3.Here each blocklet has multiple pages with size 32000, number of pages in blocklet is configurable. Since we keep the page with in short limit so no need compress the inverted index here.And maintain the max/min for each page to further prune the filter queries.Read the blocklet with pages at once and keep in offheap memory.During filter first check the max/min range and if it is valid then go for decompressing the page to filter further.
issueID:CARBONDATA-73
type:Bug
changed files:
texts:Disable Autodetect highcardinality on column group
If any column is high cardinality and its part of column group then data loading fails.We need to disable auto highcardinality check on column group column
issueID:CARBONDATA-730
type:Improvement
changed files:
texts:unsupported type: DecimalType
Below exception is thrown while trying to save dataframe with a decimal column type. scala> df.printSchema&#8211; account: integer (nullable = true)&#8211; currency: integer (nullable = true)&#8211; branch: integer (nullable = true)&#8211; country: integer (nullable = true)&#8211; date: date (nullable = true)&#8211; fcbalance: decimal(16,3) (nullable = true)&#8211; lcbalance: decimal(16,3) (nullable = true)scala> df.write.format("carbondata").option("tableName", "accBal").option("compress", "true").mode(SaveMode.Overwrite).save()java.lang.RuntimeException: unsupported type: DecimalType(16,3)        at scala.sys.package$.error(package.scala:27)        at org.apache.carbondata.spark.CarbonDataFrameWriter.org$apache$carbondata$spark$CarbonDataFrameWriter$$convertToCarbonType(CarbonDataFrameWriter.scala:172)        at org.apache.carbondata.spark.CarbonDataFrameWriter$$anonfun$2.apply(CarbonDataFrameWriter.scala:178)        at org.apache.carbondata.spark.CarbonDataFrameWriter$$anonfun$2.apply(CarbonDataFrameWriter.scala:177)        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)This is working fine with below change : git diffdiff --git a/integration/spark/src/main/scala/org/apache/carbondata/spark/CarbonDataFrameWriter.scala b/integration/spark/src/main/scala/org/apache/carbondata/spark/CarbonDataFrameWriter.scalaindex b843f59..cf9a775 100644&#8212; a/integration/spark/src/main/scala/org/apache/carbondata/spark/CarbonDataFrameWriter.scala+++ b/integration/spark/src/main/scala/org/apache/carbondata/spark/CarbonDataFrameWriter.scala@@ -169,6 +169,7 @@ class CarbonDataFrameWriter(val dataFrame: DataFrame) {       case BooleanType => CarbonType.DOUBLE.getName       case TimestampType => CarbonType.TIMESTAMP.getName       case DateType => CarbonType.DATE.getName+      case dt: DecimalType => s"${CarbonType.DECIMAL.getName}(${dt.precision}, ${dt.scale})"       case other => sys.error(s"unsupported type: $other")     }   }Can I create a pull request?
issueID:CARBONDATA-731
type:Bug
changed files:
texts:Enhance and correct quick start and installation guides
1.Enhance Quick Start Guide to have correct formatting and highlight2.Correct installation guide for cluster installation
issueID:CARBONDATA-732
type:Bug
changed files:
texts:User unable to execute the select/Load query using thrift server.
Result does not display to user while hit Select/Load query.Steps to reproduce:1:Hit the query :0: jdbc:hive2://localhost:10000> select * from t4;Note: Cursor Keep blinking on beeline.2: Logs on Thrift server:Error sending result StreamResponse{streamId=/jars/carbondata_2.11-1.0.0-incubating-SNAPSHOT-shade-hadoop2.2.0.jar, byteCount=19350001, body=FileSegmentManagedBuffer{file=/opt/spark-2.1.0/carbonlib/carbondata_2.11-1.0.0-incubating-SNAPSHOT-shade-hadoop2.2.0.jar, offset=0, length=19350001}} to /192.168.2.179:48291; closing connectionjava.lang.AbstractMethodError at io.netty.util.ReferenceCountUtil.touch(ReferenceCountUtil.java:73) at io.netty.channel.DefaultChannelPipeline.touch(DefaultChannelPipeline.java:107) at io.netty.channel.AbstractChannelHandlerContext.write(AbstractChannelHandlerContext.java:811) at io.netty.channel.AbstractChannelHandlerContext.write(AbstractChannelHandlerContext.java:724) at io.netty.handler.codec.MessageToMessageEncoder.write(MessageToMessageEncoder.java:111) at io.netty.channel.AbstractChannelHandlerContext.invokeWrite0(AbstractChannelHandlerContext.java:739) at io.netty.channel.AbstractChannelHandlerContext.invokeWrite(AbstractChannelHandlerContext.java:731) at io.netty.channel.AbstractChannelHandlerContext.write(AbstractChannelHandlerContext.java:817) at io.netty.channel.AbstractChannelHandlerContext.write(AbstractChannelHandlerContext.java:724) at io.netty.handler.timeout.IdleStateHandler.write(IdleStateHandler.java:305) at io.netty.channel.AbstractChannelHandlerContext.invokeWrite0(AbstractChannelHandlerContext.java:739) at io.netty.channel.AbstractChannelHandlerContext.invokeWriteAndFlush(AbstractChannelHandlerContext.java:802) at io.netty.channel.AbstractChannelHandlerContext.write(AbstractChannelHandlerContext.java:815) at io.netty.channel.AbstractChannelHandlerContext.writeAndFlush(AbstractChannelHandlerContext.java:795) at io.netty.channel.AbstractChannelHandlerContext.writeAndFlush(AbstractChannelHandlerContext.java:832) at io.netty.channel.DefaultChannelPipeline.writeAndFlush(DefaultChannelPipeline.java:1032) at io.netty.channel.AbstractChannel.writeAndFlush(AbstractChannel.java:296) at org.apache.spark.network.server.TransportRequestHandler.respond(TransportRequestHandler.java:194) at org.apache.spark.network.server.TransportRequestHandler.processStreamRequest(TransportRequestHandler.java:150) at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111) at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:119) at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:51) at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:363) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:349) at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:341) at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:287) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:363) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:349) at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:341) at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:363) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:349) at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:341) at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:363) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:349) at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:341) at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1334) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:363) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:349) at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:926) at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:129) at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:642) at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:565) at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:479) at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:441) at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858) at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144) at java.lang.Thread.run(Thread.java:745)ERROR 28-02 13:43:44,656 - Still have 1 requests outstanding when connection from /192.168.2.179:58030 is closedINFO  28-02 13:46:57,954 - Session disconnected without closing properly, close it nowERROR 28-02 13:46:57,958 - Error executing query, currentState CLOSED, java.lang.InterruptedException at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:998) at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1304) at scala.concurrent.impl.Promise$DefaultPromise.tryAwait(Promise.scala:202) at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:218) at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:153) at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:619) at org.apache.spark.SparkContext.runJob(SparkContext.scala:1918) at org.apache.spark.SparkContext.runJob(SparkContext.scala:1931) at org.apache.spark.SparkContext.runJob(SparkContext.scala:1944) at org.apache.spark.SparkContext.runJob(SparkContext.scala:1958) at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:935) at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151) at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112) at org.apache.spark.rdd.RDD.withScope(RDD.scala:362) at org.apache.spark.rdd.RDD.collect(RDD.scala:934) at org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:275) at org.apache.spark.sql.Dataset$$anonfun$org$apache$spark$sql$Dataset$$execute$1$1.apply(Dataset.scala:2371) at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57) at org.apache.spark.sql.Dataset.withNewExecutionId(Dataset.scala:2765) at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$execute$1(Dataset.scala:2370) at org.apache.spark.sql.Dataset$$anonfun$org$apache$spark$sql$Dataset$$collect$1.apply(Dataset.scala:2375) at org.apache.spark.sql.Dataset$$anonfun$org$apache$spark$sql$Dataset$$collect$1.apply(Dataset.scala:2375) at org.apache.spark.sql.Dataset.withCallback(Dataset.scala:2778) at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collect(Dataset.scala:2375) at org.apache.spark.sql.Dataset.collect(Dataset.scala:2351) at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:235) at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1$$anon$2.run(SparkExecuteStatementOperation.scala:163) at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1$$anon$2.run(SparkExecuteStatementOperation.scala:160) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1698) at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1.run(SparkExecuteStatementOperation.scala:173) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745)ERROR 28-02 13:46:57,959 - Error running hive query: org.apache.hive.service.cli.HiveSQLException: Illegal Operation state transition from CLOSED to ERROR at org.apache.hive.service.cli.OperationState.validateTransition(OperationState.java:92) at org.apache.hive.service.cli.OperationState.validateTransition(OperationState.java:98) at org.apache.hive.service.cli.operation.Operation.setState(Operation.java:126) at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:255) at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1$$anon$2.run(SparkExecuteStatementOperation.scala:163) at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1$$anon$2.run(SparkExecuteStatementOperation.scala:160) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1698) at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1.run(SparkExecuteStatementOperation.scala:173) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745)Please check attached log fileExpected Result: User should be able to execute the select/Load query using thrift server.
issueID:CARBONDATA-733
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/datastore/columnar/BlockIndexerStorageForNoInvertedIndexForShort.java
processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactDataHandlerColumnar.java
texts:Fixed testcase failure issue
Fixed testcase failure issue
issueID:CARBONDATA-734
type:Bug
changed files:
texts:Can&#39;t create parquet/orc table with CarbonSession
When I create parquet table with the command below:import org.apache.spark.sql.SparkSession import org.apache.spark.sql.CarbonSession._val carbon = SparkSession.builder().config(sc.getConf).getOrCreateCarbonSession("hdfs://100.0.0.4:9000/user/hive/warehouse/carbon.store")carbon.sql("create table src(key int, value string) stored as parquet")It will failed with the error info below:java.lang.RuntimeException: [1.1] failure: identifier matching regex (?i)ALTER expectedcreate table src(key int, value string) stored as parquet^  at scala.sys.package$.error(package.scala:27)  at org.apache.spark.sql.parser.CarbonSpark2SqlParser.parse(CarbonSpark2SqlParser.scala:45)  at org.apache.spark.sql.parser.CarbonSparkSqlParser.parsePlan(CarbonSparkSqlParser.scala:51)  at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:592)  ... 50 elided
issueID:CARBONDATA-735
type:Bug
changed files:
texts:Dictionary Loading performance issue with multiple task in single node
Problem:Currently when more than 1 task is getting launched in one node for a query both the task is trying to load the dictionary data and its impacting dictionary loading performance.Solution:Need to add monitor for dictionary loading one task will load the dictionary and other task will be waiting and share the same dictionary data
issueID:CARBONDATA-736
type:Bug
changed files:
texts:Dictionary Loading issue in Decoder
Problem:Currently in Carbon dictionary decoder it is loading the dictionary files, it is using get api, when number of columns are high it can use getAll api to load dictionary data concurrently Solution:Use get All API
issueID:CARBONDATA-737
type:New Feature
changed files:
texts:Add Map datatype support as Hive
Due to the lack of "alter ... add column ..." syntax support, we’re badly need a "Map datatype" in the complex types.As Hive docs:MapsMaps in Hive are similar to Java Maps.Syntax: MAP<primitive_type, data_type>
issueID:CARBONDATA-738
type:Bug
changed files:
texts:Able to load dataframe with boolean type in a carbon table but with null values
i created a dataframe with boolean type as follows case class People(name: String, occupation: String, consultant: Boolean)val people = List(People("sangeeta", "engineer", true), People("pallavi", "consultant", true))    val peopleRDD: RDD&#91;People&#93; = cc.sc.parallelize(people)    val peopleDF: DataFrame = peopleRDD.toDF("name", "occupation", "id")     peopleDF.write      .format("carbondata")      .option("tableName", "carbon2")      .option("compress", "true")      .mode(SaveMode.Overwrite)      .save()    cc.sql("SELECT * FROM carbon2").show()currently boolean type is not supported in carbon data but table gets createdbut it shows me null values------------------    nameoccupation  id------------------ pallaviconsultantnullsangeeta  engineernull------------------for boolean type it should throw unsupported type exceptionthere is problem with carbondataframe writer
issueID:CARBONDATA-739
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/scan/collector/impl/DictionaryBasedResultCollector.java
texts:Avoid creating multiple instances of DirectDictionary in DictionaryBasedResultCollector
Avoid creating multiple instances of DirectDictionary in DictionaryBasedResultCollector.For every row, direct dictionary is creating inside DictionaryBasedResultCollector.collectData method.Please create single instance per column and reuse it
issueID:CARBONDATA-74
type:Bug
changed files:
texts:Remove code for describe command, as it will be handled in spark
Currently we are handling describe command in carbonstrategies, but not doing any transformation or output parsing. Hence "describe/desc table" can be directly sent to spark for execution and result parsing.
issueID:CARBONDATA-740
type:Bug
changed files:processing/src/main/java/org/apache/carbondata/processing/loading/AbstractDataLoadProcessorStep.java
texts:Add logger for rows processed while closing in AbstractDataLoadProcessorStep
Add logger for rows processed while closing in AbstractDataLoadProcessorStep.It is good to print the total records processed while closing the step, so please log the rows processed in AbstractDataLoadProcessorStep
issueID:CARBONDATA-741
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/scan/result/vector/impl/CarbonColumnVectorImpl.java
texts:Remove the unnecessary classes from carbondata
Please remove following classes as it is not used now.VectorChunkRowIteratorCarbonColumnVectorImpl
issueID:CARBONDATA-742
type:Improvement
changed files:processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/SortDataRows.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/merger/UnsafeIntermediateFileMerger.java
core/src/main/java/org/apache/carbondata/core/util/ByteUtil.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/IntermediateFileMerger.java
processing/src/main/java/org/apache/carbondata/processing/loading/steps/DataWriterProcessorStepImpl.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/comparator/UnsafeRowComparator.java
core/src/main/java/org/apache/carbondata/core/mutate/CarbonUpdateUtil.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/UnsafeCarbonRowPage.java
processing/src/main/java/org/apache/carbondata/processing/store/writer/AbstractFactDataWriter.java
processing/src/main/java/org/apache/carbondata/processing/loading/converter/RowConverter.java
processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactDataHandlerColumnar.java
core/src/main/java/org/apache/carbondata/core/datastore/block/TableBlockInfo.java
processing/src/main/java/org/apache/carbondata/processing/loading/converter/impl/RowConverterImpl.java
processing/src/main/java/org/apache/carbondata/processing/loading/steps/SortProcessorStepImpl.java
core/src/main/java/org/apache/carbondata/core/util/NonDictionaryUtil.java
processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactDataHandlerModel.java
processing/src/main/java/org/apache/carbondata/processing/loading/steps/DataConverterProcessorStepImpl.java
core/src/main/java/org/apache/carbondata/core/util/path/CarbonTablePath.java
processing/src/main/java/org/apache/carbondata/processing/loading/constants/DataLoadProcessorConstants.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
processing/src/main/java/org/apache/carbondata/processing/loading/converter/DictionaryCardinalityFinder.java
core/src/main/java/org/apache/carbondata/core/memory/UnsafeMemoryManager.java
processing/src/main/java/org/apache/carbondata/processing/loading/row/CarbonSortBatch.java
processing/src/main/java/org/apache/carbondata/processing/loading/DataLoadProcessBuilder.java
processing/src/main/java/org/apache/carbondata/processing/loading/row/CarbonRowBatch.java
core/src/main/java/org/apache/carbondata/core/memory/CarbonUnsafe.java
processing/src/main/java/org/apache/carbondata/processing/loading/CarbonDataLoadConfiguration.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/SortTempFileChunkHolder.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/UnsafeSortDataRows.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/merger/UnsafeSingleThreadFinalSortFilesMerger.java
texts:Add batch sort to improve the loading performance
Current Problem:Sort step is major issue as it is blocking step. It needs to receive all data and write down the sort temp files to disk, after that only data writer step can start.Solution: Make sort step as non blocking step so it avoids waiting of Data writer step.Process the data in sort step in batches with size of in-memory capability of the machine. For suppose if machine can allocate 4 GB to process data in-memory, then Sort step can sorts the data with batch size of 2GB and gives it to the data writer step. By the time data writer step consumes the data, sort step receives and sorts the data. So here all steps are continuously working and absolutely there is no disk IO in sort step.So there would not be any waiting of data writer step for sort step, As and when sort step sorts the data in memory data writer can start writing it.It can significantly improves the performance.Advantages:Increases the loading performance as there is no intermediate IO and no blocking of Sort step.There is no extra effort for compaction, the current flow can handle it.Disadvantages:Number of driver side btrees will increase. So the memory might increase but it could be controlled by current LRU cache implementation.
issueID:CARBONDATA-743
type:Bug
changed files:
texts:Remove the abundant class CarbonFilters.scala
Remove the abundant class CarbonFilters.scala from spark2 package.Right now there are two classes with name CarbonFilters in carbondata.1.Delete the CarbonFilters scala file from spark-common package2. Move the CarbonFilters scala from spark2 package to spark-common package.
issueID:CARBONDATA-744
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
texts:The property "spark.carbon.custom.distribution" should be change to carbon.custom.block.distribution and should be part of CarbonProperties
The property "spark.carbon.custom.distribution" should  be part of CarbonPropertiesAs naming style adopted in carbon we should name the key carbon.custom.distribution
issueID:CARBONDATA-745
type:Wish
changed files:
texts:Does carbondata apply to scenes that need to sort historical and current data?
Does carbondata apply to scenes that need to sort historical and current data?Now there is a new scene that needs to sort the current data and historical data in real time according to certain rules,and returns the sorted data when querying.It's just like hbase put data sorted by rowkey.We want to sort the data when we load the data without having to spend extra time to sort。Is it suitable for this scene?
issueID:CARBONDATA-746
type:Improvement
changed files:
texts:Support spark-sql CLI for spark2.1 carbon integration

issueID:CARBONDATA-747
type:Improvement
changed files:
texts:Add simple performance test for spark2.1 carbon integration

issueID:CARBONDATA-748
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/scan/filter/executer/IncludeFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
texts:"between and" filter query is very slow
Hi,Currently In include and exclude filter case when dimension column does nothave inverted index it is doing linear search , We can add binary searchwhen data for that column is sorted, to get this information we can checkin carbon table for that column whether user has selected no inverted indexor not. If user has selected No inverted index while creating a column thiscode is fine, if user has not selected then data will be sorted so we canadd binary search which will improve the performance.Please raise a Jira for this improvement-RegardsKumar VishalOn Fri, Mar 3, 2017 at 7:42 PM, 马云 <simafengyun1984@163.com> wrote:Hi Dev,I used carbondata version 0.2 in my local machine, and found that the"between and" filter query is very slow.the root caused is by the below code in IncludeFilterExecuterImpl.java.It takes about 20s in my test.The code's  time complexity is O(n*m). I think it needs to optimized,please confirm. thanks private BitSet setFilterdIndexToBitSet(DimensionColumnDataChunkdimensionColumnDataChunk,     intnumerOfRows) {   BitSet bitSet = new BitSet(numerOfRows);   if (dimensionColumnDataChunkinstanceof FixedLengthDimensionDataChunk){     FixedLengthDimensionDataChunk fixedDimensionChunk =         (FixedLengthDimensionDataChunk) dimensionColumnDataChunk;     byte[][] filterValues = dimColumnExecuterInfo.getFilterKeys();     longstart = System.currentTimeMillis();     for (intk = 0; k < filterValues.length; k++) {       for (intj = 0; j < numerOfRows; j++) {         if (ByteUtil.UnsafeComparer.INSTANCE             .compareTo(fixedDimensionChunk.getCompleteDataChunk(), j *filterValues&#91;k&#93;.length,                 filterValues&#91;k&#93;.length, filterValues&#91;k&#93;, 0,filterValues&#91;k&#93;.length) == 0) {           bitSet.set(j);         }       }     }     System.out.println("loop time: "+(System.currentTimeMillis() -start));   }
issueID:CARBONDATA-75
type:Bug
changed files:
texts:Dictionary file not getting clean on global dictionary failure
1. When data load is done for the first time and before writing the dictionary meta file and after writing dictionary file if there is any failure, then spark relaunches the task and dictionary file is not getting cleaned and same data is getting appended again to the dictionary file.2. There is path formation issue while getting the sort index file
issueID:CARBONDATA-751
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/metadata/blocklet/BlockletInfo.java
core/src/main/java/org/apache/carbondata/core/reader/CarbonFooterReaderV3.java
core/src/main/java/org/apache/carbondata/core/datastore/DataRefNode.java
core/src/main/java/org/apache/carbondata/core/util/DataFileFooterConverterV3.java
processing/src/main/java/org/apache/carbondata/processing/store/writer/v3/CarbonFactDataWriterImplV3.java
core/src/main/java/org/apache/carbondata/core/reader/CarbonHeaderReader.java
core/src/main/java/org/apache/carbondata/core/util/CarbonMetadataUtil.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/reader/dimension/v3/DimensionChunkReaderV3.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/reader/measure/v3/MeasureChunkReaderV3.java
texts:Adding Header and making footer optional
Currently carbon does not support appendable format, so below changes is to support appendable format in V3 data file format by making footer option and added header in V3 carbon data file .
issueID:CARBONDATA-752
type:Bug
changed files:
texts:creating complex type gives exception
using complex type in create table gives me exceptionspark.sql(      s""" CREATE TABLE carbon_table(    shortField short,    intField int,    bigintField long,    doubleField double,    stringField string,    timestampField timestamp,    decimalField decimal(18,2),    dateField date,    charField char(5),    floatField float,    complexData array<string> ) STORED BY 'CARBONDATA' TBLPROPERTIES('DICTIONARY_INCLUDE'='dateField, charField')       """.stripMargin)it gives me exceptionCaused by: java.lang.RuntimeException: Unsupported data type: ArrayType(StringType,true)Caused by: java.lang.RuntimeException: Unsupported data type: ArrayType(StringType,true)at scala.sys.package$.error(package.scala:27)at org.apache.carbondata.spark.util.DataTypeConverterUtil$.convertToCarbonTypeForSpark2(DataTypeConverterUtil.scala:61
issueID:CARBONDATA-753
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
texts:Fix Date and Timestamp format issues
Fix Date and Timestamp format issues:1.Optimize the description of CARBON_TIMESTAMP_FORMAT&CARBON_DATE_FORMAT  in CarbonCommonConstants.java2.Correct fields' definition of Date and Timestamp in examples.3.Add example script how to show raw data's timestamp format. currently spark.sql.show() by default using "yyyy-mm-dd hh:mm:ss.fffffffff" as Timestamp.toString() format, users always wanting the show data same as raw data format.
issueID:CARBONDATA-755
type:Bug
changed files:
texts:Confusing comment about default kettle_use
Fix comment issues of CarbonExample.scala about default use_kettlehttps://github.com/apache/incubator-carbondata/pull/640/commits
issueID:CARBONDATA-756
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/datastore/columnar/BlockIndexerStorageForShort.java
texts:RLE encoding isse
Problem: Rle index size is more than actual data sizeSolution : If rle index size is more than data size or it is more than 70 of the data size then disable rle encoding for that column
issueID:CARBONDATA-758
type:Improvement
changed files:
texts:remove kettle related code in CarbonExample.scala
https://github.com/apache/incubator-carbondata/pull/648
issueID:CARBONDATA-76
type:Bug
changed files:core/src/main/java/org/carbondata/scan/expression/conditional/EqualToExpression.java
core/src/main/java/org/carbondata/scan/expression/conditional/NotEqualsExpression.java
core/src/main/java/org/apache/carbondata/core/cache/dictionary/ColumnDictionaryInfo.java
texts:Not Equals filter display even the null members while filtering non null values
if the user was applying filte to list down non null members. When user applies Not Equals filter in any non null members the system shall not display null members in report as per Hive compatibility.Query Sample:select imei,name,country,city,productdate,enddate,age,task,sale,num,level,quest,pointid,score from big_int where  age  != 2147;
issueID:CARBONDATA-760
type:Bug
changed files:
texts:Should to avoid ERROR log for successful select query
Some table without delete or update operator maybe not have delta files. Select query shouldn't record error log.Code:SegmentUpdateStatusManager.getDeltaFilesLog detail: ERROR 06-03 19:21:37,531 - pool-475-thread-1 Invalid tuple id arbonstore/default/comparetest_carbon/Fact/0/0/0-0-0-1488799238178/0ERROR 06-03 19:21:37,948 - pool-475-thread-1 Invalid tuple id arbonstore/default/comparetest_carbon/Fact/0/0/0-0-0-1488799238178/1ERROR 06-03 19:21:38,517 - pool-475-thread-1 Invalid tuple id arbonstore/default/comparetest_carbon/Fact/0/0/0-0-0-1488799238178/2ERROR 06-03 19:21:38,909 - pool-475-thread-1 Invalid tuple id arbonstore/default/comparetest_carbon/Fact/0/0/0-0-0-1488799238178/3ERROR 06-03 19:21:39,292 - pool-475-thread-1 Invalid tuple id arbonstore/default/comparetest_carbon/Fact/0/0/0-0-0-1488799238178/4
issueID:CARBONDATA-761
type:Bug
changed files:
texts:Dictionary server should not be shutdown after loading
Code:CarbonTableSchema/LoadTable
issueID:CARBONDATA-762
type:Bug
changed files:
texts:modify all schemaName->databaseName, cubeName->tableName
modify all schemaName->databaseName, cubeName->tableName
issueID:CARBONDATA-763
type:New Feature
changed files:
texts:Add L5 loading support, global sorting like HBase
Add L5 loading support, global sorting like HBase
issueID:CARBONDATA-765
type:Bug
changed files:
texts:dataframe wirter need to first drop table unless loading said table not found
dataframe wirter need to first drop table unless loading said table not found
issueID:CARBONDATA-766
type:Bug
changed files:processing/src/main/java/org/apache/carbondata/processing/store/writer/v3/CarbonFactDataWriterImplV3.java
processing/src/main/java/org/apache/carbondata/processing/store/writer/CarbonFactDataWriter.java
core/src/main/java/org/apache/carbondata/core/util/CarbonMetadataUtil.java
processing/src/main/java/org/apache/carbondata/processing/store/writer/v3/BlockletDataHolder.java
processing/src/main/java/org/apache/carbondata/processing/store/writer/AbstractFactDataWriter.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonV3DataFormatConstants.java
processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactDataHandlerColumnar.java
core/src/main/java/org/apache/carbondata/core/util/CarbonProperties.java
texts:Size based blocklet for V3
Currently number of pages is based on configured fixed value(number of pages per blocklet) , problem with this approach is in some cases blocklet size will be less and it will cause more number of IO, to avoid this we can have size based blocklet , in this case how many pages it will fit in blocklet will based on configure size, so number of IO will be uniform
issueID:CARBONDATA-769
type:Improvement
changed files:
texts:Support Codegen in CarbonDictionaryDecoder
Support Codegen in CarbonDictionaryDecoder to leverage wholecodegen performance of Spark2.1
issueID:CARBONDATA-77
type:Bug
changed files:integration/spark-common/src/main/java/org/apache/carbondata/spark/load/DeleteLoadFolders.java
texts:Delete segment folder after segment clean up.
If a segment is deleted using the delete segment by ID DDl , then the contents inside the segment folder is getting deleted by the folder will remain in the store .The complete folder itself can be deleted.
issueID:CARBONDATA-770
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/resolverinfo/visitor/DictionaryColumnVisitor.java
core/src/main/java/org/apache/carbondata/core/util/DataTypeUtil.java
texts:Filter Query not null data mismatch issue
Problem: Not null filter query is selecting null values.Solution: Problem is while parsing the data based on data type we are not parsing for int, double, float, and long data type, need to add case for the same
issueID:CARBONDATA-771
type:Bug
changed files:processing/src/main/java/org/apache/carbondata/processing/store/writer/v3/CarbonFactDataWriterImplV3.java
core/src/main/java/org/apache/carbondata/core/util/CarbonMetadataUtil.java
texts:Dataloading fails in V3 format for TPC-DS data.
Dataloading fails in V3 format for TPC-DS data.
issueID:CARBONDATA-774
type:Bug
changed files:
texts:Not like operator does not work properly in carbondata
Not Like operator result does not display same as hive.Steps to reproduces:A): Create table in HiveCREATE TABLE uniqdata_h (CUST_ID int,CUST_NAME String,ACTIVE_EMUI_VERSION string, DOB timestamp, DOJ timestamp, BIGINT_COLUMN1 bigint,BIGINT_COLUMN2 bigint,DECIMAL_COLUMN1 decimal(30,10), DECIMAL_COLUMN2 decimal(36,10),Double_COLUMN1 double, Double_COLUMN2 double,INTEGER_COLUMN1 int) ROW FORMAT DELIMITED FIELDS TERMINATED BY ','2:Load Data in hivea)load data local inpath '/opt/TestData/Data/uniqdata/2000_UniqData.csv' into table uniqdata_hb)load data local inpath '/opt/TestData/Data/uniqdata/4000_UniqData.csv' into table uniqdata_hc)load data local inpath '/opt/TestData/Data/uniqdata/6000_UniqData.csv' into table uniqdata_hd)load data local inpath '/opt/TestData/Data/uniqdata/7000_UniqData.csv' into table uniqdata_he)load data local inpath '/opt/TestData/Data/uniqdata/3000_1_UniqData.csv' into table uniqdata_h3: Run the Query:select CUST_ID from uniqdata_h where CUST_ID NOT LIKE 1000794:Result in Hive----------+ CUST_ID  ----------+ 8999      9000      9001      9002      9003      9004      9005      9006      9007      9008      9009      9010      9011      9012      9013      9014      9015      9016      9017      9018      9019      9020      9021      9022      9023      9024      9025      9026      9027      9028      9029      9030      9031      9032      9033      9034      9035      9036      9037      9038      9039      9040      9041      9042      9043      9044      9045      9046      9047      9048      9049      9050      9051      9052      9053      9054      9055      9056      9057      9058      9059      9060      9061      9062      9063      9064      9065      9066      9067      9068      9069      9070      9071      9072      9073      9074      9075      9076      9077      9078      9079      9080      9081      9082      9083      9084      9085      9086      9087      9088      9089      9090      9091      9092      9093      9094      9095      9096      9097      9098     ----------+ CUST_ID  ----------+ 9099      9100      9101      9102      9103      9104      9105      9106      9107      9108      9109      9110      9111      9112      9113      9114      9115      9116      9117      9118      9119      9120      9121      9122      9123      9124      9125      9126      9127      9128      9129      9130      9131      9132      9133      9134      9135      9136      9137      9138      9139      9140      9141      9142      9143      9144      9145      9146      9147      9148      9149      9150      9151      9152      9153      9154      9155      9156      9157      9158      9159      9160      9161      9162      9163      9164      9165      9166      9167      9168      9169      9170      9171      9172      9173      9174      9175      9176      9177      9178      9179      9180      9181      9182      9183      9184      9185      9186      9187      9188      9189      9190      9191      9192      9193      9194      9195      9196      9197      9198     ----------+ CUST_ID  ----------+ 9199      9200      9201      9202      9203      9204      9205      9206      9207      9208      9209      9210      9211      9212      9213      9214      9215      9216      9217      9218      9219      9220      9221      9222      9223      9224      9225      9226      9227      9228      9229      9230      9231      9232      9233      9234      9235      9236      9237      9238      9239      9240      9241      9242      9243      9244      9245      9246      9247      9248      9249      9250      9251      9252      9253      9254      9255      9256      9257      9258      9259      9260      9261      9262      9263      9264      9265      9266      9267      9268      9269      9270      9271      9272      9273      9274      9275      9276      9277      9278      9279      9280      9281      9282      9283      9284      9285      9286      9287      9288      9289      9290      9291      9292      9293      9294      9295      9296      9297      9298     ----------+ CUST_ID  ----------+ 9299      9300      9301      9302      9303      9304      9305      9306      9307      9308      9309      9310      9311      9312      9313      9314      9315      9316      9317      9318      9319      9320      9321      9322      9323      9324      9325      9326      9327      9328      9329      9330      9331      9332      9333      9334      9335      9336      9337      9338      9339      9340      9341      9342      9343      9344      9345      9346      9347      9348      9349      9350      9351      9352      9353      9354      9355      9356      9357      9358      9359      9360      9361      9362      9363      9364      9365      9366      9367      9368      9369      9370      9371      9372      9373      9374      9375      9376      9377      9378      9379      9380      9381      9382      9383      9384      9385      9386      9387      9388      9389      9390      9391      9392      9393      9394      9395      9396      9397      9398     ----------+ CUST_ID  ----------+ 9399      9400      9401      9402      9403      9404      9405      9406      9407      9408      9409      9410      9411      9412      9413      9414      9415      9416      9417      9418      9419      9420      9421      9422      9423      9424      9425      9426      9427      9428      9429      9430      9431      9432      9433      9434      9435      9436      9437      9438      9439      9440      9441      9442      9443      9444      9445      9446      9447      9448      9449      9450      9451      9452      9453      9454      9455      9456      9457      9458      9459      9460      9461      9462      9463      9464      9465      9466      9467      9468      9469      9470      9471      9472      9473      9474      9475      9476      9477      9478      9479      9480      9481      9482      9483      9484      9485      9486      9487      9488      9489      9490      9491      9492      9493      9494      9495      9496      9497      9498     ----------+ CUST_ID  ----------+ 9499      9500      9501      9502      9503      9504      9505      9506      9507      9508      9509      9510      9511      9512      9513      9514      9515      9516      9517      9518      9519      9520      9521      9522      9523      9524      9525      9526      9527      9528      9529      9530      9531      9532      9533      9534      9535      9536      9537      9538      9539      9540      9541      9542      9543      9544      9545      9546      9547      9548      9549      9550      9551      9552      9553      9554      9555      9556      9557      9558      9559      9560      9561      9562      9563      9564      9565      9566      9567      9568      9569      9570      9571      9572      9573      9574      9575      9576      9577      9578      9579      9580      9581      9582      9583      9584      9585      9586      9587      9588      9589      9590      9591      9592      9593      9594      9595      9596      9597      9598     ----------+ CUST_ID  ----------+ 9599      9600      9601      9602      9603      9604      9605      9606      9607      9608      9609      9610      9611      9612      9613      9614      9615      9616      9617      9618      9619      9620      9621      9622      9623      9624      9625      9626      9627      9628      9629      9630      9631      9632      9633      9634      9635      9636      9637      9638      9639      9640      9641      9642      9643      9644      9645      9646      9647      9648      9649      9650      9651      9652      9653      9654      9655      9656      9657      9658      9659      9660      9661      9662      9663      9664      9665      9666      9667      9668      9669      9670      9671      9672      9673      9674      9675      9676      9677      9678      9679      9680      9681      9682      9683      9684      9685      9686      9687      9688      9689      9690      9691      9692      9693      9694      9695      9696      9697      9698     ----------+ CUST_ID  ----------+ 9699      9700      9701      9702      9703      9704      9705      9706      9707      9708      9709      9710      9711      9712      9713      9714      9715      9716      9717      9718      9719      9720      9721      9722      9723      9724      9725      9726      9727      9728      9729      9730      9731      9732      9733      9734      9735      9736      9737      9738      9739      9740      9741      9742      9743      9744      9745      9746      9747      9748      9749      9750      9751      9752      9753      9754      9755      9756      9757      9758      9759      9760      9761      9762      9763      9764      9765      9766      9767      9768      9769      9770      9771      9772      9773      9774      9775      9776      9777      9778      9779      9780      9781      9782      9783      9784      9785      9786      9787      9788      9789      9790      9791      9792      9793      9794      9795      9796      9797      9798     ----------+ CUST_ID  ----------+ 9799      9800      9801      9802      9803      9804      9805      9806      9807      9808      9809      9810      9811      9812      9813      9814      9815      9816      9817      9818      9819      9820      9821      9822      9823      9824      9825      9826      9827      9828      9829      9830      9831      9832      9833      9834      9835      9836      9837      9838      9839      9840      9841      9842      9843      9844      9845      9846      9847      9848      9849      9850      9851      9852      9853      9854      9855      9856      9857      9858      9859      9860      9861      9862      9863      9864      9865      9866      9867      9868      9869      9870      9871      9872      9873      9874      9875      9876      9877      9878      9879      9880      9881      9882      9883      9884      9885      9886      9887      9888      9889      9890      9891      9892      9893      9894      9895      9896      9897      9898     ----------+ CUST_ID  ----------+ 9899      9900      9901      9902      9903      9904      9905      9906      9907      9908      9909      9910      9911      9912      9913      9914      9915      9916      9917      9918      9919      9920      9921      9922      9923      9924      9925      9926      9927      9928      9929      9930      9931      9932      9933      9934      9935      9936      9937      9938      9939      9940      9941      9942      9943      9944      9945      9946      9947      9948      9949      9950      9951      9952      9953      9954      9955      9956      9957      9958      9959      9960      9961      9962      9963      9964      9965      9966      9967      9968      9969      9970      9971      9972      9973      9974      9975      9976      9977      9978      9979      9980      9981      9982      9983      9984      9985      9986      9987      9988      9989      9990      9991      9992      9993      9994      9995      9996      9997      9998     ----------+ CUST_ID  ----------+ 9999      10000     10001     10002     10003     10004     10005     10006     10007     10008     10009     10010     10011     10012     10013     10014     10015     10016     10017     10018     10019     10020     10021     10022     10023     10024     10025     10026     10027     10028     10029     10030     10031     10032     10033     10034     10035     10036     10037     10038     10039     10040     10041     10042     10043     10044     10045     10046     10047     10048     10049     10050     10051     10052     10053     10054     10055     10056     10057     10058     10059     10060     10061     10062     10063     10064     10065     10066     10067     10068     10069     10070     10071     10072     10073     10074     10075     10076     10077     10078     10079     10080     10081     10082     10083     10084     10085     10086     10087     10088     10089     10090     10091     10092     10093     10094     10095     10096     10097     10098    ----------+ CUST_ID  ----------+ 10099     10100     10101     10102     10103     10104     10105     10106     10107     10108     10109     10110     10111     10112     10113     10114     10115     10116     10117     10118     10119     10120     10121     10122     10123     10124     10125     10126     10127     10128     10129     10130     10131     10132     10133     10134     10135     10136     10137     10138     10139     10140     10141     10142     10143     10144     10145     10146     10147     10148     10149     10150     10151     10152     10153     10154     10155     10156     10157     10158     10159     10160     10161     10162     10163     10164     10165     10166     10167     10168     10169     10170     10171     10172     10173     10174     10175     10176     10177     10178     10179     10180     10181     10182     10183     10184     10185     10186     10187     10188     10189     10190     10191     10192     10193     10194     10195     10196     10197     10198    ----------+ CUST_ID  ----------+ 10199     10200     10201     10202     10203     10204     10205     10206     10207     10208     10209     10210     10211     10212     10213     10214     10215     10216     10217     10218     10219     10220     10221     10222     10223     10224     10225     10226     10227     10228     10229     10230     10231     10232     10233     10234     10235     10236     10237     10238     10239     10240     10241     10242     10243     10244     10245     10246     10247     10248     10249     10250     10251     10252     10253     10254     10255     10256     10257     10258     10259     10260     10261     10262     10263     10264     10265     10266     10267     10268     10269     10270     10271     10272     10273     10274     10275     10276     10277     10278     10279     10280     10281     10282     10283     10284     10285     10286     10287     10288     10289     10290     10291     10292     10293     10294     10295     10296     10297     10298    ----------+ CUST_ID  ----------+ 10299     10300     10301     10302     10303     10304     10305     10306     10307     10308     10309     10310     10311     10312     10313     10314     10315     10316     10317     10318     10319     10320     10321     10322     10323     10324     10325     10326     10327     10328     10329     10330     10331     10332     10333     10334     10335     10336     10337     10338     10339     10340     10341     10342     10343     10344     10345     10346     10347     10348     10349     10350     10351     10352     10353     10354     10355     10356     10357     10358     10359     10360     10361     10362     10363     10364     10365     10366     10367     10368     10369     10370     10371     10372     10373     10374     10375     10376     10377     10378     10379     10380     10381     10382     10383     10384     10385     10386     10387     10388     10389     10390     10391     10392     10393     10394     10395     10396     10397     10398    ----------+ CUST_ID  ----------+ 10399     10400     10401     10402     10403     10404     10405     10406     10407     10408     10409     10410     10411     10412     10413     10414     10415     10416     10417     10418     10419     10420     10421     10422     10423     10424     10425     10426     10427     10428     10429     10430     10431     10432     10433     10434     10435     10436     10437     10438     10439     10440     10441     10442     10443     10444     10445     10446     10447     10448     10449     10450     10451     10452     10453     10454     10455     10456     10457     10458     10459     10460     10461     10462     10463     10464     10465     10466     10467     10468     10469     10470     10471     10472     10473     10474     10475     10476     10477     10478     10479     10480     10481     10482     10483     10484     10485     10486     10487     10488     10489     10490     10491     10492     10493     10494     10495     10496     10497     10498    ----------+ CUST_ID  ----------+ 10499     10500     10501     10502     10503     10504     10505     10506     10507     10508     10509     10510     10511     10512     10513     10514     10515     10516     10517     10518     10519     10520     10521     10522     10523     10524     10525     10526     10527     10528     10529     10530     10531     10532     10533     10534     10535     10536     10537     10538     10539     10540     10541     10542     10543     10544     10545     10546     10547     10548     10549     10550     10551     10552     10553     10554     10555     10556     10557     10558     10559     10560     10561     10562     10563     10564     10565     10566     10567     10568     10569     10570     10571     10572     10573     10574     10575     10576     10577     10578     10579     10580     10581     10582     10583     10584     10585     10586     10587     10588     10589     10590     10591     10592     10593     10594     10595     10596     10597     10598    ----------+ CUST_ID  ----------+ 10599     10600     10601     10602     10603     10604     10605     10606     10607     10608     10609     10610     10611     10612     10613     10614     10615     10616     10617     10618     10619     10620     10621     10622     10623     10624     10625     10626     10627     10628     10629     10630     10631     10632     10633     10634     10635     10636     10637     10638     10639     10640     10641     10642     10643     10644     10645     10646     10647     10648     10649     10650     10651     10652     10653     10654     10655     10656     10657     10658     10659     10660     10661     10662     10663     10664     10665     10666     10667     10668     10669     10670     10671     10672     10673     10674     10675     10676     10677     10678     10679     10680     10681     10682     10683     10684     10685     10686     10687     10688     10689     10690     10691     10692     10693     10694     10695     10696     10697     10698    ----------+ CUST_ID  ----------+ 10699     10700     10701     10702     10703     10704     10705     10706     10707     10708     10709     10710     10711     10712     10713     10714     10715     10716     10717     10718     10719     10720     10721     10722     10723     10724     10725     10726     10727     10728     10729     10730     10731     10732     10733     10734     10735     10736     10737     10738     10739     10740     10741     10742     10743     10744     10745     10746     10747     10748     10749     10750     10751     10752     10753     10754     10755     10756     10757     10758     10759     10760     10761     10762     10763     10764     10765     10766     10767     10768     10769     10770     10771     10772     10773     10774     10775     10776     10777     10778     10779     10780     10781     10782     10783     10784     10785     10786     10787     10788     10789     10790     10791     10792     10793     10794     10795     10796     10797     10798    ----------+ CUST_ID  ----------+ 10799     10800     10801     10802     10803     10804     10805     10806     10807     10808     10809     10810     10811     10812     10813     10814     10815     10816     10817     10818     10819     10820     10821     10822     10823     10824     10825     10826     10827     10828     10829     10830     10831     10832     10833     10834     10835     10836     10837     10838     10839     10840     10841     10842     10843     10844     10845     10846     10847     10848     10849     10850     10851     10852     10853     10854     10855     10856     10857     10858     10859     10860     10861     10862     10863     10864     10865     10866     10867     10868     10869     10870     10871     10872     10873     10874     10875     10876     10877     10878     10879     10880     10881     10882     10883     10884     10885     10886     10887     10888     10889     10890     10891     10892     10893     10894     10895     10896     10897     10898    ----------+ CUST_ID  ----------+ 10899     10900     10901     10902     10903     10904     10905     10906     10907     10908     10909     10910     10911     10912     10913     10914     10915     10916     10917     10918     10919     10920     10921     10922     10923     10924     10925     10926     10927     10928     10929     10930     10931     10932     10933     10934     10935     10936     10937     10938     10939     10940     10941     10942     10943     10944     10945     10946     10947     10948     10949     10950     10951     10952     10953     10954     10955     10956     10957     10958     10959     10960     10961     10962     10963     10964     10965     10966     10967     10968     10969     10970     10971     10972     10973     10974     10975     10976     10977     10978     10979     10980     10981     10982     10983     10984     10985     10986     10987     10988     10989     10990     10991     10992     10993     10994     10995     10996     10997     10998    ----------+ CUST_ID  ----------+ 10999    ----------+B) Create table in carbonCREATE TABLE uniqdata (CUST_ID int,CUST_NAME String,ACTIVE_EMUI_VERSION string, DOB timestamp, DOJ timestamp, BIGINT_COLUMN1 bigint,BIGINT_COLUMN2 bigint,DECIMAL_COLUMN1 decimal(30,10), DECIMAL_COLUMN2 decimal(36,10),Double_COLUMN1 double, Double_COLUMN2 double,INTEGER_COLUMN1 int) STORED BY 'org.apache.carbondata.format' TBLPROPERTIES ("TABLE_BLOCKSIZE"= "256 MB")2:Load Data in hivea)LOAD DATA INPATH 'HDFS_URL/BabuStore/Data/uniqdata/2000_UniqData.csv' into table uniqdata OPTIONS('DELIMITER'=',' , 'QUOTECHAR'='"','BAD_RECORDS_ACTION'='FORCE','FILEHEADER'='CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1,Double_COLUMN2,INTEGER_COLUMN1')b)LOAD DATA INPATH 'HDFS_URL/BabuStore/Data/uniqdata/4000_UniqData.csv' into table uniqdata OPTIONS('DELIMITER'=',' , 'QUOTECHAR'='"','BAD_RECORDS_ACTION'='FORCE','FILEHEADER'='CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1,Double_COLUMN2,INTEGER_COLUMN1')c)LOAD DATA INPATH 'HDFS_URL/BabuStore/Data/uniqdata/6000_UniqData.csv' into table uniqdata OPTIONS('DELIMITER'=',' , 'QUOTECHAR'='"','BAD_RECORDS_ACTION'='FORCE','FILEHEADER'='CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1,Double_COLUMN2,INTEGER_COLUMN1')d)LOAD DATA INPATH 'HDFS_URL/BabuStore/Data/uniqdata/7000_UniqData.csv' into table uniqdata OPTIONS('DELIMITER'=',' , 'QUOTECHAR'='"','BAD_RECORDS_ACTION'='FORCE','FILEHEADER'='CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1,Double_COLUMN2,INTEGER_COLUMN1')e)LOAD DATA INPATH 'HDFS_URL/BabuStore/Data/uniqdata/3000_1_UniqData.csv' into table uniqdata OPTIONS('DELIMITER'=',' , 'QUOTECHAR'='"','BAD_RECORDS_ACTION'='FORCE','FILEHEADER'='CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1,Double_COLUMN2,INTEGER_COLUMN1')3: Run the Query:select CUST_ID from uniqdata_h where CUST_ID NOT LIKE 1000794:Result in carbon data0: jdbc:hive2://localhost:10000> select CUST_ID from UNIQDATA where CUST_ID NOT LIKE 100079;----------+ CUST_ID  ----------+----------+No rows selected (59.188 seconds)Expected Result: Correct result should be displayed while using not like operator.
issueID:CARBONDATA-775
type:Improvement
changed files:
texts:Update Documentation for Supported Datatypes

issueID:CARBONDATA-777
type:Sub-task
changed files:core/src/main/java/org/apache/carbondata/core/datastore/block/SegmentProperties.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/CarbonTable.java
texts:Alter table support for spark 2.1
Alter table need to be supported for spark 2.1As part of this jira following features will be supported.1. Support alter table result preparation.2. Support reading data with different block key generators.3. Support addition of a new column.4. Support deletion of a column.5. Support change in data type form INT to BIGINT6. Support Change of decimal datatype from lower to higher precision.7. Support filtering on newly added columns.8. Support rename table9. Parsing support for the new DDL commands added.
issueID:CARBONDATA-78
type:Bug
changed files:
texts:Update the ReadMe and related documents as per the latest changes
Update the docs with following changes.1. Carbon support all Spark 1.5 and 1.6 versions2. Update with carbon-spark-shell3. Update with carbon-sql-cli4. Update with latest usability changes
issueID:CARBONDATA-780
type:Sub-task
changed files:processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/SortParameters.java
processing/src/main/java/org/apache/carbondata/processing/merger/CarbonCompactionUtil.java
processing/src/main/java/org/apache/carbondata/processing/merger/CarbonCompactionExecutor.java
processing/src/main/java/org/apache/carbondata/processing/merger/RowResultMergerProcessor.java
processing/src/main/java/org/apache/carbondata/processing/loading/steps/DataWriterProcessorStepImpl.java
processing/src/main/java/org/apache/carbondata/processing/util/CarbonLoaderUtil.java
core/src/main/java/org/apache/carbondata/core/scan/collector/impl/RestructureBasedRawResultCollector.java
processing/src/main/java/org/apache/carbondata/processing/exception/SliceMergerException.java
processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactDataHandlerModel.java
processing/src/main/java/org/apache/carbondata/processing/merger/CarbonDataMergerUtilResult.java
processing/src/main/java/org/apache/carbondata/processing/merger/CarbonDataMergerUtil.java
processing/src/main/java/org/apache/carbondata/processing/util/CarbonDataProcessorUtil.java
processing/src/main/java/org/apache/carbondata/processing/merger/CompactionType.java
core/src/main/java/org/apache/carbondata/core/scan/collector/impl/DictionaryBasedVectorResultCollector.java
core/src/main/java/org/apache/carbondata/core/locks/CarbonLockFactory.java
processing/src/main/java/org/apache/carbondata/processing/merger/NodeMultiBlockRelation.java
processing/src/main/java/org/apache/carbondata/processing/loading/DataLoadProcessBuilder.java
processing/src/main/java/org/apache/carbondata/processing/merger/CompactionResultSortProcessor.java
processing/src/main/java/org/apache/carbondata/processing/merger/AbstractResultProcessor.java
processing/src/main/java/org/apache/carbondata/processing/merger/NodeBlockRelation.java
texts:Alter table support for compaction through sort step
Alter table need to support compaction process where complete data need to be sorted again and then written to file.Currently in compaction process data is directly given to writer step where it is splitted into columns and written. But as columns are sorted from left to right, on dropping a column data will again become unorganized as dropped column data will not be considered during compaction. In these scenarios complete data need to be sorted again and then submitted to writer step.
issueID:CARBONDATA-781
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/metadata/schema/table/column/ColumnSchema.java
texts:Some SegmentProperties objects occupy too much memory in driver
When I load carbondata 1000+ times with 35 nodes, I found SegmentProperties objects occupy 2.5+G(76K * 35 * 1000) memory in driver. I don't have small files so I don't want to compact the segments. I analyzed the dump file and found the values of SegmentProperties are the same, so I think we can reuse the SegmentProperties object if possible.
issueID:CARBONDATA-782
type:New Feature
changed files:processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/SortDataRows.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/SortParameters.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/NewRowComparator.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/comparator/UnsafeRowComparator.java
core/src/main/java/org/apache/carbondata/core/util/DataTypeUtil.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/SingleThreadFinalSortFilesMerger.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/NewRowComparatorForNormalDims.java
processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactDataHandlerColumnar.java
core/src/main/java/org/apache/carbondata/core/scan/result/iterator/AbstractDetailQueryResultIterator.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/resolverinfo/visitor/NoDictionaryTypeVisitor.java
core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/comparator/UnsafeRowComparatorForNormalDIms.java
core/src/main/java/org/apache/carbondata/core/datastore/block/SegmentProperties.java
integration/spark2/src/main/java/org/apache/carbondata/spark/readsupport/SparkRowReadSupportImpl.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/impl/ParallelReadMergeSorterImpl.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/impl/ParallelReadMergeSorterWithBucketingImpl.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/holder/UnsafeSortTempFileChunkHolder.java
core/src/main/java/org/apache/carbondata/core/scan/filter/FilterExpressionProcessor.java
processing/src/main/java/org/apache/carbondata/processing/loading/DataLoadProcessBuilder.java
processing/src/main/java/org/apache/carbondata/processing/merger/CompactionResultSortProcessor.java
core/src/main/java/org/apache/carbondata/core/scan/filter/FilterUtil.java
core/src/main/java/org/apache/carbondata/core/util/AbstractDataFileFooterConverter.java
processing/src/main/java/org/apache/carbondata/processing/loading/CarbonDataLoadConfiguration.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/UnsafeSortDataRows.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelRangeLessThanFilterExecuterImpl.java
integration/spark-datasource/src/main/scala/org/apache/carbondata/spark/vectorreader/ColumnarVectorWrapper.java
processing/src/main/java/org/apache/carbondata/processing/loading/converter/impl/NonDictionaryFieldConverterImpl.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/holder/UnsafeFinalMergePageHolder.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/SortTempFileChunkHolder.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/holder/UnsafeInmemoryHolder.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/impl/VariableLengthDimensionColumnPage.java
processing/src/main/java/org/apache/carbondata/processing/merger/RowResultMergerProcessor.java
core/src/main/java/org/apache/carbondata/core/util/ByteUtil.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/CarbonTable.java
hadoop/src/main/java/org/apache/carbondata/hadoop/readsupport/impl/DictionaryDecodeReadSupport.java
core/src/main/java/org/apache/carbondata/core/scan/collector/impl/DictionaryBasedResultCollector.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/UnsafeCarbonRowPage.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/column/ColumnSchema.java
core/src/main/java/org/apache/carbondata/core/keygenerator/mdkey/Bits.java
core/src/main/java/org/apache/carbondata/core/scan/collector/impl/RestructureBasedDictionaryResultCollector.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelRangeLessThanEqualFilterExecuterImpl.java
processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactDataHandlerModel.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
core/src/main/java/org/apache/carbondata/core/scan/result/vector/CarbonColumnVector.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/resolverinfo/visitor/RangeNoDictionaryTypeVisitor.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/IntermediateFileMerger.java
core/src/main/java/org/apache/carbondata/core/metadata/converter/ThriftWrapperSchemaConverterImpl.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/merger/UnsafeSingleThreadFinalSortFilesMerger.java
texts:Support SORT_COLUMNS
The tasks of SORT_COLUMNS:1.Support create table with sort_columns property.e.g. tblproperties('sort_columns' = 'col7,col3')The table with SORT_COLUMNS property will be sorted by SORT_COLUMNS. The order of columns is decided by SORT_COLUMNS.2.Change the encoding rule of SORT_COLUMNSFirstly, the rule of column encoding will keep consistent with previous.Secondly, if a column of SORT_COLUMNS is a measure before, now this column will be created as a dimension. And this dimension is a no-dicitonary column(Better to use other direct-dictionary).Thirdly, the dimension of SORT_COLUMNS have RLE and ROWID page, other dimension have only RLE(not sorted).3.The start/end key should be composed of SORT_COLUMNS.Using SORT_COLUMNS to build start/end key during data loading and select query.
issueID:CARBONDATA-783
type:Bug
changed files:
texts:Loading data with Single Pass &#39;true&#39; option is throwing an exception
I tried to create table using the following query:CREATE TABLE uniq_include_dictionary (CUST_ID int,CUST_NAME String,ACTIVE_EMUI_VERSION string, DOB timestamp, DOJ timestamp, BIGINT_COLUMN1 bigint,BIGINT_COLUMN2 bigint,DECIMAL_COLUMN1 decimal(30,10), DECIMAL_COLUMN2 decimal(36,10),Double_COLUMN1 double, Double_COLUMN2 double,INTEGER_COLUMN1 int) STORED BY 'org.apache.carbondata.format' TBLPROPERTIES('DICTIONARY_INCLUDE'='CUST_ID,Double_COLUMN2,DECIMAL_COLUMN2');Table creation was successfull but when I tried to load data into the table It showed the following error:ERROR 16-03 13:41:32,354 - nioEventLoopGroup-8-2 java.lang.IndexOutOfBoundsException: readerIndex(64) + length(25) exceeds writerIndex(80): UnpooledUnsafeDirectByteBuf(ridx: 64, widx: 80, cap: 80) at io.netty.buffer.AbstractByteBuf.checkReadableBytes0(AbstractByteBuf.java:1161) at io.netty.buffer.AbstractByteBuf.checkReadableBytes(AbstractByteBuf.java:1155) at io.netty.buffer.AbstractByteBuf.readBytes(AbstractByteBuf.java:694) at io.netty.buffer.AbstractByteBuf.readBytes(AbstractByteBuf.java:702) at org.apache.carbondata.core.dictionary.generator.key.DictionaryMessage.readData(DictionaryMessage.java:70) at org.apache.carbondata.core.dictionary.server.DictionaryServerHandler.channelRead(DictionaryServerHandler.java:59) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:367) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:353) at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:346) at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1294) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:367) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:353) at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:911) at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131) at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:652) at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:575) at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:489) at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:451) at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:140) at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144) at java.lang.Thread.run(Thread.java:745)ERROR 16-03 13:41:32,355 - nioEventLoopGroup-8-2 exceptionCaughtjava.lang.IndexOutOfBoundsException: readerIndex(64) + length(25) exceeds writerIndex(80): UnpooledUnsafeDirectByteBuf(ridx: 64, widx: 80, cap: 80) at io.netty.buffer.AbstractByteBuf.checkReadableBytes0(AbstractByteBuf.java:1161) at io.netty.buffer.AbstractByteBuf.checkReadableBytes(AbstractByteBuf.java:1155) at io.netty.buffer.AbstractByteBuf.readBytes(AbstractByteBuf.java:694) at io.netty.buffer.AbstractByteBuf.readBytes(AbstractByteBuf.java:702) at org.apache.carbondata.core.dictionary.generator.key.DictionaryMessage.readData(DictionaryMessage.java:70) at org.apache.carbondata.core.dictionary.server.DictionaryServerHandler.channelRead(DictionaryServerHandler.java:59) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:367) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:353) at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:346) at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1294) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:367) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:353) at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:911) at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131) at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:652) at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:575) at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:489) at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:451) at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:140) at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144) at java.lang.Thread.run(Thread.java:745)WARN  16-03 13:43:17,223 - Lost task 0.0 in stage 0.0 (TID 0, 192.168.2.130, executor 0): org.apache.carbondata.processing.newflow.exception.CarbonDataLoadingException:  at org.apache.carbondata.processing.newflow.sort.impl.ParallelReadMergeSorterImpl.checkError(ParallelReadMergeSorterImpl.java:164) at org.apache.carbondata.processing.newflow.sort.impl.ParallelReadMergeSorterImpl.sort(ParallelReadMergeSorterImpl.java:117) at org.apache.carbondata.processing.newflow.steps.SortProcessorStepImpl.execute(SortProcessorStepImpl.java:76) at org.apache.carbondata.processing.newflow.steps.DataWriterProcessorStepImpl.execute(DataWriterProcessorStepImpl.java:92) at org.apache.carbondata.processing.newflow.DataLoadExecutor.execute(DataLoadExecutor.java:48) at org.apache.carbondata.spark.rdd.NewCarbonDataLoadRDD$$anon$1.<init>(NewCarbonDataLoadRDD.scala:166) at org.apache.carbondata.spark.rdd.NewCarbonDataLoadRDD.compute(NewCarbonDataLoadRDD.scala:142) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) at org.apache.spark.rdd.RDD.iterator(RDD.scala:287) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87) at org.apache.spark.scheduler.Task.run(Task.scala:99) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745)Caused by: java.lang.RuntimeException: java.lang.RuntimeException: Request timed out for key : DictionaryKey{ columnName='ACTIVE_EMUI_VERSION', data='ACTIVE_EMUI_VERSION_20014', dictionaryValue=-1, type=DICT_GENERATION} at org.apache.carbondata.core.dictionary.client.DictionaryClientHandler.getDictionary(DictionaryClientHandler.java:99) at org.apache.carbondata.core.dictionary.client.DictionaryClient.getDictionary(DictionaryClient.java:74) at org.apache.carbondata.processing.newflow.dictionary.DictionaryServerClientDictionary.getOrGenerateKey(DictionaryServerClientDictionary.java:57) at org.apache.carbondata.processing.newflow.dictionary.DictionaryServerClientDictionary.getOrGenerateKey(DictionaryServerClientDictionary.java:32) at org.apache.carbondata.processing.newflow.converter.impl.DictionaryFieldConverterImpl.convert(DictionaryFieldConverterImpl.java:101) at org.apache.carbondata.processing.newflow.converter.impl.RowConverterImpl.convert(RowConverterImpl.java:150) at org.apache.carbondata.processing.newflow.steps.DataConverterProcessorStepImpl.processRowBatch(DataConverterProcessorStepImpl.java:103) at org.apache.carbondata.processing.newflow.steps.DataConverterProcessorStepImpl$1.next(DataConverterProcessorStepImpl.java:89) at org.apache.carbondata.processing.newflow.steps.DataConverterProcessorStepImpl$1.next(DataConverterProcessorStepImpl.java:78) at org.apache.carbondata.processing.newflow.sort.impl.ParallelReadMergeSorterImpl$SortIteratorThread.call(ParallelReadMergeSorterImpl.java:227) at org.apache.carbondata.processing.newflow.sort.impl.ParallelReadMergeSorterImpl$SortIteratorThread.call(ParallelReadMergeSorterImpl.java:201) at java.util.concurrent.FutureTask.run(FutureTask.java:266) ... 3 moreCaused by: java.lang.RuntimeException: Request timed out for key : DictionaryKey{ columnName='ACTIVE_EMUI_VERSION', data='ACTIVE_EMUI_VERSION_20014', dictionaryValue=-1, type=DICT_GENERATION} at org.apache.carbondata.core.dictionary.client.DictionaryClientHandler.getDictionary(DictionaryClientHandler.java:94) ... 14 moreINFO  16-03 13:43:17,242 - Starting task 0.1 in stage 0.0 (TID 1, 192.168.2.130, executor 0, partition 0, NODE_LOCAL, 6989 bytes)AUDIT 16-03 13:43:17,375 - &#91;knoldus-Vostro-2520&#93;&#91;hduser&#93;&#91;Thread-134&#93;Connected io.netty.channel.DefaultChannelHandlerContext@6904f1c6AUDIT 16-03 13:43:18,471 - &#91;knoldus-Vostro-2520&#93;&#91;hduser&#93;&#91;Thread-135&#93;Connected io.netty.channel.DefaultChannelHandlerContext@38d40b06ERROR 16-03 13:43:19,577 - nioEventLoopGroup-8-4 java.lang.IndexOutOfBoundsException: readerIndex(64) + length(25) exceeds writerIndex(80): UnpooledUnsafeDirectByteBuf(ridx: 64, widx: 80, cap: 80) at io.netty.buffer.AbstractByteBuf.checkReadableBytes0(AbstractByteBuf.java:1161) at io.netty.buffer.AbstractByteBuf.checkReadableBytes(AbstractByteBuf.java:1155) at io.netty.buffer.AbstractByteBuf.readBytes(AbstractByteBuf.java:694) at io.netty.buffer.AbstractByteBuf.readBytes(AbstractByteBuf.java:702) at org.apache.carbondata.core.dictionary.generator.key.DictionaryMessage.readData(DictionaryMessage.java:70) at org.apache.carbondata.core.dictionary.server.DictionaryServerHandler.channelRead(DictionaryServerHandler.java:59) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:367) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:353) at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:346) at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1294) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:367) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:353) at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:911) at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131) at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:652) at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:575) at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:489) at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:451) at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:140) at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144) at java.lang.Thread.run(Thread.java:745)ERROR 16-03 13:43:19,578 - nioEventLoopGroup-8-4 exceptionCaughtjava.lang.IndexOutOfBoundsException: readerIndex(64) + length(25) exceeds writerIndex(80): UnpooledUnsafeDirectByteBuf(ridx: 64, widx: 80, cap: 80) at io.netty.buffer.AbstractByteBuf.checkReadableBytes0(AbstractByteBuf.java:1161) at io.netty.buffer.AbstractByteBuf.checkReadableBytes(AbstractByteBuf.java:1155) at io.netty.buffer.AbstractByteBuf.readBytes(AbstractByteBuf.java:694) at io.netty.buffer.AbstractByteBuf.readBytes(AbstractByteBuf.java:702) at org.apache.carbondata.core.dictionary.generator.key.DictionaryMessage.readData(DictionaryMessage.java:70) at org.apache.carbondata.core.dictionary.server.DictionaryServerHandler.channelRead(DictionaryServerHandler.java:59) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:367) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:353) at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:346) at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1294) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:367) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:353) at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:911) at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131) at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:652) at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:575) at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:489) at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:451) at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:140) at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144) at java.lang.Thread.run(Thread.java:745)WARN  16-03 13:45:04,012 - Lost task 0.1 in stage 0.0 (TID 1, 192.168.2.130, executor 0): org.apache.carbondata.processing.newflow.exception.CarbonDataLoadingException:  at org.apache.carbondata.processing.newflow.sort.impl.ParallelReadMergeSorterImpl.checkError(ParallelReadMergeSorterImpl.java:164) at org.apache.carbondata.processing.newflow.sort.impl.ParallelReadMergeSorterImpl.sort(ParallelReadMergeSorterImpl.java:117) at org.apache.carbondata.processing.newflow.steps.SortProcessorStepImpl.execute(SortProcessorStepImpl.java:76) at org.apache.carbondata.processing.newflow.steps.DataWriterProcessorStepImpl.execute(DataWriterProcessorStepImpl.java:92) at org.apache.carbondata.processing.newflow.DataLoadExecutor.execute(DataLoadExecutor.java:48) at org.apache.carbondata.spark.rdd.NewCarbonDataLoadRDD$$anon$1.<init>(NewCarbonDataLoadRDD.scala:166) at org.apache.carbondata.spark.rdd.NewCarbonDataLoadRDD.compute(NewCarbonDataLoadRDD.scala:142) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) at org.apache.spark.rdd.RDD.iterator(RDD.scala:287) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87) at org.apache.spark.scheduler.Task.run(Task.scala:99) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745)Caused by: java.lang.RuntimeException: java.lang.RuntimeException: Request timed out for key : DictionaryKey{ columnName='ACTIVE_EMUI_VERSION', data='ACTIVE_EMUI_VERSION_20014', dictionaryValue=-1, type=DICT_GENERATION} at org.apache.carbondata.core.dictionary.client.DictionaryClientHandler.getDictionary(DictionaryClientHandler.java:99) at org.apache.carbondata.core.dictionary.client.DictionaryClient.getDictionary(DictionaryClient.java:74) at org.apache.carbondata.processing.newflow.dictionary.DictionaryServerClientDictionary.getOrGenerateKey(DictionaryServerClientDictionary.java:57) at org.apache.carbondata.processing.newflow.dictionary.DictionaryServerClientDictionary.getOrGenerateKey(DictionaryServerClientDictionary.java:32) at org.apache.carbondata.processing.newflow.converter.impl.DictionaryFieldConverterImpl.convert(DictionaryFieldConverterImpl.java:101) at org.apache.carbondata.processing.newflow.converter.impl.RowConverterImpl.convert(RowConverterImpl.java:150) at org.apache.carbondata.processing.newflow.steps.DataConverterProcessorStepImpl.processRowBatch(DataConverterProcessorStepImpl.java:103) at org.apache.carbondata.processing.newflow.steps.DataConverterProcessorStepImpl$1.next(DataConverterProcessorStepImpl.java:89) at org.apache.carbondata.processing.newflow.steps.DataConverterProcessorStepImpl$1.next(DataConverterProcessorStepImpl.java:78) at org.apache.carbondata.processing.newflow.sort.impl.ParallelReadMergeSorterImpl$SortIteratorThread.call(ParallelReadMergeSorterImpl.java:227) at org.apache.carbondata.processing.newflow.sort.impl.ParallelReadMergeSorterImpl$SortIteratorThread.call(ParallelReadMergeSorterImpl.java:201) at java.util.concurrent.FutureTask.run(FutureTask.java:266) ... 3 moreCaused by: java.lang.RuntimeException: Request timed out for key : DictionaryKey{ columnName='ACTIVE_EMUI_VERSION', data='ACTIVE_EMUI_VERSION_20014', dictionaryValue=-1, type=DICT_GENERATION} at org.apache.carbondata.core.dictionary.client.DictionaryClientHandler.getDictionary(DictionaryClientHandler.java:94) ... 14 moreINFO  16-03 13:45:04,019 - Starting task 0.2 in stage 0.0 (TID 2, 192.168.2.130, executor 0, partition 0, NODE_LOCAL, 6989 bytes)AUDIT 16-03 13:45:04,072 - &#91;knoldus-Vostro-2520&#93;&#91;hduser&#93;&#91;Thread-136&#93;Connected io.netty.channel.DefaultChannelHandlerContext@68a91decAUDIT 16-03 13:45:05,287 - &#91;knoldus-Vostro-2520&#93;&#91;hduser&#93;&#91;Thread-137&#93;Connected io.netty.channel.DefaultChannelHandlerContext@16aa3381ERROR 16-03 13:45:06,776 - nioEventLoopGroup-8-6 java.lang.IndexOutOfBoundsException: readerIndex(64) + length(25) exceeds writerIndex(80): UnpooledUnsafeDirectByteBuf(ridx: 64, widx: 80, cap: 80) at io.netty.buffer.AbstractByteBuf.checkReadableBytes0(AbstractByteBuf.java:1161) at io.netty.buffer.AbstractByteBuf.checkReadableBytes(AbstractByteBuf.java:1155) at io.netty.buffer.AbstractByteBuf.readBytes(AbstractByteBuf.java:694) at io.netty.buffer.AbstractByteBuf.readBytes(AbstractByteBuf.java:702) at org.apache.carbondata.core.dictionary.generator.key.DictionaryMessage.readData(DictionaryMessage.java:70) at org.apache.carbondata.core.dictionary.server.DictionaryServerHandler.channelRead(DictionaryServerHandler.java:59) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:367) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:353) at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:346) at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1294) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:367) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:353) at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:911) at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131) at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:652) at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:575) at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:489) at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:451) at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:140) at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144) at java.lang.Thread.run(Thread.java:745)ERROR 16-03 13:45:06,777 - nioEventLoopGroup-8-6 exceptionCaughtjava.lang.IndexOutOfBoundsException: readerIndex(64) + length(25) exceeds writerIndex(80): UnpooledUnsafeDirectByteBuf(ridx: 64, widx: 80, cap: 80) at io.netty.buffer.AbstractByteBuf.checkReadableBytes0(AbstractByteBuf.java:1161) at io.netty.buffer.AbstractByteBuf.checkReadableBytes(AbstractByteBuf.java:1155) at io.netty.buffer.AbstractByteBuf.readBytes(AbstractByteBuf.java:694) at io.netty.buffer.AbstractByteBuf.readBytes(AbstractByteBuf.java:702) at org.apache.carbondata.core.dictionary.generator.key.DictionaryMessage.readData(DictionaryMessage.java:70) at org.apache.carbondata.core.dictionary.server.DictionaryServerHandler.channelRead(DictionaryServerHandler.java:59) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:367) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:353) at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:346) at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1294) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:367) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:353) at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:911) at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131) at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:652) at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:575) at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:489) at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:451) at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:140) at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144) at java.lang.Thread.run(Thread.java:745)WARN  16-03 13:46:51,208 - Lost task 0.2 in stage 0.0 (TID 2, 192.168.2.130, executor 0): org.apache.carbondata.processing.newflow.exception.CarbonDataLoadingException:  at org.apache.carbondata.processing.newflow.sort.impl.ParallelReadMergeSorterImpl.checkError(ParallelReadMergeSorterImpl.java:164) at org.apache.carbondata.processing.newflow.sort.impl.ParallelReadMergeSorterImpl.sort(ParallelReadMergeSorterImpl.java:117) at org.apache.carbondata.processing.newflow.steps.SortProcessorStepImpl.execute(SortProcessorStepImpl.java:76) at org.apache.carbondata.processing.newflow.steps.DataWriterProcessorStepImpl.execute(DataWriterProcessorStepImpl.java:92) at org.apache.carbondata.processing.newflow.DataLoadExecutor.execute(DataLoadExecutor.java:48) at org.apache.carbondata.spark.rdd.NewCarbonDataLoadRDD$$anon$1.<init>(NewCarbonDataLoadRDD.scala:166) at org.apache.carbondata.spark.rdd.NewCarbonDataLoadRDD.compute(NewCarbonDataLoadRDD.scala:142) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) at org.apache.spark.rdd.RDD.iterator(RDD.scala:287) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87) at org.apache.spark.scheduler.Task.run(Task.scala:99) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745)Caused by: java.lang.RuntimeException: java.lang.RuntimeException: Request timed out for key : DictionaryKey{ columnName='ACTIVE_EMUI_VERSION', data='ACTIVE_EMUI_VERSION_20014', dictionaryValue=-1, type=DICT_GENERATION} at org.apache.carbondata.core.dictionary.client.DictionaryClientHandler.getDictionary(DictionaryClientHandler.java:99) at org.apache.carbondata.core.dictionary.client.DictionaryClient.getDictionary(DictionaryClient.java:74) at org.apache.carbondata.processing.newflow.dictionary.DictionaryServerClientDictionary.getOrGenerateKey(DictionaryServerClientDictionary.java:57) at org.apache.carbondata.processing.newflow.dictionary.DictionaryServerClientDictionary.getOrGenerateKey(DictionaryServerClientDictionary.java:32) at org.apache.carbondata.processing.newflow.converter.impl.DictionaryFieldConverterImpl.convert(DictionaryFieldConverterImpl.java:101) at org.apache.carbondata.processing.newflow.converter.impl.RowConverterImpl.convert(RowConverterImpl.java:150) at org.apache.carbondata.processing.newflow.steps.DataConverterProcessorStepImpl.processRowBatch(DataConverterProcessorStepImpl.java:103) at org.apache.carbondata.processing.newflow.steps.DataConverterProcessorStepImpl$1.next(DataConverterProcessorStepImpl.java:89) at org.apache.carbondata.processing.newflow.steps.DataConverterProcessorStepImpl$1.next(DataConverterProcessorStepImpl.java:78) at org.apache.carbondata.processing.newflow.sort.impl.ParallelReadMergeSorterImpl$SortIteratorThread.call(ParallelReadMergeSorterImpl.java:227) at org.apache.carbondata.processing.newflow.sort.impl.ParallelReadMergeSorterImpl$SortIteratorThread.call(ParallelReadMergeSorterImpl.java:201) at java.util.concurrent.FutureTask.run(FutureTask.java:266) ... 3 moreCaused by: java.lang.RuntimeException: Request timed out for key : DictionaryKey{ columnName='ACTIVE_EMUI_VERSION', data='ACTIVE_EMUI_VERSION_20014', dictionaryValue=-1, type=DICT_GENERATION} at org.apache.carbondata.core.dictionary.client.DictionaryClientHandler.getDictionary(DictionaryClientHandler.java:94) ... 14 moreINFO  16-03 13:46:51,211 - Starting task 0.3 in stage 0.0 (TID 3, 192.168.2.130, executor 0, partition 0, NODE_LOCAL, 6989 bytes)AUDIT 16-03 13:46:51,284 - &#91;knoldus-Vostro-2520&#93;&#91;hduser&#93;&#91;Thread-138&#93;Connected io.netty.channel.DefaultChannelHandlerContext@a31f326AUDIT 16-03 13:46:52,740 - &#91;knoldus-Vostro-2520&#93;&#91;hduser&#93;&#91;Thread-139&#93;Connected io.netty.channel.DefaultChannelHandlerContext@54d6fe9eERROR 16-03 13:46:53,851 - nioEventLoopGroup-8-8 java.lang.IndexOutOfBoundsException: readerIndex(64) + length(25) exceeds writerIndex(80): UnpooledUnsafeDirectByteBuf(ridx: 64, widx: 80, cap: 80) at io.netty.buffer.AbstractByteBuf.checkReadableBytes0(AbstractByteBuf.java:1161) at io.netty.buffer.AbstractByteBuf.checkReadableBytes(AbstractByteBuf.java:1155) at io.netty.buffer.AbstractByteBuf.readBytes(AbstractByteBuf.java:694) at io.netty.buffer.AbstractByteBuf.readBytes(AbstractByteBuf.java:702) at org.apache.carbondata.core.dictionary.generator.key.DictionaryMessage.readData(DictionaryMessage.java:70) at org.apache.carbondata.core.dictionary.server.DictionaryServerHandler.channelRead(DictionaryServerHandler.java:59) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:367) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:353) at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:346) at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1294) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:367) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:353) at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:911) at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131) at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:652) at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:575) at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:489) at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:451) at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:140) at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144) at java.lang.Thread.run(Thread.java:745)ERROR 16-03 13:46:53,852 - nioEventLoopGroup-8-8 exceptionCaughtjava.lang.IndexOutOfBoundsException: readerIndex(64) + length(25) exceeds writerIndex(80): UnpooledUnsafeDirectByteBuf(ridx: 64, widx: 80, cap: 80) at io.netty.buffer.AbstractByteBuf.checkReadableBytes0(AbstractByteBuf.java:1161) at io.netty.buffer.AbstractByteBuf.checkReadableBytes(AbstractByteBuf.java:1155) at io.netty.buffer.AbstractByteBuf.readBytes(AbstractByteBuf.java:694) at io.netty.buffer.AbstractByteBuf.readBytes(AbstractByteBuf.java:702) at org.apache.carbondata.core.dictionary.generator.key.DictionaryMessage.readData(DictionaryMessage.java:70) at org.apache.carbondata.core.dictionary.server.DictionaryServerHandler.channelRead(DictionaryServerHandler.java:59) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:367) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:353) at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:346) at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1294) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:367) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:353) at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:911) at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131) at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:652) at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:575) at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:489) at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:451) at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:140) at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144) at java.lang.Thread.run(Thread.java:745)WARN  16-03 13:48:38,277 - Lost task 0.3 in stage 0.0 (TID 3, 192.168.2.130, executor 0): org.apache.carbondata.processing.newflow.exception.CarbonDataLoadingException:  at org.apache.carbondata.processing.newflow.sort.impl.ParallelReadMergeSorterImpl.checkError(ParallelReadMergeSorterImpl.java:164) at org.apache.carbondata.processing.newflow.sort.impl.ParallelReadMergeSorterImpl.sort(ParallelReadMergeSorterImpl.java:117) at org.apache.carbondata.processing.newflow.steps.SortProcessorStepImpl.execute(SortProcessorStepImpl.java:76) at org.apache.carbondata.processing.newflow.steps.DataWriterProcessorStepImpl.execute(DataWriterProcessorStepImpl.java:92) at org.apache.carbondata.processing.newflow.DataLoadExecutor.execute(DataLoadExecutor.java:48) at org.apache.carbondata.spark.rdd.NewCarbonDataLoadRDD$$anon$1.<init>(NewCarbonDataLoadRDD.scala:166) at org.apache.carbondata.spark.rdd.NewCarbonDataLoadRDD.compute(NewCarbonDataLoadRDD.scala:142) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) at org.apache.spark.rdd.RDD.iterator(RDD.scala:287) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87) at org.apache.spark.scheduler.Task.run(Task.scala:99) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745)Caused by: java.lang.RuntimeException: java.lang.RuntimeException: Request timed out for key : DictionaryKey{ columnName='ACTIVE_EMUI_VERSION', data='ACTIVE_EMUI_VERSION_20014', dictionaryValue=-1, type=DICT_GENERATION} at org.apache.carbondata.core.dictionary.client.DictionaryClientHandler.getDictionary(DictionaryClientHandler.java:99) at org.apache.carbondata.core.dictionary.client.DictionaryClient.getDictionary(DictionaryClient.java:74) at org.apache.carbondata.processing.newflow.dictionary.DictionaryServerClientDictionary.getOrGenerateKey(DictionaryServerClientDictionary.java:57) at org.apache.carbondata.processing.newflow.dictionary.DictionaryServerClientDictionary.getOrGenerateKey(DictionaryServerClientDictionary.java:32) at org.apache.carbondata.processing.newflow.converter.impl.DictionaryFieldConverterImpl.convert(DictionaryFieldConverterImpl.java:101) at org.apache.carbondata.processing.newflow.converter.impl.RowConverterImpl.convert(RowConverterImpl.java:150) at org.apache.carbondata.processing.newflow.steps.DataConverterProcessorStepImpl.processRowBatch(DataConverterProcessorStepImpl.java:103) at org.apache.carbondata.processing.newflow.steps.DataConverterProcessorStepImpl$1.next(DataConverterProcessorStepImpl.java:89) at org.apache.carbondata.processing.newflow.steps.DataConverterProcessorStepImpl$1.next(DataConverterProcessorStepImpl.java:78) at org.apache.carbondata.processing.newflow.sort.impl.ParallelReadMergeSorterImpl$SortIteratorThread.call(ParallelReadMergeSorterImpl.java:227) at org.apache.carbondata.processing.newflow.sort.impl.ParallelReadMergeSorterImpl$SortIteratorThread.call(ParallelReadMergeSorterImpl.java:201) at java.util.concurrent.FutureTask.run(FutureTask.java:266) ... 3 moreCaused by: java.lang.RuntimeException: Request timed out for key : DictionaryKey{ columnName='ACTIVE_EMUI_VERSION', data='ACTIVE_EMUI_VERSION_20014', dictionaryValue=-1, type=DICT_GENERATION} at org.apache.carbondata.core.dictionary.client.DictionaryClientHandler.getDictionary(DictionaryClientHandler.java:94) ... 14 moreERROR 16-03 13:48:38,352 - Task 0 in stage 0.0 failed 4 times; aborting jobINFO  16-03 13:48:38,551 - Removed TaskSet 0.0, whose tasks have all completed, from pool INFO  16-03 13:48:38,697 - Cancelling stage 0INFO  16-03 13:48:38,701 - ResultStage 0 (collect at CarbonDataRDDFactory.scala:653) failed in 443.412 s due to Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 3, 192.168.2.130, executor 0): org.apache.carbondata.processing.newflow.exception.CarbonDataLoadingException:  at org.apache.carbondata.processing.newflow.sort.impl.ParallelReadMergeSorterImpl.checkError(ParallelReadMergeSorterImpl.java:164) at org.apache.carbondata.processing.newflow.sort.impl.ParallelReadMergeSorterImpl.sort(ParallelReadMergeSorterImpl.java:117) at org.apache.carbondata.processing.newflow.steps.SortProcessorStepImpl.execute(SortProcessorStepImpl.java:76) at org.apache.carbondata.processing.newflow.steps.DataWriterProcessorStepImpl.execute(DataWriterProcessorStepImpl.java:92) at org.apache.carbondata.processing.newflow.DataLoadExecutor.execute(DataLoadExecutor.java:48) at org.apache.carbondata.spark.rdd.NewCarbonDataLoadRDD$$anon$1.<init>(NewCarbonDataLoadRDD.scala:166) at org.apache.carbondata.spark.rdd.NewCarbonDataLoadRDD.compute(NewCarbonDataLoadRDD.scala:142) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) at org.apache.spark.rdd.RDD.iterator(RDD.scala:287) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87) at org.apache.spark.scheduler.Task.run(Task.scala:99) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745)Caused by: java.lang.RuntimeException: java.lang.RuntimeException: Request timed out for key : DictionaryKey{ columnName='ACTIVE_EMUI_VERSION', data='ACTIVE_EMUI_VERSION_20014', dictionaryValue=-1, type=DICT_GENERATION} at org.apache.carbondata.core.dictionary.client.DictionaryClientHandler.getDictionary(DictionaryClientHandler.java:99) at org.apache.carbondata.core.dictionary.client.DictionaryClient.getDictionary(DictionaryClient.java:74) at org.apache.carbondata.processing.newflow.dictionary.DictionaryServerClientDictionary.getOrGenerateKey(DictionaryServerClientDictionary.java:57) at org.apache.carbondata.processing.newflow.dictionary.DictionaryServerClientDictionary.getOrGenerateKey(DictionaryServerClientDictionary.java:32) at org.apache.carbondata.processing.newflow.converter.impl.DictionaryFieldConverterImpl.convert(DictionaryFieldConverterImpl.java:101) at org.apache.carbondata.processing.newflow.converter.impl.RowConverterImpl.convert(RowConverterImpl.java:150) at org.apache.carbondata.processing.newflow.steps.DataConverterProcessorStepImpl.processRowBatch(DataConverterProcessorStepImpl.java:103) at org.apache.carbondata.processing.newflow.steps.DataConverterProcessorStepImpl$1.next(DataConverterProcessorStepImpl.java:89) at org.apache.carbondata.processing.newflow.steps.DataConverterProcessorStepImpl$1.next(DataConverterProcessorStepImpl.java:78) at org.apache.carbondata.processing.newflow.sort.impl.ParallelReadMergeSorterImpl$SortIteratorThread.call(ParallelReadMergeSorterImpl.java:227) at org.apache.carbondata.processing.newflow.sort.impl.ParallelReadMergeSorterImpl$SortIteratorThread.call(ParallelReadMergeSorterImpl.java:201) at java.util.concurrent.FutureTask.run(FutureTask.java:266) ... 3 moreCaused by: java.lang.RuntimeException: Request timed out for key : DictionaryKey{ columnName='ACTIVE_EMUI_VERSION', data='ACTIVE_EMUI_VERSION_20014', dictionaryValue=-1, type=DICT_GENERATION} at org.apache.carbondata.core.dictionary.client.DictionaryClientHandler.getDictionary(DictionaryClientHandler.java:94) ... 14 moreDriver stacktrace:INFO  16-03 13:48:38,731 - Job 1 failed: collect at CarbonDataRDDFactory.scala:653, took 443.761474 sINFO  16-03 13:48:38,732 - pool-24-thread-5 DataLoad failure: ERROR 16-03 13:48:38,733 - pool-24-thread-5 org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 3, 192.168.2.130, executor 0): org.apache.carbondata.processing.newflow.exception.CarbonDataLoadingException:  at org.apache.carbondata.processing.newflow.sort.impl.ParallelReadMergeSorterImpl.checkError(ParallelReadMergeSorterImpl.java:164) at org.apache.carbondata.processing.newflow.sort.impl.ParallelReadMergeSorterImpl.sort(ParallelReadMergeSorterImpl.java:117) at org.apache.carbondata.processing.newflow.steps.SortProcessorStepImpl.execute(SortProcessorStepImpl.java:76) at org.apache.carbondata.processing.newflow.steps.DataWriterProcessorStepImpl.execute(DataWriterProcessorStepImpl.java:92) at org.apache.carbondata.processing.newflow.DataLoadExecutor.execute(DataLoadExecutor.java:48) at org.apache.carbondata.spark.rdd.NewCarbonDataLoadRDD$$anon$1.<init>(NewCarbonDataLoadRDD.scala:166) at org.apache.carbondata.spark.rdd.NewCarbonDataLoadRDD.compute(NewCarbonDataLoadRDD.scala:142) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) at org.apache.spark.rdd.RDD.iterator(RDD.scala:287) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87) at org.apache.spark.scheduler.Task.run(Task.scala:99) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745)Caused by: java.lang.RuntimeException: java.lang.RuntimeException: Request timed out for key : DictionaryKey{ columnName='ACTIVE_EMUI_VERSION', data='ACTIVE_EMUI_VERSION_20014', dictionaryValue=-1, type=DICT_GENERATION} at org.apache.carbondata.core.dictionary.client.DictionaryClientHandler.getDictionary(DictionaryClientHandler.java:99) at org.apache.carbondata.core.dictionary.client.DictionaryClient.getDictionary(DictionaryClient.java:74) at org.apache.carbondata.processing.newflow.dictionary.DictionaryServerClientDictionary.getOrGenerateKey(DictionaryServerClientDictionary.java:57) at org.apache.carbondata.processing.newflow.dictionary.DictionaryServerClientDictionary.getOrGenerateKey(DictionaryServerClientDictionary.java:32) at org.apache.carbondata.processing.newflow.converter.impl.DictionaryFieldConverterImpl.convert(DictionaryFieldConverterImpl.java:101) at org.apache.carbondata.processing.newflow.converter.impl.RowConverterImpl.convert(RowConverterImpl.java:150) at org.apache.carbondata.processing.newflow.steps.DataConverterProcessorStepImpl.processRowBatch(DataConverterProcessorStepImpl.java:103) at org.apache.carbondata.processing.newflow.steps.DataConverterProcessorStepImpl$1.next(DataConverterProcessorStepImpl.java:89) at org.apache.carbondata.processing.newflow.steps.DataConverterProcessorStepImpl$1.next(DataConverterProcessorStepImpl.java:78) at org.apache.carbondata.processing.newflow.sort.impl.ParallelReadMergeSorterImpl$SortIteratorThread.call(ParallelReadMergeSorterImpl.java:227) at org.apache.carbondata.processing.newflow.sort.impl.ParallelReadMergeSorterImpl$SortIteratorThread.call(ParallelReadMergeSorterImpl.java:201) at java.util.concurrent.FutureTask.run(FutureTask.java:266) ... 3 moreCaused by: java.lang.RuntimeException: Request timed out for key : DictionaryKey{ columnName='ACTIVE_EMUI_VERSION', data='ACTIVE_EMUI_VERSION_20014', dictionaryValue=-1, type=DICT_GENERATION} at org.apache.carbondata.core.dictionary.client.DictionaryClientHandler.getDictionary(DictionaryClientHandler.java:94) ... 14 moreDriver stacktrace: at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435) at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423) at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422) at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59) at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48) at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422) at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802) at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802) at scala.Option.foreach(Option.scala:257) at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802) at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650) at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605) at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594) at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48) at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628) at org.apache.spark.SparkContext.runJob(SparkContext.scala:1918) at org.apache.spark.SparkContext.runJob(SparkContext.scala:1931) at org.apache.spark.SparkContext.runJob(SparkContext.scala:1944) at org.apache.spark.SparkContext.runJob(SparkContext.scala:1958) at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:935) at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151) at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112) at org.apache.spark.rdd.RDD.withScope(RDD.scala:362) at org.apache.spark.rdd.RDD.collect(RDD.scala:934) at org.apache.carbondata.spark.rdd.CarbonDataRDDFactory$.loadDataFile$1(CarbonDataRDDFactory.scala:653) at org.apache.carbondata.spark.rdd.CarbonDataRDDFactory$.loadCarbonData(CarbonDataRDDFactory.scala:807) at org.apache.spark.sql.execution.command.LoadTable.run(carbonTableSchema.scala:531) at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58) at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56) at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74) at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114) at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114) at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:135) at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151) at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:132) at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:113) at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:87) at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:87) at org.apache.spark.sql.Dataset.<init>(Dataset.scala:185) at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:64) at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:592) at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:699) at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:220) at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1$$anon$2.run(SparkExecuteStatementOperation.scala:163) at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1$$anon$2.run(SparkExecuteStatementOperation.scala:160) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1698) at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1.run(SparkExecuteStatementOperation.scala:173) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745)Caused by: org.apache.carbondata.processing.newflow.exception.CarbonDataLoadingException:  at org.apache.carbondata.processing.newflow.sort.impl.ParallelReadMergeSorterImpl.checkError(ParallelReadMergeSorterImpl.java:164) at org.apache.carbondata.processing.newflow.sort.impl.ParallelReadMergeSorterImpl.sort(ParallelReadMergeSorterImpl.java:117) at org.apache.carbondata.processing.newflow.steps.SortProcessorStepImpl.execute(SortProcessorStepImpl.java:76) at org.apache.carbondata.processing.newflow.steps.DataWriterProcessorStepImpl.execute(DataWriterProcessorStepImpl.java:92) at org.apache.carbondata.processing.newflow.DataLoadExecutor.execute(DataLoadExecutor.java:48) at org.apache.carbondata.spark.rdd.NewCarbonDataLoadRDD$$anon$1.<init>(NewCarbonDataLoadRDD.scala:166) at org.apache.carbondata.spark.rdd.NewCarbonDataLoadRDD.compute(NewCarbonDataLoadRDD.scala:142) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) at org.apache.spark.rdd.RDD.iterator(RDD.scala:287) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87) at org.apache.spark.scheduler.Task.run(Task.scala:99) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282) ... 3 moreCaused by: java.lang.RuntimeException: java.lang.RuntimeException: Request timed out for key : DictionaryKey{ columnName='ACTIVE_EMUI_VERSION', data='ACTIVE_EMUI_VERSION_20014', dictionaryValue=-1, type=DICT_GENERATION} at org.apache.carbondata.core.dictionary.client.DictionaryClientHandler.getDictionary(DictionaryClientHandler.java:99) at org.apache.carbondata.core.dictionary.client.DictionaryClient.getDictionary(DictionaryClient.java:74) at org.apache.carbondata.processing.newflow.dictionary.DictionaryServerClientDictionary.getOrGenerateKey(DictionaryServerClientDictionary.java:57) at org.apache.carbondata.processing.newflow.dictionary.DictionaryServerClientDictionary.getOrGenerateKey(DictionaryServerClientDictionary.java:32) at org.apache.carbondata.processing.newflow.converter.impl.DictionaryFieldConverterImpl.convert(DictionaryFieldConverterImpl.java:101) at org.apache.carbondata.processing.newflow.converter.impl.RowConverterImpl.convert(RowConverterImpl.java:150) at org.apache.carbondata.processing.newflow.steps.DataConverterProcessorStepImpl.processRowBatch(DataConverterProcessorStepImpl.java:103) at org.apache.carbondata.processing.newflow.steps.DataConverterProcessorStepImpl$1.next(DataConverterProcessorStepImpl.java:89) at org.apache.carbondata.processing.newflow.steps.DataConverterProcessorStepImpl$1.next(DataConverterProcessorStepImpl.java:78) at org.apache.carbondata.processing.newflow.sort.impl.ParallelReadMergeSorterImpl$SortIteratorThread.call(ParallelReadMergeSorterImpl.java:227) at org.apache.carbondata.processing.newflow.sort.impl.ParallelReadMergeSorterImpl$SortIteratorThread.call(ParallelReadMergeSorterImpl.java:201) at java.util.concurrent.FutureTask.run(FutureTask.java:266) ... 3 moreCaused by: java.lang.RuntimeException: Request timed out for key : DictionaryKey{ columnName='ACTIVE_EMUI_VERSION', data='ACTIVE_EMUI_VERSION_20014', dictionaryValue=-1, type=DICT_GENERATION} at org.apache.carbondata.core.dictionary.client.DictionaryClientHandler.getDictionary(DictionaryClientHandler.java:94) ... 14 moreINFO  16-03 13:48:38,734 - pool-24-thread-5 *******starting clean up*********INFO  16-03 13:48:39,103 - pool-24-thread-5 *******clean up done*********AUDIT 16-03 13:48:39,104 - &#91;knoldus-Vostro-2520&#93;&#91;hduser&#93;&#91;Thread-122&#93;Data load is failed for default.uniq_include_dictionaryWARN  16-03 13:48:39,104 - pool-24-thread-5 Cannot write load metadata file as data load failedERROR 16-03 13:48:41,311 - pool-24-thread-5 java.lang.Exception: DataLoad failure:  at org.apache.carbondata.spark.rdd.CarbonDataRDDFactory$.loadCarbonData(CarbonDataRDDFactory.scala:950) at org.apache.spark.sql.execution.command.LoadTable.run(carbonTableSchema.scala:531) at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58) at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56) at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74) at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114) at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114) at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:135) at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151) at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:132) at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:113) at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:87) at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:87) at org.apache.spark.sql.Dataset.<init>(Dataset.scala:185) at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:64) at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:592) at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:699) at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:220) at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1$$anon$2.run(SparkExecuteStatementOperation.scala:163) at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1$$anon$2.run(SparkExecuteStatementOperation.scala:160) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1698) at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1.run(SparkExecuteStatementOperation.scala:173) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745)AUDIT 16-03 13:48:41,312 - &#91;knoldus-Vostro-2520&#93;&#91;hduser&#93;&#91;Thread-122&#93;Dataload failure for default.uniq_include_dictionary. Please check the logsINFO  16-03 13:48:41,471 - pool-24-thread-5 Successfully deleted the lock file /tmp/default/uniq_include_dictionary/meta.lockINFO  16-03 13:48:41,471 - pool-24-thread-5 Table MetaData Unlocked Successfully after data loadERROR 16-03 13:48:41,471 - Error executing query, currentState RUNNING, java.lang.Exception: DataLoad failure:  at org.apache.carbondata.spark.rdd.CarbonDataRDDFactory$.loadCarbonData(CarbonDataRDDFactory.scala:950) at org.apache.spark.sql.execution.command.LoadTable.run(carbonTableSchema.scala:531) at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58) at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56) at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74) at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114) at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114) at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:135) at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151) at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:132) at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:113) at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:87) at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:87) at org.apache.spark.sql.Dataset.<init>(Dataset.scala:185) at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:64) at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:592) at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:699) at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:220) at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1$$anon$2.run(SparkExecuteStatementOperation.scala:163) at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1$$anon$2.run(SparkExecuteStatementOperation.scala:160) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1698) at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1.run(SparkExecuteStatementOperation.scala:173) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745)ERROR 16-03 13:48:41,472 - Error running hive query: org.apache.hive.service.cli.HiveSQLException: java.lang.Exception: DataLoad failure:  at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:258) at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1$$anon$2.run(SparkExecuteStatementOperation.scala:163) at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1$$anon$2.run(SparkExecuteStatementOperation.scala:160) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1698) at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1.run(SparkExecuteStatementOperation.scala:173) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745)Here is the load command that I used :LOAD DATA INPATH 'hdfs://localhost:54311/BabuStore/DATA/uniqdata/7000_UniqData.csv' into table uniq_include_dictionary OPTIONS('DELIMITER'=',' , 'QUOTECHAR'='"','BAD_RECORDS_LOGGER_ENABLE'='TRUE', 'BAD_RECORDS_ACTION'='FORCE','FILEHEADER'='CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1,Double_COLUMN2,INTEGER_COLUMN1','SINGLE_Pass'='true');PFA the csv for input data
issueID:CARBONDATA-784
type:Improvement
changed files:processing/src/main/java/org/apache/carbondata/processing/loading/converter/impl/FieldEncoderFactory.java
processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactDataHandlerModel.java
processing/src/main/java/org/apache/carbondata/processing/loading/steps/DataConverterProcessorStepImpl.java
processing/src/main/java/org/apache/carbondata/processing/loading/converter/impl/DirectDictionaryFieldConverterImpl.java
processing/src/main/java/org/apache/carbondata/processing/loading/constants/DataLoadProcessorConstants.java
processing/src/main/java/org/apache/carbondata/processing/loading/converter/BadRecordLogHolder.java
processing/src/main/java/org/apache/carbondata/processing/loading/csvinput/CSVInputFormat.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactDataHandlerColumnar.java
processing/src/main/java/org/apache/carbondata/processing/loading/DataLoadProcessBuilder.java
processing/src/main/java/org/apache/carbondata/processing/loading/converter/impl/RowConverterImpl.java
processing/src/main/java/org/apache/carbondata/processing/loading/converter/impl/MeasureFieldConverterImpl.java
processing/src/main/java/org/apache/carbondata/processing/loading/model/CarbonLoadModel.java
processing/src/main/java/org/apache/carbondata/processing/loading/converter/impl/NonDictionaryFieldConverterImpl.java
texts:Make configurable empty data to be treated as bad record or not And Expose BAD_RECORDS_ACTION default value to be configurable from out side.
Make configurable empty data to be treated as bad record or not And Expose BAD_RECORDS_ACTION default value to be configurable from out side.1. Currently carbon Empty data is considered as bad record.A property in the load option should be provided so asthe user can tell weather to consider empty data as bad record or not.IS_EMPTY_DATA_BAD_RECORD = false/true*default to false *For example if IS_EMPTY_DATA_BAD_RECORD false below data will not treated as bad record vice versa"","","",,*2.Expose BAD_RECORDS_ACTION default value to be configurable from out side. *A property carbon.bad.records.action will be added in carbon properties so that user can configure default value of BAD_RECORDS_ACTION.default to FAIL
issueID:CARBONDATA-785
type:Bug
changed files:
texts:Compilation error in core module util package.
Getting error while build creation.Error:&#91;INFO&#93; -------------------------------------------------------------&#91;ERROR&#93; COMPILATION ERROR : &#91;INFO&#93; -------------------------------------------------------------&#91;ERROR&#93; /opt/jenkins/workspace/CarbonData_Functional_Suite_New/incubator-carbondata/core/src/main/java/org/apache/carbondata/core/util/DataFileFooterConverterV3.java:&#91;60,56&#93; cannot find symbol  symbol:   method getTime_stamp()  location: variable fileHeader of type org.apache.carbondata.format.FileHeader&#91;ERROR&#93; /opt/jenkins/workspace/CarbonData_Functional_Suite_New/incubator-carbondata/core/src/main/java/org/apache/carbondata/core/util/CarbonMetadataUtil.java:&#91;903,15&#93; cannot find symbol  symbol:   method setTime_stamp(long)  location: variable fileHeader of type org.apache.carbondata.format.FileHeader&#91;INFO&#93; 2 errors Expected Result: Compilation error should not exist while build creation.
issueID:CARBONDATA-786
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/util/CarbonMetadataUtil.java
texts:Data mismatch if the data data is loaded across blocklet groups
Data mismatch if the data data is loaded across blocklet groups and filter applied on second column onwards.Follow testcase CarbonProperties.getInstance()      .addProperty("carbon.blockletgroup.size.in.mb", "16")      .addProperty("carbon.enable.vector.reader", "true")      .addProperty("enable.unsafe.sort", "true")    val rdd = sqlContext.sparkContext      .parallelize(1 to 1200000, 4)      .map { x =>        ("city" + x % 8, "country" + x % 1103, "planet" + x % 10007, x.toString,          (x % 16).toShort, x / 2, (x << 1).toLong, x.toDouble / 13, x.toDouble / 11)      }.map { x =>      Row(x._1, x._2, x._3, x._4, x._5, x._6, x._7, x._8, x._9)    }    val schema = StructType(      Seq(        StructField("city", StringType, nullable = false),        StructField("country", StringType, nullable = false),        StructField("planet", StringType, nullable = false),        StructField("id", StringType, nullable = false),        StructField("m1", ShortType, nullable = false),        StructField("m2", IntegerType, nullable = false),        StructField("m3", LongType, nullable = false),        StructField("m4", DoubleType, nullable = false),        StructField("m5", DoubleType, nullable = false)      )    )    val input = sqlContext.createDataFrame(rdd, schema)    sql(s"drop table if exists testBigData")    input.write      .format("carbondata")      .option("tableName", "testBigData")      .option("tempCSV", "false")      .option("single_pass", "true")      .option("dictionary_exclude", "id") // id is high cardinality column      .mode(SaveMode.Overwrite)      .save()sql(s"select city, sum(m1) from testBigData " +          s"where country='country12' group by city order by city").show()The above code supposed return following data, but not returning it.+-----+-------+| city|sum(m1)|+-----+-------+|city0|    544||city1|    680||city2|    816||city3|    952||city4|   1088||city5|   1224||city6|   1360||city7|   1496|+-----+-------+
issueID:CARBONDATA-787
type:Bug
changed files:processing/src/main/java/org/apache/carbondata/processing/store/writer/v3/CarbonFactDataWriterImplV3.java
core/src/main/java/org/apache/carbondata/core/stats/QueryStatisticsConstants.java
core/src/main/java/org/apache/carbondata/core/scan/result/BlockletScannedResult.java
core/src/main/java/org/apache/carbondata/core/stats/QueryStatisticsRecorderImpl.java
core/src/main/java/org/apache/carbondata/core/scan/scanner/impl/BlockletFullScanner.java
core/src/main/java/org/apache/carbondata/core/util/BitSetGroup.java
core/src/main/java/org/apache/carbondata/core/scan/scanner/impl/BlockletFilterScanner.java
core/src/main/java/org/apache/carbondata/core/scan/result/iterator/AbstractDetailQueryResultIterator.java
texts:Fixed Memory leak in Offheap Query + added statistics for V3
Problem: Memory leak during off heap query + added statistics for V3 during querySolution: In data Block iterator need to free memory for occupied during queryAdded statistics for V3 format(Number of valid pages and total number of pages)
issueID:CARBONDATA-788
type:Bug
changed files:
texts:Like operator is not working properly
I tried to create a table using the following command:CREATE TABLE uniqdata_INC(CUST_ID int,CUST_NAME String,ACTIVE_EMUI_VERSION string, DOB timestamp, DOJ timestamp, BIGINT_COLUMN1 bigint,BIGINT_COLUMN2 bigint,DECIMAL_COLUMN1 decimal(30,10), DECIMAL_COLUMN2 decimal(36,10),Double_COLUMN1 double, Double_COLUMN2 double,INTEGER_COLUMN1 int) STORED BY 'org.apache.carbondata.format';Load command for the table :LOAD DATA INPATH 'hdfs://localhost:54311/BabuStore/DATA/uniqdata/2000_UniqData.csv' into table uniqdata_INC OPTIONS('DELIMITER'=',' , 'QUOTECHAR'='"','FILEHEADER'='CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1,Double_COLUMN2,INTEGER_COLUMN1');When I performed the below query on the table using 'like' operator, it displayed no results.select cust_id from uniqdata_INC where cust_id like 8999;Result:----------+ cust_id  ----------+----------+No rows selected (0.515 seconds)PFA the csv used for input data.
issueID:CARBONDATA-789
type:Bug
changed files:
texts:Join operation does not work properly  in Carbon data.
Join operation does not work properly  in Carbon data for the int data type.Steps to Reproduces:A) Create Table in Hive:First table:CREATE TABLE uniqdata_nobucket11_Hive (CUST_ID int,CUST_NAME String,ACTIVE_EMUI_VERSION string, DOB timestamp, DOJ timestamp, BIGINT_COLUMN1 bigint,BIGINT_COLUMN2 bigint,DECIMAL_COLUMN1 decimal(30,10), DECIMAL_COLUMN2 decimal(36,10),Double_COLUMN1 double, Double_COLUMN2 double,INTEGER_COLUMN1 int) ROW FORMAT DELIMITED FIELDS TERMINATED BY ",";First table Load:LOAD DATA LOCAL INPATH '/home/vinod/Desktop/AllCSV/2000_UniqData.csv'OVERWRITE INTO TABLE uniqdata_nobucket11_Hive;Second  table :CREATE TABLE uniqdata_nobucket22_Hive (CUST_ID int,CUST_NAME String,ACTIVE_EMUI_VERSION string, DOB timestamp, DOJ timestamp, BIGINT_COLUMN1 bigint,BIGINT_COLUMN2 bigint,DECIMAL_COLUMN1 decimal(30,10), DECIMAL_COLUMN2 decimal(36,10),Double_COLUMN1 double, Double_COLUMN2 double,INTEGER_COLUMN1 int) ROW FORMAT DELIMITED FIELDS TERMINATED BY ",";Second  table Load:LOAD DATA LOCAL INPATH '/home/vinod/Desktop/AllCSV/2000_UniqData.csv'OVERWRITE INTO TABLE uniqdata_nobucket22_Hive;Results in Hive: CUST_ID      CUST_NAME         ACTIVE_EMUI_VERSION               DOB                     DOJ            BIGINT_COLUMN1   BIGINT_COLUMN2       DECIMAL_COLUMN1          DECIMAL_COLUMN2         Double_COLUMN1        Double_COLUMN2      INTEGER_COLUMN1   CUST_ID      CUST_NAME         ACTIVE_EMUI_VERSION               DOB                     DOJ            BIGINT_COLUMN1   BIGINT_COLUMN2       DECIMAL_COLUMN1          DECIMAL_COLUMN2         Double_COLUMN1        Double_COLUMN2      INTEGER_COLUMN1  ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ 10999     CUST_NAME_01999   ACTIVE_EMUI_VERSION_01999   1975-06-23 01:00:03.0   1975-06-23 02:00:03.0   123372038853     -223372034855    12345680900.1234000000   22345680900.1234000000   1.12345674897976E10   -1.12345674897976E10   2000              10999     CUST_NAME_01999   ACTIVE_EMUI_VERSION_01999   1975-06-23 01:00:03.0   1975-06-23 02:00:03.0   123372038853     -223372034855    12345680900.1234000000   22345680900.1234000000   1.12345674897976E10   -1.12345674897976E10   2000             ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------2,001 rows selected (3.369 seconds)B) Create table in Carbon data First Table:CREATE TABLE uniqdata_nobucket11 (CUST_ID int,CUST_NAME String,ACTIVE_EMUI_VERSION string, DOB timestamp, DOJ timestamp, BIGINT_COLUMN1 bigint,BIGINT_COLUMN2 bigint,DECIMAL_COLUMN1 decimal(30,10), DECIMAL_COLUMN2 decimal(36,10),Double_COLUMN1 double, Double_COLUMN2 double,INTEGER_COLUMN1 int) STORED BY 'org.apache.carbondata.format' ;Load Data in table:LOAD DATA INPATH 'hdfs://localhost:54310/2000_UniqData.csv' into table uniqdata_nobucket11 OPTIONS('DELIMITER'=',' , 'QUOTECHAR'='"','FILEHEADER'='CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1,Double_COLUMN2,INTEGER_COLUMN1')Create Second table:CREATE TABLE uniqdata_nobucket22 (CUST_ID int,CUST_NAME String,ACTIVE_EMUI_VERSION string, DOB timestamp, DOJ timestamp, BIGINT_COLUMN1 bigint,BIGINT_COLUMN2 bigint,DECIMAL_COLUMN1 decimal(30,10), DECIMAL_COLUMN2 decimal(36,10),Double_COLUMN1 double, Double_COLUMN2 double,INTEGER_COLUMN1 int) STORED BY 'org.apache.carbondata.format';Load data in Table:LOAD DATA INPATH 'hdfs://localhost:54310/2000_UniqData.csv' into table uniqdata_nobucket22 OPTIONS('DELIMITER'=',' , 'QUOTECHAR'='"','FILEHEADER'='CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1,Double_COLUMN2,INTEGER_COLUMN1')Result in CarbonData:0: jdbc:hive2://localhost:10000> select * from uniqdata_nobucket11 u1, uniqdata_nobucket22 u2 where u1.cust_id = u2.cust_id;Error: java.util.EmptyStackException (state=,code=0)Expected Result: Results should be displayed in carbon data.
issueID:CARBONDATA-79
type:Bug
changed files:
texts:Data load fails when complex type column with timestamp primitives
When complex type column with timestamp direct dictionary is in the middle of other complex types, data load is failing.eg., CREATE TABLE testtimestampcarbon(imei string,rat array<string>, sid array<int>, end_time array<Timestamp>, probeid array<double>, contact struct<name:string, id:string>)STORED BY 'org.apache.carbondata.format'LOAD DATA local inpath './src/test/resources/timestampdata.csv' INTO TABLE testtimestampcarbon options('DELIMITER'=',', 'QUOTECHAR'='\"','COMPLEX_DELIMITER_LEVEL_1'='$', 'FILEHEADER'='imei,rat,sid,end_time,probeid,contact')Load fails with ArrayIndexOutOfBoundException.sample dataimei001,rat$aaa1,111$111,2015-01-01 13:00:00.000$2015-01-01 13:00:00.000,16$64,babu$00
issueID:CARBONDATA-790
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/stats/QueryStatisticsConstants.java
core/src/main/java/org/apache/carbondata/core/scan/result/iterator/DetailQueryResultIterator.java
core/src/main/java/org/apache/carbondata/core/stats/QueryStatisticsRecorderImpl.java
core/src/main/java/org/apache/carbondata/core/scan/scanner/impl/BlockletFullScanner.java
core/src/main/java/org/apache/carbondata/core/scan/scanner/impl/BlockletFilterScanner.java
core/src/main/java/org/apache/carbondata/core/scan/result/iterator/AbstractDetailQueryResultIterator.java
texts:Added statistics for exclusive carbon read(I/O) and scan time
Added statistics to capture exclusive carbon scan and read time during query, excluding sparks records consumption time.
issueID:CARBONDATA-791
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/scan/result/iterator/AbstractDetailQueryResultIterator.java
texts:Exists queries of TPC-DS are failing in carbon
Exists queries are failing in carbon.These are required in TPC-DS test.Testcase to reproduce.val df = sqlContext.sparkContext.parallelize(1 to 1000).map(x => (x+"", (x+100)+"")).toDF("c1", "c2")    df.write      .format("carbondata")      .mode(SaveMode.Overwrite)      .option("tableName", "carbon")      .save()sql("select * from carbon where c1='200' and exists(select * from carbon)")It fails in carbon.
issueID:CARBONDATA-792
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RangeValueFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/scan/expression/ColumnExpression.java
core/src/main/java/org/apache/carbondata/core/scan/expression/RangeExpressionEvaluator.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/resolverinfo/TrueConditionalResolverImpl.java
core/src/main/java/org/apache/carbondata/core/scan/expression/conditional/ListExpression.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/RowLevelFilterResolverImpl.java
core/src/main/java/org/apache/carbondata/core/scan/expression/logical/TrueExpression.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/TrueFilterExecutor.java
core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
core/src/main/java/org/apache/carbondata/core/scan/filter/intf/ExpressionType.java
core/src/main/java/org/apache/carbondata/core/scan/expression/BinaryExpression.java
core/src/main/java/org/apache/carbondata/core/scan/filter/optimizer/RangeFilterOptmizer.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/resolverinfo/visitor/RangeDictionaryColumnVisitor.java
core/src/main/java/org/apache/carbondata/core/scan/expression/LiteralExpression.java
core/src/main/java/org/apache/carbondata/core/scan/filter/intf/FilterExecuterType.java
core/src/main/java/org/apache/carbondata/core/scan/expression/logical/RangeExpression.java
core/src/main/java/org/apache/carbondata/core/scan/expression/Expression.java
hadoop/src/main/java/org/apache/carbondata/hadoop/util/CarbonInputFormatUtil.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/resolverinfo/visitor/CustomTypeDictionaryVisitor.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/resolverinfo/visitor/FilterInfoTypeVisitorFactory.java
core/src/main/java/org/apache/carbondata/core/scan/expression/FilterModificationNode.java
core/src/main/java/org/apache/carbondata/core/scan/filter/FilterExpressionProcessor.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/resolverinfo/visitor/RangeNoDictionaryTypeVisitor.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/resolverinfo/visitor/RangeDirectDictionaryVisitor.java
core/src/main/java/org/apache/carbondata/core/scan/filter/FilterUtil.java
core/src/main/java/org/apache/carbondata/core/scan/filter/intf/FilterOptimizer.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/ConditionalFilterResolverImpl.java
core/src/main/java/org/apache/carbondata/core/scan/filter/resolver/RowLevelRangeFilterResolverImpl.java
texts:Range Filter Optimization
Combine separate less than and greater than filter expression into a single expression in order to gain performance. As within a single expression both Min and Max values will be there, evaluation of rows will be come much easier.
issueID:CARBONDATA-793
type:Bug
changed files:
texts:Count with null values is giving wrong result.
if the data has null values then it should not count the data. But it is counting now.
issueID:CARBONDATA-794
type:Bug
changed files:processing/src/main/java/org/apache/carbondata/processing/loading/converter/impl/FieldEncoderFactory.java
processing/src/main/java/org/apache/carbondata/processing/loading/converter/impl/RowConverterImpl.java
texts:Numeric dimension column value should be validated for the bad record

issueID:CARBONDATA-795
type:Bug
changed files:
texts:Table Rename command is changing the db name of provided table to current db
When user trigger rename table command not on current database, table is getting renamed from provided db to current db.eg., alter table testdb.test1 rename to testdb.test2Assume current db is default,Above query is renaming the table from testdb.test1 to default.test2This needs to be corrected.
issueID:CARBONDATA-796
type:Bug
changed files:
texts:Drop database command is deleting all the carbon files from database folder even if the user does not provide cascade
When user trigger drop database command, Carbon is deleted all the files from <carbon.store>/database/ folder without check whether any file existeg., create database db1;create table db1.test1(id int, name string) stored as 'carbondata';drop database db1; --> This command clears all the files in <carbon.store>/database/ folder.
issueID:CARBONDATA-797
type:Bug
changed files:
texts:Data loss for BigInt datatype if data contains long max and min values
When data contains long max and min values for a measure column with bigInt datatype, the delta compression selected is DATA_BYTE which is incorrect. For selecting the delta compression min value is decremented from max value and here the min value is negative, so it performs addition operation and goes out of long range. When a long value goes out of range it starts again from the long max negative value which results in wrong compression selection.This leads to data loss and incorrect query results.
issueID:CARBONDATA-798
type:Bug
changed files:
texts:Update Bad Records folder name during table rename
Currently the bad records location is not being updated during table rename.
issueID:CARBONDATA-799
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/scan/result/impl/NonFilterQueryScannedResult.java
core/src/main/java/org/apache/carbondata/core/scan/result/BlockletScannedResult.java
core/src/main/java/org/apache/carbondata/core/scan/result/impl/FilterQueryScannedResult.java
core/src/main/java/org/apache/carbondata/core/scan/collector/impl/DictionaryBasedResultCollector.java
core/src/main/java/org/apache/carbondata/core/scan/collector/impl/AbstractScannedResultCollector.java
core/src/main/java/org/apache/carbondata/core/scan/collector/impl/RestructureBasedRawResultCollector.java
core/src/main/java/org/apache/carbondata/core/scan/collector/impl/RawBasedResultCollector.java
core/src/main/java/org/apache/carbondata/core/scan/collector/impl/RestructureBasedDictionaryResultCollector.java
texts:change word from currenr to current
change word from currenr to current
issueID:CARBONDATA-8
type:Test
changed files:
texts:Use create table instead of cube in all test cases
1. Use create table instead of cube in all test cases2. Remove unnecessary & duplicate  test cases
issueID:CARBONDATA-80
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/cache/dictionary/ColumnDictionaryInfo.java
core/src/main/java/org/apache/carbondata/core/cache/dictionary/AbstractColumnDictionaryInfo.java
core/src/main/java/org/apache/carbondata/core/cache/dictionary/DictionaryInfo.java
core/src/main/java/org/apache/carbondata/core/reader/CarbonDictionaryReader.java
core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
texts:Dictionary values should be equally distributed in buckets while loading in memory
Whenever a query is executed, dictionary for columns queried is loaded in memory. For incremental loads dictionary values are loaded incrementally and thus one list contains several sub lists with dictionary values.The dictionary values on incremental load may not be equally distributed in the sub buckets and this might increase the search time of a value if there are too many incremental loads.Therefore the dictionary values should be divided equally in the sub buckets.
issueID:CARBONDATA-801
type:Bug
changed files:
texts:[Documentation] Examples format to be fixed
Some examples provided in DDL are enclosed in “” which might not work in some scenarios. Need to replace the “” in the examples to ‘’.
issueID:CARBONDATA-802
type:Bug
changed files:
texts:Select query is throwing exception if new dictionary column is added without any default value
Select query is throwing exception if new dictionary column is added without any default valueeg., create table test(int id, name string) stored as 'carbondata'alter table test add columns(country string) tblproperties('default.value.country'='india') -->select query is passingalter table test add columns(state string) -->select query is failing
issueID:CARBONDATA-803
type:Bug
changed files:
texts:Incorrect results returned by not equal to filter on dictionary column with numeric data type
Whenever a not equal to filter is applied on dictionary column with numeric datatype, the cast added by spark plan is removed while creating carbon filters from spark filter. Due to this plan modification incorrect results are returned by spark.Steps to reproduce the issue:1. CREATE TABLE IF NOT EXISTS carbon(ID Int, date Timestamp, country String, name String, phonetype String, serialname String, salary Int) STORED BY 'org.apache.carbondata.format' TBLPROPERTIES('dictionary_include'='id')2. LOAD DATA LOCAL INPATH '$csvFilePath' into table carbon3. select Id from test_not_equal_to_carbon where id != '7'
issueID:CARBONDATA-804
type:Bug
changed files:
texts:Update file structure info as per V3 format definition
Update file structure info as per V3 format definition, the master has merged new V3 format for further improving performance.
issueID:CARBONDATA-805
type:Sub-task
changed files:integration/presto/src/main/java/org/apache/carbondata/presto/CarbondataModule.java
integration/presto/src/main/java/org/apache/carbondata/presto/CarbondataConnectorFactory.java
integration/presto/src/main/java/org/apache/carbondata/presto/Types.java
integration/presto/src/main/java/org/apache/carbondata/presto/impl/CarbonTableConfig.java
integration/presto/src/main/java/org/apache/carbondata/presto/CarbondataColumnConstraint.java
integration/presto/src/main/java/org/apache/carbondata/presto/impl/CarbonTableCacheModel.java
integration/presto/src/main/java/org/apache/carbondata/presto/CarbondataPlugin.java
integration/presto/src/main/java/org/apache/carbondata/presto/impl/CarbonLocalInputSplit.java
integration/presto/src/main/java/org/apache/carbondata/presto/impl/CarbonTableReader.java
integration/presto/src/main/java/org/apache/carbondata/presto/CarbondataSplitManager.java
texts:Fix groupid,package name,Class name issues
Fix groupid,package name,Class name issues
issueID:CARBONDATA-807
type:Sub-task
changed files:
texts:Add the basic presto integration code

issueID:CARBONDATA-809
type:Bug
changed files:
texts:Union with alias is returning wrong result.
Union with alias is returning wrong result.Testcase SELECT t.c1 a FROM (select c1 from  carbon_table1 union all  select c1 from  carbon_table2) tThe above query returns the data from only one table and also duplicated.
issueID:CARBONDATA-81
type:Wish
changed files:
texts:please support that carbon-spark-sql can load property-file
https://github.com/apache/incubator-carbondata/pull/33#issuecomment-233589995
issueID:CARBONDATA-810
type:Bug
changed files:
texts:Index builder exception display to user using select query.
Error message display as Job aborted due to stage failure.Steps to reproduce:1: Create the table in Carbon:create table IF NOT EXISTS traffic_2g_3g_4g (SOURCE_INFO String ,APP_CATEGORY_ID String ,APP_CATEGORY_NAME String ,APP_SUB_CATEGORY_ID String ,APP_SUB_CATEGORY_NAME String ,RAT_NAME String ,IMSI String ,OFFER_MSISDN String ,OFFER_ID String ,OFFER_OPTION_1 String ,OFFER_OPTION_2 String ,OFFER_OPTION_3 String ,MSISDN String ,PACKAGE_TYPE String ,PACKAGE_PRICE String ,TAG_IMSI String ,TAG_MSISDN String ,PROVINCE String ,CITY String ,AREA_CODE String ,TAC String ,IMEI String ,TERMINAL_TYPE String ,TERMINAL_BRAND String ,TERMINAL_MODEL String ,PRICE_LEVEL String ,NETWORK String ,SHIPPED_OS String ,WIFI String ,WIFI_HOTSPOT String ,GSM String ,WCDMA String ,TD_SCDMA String ,LTE_FDD String ,LTE_TDD String ,CDMA String ,SCREEN_SIZE String ,SCREEN_RESOLUTION String ,HOST_NAME String ,WEBSITE_NAME String ,OPERATOR String ,SRV_TYPE_NAME String ,TAG_HOST String ,CGI String ,CELL_NAME String ,COVERITY_TYPE1 String ,COVERITY_TYPE2 String ,COVERITY_TYPE3 String ,COVERITY_TYPE4 String ,COVERITY_TYPE5 String ,LATITUDE String ,LONGITUDE String ,AZIMUTH String ,TAG_CGI String ,APN String ,USER_AGENT String ,DAY String ,HOUR String ,MIN String ,IS_DEFAULT_BEAR int ,EPS_BEARER_ID String ,QCI int ,USER_FILTER String ,ANALYSIS_PERIOD String, UP_THROUGHPUT int,DOWN_THROUGHPUT int,UP_PKT_NUM int,DOWN_PKT_NUM int,APP_REQUEST_NUM int,PKT_NUM_LEN_1_64 int,PKT_NUM_LEN_64_128 int,PKT_NUM_LEN_128_256 int,PKT_NUM_LEN_256_512 int,PKT_NUM_LEN_512_768 int,PKT_NUM_LEN_768_1024 int,PKT_NUM_LEN_1024_ALL int,IP_FLOW_MARK int) STORED BY 'org.apache.carbondata.format'2: Load the DataLOAD DATA INPATH 'HDFS_URL/FACT_UNITED_DATA_INFO_sample_cube.csv' INTO table traffic_2g_3g_4g OPTIONS ('DELIMITER'=',' , 'QUOTECHAR'='"','BAD_RECORDS_ACTION'='FORCE','FILEHEADER'='')3: Perform the Select Queryselect MSISDN,sum(UP_THROUGHPUT)+sum(DOWN_THROUGHPUT) as total from  traffic_2g_3g_4g  where TERMINAL_BRAND='HTC' and APP_CATEGORY_NAME='Web_Browsing' group by MSISDNError on ThriftServerJob aborted due to stage failure: Task 1 in stage 12259.0 failed 4 times, most recent failure: Lost task 1.3 in stage 12259.0 (TID 642677, hadoop-slave-3, executor 9): org.apache.carbondata.core.datastore.exception.IndexBuilderException: Block B-tree loading failed at org.apache.carbondata.core.datastore.BlockIndexStore.fillLoadedBlocks(BlockIndexStore.java:264) at org.apache.carbondata.core.datastore.BlockIndexStore.getAll(BlockIndexStore.java:189) at org.apache.carbondata.core.scan.executor.impl.AbstractQueryExecutor.initQuery(AbstractQueryExecutor.java:131) at org.apache.carbondata.core.scan.executor.impl.AbstractQueryExecutor.getBlockExecutionInfos(AbstractQueryExecutor.java:186) at org.apache.carbondata.core.scan.executor.impl.DetailQueryExecutor.execute(DetailQueryExecutor.java:39) at org.apache.carbondata.hadoop.CarbonRecordReader.initialize(CarbonRecordReader.java:79) at org.apache.carbondata.spark.rdd.CarbonScanRDD.compute(CarbonScanRDD.scala:204) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) at org.apache.spark.rdd.RDD.iterator(RDD.scala:287) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) at org.apache.spark.rdd.RDD.iterator(RDD.scala:287) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) at org.apache.spark.rdd.RDD.iterator(RDD.scala:287) at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96) at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53) at org.apache.spark.scheduler.Task.run(Task.scala:99) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745)Caused by: java.util.concurrent.ExecutionException: java.io.IOException: org.apache.thrift.protocol.TProtocolException: don't know what type: 15 at java.util.concurrent.FutureTask.report(FutureTask.java:122) at java.util.concurrent.FutureTask.get(FutureTask.java:192) at org.apache.carbondata.core.datastore.BlockIndexStore.fillLoadedBlocks(BlockIndexStore.java:254) ... 21 moreCaused by: java.io.IOException: org.apache.thrift.protocol.TProtocolException: don't know what type: 15 at org.apache.carbondata.core.reader.ThriftReader.read(ThriftReader.java:108) at org.apache.carbondata.core.reader.CarbonHeaderReader.readHeader(CarbonHeaderReader.java:47) at org.apache.carbondata.core.util.DataFileFooterConverterV3.readDataFileFooter(DataFileFooterConverterV3.java:53) at org.apache.carbondata.core.util.CarbonUtil.readMetadatFile(CarbonUtil.java:965) at org.apache.carbondata.core.datastore.AbstractBlockIndexStoreCache.checkAndLoadTableBlocks(AbstractBlockIndexStoreCache.java:98) at org.apache.carbondata.core.datastore.BlockIndexStore.loadBlock(BlockIndexStore.java:304) at org.apache.carbondata.core.datastore.BlockIndexStore.get(BlockIndexStore.java:109) at org.apache.carbondata.core.datastore.BlockIndexStore$BlockLoaderThread.call(BlockIndexStore.java:294) at org.apache.carbondata.core.datastore.BlockIndexStore$BlockLoaderThread.call(BlockIndexStore.java:284) at java.util.concurrent.FutureTask.run(FutureTask.java:266) ... 3 moreCaused by: org.apache.thrift.protocol.TProtocolException: don't know what type: 15 at org.apache.thrift.protocol.TCompactProtocol.getTType(TCompactProtocol.java:896) at org.apache.thrift.protocol.TCompactProtocol.readFieldBegin(TCompactProtocol.java:558) at org.apache.carbondata.format.FileHeader$FileHeaderStandardScheme.read(FileHeader.java:590) at org.apache.carbondata.format.FileHeader$FileHeaderStandardScheme.read(FileHeader.java:583) at org.apache.carbondata.format.FileHeader.read(FileHeader.java:511) at org.apache.carbondata.core.reader.ThriftReader.read(ThriftReader.java:106) ... 12 moreDriver stacktrace:
issueID:CARBONDATA-811
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/scan/collector/impl/RestructureBasedDictionaryResultCollector.java
core/src/main/java/org/apache/carbondata/core/scan/collector/impl/DictionaryBasedResultCollector.java
texts:Refactor dictionary based result collector class
Problem: For each batch result collector class is filling all the class level variable this may hit the performanceSolution: fill it in constructor, so only once it will be initialize.
issueID:CARBONDATA-812
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
texts:make vectorized reader as default reader

issueID:CARBONDATA-813
type:Sub-task
changed files:integration/presto/src/main/java/org/apache/carbondata/presto/CarbondataColumnConstraint.java
integration/presto/src/main/java/org/apache/carbondata/presto/CarbondataSplitManager.java
texts:Fix pom issues and add the correct dependency jar to build success for integration/presto
Fix all issues to build success for integration/presto
issueID:CARBONDATA-814
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/scan/expression/RangeExpressionEvaluator.java
processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactDataHandlerColumnar.java
processing/src/main/java/org/apache/carbondata/processing/loading/steps/DataConverterProcessorStepImpl.java
texts:bad record log file writing is not correct

issueID:CARBONDATA-815
type:Sub-task
changed files:
texts:Add basic hive integration code

issueID:CARBONDATA-816
type:Sub-task
changed files:
texts:Add examples for hive integration under /Examples

issueID:CARBONDATA-818
type:Bug
changed files:processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactDataHandlerColumnar.java
texts:The file_name stored in carbonindex is wrong
The file_name stored in carbonindex is a local path which used on executor as temp dir /tmp/6937581525189542/0/default/carbon_v3/Fact/Part0/Segment_0/0/part-0-0_batchno0-0-1490345609845.carbondataBut I think we want to store the actual carbondata path likepart-0-0_batchno0-0-1490345609845.carbondata
issueID:CARBONDATA-82
type:Test
changed files:
texts:NullPointerException by ColumnSchemaDetailsWrapper.<init>(ColumnSchemaDetailsWrapper.java:75)
csv file：ss_sold_date_sk,ss_sold_time_sk,ss_item_sk,ss_customer_sk,ss_cdemo_sk,ss_hdemo_sk,ss_addr_sk,ss_store_sk,ss_promo_sk,ss_ticket_number,ss_quantity,ss_wholesale_cost,ss_list_price,ss_sales_price,ss_ext_discount_amt,ss_ext_sales_price,ss_ext_wholesale_cost,ss_ext_list_price,ss_ext_tax,ss_coupon_amt,ss_net_paid,ss_net_paid_inc_tax,ss_net_profit2451813,65495,7649,79006,591617,3428,44839,10,5,1,79,11.41,18.71,2.80,99.54,221.20,901.39,1478.09,6.08,99.54,121.66,127.74,-779.73DDL：create table if not exists  store_sales(    ss_sold_date_sk           int,    ss_sold_time_sk           int,    ss_item_sk                int,    ss_customer_sk            int,    ss_cdemo_sk               int,    ss_hdemo_sk               int,    ss_addr_sk                int,    ss_store_sk               int,    ss_promo_sk               int,    ss_ticket_number          int,    ss_quantity               int,    ss_wholesale_cost         double,    ss_list_price             double,    ss_sales_price            double,    ss_ext_discount_amt       double,    ss_ext_sales_price        double,    ss_ext_wholesale_cost     double,    ss_ext_list_price         double,    ss_ext_tax                double,    ss_coupon_amt             double,    ss_net_paid               double,    ss_net_paid_inc_tax       double,    ss_net_profit             double)STORED BY 'org.apache.carbondata.format';Log：> LOAD DATA  inpath 'hdfs://holodesk01/user/carbon-spark-sql/tpcds/2/store_sales' INTO table store_sales;INFO  20-07 13:43:39,249 - main Query &#91;LOAD DATA  INPATH &#39;HDFS://HOLODESK01/USER/CARBON-SPARK-SQL/TPCDS/2/STORE_SALES&#39; INTO TABLE STORE_SALES&#93;INFO  20-07 13:43:39,307 - Successfully able to get the table metadata file lockINFO  20-07 13:43:39,324 - main Initiating Direct Load for the Table : (tpcds_carbon_2.store_sales)INFO  20-07 13:43:39,331 - &#91;Block Distribution&#93;INFO  20-07 13:43:39,332 - totalInputSpaceConsumed : 778266079 , defaultParallelism : 24INFO  20-07 13:43:39,332 - mapreduce.input.fileinputformat.split.maxsize : 32427753INFO  20-07 13:43:39,392 - Block broadcast_8 stored as values in memory (estimated size 264.0 KB, free 573.6 KB)INFO  20-07 13:43:39,465 - Block broadcast_8_piece0 stored as bytes in memory (estimated size 23.9 KB, free 597.4 KB)INFO  20-07 13:43:39,467 - Added broadcast_8_piece0 in memory on localhost:50762 (size: 23.9 KB, free: 511.4 MB)INFO  20-07 13:43:39,468 - Created broadcast 8 from NewHadoopRDD at CarbonTextFile.scala:45INFO  20-07 13:43:39,478 - Total input paths to process : 1INFO  20-07 13:43:39,493 - Starting job: take at CarbonCsvRelation.scala:175INFO  20-07 13:43:39,494 - Got job 5 (take at CarbonCsvRelation.scala:175) with 1 output partitionsINFO  20-07 13:43:39,494 - Final stage: ResultStage 6 (take at CarbonCsvRelation.scala:175)INFO  20-07 13:43:39,494 - Parents of final stage: List()INFO  20-07 13:43:39,495 - Missing parents: List()INFO  20-07 13:43:39,496 - Submitting ResultStage 6 (MapPartitionsRDD&#91;23&#93; at map at CarbonTextFile.scala:55), which has no missing parentsINFO  20-07 13:43:39,499 - Block broadcast_9 stored as values in memory (estimated size 2.6 KB, free 600.0 KB)INFO  20-07 13:43:39,511 - Block broadcast_9_piece0 stored as bytes in memory (estimated size 1600.0 B, free 601.5 KB)INFO  20-07 13:43:39,512 - Added broadcast_9_piece0 in memory on localhost:50762 (size: 1600.0 B, free: 511.4 MB)INFO  20-07 13:43:39,513 - Created broadcast 9 from broadcast at DAGScheduler.scala:1006INFO  20-07 13:43:39,514 - Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD&#91;23&#93; at map at CarbonTextFile.scala:55)INFO  20-07 13:43:39,514 - Adding task set 6.0 with 1 tasksINFO  20-07 13:43:39,517 - Starting task 0.0 in stage 6.0 (TID 9, localhost, partition 0,ANY, 2302 bytes)INFO  20-07 13:43:39,518 - Running task 0.0 in stage 6.0 (TID 9)INFO  20-07 13:43:39,523 - Input split: hdfs://holodesk01/user/carbon-spark-sql/tpcds/2/store_sales/data-m-00001.csv:0+32427753INFO  20-07 13:43:39,545 - Finished task 0.0 in stage 6.0 (TID 9). 3580 bytes result sent to driverINFO  20-07 13:43:39,558 - Finished task 0.0 in stage 6.0 (TID 9) in 42 ms on localhost (1/1)INFO  20-07 13:43:39,558 - ResultStage 6 (take at CarbonCsvRelation.scala:175) finished in 0.042 sINFO  20-07 13:43:39,558 - Removed TaskSet 6.0, whose tasks have all completed, from pool INFO  20-07 13:43:39,558 - Job 5 finished: take at CarbonCsvRelation.scala:175, took 0.065209 sINFO  20-07 13:43:39,558 - Finished stage: org.apache.spark.scheduler.StageInfo@6c7379d3INFO  20-07 13:43:39,561 - task runtime:(count: 1, mean: 42.000000, stdev: 0.000000, max: 42.000000, min: 42.000000)INFO  20-07 13:43:39,561 -  0% 5% 10% 25% 50% 75% 90% 95% 100%INFO  20-07 13:43:39,561 -  42.0 ms 42.0 ms 42.0 ms 42.0 ms 42.0 ms 42.0 ms 42.0 ms 42.0 ms 42.0 msINFO  20-07 13:43:39,563 - task result size:(count: 1, mean: 3580.000000, stdev: 0.000000, max: 3580.000000, min: 3580.000000)INFO  20-07 13:43:39,563 -  0% 5% 10% 25% 50% 75% 90% 95% 100%INFO  20-07 13:43:39,563 -  3.5 KB 3.5 KB 3.5 KB 3.5 KB 3.5 KB 3.5 KB 3.5 KB 3.5 KB 3.5 KBINFO  20-07 13:43:39,564 - have no column need to generate global dictionaryAUDIT 20-07 13:43:39,564 - &#91;holodesk01&#93;&#91;hdfs&#93;&#91;Thread-1&#93;Data load request has been received for table tpcds_carbon_2.store_salesINFO  20-07 13:43:39,565 - executor (non-fetch) time pct: (count: 1, mean: 26.190476, stdev: 0.000000, max: 26.190476, min: 26.190476)INFO  20-07 13:43:39,565 -  0% 5% 10% 25% 50% 75% 90% 95% 100%INFO  20-07 13:43:39,565 -  26 % 26 % 26 % 26 % 26 % 26 % 26 % 26 % 26 %INFO  20-07 13:43:39,567 - other time pct: (count: 1, mean: 73.809524, stdev: 0.000000, max: 73.809524, min: 73.809524)INFO  20-07 13:43:39,567 -  0% 5% 10% 25% 50% 75% 90% 95% 100%INFO  20-07 13:43:39,568 -  74 % 74 % 74 % 74 % 74 % 74 % 74 % 74 % 74 %INFO  20-07 13:43:39,582 - main compaction need status is falseINFO  20-07 13:43:39,583 - &#91;Block Distribution&#93;INFO  20-07 13:43:39,584 - totalInputSpaceConsumed : 778266079 , defaultParallelism : 24INFO  20-07 13:43:39,584 - mapreduce.input.fileinputformat.split.maxsize : 32427753INFO  20-07 13:43:39,586 - Total input paths to process : 1INFO  20-07 13:43:39,599 - Total no of blocks : 24, No.of Nodes : 4INFO  20-07 13:43:39,599 - #Node: holodesk02 no.of.blocks: 6#Node: holodesk01 no.of.blocks: 6#Node: holodesk04 no.of.blocks: 6#Node: holodesk03 no.of.blocks: 6INFO  20-07 13:43:40,605 - Starting job: collect at CarbonDataRDDFactory.scala:717INFO  20-07 13:43:40,606 - Got job 6 (collect at CarbonDataRDDFactory.scala:717) with 4 output partitionsINFO  20-07 13:43:40,606 - Final stage: ResultStage 7 (collect at CarbonDataRDDFactory.scala:717)INFO  20-07 13:43:40,607 - Parents of final stage: List()INFO  20-07 13:43:40,607 - Missing parents: List()INFO  20-07 13:43:40,607 - Submitting ResultStage 7 (CarbonDataLoadRDD&#91;24&#93; at RDD at CarbonDataLoadRDD.scala:94), which has no missing parentsINFO  20-07 13:43:40,608 - Prefered Location for split : holodesk02INFO  20-07 13:43:40,608 - Prefered Location for split : holodesk01INFO  20-07 13:43:40,608 - Prefered Location for split : holodesk04INFO  20-07 13:43:40,608 - Prefered Location for split : holodesk03INFO  20-07 13:43:40,613 - Block broadcast_10 stored as values in memory (estimated size 15.8 KB, free 617.3 KB)INFO  20-07 13:43:40,625 - Block broadcast_10_piece0 stored as bytes in memory (estimated size 5.9 KB, free 623.2 KB)INFO  20-07 13:43:40,627 - Added broadcast_10_piece0 in memory on localhost:50762 (size: 5.9 KB, free: 511.4 MB)INFO  20-07 13:43:40,627 - Created broadcast 10 from broadcast at DAGScheduler.scala:1006INFO  20-07 13:43:40,628 - Submitting 4 missing tasks from ResultStage 7 (CarbonDataLoadRDD&#91;24&#93; at RDD at CarbonDataLoadRDD.scala:94)INFO  20-07 13:43:40,628 - Adding task set 7.0 with 4 tasksINFO  20-07 13:43:40,631 - Starting task 0.0 in stage 7.0 (TID 10, localhost, partition 0,ANY, 2892 bytes)INFO  20-07 13:43:40,632 - Starting task 1.0 in stage 7.0 (TID 11, localhost, partition 1,ANY, 2892 bytes)INFO  20-07 13:43:40,633 - Starting task 2.0 in stage 7.0 (TID 12, localhost, partition 2,ANY, 2892 bytes)INFO  20-07 13:43:40,634 - Starting task 3.0 in stage 7.0 (TID 13, localhost, partition 3,ANY, 2892 bytes)INFO  20-07 13:43:40,634 - Running task 0.0 in stage 7.0 (TID 10)INFO  20-07 13:43:40,635 - Running task 1.0 in stage 7.0 (TID 11)INFO  20-07 13:43:40,635 - Running task 2.0 in stage 7.0 (TID 12)INFO  20-07 13:43:40,635 - Running task 3.0 in stage 7.0 (TID 13)INFO  20-07 13:43:40,648 - Input split: holodesk04INFO  20-07 13:43:40,648 - The Block Count in this node :6INFO  20-07 13:43:40,649 - Input split: holodesk01INFO  20-07 13:43:40,649 - The Block Count in this node :6INFO  20-07 13:43:40,649 - &#91;Executor task launch worker-7&#93;&#91;partitionID:tpcds_carbon_2_store_sales_00be80d1-400a-425d-9c7f-4acf3b3a7bb3&#93; ************* Is Columnar StoragetrueINFO  20-07 13:43:40,649 - &#91;Executor task launch worker-6&#93;&#91;partitionID:tpcds_carbon_2_store_sales_6302551d-dc77-4440-a26e-cbafb9d22c8c&#93; ************* Is Columnar StoragetrueINFO  20-07 13:43:40,649 - Input split: holodesk03INFO  20-07 13:43:40,650 - The Block Count in this node :6INFO  20-07 13:43:40,650 - &#91;Executor task launch worker-8&#93;&#91;partitionID:tpcds_carbon_2_store_sales_94282d67-f4de-42dd-b61c-af8483cf3d21&#93; ************* Is Columnar StoragetrueINFO  20-07 13:43:40,649 - Input split: holodesk02INFO  20-07 13:43:40,651 - The Block Count in this node :6INFO  20-07 13:43:40,651 - &#91;Executor task launch worker-5&#93;&#91;partitionID:tpcds_carbon_2_store_sales_3e4ba964-bcdc-4196-8d81-c590f2c67605&#93; ************* Is Columnar StoragetrueINFO  20-07 13:43:40,701 - &#91;Executor task launch worker-6&#93;&#91;partitionID:tpcds_carbon_2_store_sales_6302551d-dc77-4440-a26e-cbafb9d22c8c&#93; Kettle environment initializedINFO  20-07 13:43:40,706 - &#91;Executor task launch worker-8&#93;&#91;partitionID:tpcds_carbon_2_store_sales_94282d67-f4de-42dd-b61c-af8483cf3d21&#93; Kettle environment initializedINFO  20-07 13:43:40,707 - &#91;Executor task launch worker-7&#93;&#91;partitionID:tpcds_carbon_2_store_sales_00be80d1-400a-425d-9c7f-4acf3b3a7bb3&#93; Kettle environment initializedINFO  20-07 13:43:40,713 - &#91;Executor task launch worker-5&#93;&#91;partitionID:tpcds_carbon_2_store_sales_3e4ba964-bcdc-4196-8d81-c590f2c67605&#93; Kettle environment initializedINFO  20-07 13:43:40,751 - &#91;Executor task launch worker-8&#93;&#91;partitionID:tpcds_carbon_2_store_sales_94282d67-f4de-42dd-b61c-af8483cf3d21&#93; ** Using csv file **INFO  20-07 13:43:40,756 - &#91;Executor task launch worker-6&#93;&#91;partitionID:tpcds_carbon_2_store_sales_6302551d-dc77-4440-a26e-cbafb9d22c8c&#93; ** Using csv file **INFO  20-07 13:43:40,764 - store_sales: Graph - CSV Input ****************Started all csv reading**********INFO  20-07 13:43:40,774 - &#91;pool-40-thread-1&#93;&#91;partitionID:PROCESS_BLOCKS;queryID:pool-40-thread-1&#93; ****************started csv reading by thread**********INFO  20-07 13:43:40,788 - &#91;pool-40-thread-2&#93;&#91;partitionID:PROCESS_BLOCKS;queryID:pool-40-thread-2&#93; ****************started csv reading by thread**********INFO  20-07 13:43:40,795 - &#91;Executor task launch worker-8&#93;&#91;partitionID:tpcds_carbon_2_store_sales_94282d67-f4de-42dd-b61c-af8483cf3d21&#93; Graph execution is started /mnt/disk1/spark/438978154880668/3/etl/tpcds_carbon_2/store_sales/0/3/store_sales.ktrINFO  20-07 13:43:40,798 - store_sales: Graph - CSV Input ****************Started all csv reading**********INFO  20-07 13:43:40,809 - &#91;Executor task launch worker-6&#93;&#91;partitionID:tpcds_carbon_2_store_sales_6302551d-dc77-4440-a26e-cbafb9d22c8c&#93; Graph execution is started /mnt/disk1/spark/438978153902729/1/etl/tpcds_carbon_2/store_sales/0/1/store_sales.ktrINFO  20-07 13:43:40,813 - &#91;pool-41-thread-1&#93;&#91;partitionID:PROCESS_BLOCKS;queryID:pool-41-thread-1&#93; ****************started csv reading by thread**********INFO  20-07 13:43:40,814 - &#91;pool-41-thread-2&#93;&#91;partitionID:PROCESS_BLOCKS;queryID:pool-41-thread-2&#93; ****************started csv reading by thread**********ERROR 20-07 13:43:40,819 - &#91;store_sales: Graph - Carbon Surrogate Key Generator&#93;&#91;partitionID:0&#93; java.lang.NullPointerException at org.carbondata.processing.schema.metadata.ColumnSchemaDetailsWrapper.<init>(ColumnSchemaDetailsWrapper.java:75) at org.carbondata.processing.surrogatekeysgenerator.csvbased.CarbonCSVBasedSeqGenMeta.initialize(CarbonCSVBasedSeqGenMeta.java:787) at org.carbondata.processing.surrogatekeysgenerator.csvbased.CarbonCSVBasedSeqGenStep.processRow(CarbonCSVBasedSeqGenStep.java:294) at org.pentaho.di.trans.step.RunThread.run(RunThread.java:50) at java.lang.Thread.run(Thread.java:745)INFO  20-07 13:43:40,819 - &#91;store_sales: Graph - Sort Key: Sort keysstore_sales&#93;&#91;partitionID:0&#93; Record Processed For table: store_salesINFO  20-07 13:43:40,819 - &#91;store_sales: Graph - Sort Key: Sort keysstore_sales&#93;&#91;partitionID:0&#93; Number of Records was ZeroINFO  20-07 13:43:40,819 - &#91;store_sales: Graph - Sort Key: Sort keysstore_sales&#93;&#91;partitionID:0&#93; Summary: Carbon Sort Key Step: Read: 0: Write: 0INFO  20-07 13:43:40,820 - &#91;store_sales: Graph - Carbon Slice Mergerstore_sales&#93;&#91;partitionID:sales&#93; Record Procerssed For table: store_salesINFO  20-07 13:43:40,820 - &#91;store_sales: Graph - Carbon Slice Mergerstore_sales&#93;&#91;partitionID:sales&#93; Summary: Carbon Slice Merger Step: Read: 0: Write: 0INFO  20-07 13:43:40,820 - &#91;Executor task launch worker-5&#93;&#91;partitionID:tpcds_carbon_2_store_sales_3e4ba964-bcdc-4196-8d81-c590f2c67605&#93; ** Using csv file **ERROR 20-07 13:43:40,821 - &#91;store_sales: Graph - MDKeyGenstore_sales&#93;&#91;partitionID:0&#93; Local data load folder location does not exist: /mnt/disk1/spark/438978154880668/3/tpcds_carbon_2/store_sales/Fact/Part0/Segment_0/3INFO  20-07 13:43:40,841 - &#91;Executor task launch worker-7&#93;&#91;partitionID:tpcds_carbon_2_store_sales_00be80d1-400a-425d-9c7f-4acf3b3a7bb3&#93; ** Using csv file **INFO  20-07 13:43:40,854 - &#91;Executor task launch worker-5&#93;&#91;partitionID:tpcds_carbon_2_store_sales_3e4ba964-bcdc-4196-8d81-c590f2c67605&#93; Graph execution is started /mnt/disk2/spark/438978155737218/0/etl/tpcds_carbon_2/store_sales/0/0/store_sales.ktrERROR 20-07 13:43:40,854 - &#91;store_sales: Graph - Carbon Surrogate Key Generator&#93;&#91;partitionID:0&#93; java.lang.NullPointerException at org.carbondata.processing.schema.metadata.ColumnSchemaDetailsWrapper.<init>(ColumnSchemaDetailsWrapper.java:75) at org.carbondata.processing.surrogatekeysgenerator.csvbased.CarbonCSVBasedSeqGenMeta.initialize(CarbonCSVBasedSeqGenMeta.java:787) at org.carbondata.processing.surrogatekeysgenerator.csvbased.CarbonCSVBasedSeqGenStep.processRow(CarbonCSVBasedSeqGenStep.java:294) at org.pentaho.di.trans.step.RunThread.run(RunThread.java:50) at java.lang.Thread.run(Thread.java:745)ERROR 20-07 13:43:40,855 - &#91;store_sales: Graph - MDKeyGenstore_sales&#93;&#91;partitionID:0&#93; Local data load folder location does not exist: /mnt/disk1/spark/438978153902729/1/tpcds_carbon_2/store_sales/Fact/Part0/Segment_0/1INFO  20-07 13:43:40,855 - &#91;store_sales: Graph - Sort Key: Sort keysstore_sales&#93;&#91;partitionID:0&#93; Record Processed For table: store_salesINFO  20-07 13:43:40,855 - &#91;store_sales: Graph - Sort Key: Sort keysstore_sales&#93;&#91;partitionID:0&#93; Number of Records was ZeroINFO  20-07 13:43:40,855 - &#91;store_sales: Graph - Sort Key: Sort keysstore_sales&#93;&#91;partitionID:0&#93; Summary: Carbon Sort Key Step: Read: 0: Write: 0INFO  20-07 13:43:40,856 - store_sales: Graph - CSV Input ****************Started all csv reading**********INFO  20-07 13:43:40,857 - &#91;store_sales: Graph - Carbon Slice Mergerstore_sales&#93;&#91;partitionID:sales&#93; Record Procerssed For table: store_salesINFO  20-07 13:43:40,857 - &#91;store_sales: Graph - Carbon Slice Mergerstore_sales&#93;&#91;partitionID:sales&#93; Summary: Carbon Slice Merger Step: Read: 0: Write: 0INFO  20-07 13:43:40,867 - &#91;pool-42-thread-2&#93;&#91;partitionID:PROCESS_BLOCKS;queryID:pool-42-thread-2&#93; ****************started csv reading by thread**********INFO  20-07 13:43:40,869 - &#91;pool-42-thread-1&#93;&#91;partitionID:PROCESS_BLOCKS;queryID:pool-42-thread-1&#93; ****************started csv reading by thread**********INFO  20-07 13:43:40,872 - store_sales: Graph - CSV Input ****************Started all csv reading**********INFO  20-07 13:43:40,878 - &#91;pool-43-thread-1&#93;&#91;partitionID:PROCESS_BLOCKS;queryID:pool-43-thread-1&#93; ****************started csv reading by thread**********INFO  20-07 13:43:40,881 - &#91;pool-43-thread-2&#93;&#91;partitionID:PROCESS_BLOCKS;queryID:pool-43-thread-2&#93; ****************started csv reading by thread**********INFO  20-07 13:43:40,886 - &#91;Executor task launch worker-7&#93;&#91;partitionID:tpcds_carbon_2_store_sales_00be80d1-400a-425d-9c7f-4acf3b3a7bb3&#93; Graph execution is started /mnt/disk1/spark/438978153678637/2/etl/tpcds_carbon_2/store_sales/0/2/store_sales.ktrERROR 20-07 13:43:40,898 - &#91;store_sales: Graph - Carbon Surrogate Key Generator&#93;&#91;partitionID:0&#93; java.lang.NullPointerException at org.carbondata.processing.schema.metadata.ColumnSchemaDetailsWrapper.<init>(ColumnSchemaDetailsWrapper.java:75) at org.carbondata.processing.surrogatekeysgenerator.csvbased.CarbonCSVBasedSeqGenMeta.initialize(CarbonCSVBasedSeqGenMeta.java:787) at org.carbondata.processing.surrogatekeysgenerator.csvbased.CarbonCSVBasedSeqGenStep.processRow(CarbonCSVBasedSeqGenStep.java:294) at org.pentaho.di.trans.step.RunThread.run(RunThread.java:50) at java.lang.Thread.run(Thread.java:745)INFO  20-07 13:43:40,899 - &#91;store_sales: Graph - Carbon Slice Mergerstore_sales&#93;&#91;partitionID:sales&#93; Record Procerssed For table: store_salesERROR 20-07 13:43:40,899 - &#91;store_sales: Graph - MDKeyGenstore_sales&#93;&#91;partitionID:0&#93; Local data load folder location does not exist: /mnt/disk2/spark/438978155737218/0/tpcds_carbon_2/store_sales/Fact/Part0/Segment_0/0INFO  20-07 13:43:40,899 - &#91;store_sales: Graph - Carbon Slice Mergerstore_sales&#93;&#91;partitionID:sales&#93; Summary: Carbon Slice Merger Step: Read: 0: Write: 0INFO  20-07 13:43:40,899 - &#91;store_sales: Graph - Sort Key: Sort keysstore_sales&#93;&#91;partitionID:0&#93; Record Processed For table: store_salesINFO  20-07 13:43:40,899 - &#91;store_sales: Graph - Sort Key: Sort keysstore_sales&#93;&#91;partitionID:0&#93; Number of Records was ZeroINFO  20-07 13:43:40,900 - &#91;store_sales: Graph - Sort Key: Sort keysstore_sales&#93;&#91;partitionID:0&#93; Summary: Carbon Sort Key Step: Read: 0: Write: 0ERROR 20-07 13:43:40,904 - &#91;store_sales: Graph - Carbon Surrogate Key Generator&#93;&#91;partitionID:0&#93; java.lang.NullPointerException at org.carbondata.processing.schema.metadata.ColumnSchemaDetailsWrapper.<init>(ColumnSchemaDetailsWrapper.java:75) at org.carbondata.processing.surrogatekeysgenerator.csvbased.CarbonCSVBasedSeqGenMeta.initialize(CarbonCSVBasedSeqGenMeta.java:787) at org.carbondata.processing.surrogatekeysgenerator.csvbased.CarbonCSVBasedSeqGenStep.processRow(CarbonCSVBasedSeqGenStep.java:294) at org.pentaho.di.trans.step.RunThread.run(RunThread.java:50) at java.lang.Thread.run(Thread.java:745)INFO  20-07 13:43:40,906 - &#91;store_sales: Graph - Sort Key: Sort keysstore_sales&#93;&#91;partitionID:0&#93; Record Processed For table: store_salesINFO  20-07 13:43:40,906 - &#91;store_sales: Graph - Carbon Slice Mergerstore_sales&#93;&#91;partitionID:sales&#93; Record Procerssed For table: store_salesERROR 20-07 13:43:40,907 - &#91;store_sales: Graph - MDKeyGenstore_sales&#93;&#91;partitionID:0&#93; Local data load folder location does not exist: /mnt/disk1/spark/438978153678637/2/tpcds_carbon_2/store_sales/Fact/Part0/Segment_0/2INFO  20-07 13:43:40,907 - &#91;store_sales: Graph - Sort Key: Sort keysstore_sales&#93;&#91;partitionID:0&#93; Number of Records was ZeroINFO  20-07 13:43:40,907 - &#91;store_sales: Graph - Carbon Slice Mergerstore_sales&#93;&#91;partitionID:sales&#93; Summary: Carbon Slice Merger Step: Read: 0: Write: 0INFO  20-07 13:43:40,907 - &#91;store_sales: Graph - Sort Key: Sort keysstore_sales&#93;&#91;partitionID:0&#93; Summary: Carbon Sort Key Step: Read: 0: Write: 0INFO  20-07 13:43:41,464 - Cleaned accumulator 18INFO  20-07 13:43:41,492 - Removed broadcast_8_piece0 on localhost:50762 in memory (size: 23.9 KB, free: 511.5 MB)INFO  20-07 13:43:41,497 - Removed broadcast_7_piece0 on localhost:50762 in memory (size: 23.9 KB, free: 511.5 MB)INFO  20-07 13:43:41,499 - Removed broadcast_9_piece0 on localhost:50762 in memory (size: 1600.0 B, free: 511.5 MB)INFO  20-07 13:43:49,599 - &#91;pool-41-thread-2&#93;&#91;partitionID:PROCESS_BLOCKS;queryID:pool-41-thread-2&#93; ****************Completed csv reading by thread**********INFO  20-07 13:43:49,855 - &#91;pool-41-thread-1&#93;&#91;partitionID:PROCESS_BLOCKS;queryID:pool-41-thread-1&#93; ****************Completed csv reading by thread**********INFO  20-07 13:43:49,957 - store_sales: Graph - CSV Input ****************Completed all csv reading**********INFO  20-07 13:43:49,957 - &#91;Executor task launch worker-6&#93;&#91;partitionID:tpcds_carbon_2_store_sales_6302551d-dc77-4440-a26e-cbafb9d22c8c&#93; Graph execution is finished.ERROR 20-07 13:43:49,957 - &#91;Executor task launch worker-6&#93;&#91;partitionID:tpcds_carbon_2_store_sales_6302551d-dc77-4440-a26e-cbafb9d22c8c&#93; Graph Execution had errorsERROR 20-07 13:43:49,957 - &#91;Executor task launch worker-6&#93;&#91;partitionID:tpcds_carbon_2_store_sales_6302551d-dc77-4440-a26e-cbafb9d22c8c&#93; org.carbondata.processing.etl.DataLoadingException: Internal Errors at org.carbondata.processing.csvload.DataGraphExecuter.execute(DataGraphExecuter.java:253) at org.carbondata.processing.csvload.DataGraphExecuter.executeGraph(DataGraphExecuter.java:168) at org.carbondata.spark.load.CarbonLoaderUtil.executeGraph(CarbonLoaderUtil.java:189) at org.carbondata.spark.rdd.CarbonDataLoadRDD$$anon$1.<init>(CarbonDataLoadRDD.scala:189) at org.carbondata.spark.rdd.CarbonDataLoadRDD.compute(CarbonDataLoadRDD.scala:148) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306) at org.apache.spark.rdd.RDD.iterator(RDD.scala:270) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66) at org.apache.spark.scheduler.Task.run(Task.scala:89) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) at java.lang.Thread.run(Thread.java:745)INFO  20-07 13:43:49,958 - DataLoad failureINFO  20-07 13:43:49,969 - Finished task 1.0 in stage 7.0 (TID 11). 952 bytes result sent to driverINFO  20-07 13:43:49,982 - Finished task 1.0 in stage 7.0 (TID 11) in 9350 ms on localhost (1/4)INFO  20-07 13:43:50,482 - &#91;pool-40-thread-2&#93;&#91;partitionID:PROCESS_BLOCKS;queryID:pool-40-thread-2&#93; ****************Completed csv reading by thread**********INFO  20-07 13:43:50,943 - &#91;pool-42-thread-2&#93;&#91;partitionID:PROCESS_BLOCKS;queryID:pool-42-thread-2&#93; ****************Completed csv reading by thread**********INFO  20-07 13:43:51,270 - &#91;pool-40-thread-1&#93;&#91;partitionID:PROCESS_BLOCKS;queryID:pool-40-thread-1&#93; ****************Completed csv reading by thread**********INFO  20-07 13:43:51,408 - store_sales: Graph - CSV Input ****************Completed all csv reading**********INFO  20-07 13:43:51,408 - &#91;Executor task launch worker-8&#93;&#91;partitionID:tpcds_carbon_2_store_sales_94282d67-f4de-42dd-b61c-af8483cf3d21&#93; Graph execution is finished.ERROR 20-07 13:43:51,409 - &#91;Executor task launch worker-8&#93;&#91;partitionID:tpcds_carbon_2_store_sales_94282d67-f4de-42dd-b61c-af8483cf3d21&#93; Graph Execution had errorsERROR 20-07 13:43:51,409 - &#91;Executor task launch worker-8&#93;&#91;partitionID:tpcds_carbon_2_store_sales_94282d67-f4de-42dd-b61c-af8483cf3d21&#93; org.carbondata.processing.etl.DataLoadingException: Internal Errors at org.carbondata.processing.csvload.DataGraphExecuter.execute(DataGraphExecuter.java:253) at org.carbondata.processing.csvload.DataGraphExecuter.executeGraph(DataGraphExecuter.java:168) at org.carbondata.spark.load.CarbonLoaderUtil.executeGraph(CarbonLoaderUtil.java:189) at org.carbondata.spark.rdd.CarbonDataLoadRDD$$anon$1.<init>(CarbonDataLoadRDD.scala:189) at org.carbondata.spark.rdd.CarbonDataLoadRDD.compute(CarbonDataLoadRDD.scala:148) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306) at org.apache.spark.rdd.RDD.iterator(RDD.scala:270) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66) at org.apache.spark.scheduler.Task.run(Task.scala:89) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) at java.lang.Thread.run(Thread.java:745)INFO  20-07 13:43:51,409 - DataLoad failureINFO  20-07 13:43:51,420 - Finished task 3.0 in stage 7.0 (TID 13). 952 bytes result sent to driverINFO  20-07 13:43:51,434 - Finished task 3.0 in stage 7.0 (TID 13) in 10800 ms on localhost (2/4)INFO  20-07 13:43:51,435 - &#91;pool-43-thread-2&#93;&#91;partitionID:PROCESS_BLOCKS;queryID:pool-43-thread-2&#93; ****************Completed csv reading by thread**********INFO  20-07 13:43:52,466 - &#91;pool-42-thread-1&#93;&#91;partitionID:PROCESS_BLOCKS;queryID:pool-42-thread-1&#93; ****************Completed csv reading by thread**********INFO  20-07 13:43:52,588 - store_sales: Graph - CSV Input ****************Completed all csv reading**********INFO  20-07 13:43:52,590 - &#91;Executor task launch worker-5&#93;&#91;partitionID:tpcds_carbon_2_store_sales_3e4ba964-bcdc-4196-8d81-c590f2c67605&#93; Graph execution is finished.ERROR 20-07 13:43:52,590 - &#91;Executor task launch worker-5&#93;&#91;partitionID:tpcds_carbon_2_store_sales_3e4ba964-bcdc-4196-8d81-c590f2c67605&#93; Graph Execution had errorsERROR 20-07 13:43:52,590 - &#91;Executor task launch worker-5&#93;&#91;partitionID:tpcds_carbon_2_store_sales_3e4ba964-bcdc-4196-8d81-c590f2c67605&#93; org.carbondata.processing.etl.DataLoadingException: Internal Errors at org.carbondata.processing.csvload.DataGraphExecuter.execute(DataGraphExecuter.java:253) at org.carbondata.processing.csvload.DataGraphExecuter.executeGraph(DataGraphExecuter.java:168) at org.carbondata.spark.load.CarbonLoaderUtil.executeGraph(CarbonLoaderUtil.java:189) at org.carbondata.spark.rdd.CarbonDataLoadRDD$$anon$1.<init>(CarbonDataLoadRDD.scala:189) at org.carbondata.spark.rdd.CarbonDataLoadRDD.compute(CarbonDataLoadRDD.scala:148) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306) at org.apache.spark.rdd.RDD.iterator(RDD.scala:270) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66) at org.apache.spark.scheduler.Task.run(Task.scala:89) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) at java.lang.Thread.run(Thread.java:745)INFO  20-07 13:43:52,591 - DataLoad failureINFO  20-07 13:43:52,603 - Finished task 0.0 in stage 7.0 (TID 10). 952 bytes result sent to driverINFO  20-07 13:43:52,614 - Finished task 0.0 in stage 7.0 (TID 10) in 11984 ms on localhost (3/4)INFO  20-07 13:43:52,638 - &#91;pool-43-thread-1&#93;&#91;partitionID:PROCESS_BLOCKS;queryID:pool-43-thread-1&#93; ****************Completed csv reading by thread**********INFO  20-07 13:43:52,824 - store_sales: Graph - CSV Input ****************Completed all csv reading**********INFO  20-07 13:43:52,824 - &#91;Executor task launch worker-7&#93;&#91;partitionID:tpcds_carbon_2_store_sales_00be80d1-400a-425d-9c7f-4acf3b3a7bb3&#93; Graph execution is finished.ERROR 20-07 13:43:52,825 - &#91;Executor task launch worker-7&#93;&#91;partitionID:tpcds_carbon_2_store_sales_00be80d1-400a-425d-9c7f-4acf3b3a7bb3&#93; Graph Execution had errorsERROR 20-07 13:43:52,825 - &#91;Executor task launch worker-7&#93;&#91;partitionID:tpcds_carbon_2_store_sales_00be80d1-400a-425d-9c7f-4acf3b3a7bb3&#93; org.carbondata.processing.etl.DataLoadingException: Internal Errors at org.carbondata.processing.csvload.DataGraphExecuter.execute(DataGraphExecuter.java:253) at org.carbondata.processing.csvload.DataGraphExecuter.executeGraph(DataGraphExecuter.java:168) at org.carbondata.spark.load.CarbonLoaderUtil.executeGraph(CarbonLoaderUtil.java:189) at org.carbondata.spark.rdd.CarbonDataLoadRDD$$anon$1.<init>(CarbonDataLoadRDD.scala:189) at org.carbondata.spark.rdd.CarbonDataLoadRDD.compute(CarbonDataLoadRDD.scala:148) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306) at org.apache.spark.rdd.RDD.iterator(RDD.scala:270) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66) at org.apache.spark.scheduler.Task.run(Task.scala:89) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) at java.lang.Thread.run(Thread.java:745)INFO  20-07 13:43:52,825 - DataLoad failureINFO  20-07 13:43:52,837 - Finished task 2.0 in stage 7.0 (TID 12). 952 bytes result sent to driverINFO  20-07 13:43:52,849 - Finished task 2.0 in stage 7.0 (TID 12) in 12216 ms on localhost (4/4)INFO  20-07 13:43:52,849 - ResultStage 7 (collect at CarbonDataRDDFactory.scala:717) finished in 12.219 sINFO  20-07 13:43:52,849 - Removed TaskSet 7.0, whose tasks have all completed, from pool INFO  20-07 13:43:52,849 - Finished stage: org.apache.spark.scheduler.StageInfo@46ffcf8bINFO  20-07 13:43:52,849 - Job 6 finished: collect at CarbonDataRDDFactory.scala:717, took 12.244086 sINFO  20-07 13:43:52,850 - *******starting clean up*********INFO  20-07 13:43:52,851 - task runtime:(count: 4, mean: 11087.500000, stdev: 1137.847419, max: 12216.000000, min: 9350.000000)INFO  20-07 13:43:52,851 -  0% 5% 10% 25% 50% 75% 90% 95% 100%INFO  20-07 13:43:52,851 -  9.4 s 9.4 s 9.4 s 10.8 s 12.0 s 12.2 s 12.2 s 12.2 s 12.2 sINFO  20-07 13:43:52,853 - task result size:(count: 4, mean: 952.000000, stdev: 0.000000, max: 952.000000, min: 952.000000)INFO  20-07 13:43:52,853 -  0% 5% 10% 25% 50% 75% 90% 95% 100%INFO  20-07 13:43:52,853 -  952.0 B 952.0 B 952.0 B 952.0 B 952.0 B 952.0 B 952.0 B 952.0 B 952.0 BINFO  20-07 13:43:52,855 - executor (non-fetch) time pct: (count: 4, mean: 99.639701, stdev: 0.042276, max: 99.688933, min: 99.572193)INFO  20-07 13:43:52,855 -  0% 5% 10% 25% 50% 75% 90% 95% 100%INFO  20-07 13:43:52,855 -  100 % 100 % 100 % 100 % 100 % 100 % 100 % 100 % 100 %INFO  20-07 13:43:52,857 - other time pct: (count: 4, mean: 0.360299, stdev: 0.042276, max: 0.427807, min: 0.311067)INFO  20-07 13:43:52,857 -  0% 5% 10% 25% 50% 75% 90% 95% 100%INFO  20-07 13:43:52,857 -   0 %  0 %  0 %  0 %  0 %  0 %  0 %  0 %  0 %INFO  20-07 13:43:53,079 - *******clean up done*********AUDIT 20-07 13:43:53,079 - &#91;holodesk01&#93;&#91;hdfs&#93;&#91;Thread-1&#93;Data load is failed for tpcds_carbon_2.store_salesWARN  20-07 13:43:53,080 - Unable to write load metadata fileERROR 20-07 13:43:53,080 - main java.lang.Exception: Dataload failure at org.carbondata.spark.rdd.CarbonDataRDDFactory$.loadCarbonData(CarbonDataRDDFactory.scala:779) at org.apache.spark.sql.execution.command.LoadTable.run(carbonTableSchema.scala:1146) at org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult$lzycompute(commands.scala:58) at org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult(commands.scala:56) at org.apache.spark.sql.execution.ExecutedCommand.doExecute(commands.scala:70) at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$5.apply(SparkPlan.scala:132) at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$5.apply(SparkPlan.scala:130) at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150) at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:130) at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:55) at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:55) at org.apache.spark.sql.DataFrame.<init>(DataFrame.scala:145) at org.apache.spark.sql.DataFrame.<init>(DataFrame.scala:130) at org.carbondata.spark.rdd.CarbonDataFrameRDD.<init>(CarbonDataFrameRDD.scala:23) at org.apache.spark.sql.CarbonContext.sql(CarbonContext.scala:109) at org.apache.spark.sql.hive.thriftserver.SparkSQLDriver.run(SparkSQLDriver.scala:63) at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.processCmd(SparkSQLCLIDriver.scala:311) at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:376) at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver$.main(SparkSQLCLIDriver.scala:226) at org.apache.spark.sql.hive.cli.CarbonSQLCLIDriver$.main(CarbonSQLCLIDriver.scala:40) at org.apache.spark.sql.hive.cli.CarbonSQLCLIDriver.main(CarbonSQLCLIDriver.scala) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:606) at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731) at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181) at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206) at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121) at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)AUDIT 20-07 13:43:53,081 - &#91;holodesk01&#93;&#91;hdfs&#93;&#91;Thread-1&#93;Dataload failure for tpcds_carbon_2.store_sales. Please check the logsINFO  20-07 13:43:53,083 - Table MetaData Unlocked Successfully after data loadERROR 20-07 13:43:53,083 - Failed in &#91;LOAD DATA  inpath &#39;hdfs://holodesk01/user/carbon-spark-sql/tpcds/2/store_sales&#39; INTO table store_sales&#93;java.lang.Exception: Dataload failure at org.carbondata.spark.rdd.CarbonDataRDDFactory$.loadCarbonData(CarbonDataRDDFactory.scala:779) at org.apache.spark.sql.execution.command.LoadTable.run(carbonTableSchema.scala:1146) at org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult$lzycompute(commands.scala:58) at org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult(commands.scala:56) at org.apache.spark.sql.execution.ExecutedCommand.doExecute(commands.scala:70) at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$5.apply(SparkPlan.scala:132) at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$5.apply(SparkPlan.scala:130) at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150) at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:130) at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:55) at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:55) at org.apache.spark.sql.DataFrame.<init>(DataFrame.scala:145) at org.apache.spark.sql.DataFrame.<init>(DataFrame.scala:130) at org.carbondata.spark.rdd.CarbonDataFrameRDD.<init>(CarbonDataFrameRDD.scala:23) at org.apache.spark.sql.CarbonContext.sql(CarbonContext.scala:109) at org.apache.spark.sql.hive.thriftserver.SparkSQLDriver.run(SparkSQLDriver.scala:63) at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.processCmd(SparkSQLCLIDriver.scala:311) at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:376) at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver$.main(SparkSQLCLIDriver.scala:226) at org.apache.spark.sql.hive.cli.CarbonSQLCLIDriver$.main(CarbonSQLCLIDriver.scala:40) at org.apache.spark.sql.hive.cli.CarbonSQLCLIDriver.main(CarbonSQLCLIDriver.scala) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:606) at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731) at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181) at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206) at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121) at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)java.lang.Exception: Dataload failure at org.carbondata.spark.rdd.CarbonDataRDDFactory$.loadCarbonData(CarbonDataRDDFactory.scala:779) at org.apache.spark.sql.execution.command.LoadTable.run(carbonTableSchema.scala:1146) at org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult$lzycompute(commands.scala:58) at org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult(commands.scala:56) at org.apache.spark.sql.execution.ExecutedCommand.doExecute(commands.scala:70) at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$5.apply(SparkPlan.scala:132) at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$5.apply(SparkPlan.scala:130) at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150) at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:130) at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:55) at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:55) at org.apache.spark.sql.DataFrame.<init>(DataFrame.scala:145) at org.apache.spark.sql.DataFrame.<init>(DataFrame.scala:130) at org.carbondata.spark.rdd.CarbonDataFrameRDD.<init>(CarbonDataFrameRDD.scala:23) at org.apache.spark.sql.CarbonContext.sql(CarbonContext.scala:109) at org.apache.spark.sql.hive.thriftserver.SparkSQLDriver.run(SparkSQLDriver.scala:63) at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.processCmd(SparkSQLCLIDriver.scala:311) at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:376) at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver$.main(SparkSQLCLIDriver.scala:226) at org.apache.spark.sql.hive.cli.CarbonSQLCLIDriver$.main(CarbonSQLCLIDriver.scala:40) at org.apache.spark.sql.hive.cli.CarbonSQLCLIDriver.main(CarbonSQLCLIDriver.scala) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:606) at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731) at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181) at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206) at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121) at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)```
issueID:CARBONDATA-820
type:Bug
changed files:processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactDataHandlerColumnar.java
texts:Redundant BitSet created in data load
In CarbonFactDataHandlerColumnar.getMeasureNullValueIndexBitSet method
issueID:CARBONDATA-821
type:Bug
changed files:processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/SortDataRows.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/SortParameters.java
processing/src/main/java/org/apache/carbondata/processing/loading/csvinput/BlockDetails.java
core/src/main/java/org/apache/carbondata/core/scan/wrappers/ByteArrayWrapper.java
processing/src/main/java/org/apache/carbondata/processing/merger/RowResultMergerProcessor.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/IntermediateFileMerger.java
processing/src/main/java/org/apache/carbondata/processing/datatypes/PrimitiveDataType.java
processing/src/main/java/org/apache/carbondata/processing/loading/steps/DataWriterProcessorStepImpl.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/SingleThreadFinalSortFilesMerger.java
processing/src/main/java/org/apache/carbondata/processing/store/writer/AbstractFactDataWriter.java
processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactDataHandlerColumnar.java
core/src/main/java/org/apache/carbondata/core/scan/executor/impl/AbstractQueryExecutor.java
processing/src/main/java/org/apache/carbondata/processing/util/CarbonLoaderUtil.java
processing/src/main/java/org/apache/carbondata/processing/datatypes/StructDataType.java
processing/src/main/java/org/apache/carbondata/processing/datatypes/GenericDataType.java
core/src/main/java/org/apache/carbondata/core/scan/executor/util/QueryUtil.java
core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactDataHandlerModel.java
processing/src/main/java/org/apache/carbondata/processing/merger/exeception/SliceMergerException.java
processing/src/main/java/org/apache/carbondata/processing/datatypes/ArrayDataType.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
processing/src/main/java/org/apache/carbondata/processing/util/CarbonDataProcessorUtil.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/impl/ParallelReadMergeSorterImpl.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/impl/ParallelReadMergeSorterWithBucketingImpl.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/SortTempFileChunkHolder.java
texts:Remove Kettle related code and flow from carbon.
Remove Kettle related code and flow from carbon. It becomes difficult to developers to handle all bugs and features in both the flows.
issueID:CARBONDATA-822
type:Improvement
changed files:processing/src/main/java/org/apache/carbondata/processing/loading/sort/impl/ParallelReadMergeSorterWithBucketingImpl.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/impl/UnsafeParallelReadMergeSorterImpl.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/SorterFactory.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/UnsafeSortDataRows.java
processing/src/main/java/org/apache/carbondata/processing/loading/steps/SortProcessorStepImpl.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/impl/UnsafeParallelReadMergeSorterWithBucketingImpl.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/merger/UnsafeSingleThreadFinalSortFilesMerger.java
texts:Add unsafe sort for bucketing feature
Currently there is no unsafe sort in case of bucketing enabled. To improve the bucketing load performance enable unsafe sort for bucketing as well.
issueID:CARBONDATA-823
type:Improvement
changed files:processing/src/main/java/org/apache/carbondata/processing/store/writer/v3/CarbonFactDataWriterImplV3.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/SortDataRows.java
processing/src/main/java/org/apache/carbondata/processing/store/writer/CarbonFactDataWriter.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/merger/UnsafeIntermediateFileMerger.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
core/src/main/java/org/apache/carbondata/core/util/CarbonMetadataUtil.java
core/src/main/java/org/apache/carbondata/core/util/DataTypeUtil.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/UnsafeCarbonRowPage.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/holder/UnsafeSortTempFileChunkHolder.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/SortTempFileChunkHolder.java
processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactDataHandlerColumnar.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/IntermediateFileMerger.java
core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
texts:Refactory of data write step

issueID:CARBONDATA-824
type:Bug
changed files:
texts:Null pointer Exception display to user while performance Testing
Displays null pointer exception to the user while select Query.Steps to reproduces:1: Create table:CREATE TABLE oscon_new_1 (ACTIVE_AREA_ID String, ACTIVE_CHECK_DY String, ACTIVE_CHECK_HOUR String, ACTIVE_CHECK_MM String, ACTIVE_CHECK_TIME String, ACTIVE_CHECK_YR String, ACTIVE_CITY String, ACTIVE_COUNTRY String, ACTIVE_DISTRICT String, ACTIVE_EMUI_VERSION String, ACTIVE_FIRMWARE_VER String, ACTIVE_NETWORK String, ACTIVE_OS_VERSION String, ACTIVE_PROVINCE String, BOM String, CHECK_DATE String, CHECK_DY String, CHECK_HOUR String, CHECK_MM String, CHECK_YR String, CUST_ADDRESS_ID String, CUST_AGE String, CUST_BIRTH_COUNTRY String, CUST_BIRTH_DY String, CUST_BIRTH_MM String, CUST_BIRTH_YR String, CUST_BUY_POTENTIAL String, CUST_CITY String, CUST_STATE String, CUST_COUNTRY String, CUST_COUNTY String, CUST_EMAIL_ADDR String, CUST_LAST_RVW_DATE TIMESTAMP, CUST_FIRST_NAME String, CUST_ID String, CUST_JOB_TITLE String, CUST_LAST_NAME String, CUST_LOGIN String, CUST_NICK_NAME String, CUST_PRFRD_FLG String, CUST_SEX String, CUST_STREET_NAME String, CUST_STREET_NO String, CUST_SUITE_NO String, CUST_ZIP String, DELIVERY_CITY String, DELIVERY_STATE String, DELIVERY_COUNTRY String, DELIVERY_DISTRICT String, DELIVERY_PROVINCE String, DEVICE_NAME String, INSIDE_NAME String, ITM_BRAND String, ITM_BRAND_ID String, ITM_CATEGORY String, ITM_CATEGORY_ID String, ITM_CLASS String, ITM_CLASS_ID String, ITM_COLOR String, ITM_CONTAINER String, ITM_FORMULATION String, ITM_MANAGER_ID String, ITM_MANUFACT String, ITM_MANUFACT_ID String, ITM_ID String, ITM_NAME String, ITM_REC_END_DATE String, ITM_REC_START_DATE String, LATEST_AREAID String, LATEST_CHECK_DY String, LATEST_CHECK_HOUR String, LATEST_CHECK_MM String, LATEST_CHECK_TIME String, LATEST_CHECK_YR String, LATEST_CITY String, LATEST_COUNTRY String, LATEST_DISTRICT String, LATEST_EMUI_VERSION String, LATEST_FIRMWARE_VER String, LATEST_NETWORK String, LATEST_OS_VERSION String, LATEST_PROVINCE String, OL_ORDER_DATE String, OL_ORDER_NO INT, OL_RET_ORDER_NO String, OL_RET_DATE String, OL_SITE String, OL_SITE_DESC String, PACKING_DATE String, PACKING_DY String, PACKING_HOUR String, PACKING_LIST_NO String, PACKING_MM String, PACKING_YR String, PRMTION_ID String, PRMTION_NAME String, PRM_CHANNEL_CAT String, PRM_CHANNEL_DEMO String, PRM_CHANNEL_DETAILS String, PRM_CHANNEL_DMAIL String, PRM_CHANNEL_EMAIL String, PRM_CHANNEL_EVENT String, PRM_CHANNEL_PRESS String, PRM_CHANNEL_RADIO String, PRM_CHANNEL_TV String, PRM_DSCNT_ACTIVE String, PRM_END_DATE String, PRM_PURPOSE String, PRM_START_DATE String, PRODUCT_ID String, PROD_BAR_CODE String, PROD_BRAND_NAME String, PRODUCT_NAME String, PRODUCT_MODEL String, PROD_MODEL_ID String, PROD_COLOR String, PROD_SHELL_COLOR String, PROD_CPU_CLOCK String, PROD_IMAGE String, PROD_LIVE String, PROD_LOC String, PROD_LONG_DESC String, PROD_RAM String, PROD_ROM String, PROD_SERIES String, PROD_SHORT_DESC String, PROD_THUMB String, PROD_UNQ_DEVICE_ADDR String, PROD_UNQ_MDL_ID String, PROD_UPDATE_DATE String, PROD_UQ_UUID String, SHP_CARRIER String, SHP_CODE String, SHP_CONTRACT String, SHP_MODE_ID String, SHP_MODE String, STR_ORDER_DATE String, STR_ORDER_NO String, TRACKING_NO String, WH_CITY String, WH_COUNTRY String, WH_COUNTY String, WH_ID String, WH_NAME String, WH_STATE String, WH_STREET_NAME String, WH_STREET_NO String, WH_STREET_TYPE String, WH_SUITE_NO String, WH_ZIP String, CUST_DEP_COUNT DOUBLE, CUST_VEHICLE_COUNT DOUBLE, CUST_ADDRESS_CNT DOUBLE, CUST_CRNT_CDEMO_CNT DOUBLE, CUST_CRNT_HDEMO_CNT DOUBLE, CUST_CRNT_ADDR_DM DOUBLE, CUST_FIRST_SHIPTO_CNT DOUBLE, CUST_FIRST_SALES_CNT DOUBLE, CUST_GMT_OFFSET DOUBLE, CUST_DEMO_CNT DOUBLE, CUST_INCOME DOUBLE, PROD_UNLIMITED INT, PROD_OFF_PRICE DOUBLE, PROD_UNITS INT, TOTAL_PRD_COST DOUBLE, TOTAL_PRD_DISC DOUBLE, PROD_WEIGHT DOUBLE, REG_UNIT_PRICE DOUBLE, EXTENDED_AMT DOUBLE, UNIT_PRICE_DSCNT_PCT DOUBLE, DSCNT_AMT DOUBLE, PROD_STD_CST DOUBLE, TOTAL_TX_AMT DOUBLE, FREIGHT_CHRG DOUBLE, WAITING_PERIOD DOUBLE, DELIVERY_PERIOD DOUBLE, ITM_CRNT_PRICE DOUBLE, ITM_UNITS DOUBLE, ITM_WSLE_CST DOUBLE, ITM_SIZE DOUBLE, PRM_CST DOUBLE, PRM_RESPONSE_TARGET DOUBLE, PRM_ITM_DM DOUBLE, SHP_MODE_CNT DOUBLE, WH_GMT_OFFSET DOUBLE, WH_SQ_FT DOUBLE, STR_ORD_QTY DOUBLE, STR_WSLE_CST DOUBLE, STR_LIST_PRICE DOUBLE, STR_SALES_PRICE DOUBLE, STR_EXT_DSCNT_AMT DOUBLE, STR_EXT_SALES_PRICE DOUBLE, STR_EXT_WSLE_CST DOUBLE, STR_EXT_LIST_PRICE DOUBLE, STR_EXT_TX DOUBLE, STR_COUPON_AMT DOUBLE, STR_NET_PAID DOUBLE, STR_NET_PAID_INC_TX DOUBLE, STR_NET_PRFT DOUBLE, STR_SOLD_YR_CNT DOUBLE, STR_SOLD_MM_CNT DOUBLE, STR_SOLD_ITM_CNT DOUBLE, STR_TOTAL_CUST_CNT DOUBLE, STR_AREA_CNT DOUBLE, STR_DEMO_CNT DOUBLE, STR_OFFER_CNT DOUBLE, STR_PRM_CNT DOUBLE, STR_TICKET_CNT DOUBLE, STR_NET_PRFT_DM_A DOUBLE, STR_NET_PRFT_DM_B DOUBLE, STR_NET_PRFT_DM_C DOUBLE, STR_NET_PRFT_DM_D DOUBLE, STR_NET_PRFT_DM_E DOUBLE, STR_RET_STR_ID DOUBLE, STR_RET_REASON_CNT DOUBLE, STR_RET_TICKET_NO DOUBLE, STR_RTRN_QTY DOUBLE, STR_RTRN_AMT DOUBLE, STR_RTRN_TX DOUBLE, STR_RTRN_AMT_INC_TX DOUBLE, STR_RET_FEE DOUBLE, STR_RTRN_SHIP_CST DOUBLE, STR_RFNDD_CSH DOUBLE, STR_REVERSED_CHRG DOUBLE, STR_STR_CREDIT DOUBLE, STR_RET_NET_LOSS DOUBLE, STR_RTRNED_YR_CNT DOUBLE, STR_RTRN_MM_CNT DOUBLE, STR_RET_ITM_CNT DOUBLE, STR_RET_CUST_CNT DOUBLE, STR_RET_AREA_CNT DOUBLE, STR_RET_OFFER_CNT DOUBLE, STR_RET_PRM_CNT DOUBLE, STR_RET_NET_LOSS_DM_A DOUBLE, STR_RET_NET_LOSS_DM_B DOUBLE, STR_RET_NET_LOSS_DM_C DOUBLE, STR_RET_NET_LOSS_DM_D DOUBLE, OL_ORD_QTY DOUBLE, OL_WSLE_CST DOUBLE, OL_LIST_PRICE DOUBLE, OL_SALES_PRICE DOUBLE, OL_EXT_DSCNT_AMT DOUBLE, OL_EXT_SALES_PRICE DOUBLE, OL_EXT_WSLE_CST DOUBLE, OL_EXT_LIST_PRICE DOUBLE, OL_EXT_TX DOUBLE, OL_COUPON_AMT DOUBLE, OL_EXT_SHIP_CST DOUBLE, OL_NET_PAID DOUBLE, OL_NET_PAID_INC_TX DOUBLE, OL_NET_PAID_INC_SHIP DOUBLE, OL_NET_PAID_INC_SHIP_TX DOUBLE, OL_NET_PRFT DOUBLE, OL_SOLD_YR_CNT DOUBLE, OL_SOLD_MM_CNT DOUBLE, OL_SHIP_DATE_CNT DOUBLE, OL_ITM_CNT DOUBLE, OL_BILL_CUST_CNT DOUBLE, OL_BILL_AREA_CNT DOUBLE, OL_BILL_DEMO_CNT DOUBLE, OL_BILL_OFFER_CNT DOUBLE, OL_SHIP_CUST_CNT DOUBLE, OL_SHIP_AREA_CNT DOUBLE, OL_SHIP_DEMO_CNT DOUBLE, OL_SHIP_OFFER_CNT DOUBLE, OL_WEB_PAGE_CNT DOUBLE, OL_WEB_SITE_CNT DOUBLE, OL_SHIP_MODE_CNT DOUBLE, OL_WH_CNT DOUBLE, OL_PRM_CNT DOUBLE, OL_NET_PRFT_DM_A DOUBLE, OL_NET_PRFT_DM_B DOUBLE, OL_NET_PRFT_DM_C DOUBLE, OL_NET_PRFT_DM_D DOUBLE, OL_RET_RTRN_QTY DOUBLE, OL_RTRN_AMT DOUBLE, OL_RTRN_TX DOUBLE, OL_RTRN_AMT_INC_TX DOUBLE, OL_RET_FEE DOUBLE, OL_RTRN_SHIP_CST DOUBLE, OL_RFNDD_CSH DOUBLE, OL_REVERSED_CHRG DOUBLE, OL_ACCOUNT_CREDIT DOUBLE, OL_RTRNED_YR_CNT DOUBLE, OL_RTRNED_MM_CNT DOUBLE, OL_RTRITM_CNT DOUBLE, OL_RFNDD_CUST_CNT DOUBLE, OL_RFNDD_AREA_CNT DOUBLE, OL_RFNDD_DEMO_CNT DOUBLE, OL_RFNDD_OFFER_CNT DOUBLE, OL_RTRNING_CUST_CNT DOUBLE, OL_RTRNING_AREA_CNT DOUBLE, OL_RTRNING_DEMO_CNT DOUBLE, OL_RTRNING_OFFER_CNT DOUBLE, OL_RTRWEB_PAGE_CNT DOUBLE, OL_REASON_CNT DOUBLE, OL_NET_LOSS DOUBLE, OL_NET_LOSS_DM_A DOUBLE, OL_NET_LOSS_DM_B DOUBLE, OL_NET_LOSS_DM_C DOUBLE) STORED BY 'org.apache.carbondata.format';2: Perform Select Query.TC_010,""select * from  oscon_new_1  where CUST_ID = """"ID00000000015"""" and CUST_CITY=""""CC015"""" and CUST_LAST_RVW_DATE between """"2011-02-17 00:00:00"""" and """"2011-02-18 00:00:00""""""TC_011,""select * from   oscon_new_1  where PROD_COLOR =""""GOLD"""" and ACTIVE_EMUI_VERSION  like""""%73"""" limit 5000""TC_022,""SELECT *  from  oscon_new_1  where CUST_PRFRD_FLG=""""Y"""" and PROD_BRAND_NAME = """"LG"""" and PROD_COLOR = """"BLACK"""" and CUST_LAST_RVW_DATE = """"2011-07-02 00:00:00"""" and CUST_COUNTRY = """"CC009"""" and product_name = """"LG KF311 phone"""" ""TC_019,"select * from  oscon_new_1  where CUST_CITY='CC008' and CUST_PRFRD_FLG='Y' and CUST_LAST_RVW_DATE between '2011-02-04 00:00:00' and '2011-02-05 00:00:00'"TC_008,""select * from  oscon_new_1  where CUST_CITY=""""CC015"""" and CUST_PRFRD_FLG=""""Y"""" and CUST_LAST_RVW_DATE between """"2011-02-02 00:00:00"""" and """"2011-02-03 00:00:00""""""Error :WARN TaskSetManager: Lost task 0.0 in stage 69.0 (TID 24007, hadoop-slave-7, executor 8): java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.NullPointerException at org.apache.carbondata.core.scan.processor.AbstractDataBlockIterator.updateScanner(AbstractDataBlockIterator.java:136) at org.apache.carbondata.core.scan.processor.impl.DataBlockIteratorImpl.next(DataBlockIteratorImpl.java:50) at org.apache.carbondata.core.scan.processor.impl.DataBlockIteratorImpl.next(DataBlockIteratorImpl.java:32) at org.apache.carbondata.core.scan.result.iterator.DetailQueryResultIterator.getBatchResult(DetailQueryResultIterator.java:50) at org.apache.carbondata.core.scan.result.iterator.DetailQueryResultIterator.next(DetailQueryResultIterator.java:41) at org.apache.carbondata.core.scan.result.iterator.DetailQueryResultIterator.next(DetailQueryResultIterator.java:31) at org.apache.carbondata.core.scan.result.iterator.ChunkRowIterator.<init>(ChunkRowIterator.java:41) at org.apache.carbondata.hadoop.CarbonRecordReader.initialize(CarbonRecordReader.java:79) at org.apache.carbondata.spark.rdd.CarbonScanRDD.compute(CarbonScanRDD.scala:204) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) at org.apache.spark.rdd.RDD.iterator(RDD.scala:287) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) at org.apache.spark.rdd.RDD.iterator(RDD.scala:287) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) at org.apache.spark.rdd.RDD.iterator(RDD.scala:287) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) at org.apache.spark.rdd.RDD.iterator(RDD.scala:287) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) at org.apache.spark.rdd.RDD.iterator(RDD.scala:287) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87) at org.apache.spark.scheduler.Task.run(Task.scala:99) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745)Caused by: java.util.concurrent.ExecutionException: java.lang.NullPointerException at java.util.concurrent.FutureTask.report(FutureTask.java:122) at java.util.concurrent.FutureTask.get(FutureTask.java:192) at org.apache.carbondata.core.scan.processor.AbstractDataBlockIterator.getNextScannedResult(AbstractDataBlockIterator.java:146) at org.apache.carbondata.core.scan.processor.AbstractDataBlockIterator.updateScanner(AbstractDataBlockIterator.java:124) ... 28 moreNote : Data load file (CSV) is 32 GB so which can not be upload on the Jira.
issueID:CARBONDATA-826
type:Sub-task
changed files:integration/presto/src/main/java/org/apache/carbondata/presto/CarbondataModule.java
integration/presto/src/main/java/org/apache/carbondata/presto/CarbondataConnectorFactory.java
integration/presto/src/main/java/org/apache/carbondata/presto/Types.java
integration/presto/src/main/java/org/apache/carbondata/presto/impl/CarbonTableConfig.java
integration/presto/src/main/java/org/apache/carbondata/presto/CarbondataColumnConstraint.java
integration/presto/src/main/java/org/apache/carbondata/presto/impl/CarbonTableCacheModel.java
integration/presto/src/main/java/org/apache/carbondata/presto/CarbondataPlugin.java
integration/presto/src/main/java/org/apache/carbondata/presto/impl/CarbonLocalInputSplit.java
integration/presto/src/main/java/org/apache/carbondata/presto/impl/CarbonTableReader.java
integration/presto/src/main/java/org/apache/carbondata/presto/CarbondataSplitManager.java
texts:Create carbondata-connector for query carbon data in presto
1.In CarbonData project, generate carbondata-connector of presto2.Copy carbondata-connector to presto/plugin/3.Run query in presto to read carbon data.
issueID:CARBONDATA-827
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/stats/QueryStatisticsRecorderImpl.java
texts:Query statistics log format is incorrect
The output log for query statistics has repeated numbers which is incorrect
issueID:CARBONDATA-828
type:Bug
changed files:
texts:Fix length issue of model.dimensions in CarbonGlobalDictionaryGenerateRDD
org.apache.carbondata.spark.rdd.CarbonGlobalDictionaryGenerateRDD find a bug:    code line 378-380if (model.isFirstLoad && model.highCardIdentifyEnable&& !model.isComplexes(split.index)&& model.dimensions(split.index).isColumnar) {model.dimensions(split.index).isColumnar must change to  model.primDimensions(split.index).isColumnar because model.isComplexes.length may be != model.dimensions.length  when create table use DICTIONARY_EXCLUDE or column datatype is complex
issueID:CARBONDATA-829
type:Bug
changed files:
texts:DICTIONARY_EXCLUDE is not working when using Spark Datasource DDL
When creating table for TCP-H, found that following operation will failcreate table car(                                                          L_SHIPDATE string,L_SHIPMODE string,L_SHIPINSTRUCT string,L_RETURNFLAG string,L_RECEIPTDATE string,L_ORDERKEY string,L_PARTKEY string,L_SUPPKEY   string,L_LINENUMBER int,L_QUANTITY decimal,L_EXTENDEDPRICE decimal,L_DISCOUNT decimal,L_TAX decimal,L_LINESTATUS string,L_COMMITDATE string,L_COMMENT  string                                        ) USING org.apache.spark.sql.CarbonSourceOPTIONS (tableName "car", DICTIONARY_EXCLUDE "L_ORDERKEY, L_PARTKEY, L_SUPPKEY, L_COMMENT");
issueID:CARBONDATA-83
type:Improvement
changed files:
texts:please support carbon-spark-sql CLI options
exmaple: -d,    --define <key=value>          Variable subsitution to apply to hive                                  commands. e.g. -d A=B or --define A=B    --database <databasename>     Specify the database to use -e <quoted-query-string>         SQL from command line -f <filename>                    SQL from files -H,    --help                        Print help information    --hiveconf <property=value>   Use value for given property    --hivevar <key=value>         Variable subsitution to apply to hive                                  commands. e.g. --hivevar A=B -i <filename>                    Initialization SQL file -S,    --silent                      Silent mode in interactive shell -v,    --verbose                     Verbose mode (echo executed SQL to the                                  console)
issueID:CARBONDATA-830
type:Bug
changed files:
texts:Incorrect schedule for NewCarbonDataLoadRDD
Currently NewCarbonDataLoadRDD's getPreferredLocations will return all locs rather than 1, then on Spark may pick the same node for two tasks, so one node is getting over loaded with the task and one has no task to do, and impacting the performance despite of any failure.
issueID:CARBONDATA-832
type:Bug
changed files:processing/src/main/java/org/apache/carbondata/processing/loading/parser/impl/RowParserImpl.java
core/src/main/java/org/apache/carbondata/core/util/DataTypeUtil.java
texts:Data loading is failing with duplicate header column in csv file
Problem : data mismatch issue when csv column having duplicate column header.Solution: row parser impl logic of getting indexes is having issue
issueID:CARBONDATA-834
type:Bug
changed files:
texts:Describe Table in Presto gives incorrect order of columns
Describe Table in Presto gives incorrect order of columns
issueID:CARBONDATA-835
type:Bug
changed files:
texts:Null values in carbon table gives a NullPointerException when querying from Presto
Null values in carbon table gives a NullPointerException when querying from Presto
issueID:CARBONDATA-837
type:Bug
changed files:
texts:Unable to delete records from carbondata table
As per below document I am trying to delete entries from the table :https://github.com/apache/incubator-carbondata/blob/master/docs/dml-operation-on-carbondata.mdscala> cc.sql("select * from accountentity").countres10: Long = 391351scala> cc.sql("delete from accountentity")INFO  30-03 09:03:03,099 - main Query &#91;DELETE FROM ACCOUNTENTITY&#93;INFO  30-03 09:03:03,104 - Parsing command: select tupleId from accountentityINFO  30-03 09:03:03,104 - Parse CompletedINFO  30-03 09:03:03,105 - Parsing command: select tupleId from accountentityINFO  30-03 09:03:03,105 - Parse Completedres11: org.apache.spark.sql.DataFrame = []scala> cc.sql("select * from accountentity").countres12: Long = 391351The records gets deleted only when an action such as show() is applied. scala> cc.sql("delete from accountentity").show
issueID:CARBONDATA-838
type:Bug
changed files:
texts:Alter table add decimal column with default precision and scale is failing in parser.
When we add new decimal column without specifying scale and precision, alter table command is failing in parser.eg., alter table test1 add columns(dcmlcol decimal)
issueID:CARBONDATA-839
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/locks/AbstractCarbonLock.java
core/src/main/java/org/apache/carbondata/core/locks/HdfsFileLock.java
core/src/main/java/org/apache/carbondata/core/locks/LocalFileLock.java
core/src/main/java/org/apache/carbondata/core/locks/ICarbonLock.java
texts:Table lock file is not getting deleted after table rename is successful
Table lock file is not getting deleted after table rename is successful
issueID:CARBONDATA-84
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/util/path/CarbonTablePath.java
texts:Change Locking framework to suit database related locks
Current lock frame is table specific. If we need lock for database level files, it doesn't provide so.
issueID:CARBONDATA-841
type:Sub-task
changed files:
texts:improve the compress encoding for numeric type column to give good performance
Now no-dictionary column use LV(length-value) encoding. It isn't the best choice for numeric type column.
issueID:CARBONDATA-842
type:Sub-task
changed files:processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactDataHandlerModel.java
processing/src/main/java/org/apache/carbondata/processing/merger/CarbonDataMergerUtil.java
processing/src/main/java/org/apache/carbondata/processing/loading/steps/CarbonRowDataWriterProcessorStepImpl.java
processing/src/main/java/org/apache/carbondata/processing/store/CarbonDataFileAttributes.java
processing/src/main/java/org/apache/carbondata/processing/store/writer/AbstractFactDataWriter.java
processing/src/main/java/org/apache/carbondata/processing/loading/DataLoadProcessBuilder.java
processing/src/main/java/org/apache/carbondata/processing/loading/model/CarbonLoadModel.java
texts:when SORT_COLUMN is empty, no need to sort data.

issueID:CARBONDATA-843
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/scan/collector/impl/AbstractScannedResultCollector.java
texts:null pointer exception is thrown when floor operation is done on decimal column

issueID:CARBONDATA-845
type:Bug
changed files:
texts:Insert Select into same table is not working
Insert Select from same table is not working in Spark-2.1. Insert into table1 select * from table1 giving errorError: org.apache.spark.sql.AnalysisException: Cannot insert overwrite into table that is also being read from.
issueID:CARBONDATA-846
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/metadata/converter/ThriftWrapperSchemaConverterImpl.java
core/src/main/java/org/apache/carbondata/core/util/DataTypeUtil.java
texts:Add support to revert changes to alter table commands if there is a failure while executing the changes on hive.

issueID:CARBONDATA-847
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/scan/collector/impl/RestructureBasedVectorResultCollector.java
core/src/main/java/org/apache/carbondata/core/scan/collector/impl/RestructureBasedRawResultCollector.java
texts:Select query not working properly after alter.
Execute these set of queries: CREATE TABLE uniqdata (CUST_ID int,CUST_NAME String,ACTIVE_EMUI_VERSION string, DOB timestamp, DOJ timestamp, BIGINT_COLUMN1 bigint,BIGINT_COLUMN2 bigint,DECIMAL_COLUMN1 decimal(30,10), DECIMAL_COLUMN2 decimal(36,10),Double_COLUMN1 double, Double_COLUMN2 double,INTEGER_COLUMN1 int) STORED BY 'org.apache.carbondata.format' TBLPROPERTIES ("TABLE_BLOCKSIZE"= "256 MB");LOAD DATA INPATH 'HDFS_URL/BabuStore/Data/uniqdata/2000_UniqData.csv' into table uniqdata OPTIONS('DELIMITER'=',' , 'QUOTECHAR'='"','BAD_RECORDS_ACTION'='FORCE','FILEHEADER'='CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1,Double_COLUMN2,INTEGER_COLUMN1');ALTER TABLE uniqdata RENAME TO uniqdata1;alter table uniqdata1 add columns(dict int) TBLPROPERTIES('DICTIONARY_INCLUDE'='dict','DEFAULT.VALUE.dict'= '9999');select distinct(dict) from uniqdata2 ;it will display the result but when we perform :select * from uniqdata1 ;it will display an error message :Job aborted due to stage failure: Task 3 in stage 59.0 failed 1 times, most recent failure: Lost task 3.0 in stage 59.0 (TID 714, localhost, executor driver): java.lang.NullPointerException
issueID:CARBONDATA-848
type:Bug
changed files:
texts:Select count(*) from table gives an exception in Presto
The select count is giving an ArrayIndexOutOfException in Presto connector.
issueID:CARBONDATA-849
type:Bug
changed files:
texts:if alter table ddl is executed on non existing table, then error message is wrong.
The error message getting while running alter on the non existing table is : Exception in thread "main" org.apache.carbondata.spark.exception.MalformedCarbonCommandException: Unsupported alter operation on hive tablebut this is not correct. The hive table has blocked the alter DDL on its tables. So Carbon should be consistent with HIVE.Correct msg : Operation not allowed: alter table name compact 'minor'
issueID:CARBONDATA-85
type:Wish
changed files:
texts:please support insert into carbon table from other format table
exmaple:insert into table2 select * from table1table2 is stored by org.apache.carbondata.format, and table 1 is a parquet table
issueID:CARBONDATA-850
type:Bug
changed files:
texts:Fix the comment definition issues of CarbonData thrift files
Fix the comment definition issues of CarbonData thrift files, for helping users to easier understand CarbonData file format
issueID:CARBONDATA-851
type:Bug
changed files:
texts:Incorrect result displays while range filter query.
Incorrect result displays to a user while use Greater then or equal to (>=) operator.Steps to Reproduces:1:Create table :CREATE TABLE uniqdata (CUST_ID int,CUST_NAME String,ACTIVE_EMUI_VERSION string, DOB timestamp, DOJ timestamp, BIGINT_COLUMN1 bigint,BIGINT_COLUMN2 bigint,DECIMAL_COLUMN1 decimal(30,10), DECIMAL_COLUMN2 decimal(36,10),Double_COLUMN1 double, Double_COLUMN2 double,INTEGER_COLUMN1 int) STORED BY 'org.apache.carbondata.format' TBLPROPERTIES ("TABLE_BLOCKSIZE"= "256 MB");2:Load Data:LOAD DATA INPATH 'HDFS_URL/BabuStore/Data/uniqdata/2000_UniqData.csv' into table uniqdata OPTIONS('DELIMITER'=',' , 'QUOTECHAR'='"','BAD_RECORDS_ACTION'='FORCE','FILEHEADER'='CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1,Double_COLUMN2,INTEGER_COLUMN1');3:Run the Query:select dob from uniqdata where dob <= '1972-12-10 01:00:03.0' and dob >= '1972-12-01 01:00:03.0';Result:------------------------+          dob           ------------------------+ 1972-12-02 01:00:03.0   1972-12-03 01:00:03.0   1972-12-04 01:00:03.0   1972-12-05 01:00:03.0   1972-12-06 01:00:03.0   1972-12-07 01:00:03.0   1972-12-08 01:00:03.0   1972-12-09 01:00:03.0   1972-12-10 01:00:03.0  ------------------------+Expected Result: it should include " 1972-12-01 01:00:03.0 " in the result set.
issueID:CARBONDATA-853
type:Bug
changed files:
texts:Less than or equal to operator(<=) does not work properly in Range Filter.
Less than or equal (<=) to operator does not work properly in range filter.Steps to reproduces:1)Create table:CREATE TABLE uniqdata (CUST_ID int,CUST_NAME String,ACTIVE_EMUI_VERSION string, DOB timestamp, DOJ timestamp, BIGINT_COLUMN1 bigint,BIGINT_COLUMN2 bigint,DECIMAL_COLUMN1 decimal(30,10), DECIMAL_COLUMN2 decimal(36,10),Double_COLUMN1 double, Double_COLUMN2 double,INTEGER_COLUMN1 int) STORED BY 'org.apache.carbondata.format' TBLPROPERTIES ("TABLE_BLOCKSIZE"= "256 MB");2)Load Data in a table:LOAD DATA INPATH 'HDFS_URL/BabuStore/Data/uniqdata/2000_UniqData.csv' into table uniqdata OPTIONS('DELIMITER'=',' , 'QUOTECHAR'='"','BAD_RECORDS_ACTION'='FORCE','FILEHEADER'='CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1,Double_COLUMN2,INTEGER_COLUMN1');3: Run the Query.select dob from uniqdata where dob <= '1972-12-10' and dob >= '1972-12-01';4:Result on beeline:------------------------+          dob           ------------------------+ 1972-12-01 01:00:03.0   1972-12-02 01:00:03.0   1972-12-03 01:00:03.0   1972-12-04 01:00:03.0   1972-12-05 01:00:03.0   1972-12-06 01:00:03.0   1972-12-07 01:00:03.0   1972-12-08 01:00:03.0   1972-12-09 01:00:03.0  ------------------------+Expected Result: It should include " 1972-12-10 " in the result set.
issueID:CARBONDATA-856
type:Sub-task
changed files:
texts:[Documentation] Alter Table -  TABLE RENAME
Documentation for TABLE RENAMEShould include the following:Function/DescriptionSyntaxParameter DescriptionUsage GuidelinesExample(s)
issueID:CARBONDATA-857
type:Sub-task
changed files:
texts:[Documentation] Alter Table - ADD COLUMNS
Documentation for ADD COLUMNSShould include the following:Function/DescriptionSyntaxParameter DescriptionUsage GuidelinesExample(s)
issueID:CARBONDATA-86
type:Bug
changed files:
texts:Value displayed as Null after increase in precision for decimal datatype after aggregation
While creating a table if user specifies a precision for decimal datatype column and then executes an aggregation query on that column then in case if after aggregation the precision of the resulted value becomes greater than the user configure precision value, spark displays the aggregated value as null
issueID:CARBONDATA-860
type:Bug
changed files:
texts:Carbon with Spark2.1, select query with filter on dictionary column & order by dictionary/measure with limit is failing
Select queries are failing in Spark 2.1 with ClassCastException for dictionary Columns (UTF8String cannot be cast to Int)eg., CREATE TABLE alldatatypestablesort (empno int, empname String, designation String, doj Timestamp, workgroupcategory int, workgroupcategoryname String, deptno int, deptname String, projectcode int, projectjoindate Timestamp, projectenddate Timestamp,attendance int,utilization int,salary int) STORED BY 'org.apache.carbondata.format'Below 3 queries are failing.,select * from alldatatypestablesort order by empname limit 2select * from alldatatypestablesort order by salary limit 2select * from alldatatypestablesort where empname='arvind' order by salary limit 2
issueID:CARBONDATA-862
type:Bug
changed files:
texts:USE_KETTLE option described in dml-operation-on-carbondata.md document doesn&#39;t work

issueID:CARBONDATA-863
type:Improvement
changed files:
texts:Support creation and deletion of dictionary files through RDD during alter add and drop
Currently during alter add and drop columns operation, dictionary files for columns are being added and dropped in a single thread due to which the operation becomes very slow as the number of columns increases.Instead this operation should be done through RDD which will make optimal use of executor cores configured and performance will increase by number of cores configured.
issueID:CARBONDATA-864
type:Bug
changed files:
texts:After adding column using alter query, when we put any column in "Dictionary Exclude" then perform select query on that column then it will throws an exception.
CREATE TABLE uniqdata (CUST_ID int,CUST_NAME String,ACTIVE_EMUI_VERSION string, DOB timestamp, DOJ timestamp, BIGINT_COLUMN1 bigint,BIGINT_COLUMN2 bigint,DECIMAL_COLUMN1 decimal(30,10), DECIMAL_COLUMN2 decimal(36,10),Double_COLUMN1 double, Double_COLUMN2 double,INTEGER_COLUMN1 int) STORED BY 'org.apache.carbondata.format' TBLPROPERTIES ("TABLE_BLOCKSIZE"= "256 MB");LOAD DATA INPATH 'HDFS_URL/BabuStore/Data/uniqdata/2000_UniqData.csv' into table uniqdata OPTIONS('DELIMITER'=',' , 'QUOTECHAR'='"','BAD_RECORDS_ACTION'='FORCE','FILEHEADER'='CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1,Double_COLUMN2,INTEGER_COLUMN1');ALTER TABLE uniqdata RENAME TO uniqdata1;alter table uniqdata1 add columns(dict int) TBLPROPERTIES('DICTIONARY_INCLUDE'='dict','DEFAULT.VALUE.dict'= '9999');select distinct(dict) from uniqdata1 ;when we perform select query on "DICTIONARY_INCLUDE" it is workingbut when we perform select query on "DICTIONARY_EXCLUDE" then it will throws an exception:alter table uniqdata1 add columns(nodict string) TBLPROPERTIES('DICTIONARY_EXCLUDE'='nodict', 'DEFAULT.VALUE.NoDict'= 'abcd');select distinct(nodict) from uniqdata1 ;0: jdbc:hive2://192.168.2.126:10000> select distinct(nodict) from uniqdata1 ;Error: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 21.0 failed 1 times, most recent failure: Lost task 0.0 in stage 21.0 (TID 419, localhost, executor driver): java.lang.ArrayIndexOutOfBoundsException: 4186 at org.apache.spark.sql.execution.vectorized.OnHeapColumnVector.putByteArray(OnHeapColumnVector.java:401) at org.apache.spark.sql.execution.vectorized.ColumnVector.putByteArray(ColumnVector.java:569) at org.apache.carbondata.spark.vectorreader.ColumnarVectorWrapper.putBytes(ColumnarVectorWrapper.java:77) at org.apache.carbondata.spark.vectorreader.ColumnarVectorWrapper.putBytes(ColumnarVectorWrapper.java:83) at org.apache.carbondata.core.scan.collector.impl.RestructureBasedVectorResultCollector.fillNoDictionaryData(RestructureBasedVectorResultCollector.java:167) at org.apache.carbondata.core.scan.collector.impl.RestructureBasedVectorResultCollector.fillDataForNonExistingDimensions(RestructureBasedVectorResultCollector.java:122) at org.apache.carbondata.core.scan.collector.impl.RestructureBasedVectorResultCollector.collectVectorBatch(RestructureBasedVectorResultCollector.java:97) at org.apache.carbondata.core.scan.processor.impl.DataBlockIteratorImpl.processNextBatch(DataBlockIteratorImpl.java:65) at org.apache.carbondata.core.scan.result.iterator.VectorDetailQueryResultIterator.processNextBatch(VectorDetailQueryResultIterator.java:46) at org.apache.carbondata.spark.vectorreader.VectorizedCarbonRecordReader.nextBatch(VectorizedCarbonRecordReader.java:246) at org.apache.carbondata.spark.vectorreader.VectorizedCarbonRecordReader.nextKeyValue(VectorizedCarbonRecordReader.java:140) at org.apache.carbondata.spark.rdd.CarbonScanRDD$$anon$1.hasNext(CarbonScanRDD.scala:222) at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.scan_nextBatch$(Unknown Source) at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.agg_doAggregateWithKeys$(Unknown Source) at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source) at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43) at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377) at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408) at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:126) at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96) at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53) at org.apache.spark.scheduler.Task.run(Task.scala:99) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
issueID:CARBONDATA-865
type:Bug
changed files:
texts:Remove configurations for Kettle from master/docs/installation-guide.md
Actual Result:Configurations  displays for Kettle  under installation-guide.md file.Expected Result: Remove configurations for Kettle from master/docs/installation-guide.md file.
issueID:CARBONDATA-866
type:Bug
changed files:
texts:remove kettle configuration from master/docs/configuration-parameters.md
remove kettle configuration from master/docs/configuration-parameters.md files.
issueID:CARBONDATA-868
type:Bug
changed files:
texts:Select query on decimal datatype is not working fine after adding decimal column using alter
CREATE TABLE uniqdata (CUST_ID int,CUST_NAME String,ACTIVE_EMUI_VERSION string, DOB timestamp, DOJ timestamp, BIGINT_COLUMN1 bigint,BIGINT_COLUMN2 bigint,DECIMAL_COLUMN1 decimal(30,10), DECIMAL_COLUMN2 decimal(36,10),Double_COLUMN1 double, Double_COLUMN2 double,INTEGER_COLUMN1 int) STORED BY 'org.apache.carbondata.format' TBLPROPERTIES ("TABLE_BLOCKSIZE"= "256 MB");LOAD DATA INPATH 'HDFS_URL/BabuStore/Data/uniqdata/2000_UniqData.csv' into table uniqdata OPTIONS('DELIMITER'=',' , 'QUOTECHAR'='"','BAD_RECORDS_ACTION'='FORCE','FILEHEADER'='CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1,Double_COLUMN2,INTEGER_COLUMN1');ALTER TABLE uniqdata RENAME TO uniqdata1;alter table uniqdata1 add columns(msrField decimal(5,2))TBLPROPERTIES('DEFAULT.VALUE.msrfield'= '123.45');0: jdbc:hive2://192.168.2.126:10000> select msrField from uniqdata1;Error: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 48.0 failed 1 times, most recent failure: Lost task 0.0 in stage 48.0 (TID 1041, localhost, executor driver): java.lang.ArrayIndexOutOfBoundsException: 4186 at org.apache.spark.sql.execution.vectorized.OnHeapColumnVector.putInt(OnHeapColumnVector.java:202) at org.apache.spark.sql.execution.vectorized.ColumnVector.putDecimal(ColumnVector.java:608) at org.apache.carbondata.spark.vectorreader.ColumnarVectorWrapper.putDecimal(ColumnarVectorWrapper.java:58) at org.apache.carbondata.spark.vectorreader.ColumnarVectorWrapper.putDecimals(ColumnarVectorWrapper.java:64) at org.apache.carbondata.core.scan.collector.impl.RestructureBasedVectorResultCollector.fillDataForNonExistingMeasures(RestructureBasedVectorResultCollector.java:202) at org.apache.carbondata.core.scan.collector.impl.RestructureBasedVectorResultCollector.collectVectorBatch(RestructureBasedVectorResultCollector.java:98) at org.apache.carbondata.core.scan.processor.impl.DataBlockIteratorImpl.processNextBatch(DataBlockIteratorImpl.java:65) at org.apache.carbondata.core.scan.result.iterator.VectorDetailQueryResultIterator.processNextBatch(VectorDetailQueryResultIterator.java:46) at org.apache.carbondata.spark.vectorreader.VectorizedCarbonRecordReader.nextBatch(VectorizedCarbonRecordReader.java:246) at org.apache.carbondata.spark.vectorreader.VectorizedCarbonRecordReader.nextKeyValue(VectorizedCarbonRecordReader.java:140) at org.apache.carbondata.spark.rdd.CarbonScanRDD$$anon$1.hasNext(CarbonScanRDD.scala:222) at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.scan_nextBatch$(Unknown Source) at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source) at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43) at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377) at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:231) at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:225) at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:826) at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:826) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) at org.apache.spark.rdd.RDD.iterator(RDD.scala:287) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87) at org.apache.spark.scheduler.Task.run(Task.scala:99) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745)Driver stacktrace: (state=,code=0)
issueID:CARBONDATA-869
type:Bug
changed files:
texts:Greater than operator(>) does not display correct result for some attribute in range filter
Greater than operator(>) does not display accurate result for some attributes in  range filterSteps to reproduces:1)Create table:CREATE TABLE uniqdata (CUST_ID int,CUST_NAME String,ACTIVE_EMUI_VERSION string, DOB timestamp, DOJ timestamp, BIGINT_COLUMN1 bigint,BIGINT_COLUMN2 bigint,DECIMAL_COLUMN1 decimal(30,10), DECIMAL_COLUMN2 decimal(36,10),Double_COLUMN1 double, Double_COLUMN2 double,INTEGER_COLUMN1 int) STORED BY 'org.apache.carbondata.format' TBLPROPERTIES ("TABLE_BLOCKSIZE"= "256 MB");2)Load Data in a table:LOAD DATA INPATH 'HDFS_URL/BabuStore/Data/uniqdata/2000_UniqData.csv' into table uniqdata OPTIONS('DELIMITER'=',' , 'QUOTECHAR'='"','BAD_RECORDS_ACTION'='FORCE','FILEHEADER'='CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1,Double_COLUMN2,INTEGER_COLUMN1');3: Run the Query.select dob  from uniqdata where dob > '1974-12-25' and dob <  '1975';4:Result on beeline:------------------------+          dob           ------------------------+ 1974-12-25 01:00:03.0   1974-12-26 01:00:03.0   1974-12-27 01:00:03.0   1974-12-28 01:00:03.0   1974-12-29 01:00:03.0   1974-12-30 01:00:03.0   1974-12-31 01:00:03.0  ------------------------+7 rows selected (0.132 seconds)Expected Result: resultset should include dates from "1974-12-26 to 1974-12-31" and exclude "1974-12-25"
issueID:CARBONDATA-87
type:Bug
changed files:integration/spark-common/src/main/java/org/apache/carbondata/spark/load/CarbonLoaderUtil.java
texts:Temp files not getting deleted
Temp files which are created during the data load is not getting deleted if the data load is failure. it is being deleted in the success case only.
issueID:CARBONDATA-870
type:Bug
changed files:processing/src/main/java/org/apache/carbondata/processing/util/CarbonLoaderUtil.java
texts:Folders and files not getting cleaned up created locally during data load operation
Folders and files which are created in local temp store location during data load and insert into operations are not getting cleaned up. After some time this will lead to filling up of local disk space and eventually will lead to data load failure if threshold limit is reached.For this all the folders and files created locally need to be deleted once the operation is completed.
issueID:CARBONDATA-871
type:Bug
changed files:
texts:If locktype is not configured and store type is HDFS set HDFS lock as default
if locktype is not configured and store type is HDFS set HDFS lock as default
issueID:CARBONDATA-872
type:Bug
changed files:integration/presto/src/main/java/org/apache/carbondata/presto/CarbondataConnectorFactory.java
integration/presto/src/main/java/org/apache/carbondata/presto/impl/CarbonTableConfig.java
integration/presto/src/main/java/org/apache/carbondata/presto/CarbondataColumnConstraint.java
integration/presto/src/main/java/org/apache/carbondata/presto/impl/CarbonTableCacheModel.java
integration/presto/src/main/java/org/apache/carbondata/presto/impl/CarbonTableReader.java
integration/presto/src/main/java/org/apache/carbondata/presto/CarbondataSplitManager.java
texts:Fix comment issues of integration/presto for easier reading
Fix comment issues of integration/presto for easier reading
issueID:CARBONDATA-873
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/locks/HdfsFileLock.java
texts:Drop table command throwing table already exists exception
Drop table command throwing table already exists exception.
issueID:CARBONDATA-874
type:Bug
changed files:
texts:select * from table order by limit query is failing
Query like below are failing in carbon with spark 2.1select * from alldatatypestablesort order by empname limit 10
issueID:CARBONDATA-875
type:Bug
changed files:
texts:create database ddl is creating the database folder with case sensitive name.
Create database DBNAME.  here the database name should be case insensitive.
issueID:CARBONDATA-877
type:Bug
changed files:integration/spark-datasource/src/main/scala/org/apache/carbondata/spark/vectorreader/ColumnarVectorWrapper.java
texts:String datatype is throwing an error when included in DIctionary_Exclude in a alter query
CREATE TABLE uniqdata (CUST_ID int,CUST_NAME String,ACTIVE_EMUI_VERSION string, DOB timestamp, DOJ timestamp, BIGINT_COLUMN1 bigint,BIGINT_COLUMN2 bigint,DECIMAL_COLUMN1 decimal(30,10), DECIMAL_COLUMN2 decimal(36,10),Double_COLUMN1 double, Double_COLUMN2 double,INTEGER_COLUMN1 int) STORED BY 'org.apache.carbondata.format' TBLPROPERTIES ("TABLE_BLOCKSIZE"= "256 MB");LOAD DATA INPATH 'HDFS_URL/BabuStore/Data/uniqdata/2000_UniqData.csv' into table uniqdata OPTIONS('DELIMITER'=',' , 'QUOTECHAR'='"','BAD_RECORDS_ACTION'='FORCE','FILEHEADER'='CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1,Double_COLUMN2,INTEGER_COLUMN1');ALTER TABLE uniqdata RENAME TO uniqdata1;alter table uniqdata1 drop columns(CUST_NAME);alter table uniqdata1 add columns(CUST_NAME string) TBLPROPERTIES('DICTIONARY_EXCLUDE'='CUST_NAME', 'DEFAULT.VALUE.CUST_NAME'='testuser') ;Column added successfully. But when we execute:select distinct(CUST_NAME) from uniqdata1 ; &select count(CUST_NAME) from uniqdata1 ;it throws an error :"Job aborted due to stage failure: Task 0 in stage 9.0 failed 1 times, most recent failure: Lost task 0.0 in stage 9.0 (TID 206, localhost, executor driver): java.lang.ArrayIndexOutOfBoundsException: 4186"
issueID:CARBONDATA-878
type:Improvement
changed files:
texts:Inconsistent stylin in quick-start.md file
https://github.com/apache/incubator-carbondata/pull/747
issueID:CARBONDATA-88
type:Bug
changed files:
texts:Use ./bin/carbon-spark-shell to run, generated two issues
Use ./bin/carbon-spark-shell to run, generated two issues:1. The carbonshellstore be created under root directory, propose to move carbonshellstore to ./bin directory2.Data load failure:scala> cc.sql("LOAD DATA LOCAL INPATH '/Users/apple/Downloads/spark-1.6.1-bin-hadoop2.6/carbondata/hzmeetup.csv' INTO TABLE meetupTable")java.lang.Exception: Dataload failure at org.carbondata.spark.rdd.CarbonDataRDDFactory$.loadCarbonData(CarbonDataRDDFactory.scala:791) at org.apache.spark.sql.execution.command.LoadTable.run(carbonTableSchema.scala:1167) at org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult$lzycompute(commands.scala:58) at org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult(commands.scala:56) at org.apache.spark.sql.execution.ExecutedCommand.doExecute(commands.scala:70) at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$5.apply(SparkPlan.scala:132) at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$5.apply(SparkPlan.scala:130) at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150) at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:130) at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:55) at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:55) at org.apache.spark.sql.DataFrame.<init>(DataFrame.scala:145) at org.apache.spark.sql.DataFrame.<init>(DataFrame.scala:130) at org.carbondata.spark.rdd.CarbonDataFrameRDD.<init>(CarbonDataFrameRDD.scala:23) at org.apache.spark.sql.CarbonContext.sql(CarbonContext.scala:131) at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:42)
issueID:CARBONDATA-880
type:Bug
changed files:
texts:when explain extended is done on a query then store path is getting printed to the user.
when explain extended is done on a query then store path is getting printed to the user in carbonhadooprelation. the path should not be shown to the user.example :CarbonDatasourceHadoopRelation(org.apache.spark.sql.CarbonSession@1ed6b3b9,[Ljava.lang.String;@21b14680,Map(path -> file:/D:/Carbon/incubator-carbondata/examples/spark2/target/warehouse/carbon_table, serialization.format -> 1, dbname -> default, tablepath -> D:Carbonincubator-carbondata/examples/spark2/target/storedefaultcarbon_table, tablename -> carbon_table),Some(StructType(StructField(shortField,ShortType,true), StructField(intField,IntegerType,true)
issueID:CARBONDATA-881
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/statusmanager/SegmentStatusManager.java
processing/src/main/java/org/apache/carbondata/processing/merger/CarbonDataMergerUtil.java
texts:Load status is successful even though system is fail to write status into tablestatus file
org.apache.carbondata.core.statusmanager.SegmentStatusManager.SegmentStatusManager is eating the IOException due to this even though when carbon is fail to write the load status in the tablestatus file, the final load is successful.
issueID:CARBONDATA-882
type:Improvement
changed files:
texts:Add SORT_COLUMNS option support in dataframe writer
User can should be able to specify SORT_COLUMNS option when using dataframe.write
issueID:CARBONDATA-883
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/scan/executor/util/RestructureUtil.java
core/src/main/java/org/apache/carbondata/core/scan/collector/impl/RestructureBasedVectorResultCollector.java
processing/src/main/java/org/apache/carbondata/processing/util/CarbonLoaderUtil.java
texts:Select query after alter is not working fine with integer datatype.
CREATE TABLE uniqdata (CUST_ID int,CUST_NAME String,ACTIVE_EMUI_VERSION string, DOB timestamp, DOJ timestamp, BIGINT_COLUMN1 bigint,BIGINT_COLUMN2 bigint,DECIMAL_COLUMN1 decimal(30,10), DECIMAL_COLUMN2 decimal(36,10),Double_COLUMN1 double, Double_COLUMN2 double,INTEGER_COLUMN1 int) STORED BY 'org.apache.carbondata.format' TBLPROPERTIES ("TABLE_BLOCKSIZE"= "256 MB");LOAD DATA INPATH 'HDFS_URL/BabuStore/Data/uniqdata/2000_UniqData.csv' into table uniqdata OPTIONS('DELIMITER'=',' , 'QUOTECHAR'='"','BAD_RECORDS_ACTION'='FORCE','FILEHEADER'='CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1,Double_COLUMN2,INTEGER_COLUMN1');ALTER TABLE uniqdata RENAME TO uniqdata1;0: jdbc:hive2://192.168.2.126:10000> alter table default.uniqdata1 add columns(ACTIVE_EMUI_VERSION int) TBLPROPERTIES('DEFAULT.VALUE.ACTIVE_EMUI_VERSION'='67890');---------+ Result  ---------+---------+No rows selected (0.265 seconds)0: jdbc:hive2://192.168.2.126:10000> select distinct(ACTIVE_EMUI_VERSION) from uniqdata1 ;Error: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 22.0 failed 1 times, most recent failure: Lost task 0.0 in stage 22.0 (TID 1211, localhost, executor driver): java.lang.ClassCastException: java.lang.Long cannot be cast to java.lang.Integer at org.apache.carbondata.core.scan.collector.impl.RestructureBasedVectorResultCollector.fillDataForNonExistingMeasures(RestructureBasedVectorResultCollector.java:193) at org.apache.carbondata.core.scan.collector.impl.RestructureBasedVectorResultCollector.collectVectorBatch(RestructureBasedVectorResultCollector.java:98) at org.apache.carbondata.core.scan.processor.impl.DataBlockIteratorImpl.processNextBatch(DataBlockIteratorImpl.java:65) at org.apache.carbondata.core.scan.result.iterator.VectorDetailQueryResultIterator.processNextBatch(VectorDetailQueryResultIterator.java:46) at org.apache.carbondata.spark.vectorreader.VectorizedCarbonRecordReader.nextBatch(VectorizedCarbonRecordReader.java:246) at org.apache.carbondata.spark.vectorreader.VectorizedCarbonRecordReader.nextKeyValue(VectorizedCarbonRecordReader.java:140) at org.apache.carbondata.spark.rdd.CarbonScanRDD$$anon$1.hasNext(CarbonScanRDD.scala:222) at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.scan_nextBatch$(Unknown Source) at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.agg_doAggregateWithKeys$(Unknown Source) at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source) at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43) at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377) at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408) at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:126) at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96) at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53) at org.apache.spark.scheduler.Task.run(Task.scala:99) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745)Driver stacktrace: (state=,code=0)0: jdbc:hive2://192.168.2.126:10000>
issueID:CARBONDATA-884
type:Improvement
changed files:
texts:[Documentation] information on assembly jar to be provided in Quick Start
In Quick start we have mentioned the below command:Start Spark shell by running the following command in the Spark directory:./bin/spark-shell --jars <carbondata assembly jar path>It is better to mention for the user from where to take the assembly jar.For example: the assembly jar will be present in the target folder when you build the project.
issueID:CARBONDATA-885
type:Bug
changed files:
texts:Inconsistent usage of " " in queries in ddl operations on Carbondata

issueID:CARBONDATA-886
type:Improvement
changed files:processing/src/main/java/org/apache/carbondata/processing/merger/CarbonCompactionUtil.java
core/src/main/java/org/apache/carbondata/core/scan/expression/ColumnExpression.java
core/src/main/java/org/apache/carbondata/core/mutate/CarbonUpdateUtil.java
core/src/main/java/org/apache/carbondata/core/keygenerator/mdkey/Bits.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelFilterExecuterImpl.java
core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
processing/src/main/java/org/apache/carbondata/processing/loading/steps/SortProcessorStepImpl.java
core/src/main/java/org/apache/carbondata/core/util/NonDictionaryUtil.java
core/src/main/java/org/apache/carbondata/core/datastore/impl/FileFactory.java
processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactDataHandlerModel.java
processing/src/main/java/org/apache/carbondata/processing/loading/steps/DataConverterProcessorStepImpl.java
core/src/main/java/org/apache/carbondata/core/scan/result/iterator/DetailQueryResultIterator.java
core/src/main/java/org/apache/carbondata/core/util/path/CarbonTablePath.java
core/src/main/java/org/apache/carbondata/core/scan/expression/LiteralExpression.java
processing/src/main/java/org/apache/carbondata/processing/merger/CarbonDataMergerUtil.java
processing/src/main/java/org/apache/carbondata/processing/util/CarbonDataProcessorUtil.java
core/src/main/java/org/apache/carbondata/core/reader/CarbonIndexFileReader.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/impl/ParallelReadMergeSorterWithBucketingImpl.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/holder/UnsafeSortTempFileChunkHolder.java
core/src/main/java/org/apache/carbondata/core/datastore/block/TableBlockUniqueIdentifier.java
core/src/main/java/org/apache/carbondata/core/keygenerator/directdictionary/timestamp/DateDirectDictionaryGenerator.java
processing/src/main/java/org/apache/carbondata/processing/loading/DataLoadProcessBuilder.java
processing/src/main/java/org/apache/carbondata/processing/merger/CompactionResultSortProcessor.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/SortTempFileChunkHolder.java
texts:remove all redundant local variable

issueID:CARBONDATA-887
type:Improvement
changed files:
texts:lazy rdd iterator for InsertInto

issueID:CARBONDATA-888
type:Improvement
changed files:
texts:Dictionary include / exclude option in dataframe writer
While creating a Carbondata table from dataframe, currently it is not possible to specify columns that needs to be included in or excluded from the dictionary. An option is required to specify it as below : df.write.format("carbondata")  .option("tableName", "test")  .option("compress","true")  .option("dictionary_include","incol1,intcol2")  .option("dictionary_exclude","stringcol1,stringcol2")  .mode(SaveMode.Overwrite).save()We have lot of integer columns that are dimensions, dataframe.save is used to quickly create tables instead of writing ddls, and it would be nice to have this feature to execute POCs.
issueID:CARBONDATA-890
type:Bug
changed files:
texts:For Spark 2.1 LRU cache size at driver is getting configured with the executor lru cache size.

issueID:CARBONDATA-891
type:Bug
changed files:
texts:Fix compilation issue of LocalFileLockTest generate new folder "carbon.store"
Fix compilation issue of LocalFileLockTest generate new folder "carbon.store"
issueID:CARBONDATA-892
type:Bug
changed files:
texts:IndexOutOf Bound exception while running query with 2nd level sub-query
select s_name, s_address from tpchcarbon_1.supplier where s_suppkey in (select ps_suppkey from tpchcarbon_1.partsupp where ps_partkey in (select p_partkey from tpchcarbon_1.part where p_name like 'forest%')) limit 10;Error: java.lang.IndexOutOfBoundsException: 0 (state=,code=0)
issueID:CARBONDATA-893
type:Bug
changed files:
texts:MR testcase hangs in Hadoop 2.7.2 version profile
MR testcase hangs in Hadoop 2.7.2 version profile
issueID:CARBONDATA-894
type:Sub-task
changed files:
texts:Add license header for vectorreader/AddColumnTestCases,ChangeDataTypeTestCases,DropColumnTestCases
Add license header for the three files :vectorreader/AddColumnTestCases,vectorreader/ChangeDataTypeTestCases,vectorreader/DropColumnTestCases
issueID:CARBONDATA-897
type:Bug
changed files:
texts:Redundant Fields Inside  * **Global Dictionary Configurations** in Configuration-parameters.md
In the Configuration-parameters.md file under the table Global Dictionary Configurations the row for field  high.cardinality.threshold has extra columns with redundant values in the md file.
issueID:CARBONDATA-898
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/metadata/schema/SchemaReader.java
texts:When select query and alter table rename table is triggered concurrently, NullPointerException is getting thrown
When user triggers select query and alter table rename table command concurrently, Select query is throwning NullPointerException if the files does not exist in hdfs.When dictionary file or schema file does not exist, File not found exception should be thrown
issueID:CARBONDATA-899
type:Sub-task
changed files:
texts:Added Support for DecimalType and Timestamp for spark-2.1 for integration/presto
Added Support for DecimalType and Timestamp for spark-2.1 for integration/presto.
issueID:CARBONDATA-9
type:Bug
changed files:
texts:Carbon data load bad record is not written into the bad record log file
Load csv having bad records, the row having bad columns should be logged into bad record file. The writing is failing due to FileNotFoundException: No lease on file.Enviroment:3 node clusterhaving three executors.
issueID:CARBONDATA-90
type:Bug
changed files:
texts:Struct of array query is execution is failing
Assignment of complex dimension ordinal was not correct as it was not increment and two children was getting same complex dimension ordinalSecond issue was selecting all the block indexes of complex dimension children.
issueID:CARBONDATA-900
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/scan/filter/FilterUtil.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RowLevelFilterExecuterImpl.java
texts:Is null query on a newly added measure column is not returning proper results
When is null query is executed on newly added measure column, control goes to RowLevelFilterExecuterImpl class, where measure existence is checked. In case the measure is not found, bitset group is not getting populated with default values due to which that block is not returning any result.Below queries can be executed to reproduce the issue:CREATE TABLE uniqdata110 (CUST_ID int,CUST_NAME String) STORED BY 'carbondata'LOAD DATA INPATH '<csvfilePath>' into table uniqdata110 OPTIONS('BAD_RECORDS_LOGGER_ENABLE'='TRUE', 'BAD_RECORDS_ACTION'='FORCE','FILEHEADER'='CUST_ID,CUST_NAME')ALTER TABLE uniqdata110  ADD COLUMNS (a6 int)LOAD DATA INPATH '<csvfilePath>' into table uniqdata110 OPTIONS('BAD_RECORDS_LOGGER_ENABLE'='TRUE', 'BAD_RECORDS_ACTION'='FORCE','FILEHEADER'='CUST_ID,CUST_NAME,a6')select * from uniqdata110select * from uniqdata110 where a6 is nullData:7,hello18,welcome1bye,11
issueID:CARBONDATA-901
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/datastore/block/SegmentProperties.java
texts:Fix some spelling mistakes
https://github.com/apache/incubator-carbondata/pull/782
issueID:CARBONDATA-903
type:Bug
changed files:processing/src/main/java/org/apache/carbondata/processing/loading/sort/impl/ParallelReadMergeSorterImpl.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/impl/ParallelReadMergeSorterWithBucketingImpl.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/impl/UnsafeParallelReadMergeSorterImpl.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/AbstractMergeSorter.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/merger/UnsafeSingleThreadFinalSortFilesMerger.java
texts:data load is not failing even though bad records exists in the data in case of unsafe sort or batch sort

issueID:CARBONDATA-904
type:Bug
changed files:
texts:ArrayIndexOutOfBoundsException
Or operator is not working properly.When we execute these query in hive it is working fine but when we execute the same in carbondata it throws an exception:java.lang.ArrayIndexOutOfBoundsExceptionHIVE:0: jdbc:hive2://hadoop-master:10000> create table Test_Boundary_h1 (c1_int int,c2_Bigint Bigint,c3_Decimal Decimal(38,30),c4_double double,c5_string string,c6_Timestamp Timestamp,c7_Datatype_Desc string) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' ;---------+ result  ---------+---------+No rows selected (1.177 seconds)0: jdbc:hive2://hadoop-master:10000> load data local inpath '/opt/Carbon/CarbonData/TestData/Data/Test_Data1_h1.csv' OVERWRITE INTO TABLE Test_Boundary_h1 ;---------+ Result  ---------+---------+No rows selected (0.437 seconds)0: jdbc:hive2://hadoop-master:10000> select c6_Timestamp,max(c6_Timestamp) from Test_Boundary_h1 where c6_Timestamp ='2017-07-01 12:07:28' or c6_Timestamp ='2019-07-05 13:07:30' or c6_Timestamp = '1999-01-06 10:05:29' group by c6_Timestamp ;----------------------------------------------      c6_Timestamp                _c1           ---------------------------------------------- 2017-07-01 12:07:28.0   2017-07-01 12:07:28.0  ----------------------------------------------1 row selected (1.637 seconds)CARBONDATA:0: jdbc:hive2://hadoop-master:10000> create table Test_Boundary (c1_int int,c2_Bigint Bigint,c3_Decimal Decimal(38,30),c4_double double,c5_string string,c6_Timestamp Timestamp,c7_Datatype_Desc string) STORED BY 'org.apache.carbondata.format' ;---------+ Result  ---------+---------+No rows selected (4.48 seconds)0: jdbc:hive2://hadoop-master:10000> LOAD DATA INPATH 'hdfs://192.168.2.145:54310/BabuStore/Data/Test_Data1.csv' INTO table Test_Boundary OPTIONS('DELIMITER'=',','QUOTECHAR'='','BAD_RECORDS_ACTION'='FORCE','FILEHEADER'='') ;---------+ Result  ---------+---------+No rows selected (4.445 seconds)0: jdbc:hive2://hadoop-master:10000> select c6_Timestamp,max(c6_Timestamp) from Test_Boundary where c6_Timestamp ='2017-07-01 12:07:28' or c6_Timestamp =' 2019-07-05 13:07:30' or c6_Timestamp = '1999-01-06 10:05:29' group by c6_Timestamp ;Error: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 5.0 failed 4 times, most recent failure: Lost task 0.3 in stage 5.0 (TID 8, hadoop-master): java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.ArrayIndexOutOfBoundsException: 0 at org.apache.carbondata.core.scan.processor.AbstractDataBlockIterator.updateScanner(AbstractDataBlockIterator.java:136) at org.apache.carbondata.core.scan.processor.impl.DataBlockIteratorImpl.next(DataBlockIteratorImpl.java:50) at org.apache.carbondata.core.scan.processor.impl.DataBlockIteratorImpl.next(DataBlockIteratorImpl.java:32) at org.apache.carbondata.core.scan.result.iterator.DetailQueryResultIterator.getBatchResult(DetailQueryResultIterator.java:50) at org.apache.carbondata.core.scan.result.iterator.DetailQueryResultIterator.next(DetailQueryResultIterator.java:41) at org.apache.carbondata.core.scan.result.iterator.DetailQueryResultIterator.next(DetailQueryResultIterator.java:31) at org.apache.carbondata.core.scan.result.iterator.ChunkRowIterator.<init>(ChunkRowIterator.java:41) at org.apache.carbondata.hadoop.CarbonRecordReader.initialize(CarbonRecordReader.java:79) at org.apache.carbondata.spark.rdd.CarbonScanRDD.compute(CarbonScanRDD.scala:204) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306) at org.apache.spark.rdd.RDD.iterator(RDD.scala:270) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306) at org.apache.spark.rdd.RDD.iterator(RDD.scala:270) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306) at org.apache.spark.rdd.RDD.iterator(RDD.scala:270) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306) at org.apache.spark.rdd.RDD.iterator(RDD.scala:270) at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73) at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41) at org.apache.spark.scheduler.Task.run(Task.scala:89) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745)Caused by: java.util.concurrent.ExecutionException: java.lang.ArrayIndexOutOfBoundsException: 0 at java.util.concurrent.FutureTask.report(FutureTask.java:122) at java.util.concurrent.FutureTask.get(FutureTask.java:192) at org.apache.carbondata.core.scan.processor.AbstractDataBlockIterator.getNextScannedResult(AbstractDataBlockIterator.java:146) at org.apache.carbondata.core.scan.processor.AbstractDataBlockIterator.updateScanner(AbstractDataBlockIterator.java:124) ... 26 moreCaused by: java.lang.ArrayIndexOutOfBoundsException: 0 at org.apache.carbondata.core.util.BitSetGroup.getBitSet(BitSetGroup.java:40) at org.apache.carbondata.core.util.BitSetGroup.or(BitSetGroup.java:68) at org.apache.carbondata.core.scan.filter.executer.OrFilterExecuterImpl.applyFilter(OrFilterExecuterImpl.java:40) at org.apache.carbondata.core.scan.filter.executer.OrFilterExecuterImpl.applyFilter(OrFilterExecuterImpl.java:38) at org.apache.carbondata.core.scan.scanner.impl.FilterScanner.fillScannedResult(FilterScanner.java:147) at org.apache.carbondata.core.scan.scanner.impl.FilterScanner.scanBlocklet(FilterScanner.java:92) at org.apache.carbondata.core.scan.processor.AbstractDataBlockIterator$1.call(AbstractDataBlockIterator.java:189) at org.apache.carbondata.core.scan.processor.AbstractDataBlockIterator$1.call(AbstractDataBlockIterator.java:176) at java.util.concurrent.FutureTask.run(FutureTask.java:266) ... 3 moreDriver stacktrace: (state=,code=0)
issueID:CARBONDATA-905
type:Bug
changed files:
texts:Unable to execute method public: org.apache.hadoop.hive.ql.metadata.HiveException
When we execute Same query in hive, it is working fine but when we execute in carbondata "org.apache.hadoop.hive.ql.metadata.HiveException" occurs. HIVE:0: jdbc:hive2://hadoop-master:10000> create table Test_Boundary_h1 (c1_int int,c2_Bigint Bigint,c3_Decimal Decimal(38,30),c4_double double,c5_string string,c6_Timestamp Timestamp,c7_Datatype_Desc string) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' ;---------+result---------+---------+No rows selected (1.177 seconds)0: jdbc:hive2://hadoop-master:10000> load data local inpath '/opt/Carbon/CarbonData/TestData/Data/Test_Data1_h1.csv' OVERWRITE INTO TABLE Test_Boundary_h1 ;---------+Result---------+---------+No rows selected (0.437 seconds)0: jdbc:hive2://hadoop-master:10000> select min(c1_int),max(c1_int),sum(c1_int),avg(c1_int) , count(c1_int), variance(c1_int) from Test_Boundary_h1 where rand(c1_int)=0.6201007799387834 or rand(c1_int)=0.45540022789662593 ;-------------------------------  _c0    _c1    _c2    _c3   _c4    _c5  ------------------------------- NULL   NULL   NULL   NULL   0     NULL  -------------------------------1 row selected (0.996 seconds)CARBONDATA:0: jdbc:hive2://hadoop-master:10000> create table Test_Boundary (c1_int int,c2_Bigint Bigint,c3_Decimal Decimal(38,30),c4_double double,c5_string string,c6_Timestamp Timestamp,c7_Datatype_Desc string) STORED BY 'org.apache.carbondata.format' ;---------+Result---------+---------+No rows selected (4.48 seconds)0: jdbc:hive2://hadoop-master:10000> LOAD DATA INPATH 'hdfs://192.168.2.145:54310/BabuStore/Data/Test_Data1.csv' INTO table Test_Boundary OPTIONS('DELIMITER'=',','QUOTECHAR'='','BAD_RECORDS_ACTION'='FORCE','FILEHEADER'='') ;---------+Result---------+---------+No rows selected (4.445 seconds)0: jdbc:hive2://hadoop-master:10000> select min(c1_int),max(c1_int),sum(c1_int),avg(c1_int) , count(c1_int), variance(c1_int) from Test_Boundary where rand(c1_int)=0.6201007799387834 or rand(c1_int)=0.45540022789662593 ;Error: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 19.0 failed 4 times, most recent failure: Lost task 0.3 in stage 19.0 (TID 826, hadoop-master): org.apache.hadoop.hive.ql.metadata.HiveException: Unable to execute method public org.apache.hadoop.hive.serde2.io.DoubleWritable org.apache.hadoop.hive.ql.udf.UDFRand.evaluate(org.apache.hadoop.io.LongWritable)  on object org.apache.hadoop.hive.ql.udf.UDFRand@3152da1e of class org.apache.hadoop.hive.ql.udf.UDFRand with arguments {null} of size 1 at org.apache.hadoop.hive.ql.exec.FunctionRegistry.invoke(FunctionRegistry.java:981) at org.apache.spark.sql.hive.HiveSimpleUDF.eval(hiveUDFs.scala:185) at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificPredicate.eval(Unknown Source) at org.apache.spark.sql.catalyst.expressions.codegen.GeneratePredicate$$anonfun$create$2.apply(GeneratePredicate.scala:68) at org.apache.spark.sql.catalyst.expressions.codegen.GeneratePredicate$$anonfun$create$2.apply(GeneratePredicate.scala:68) at org.apache.spark.sql.execution.Filter$$anonfun$2$$anonfun$apply$2.apply(basicOperators.scala:74) at org.apache.spark.sql.execution.Filter$$anonfun$2$$anonfun$apply$2.apply(basicOperators.scala:72) at scala.collection.Iterator$$anon$14.hasNext(Iterator.scala:390) at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327) at org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.processInputs(TungstenAggregationIterator.scala:504) at org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.<init>(TungstenAggregationIterator.scala:686) at org.apache.spark.sql.execution.aggregate.TungstenAggregate$$anonfun$doExecute$1$$anonfun$2.apply(TungstenAggregate.scala:95) at org.apache.spark.sql.execution.aggregate.TungstenAggregate$$anonfun$doExecute$1$$anonfun$2.apply(TungstenAggregate.scala:86) at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710) at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306) at org.apache.spark.rdd.RDD.iterator(RDD.scala:270) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306) at org.apache.spark.rdd.RDD.iterator(RDD.scala:270) at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73) at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41) at org.apache.spark.scheduler.Task.run(Task.scala:89) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745)Caused by: java.lang.reflect.InvocationTargetException at sun.reflect.GeneratedMethodAccessor46.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.hadoop.hive.ql.exec.FunctionRegistry.invoke(FunctionRegistry.java:957) ... 27 moreCaused by: java.lang.NullPointerException at org.apache.hadoop.hive.ql.udf.UDFRand.evaluate(UDFRand.java:57) ... 31 moreDriver stacktrace: (state=,code=0)
issueID:CARBONDATA-906
type:Bug
changed files:
texts:Always OOM error when import large dataset (100milion rows)
java.lang.OutOfMemoryError: GC overhead limit exceeded at java.util.concurrent.ConcurrentHashMap$Segment.put(ConcurrentHashMap.java:457) at java.util.concurrent.ConcurrentHashMap.put(ConcurrentHashMap.java:1130) at org.apache.carbondata.core.cache.dictionary.ColumnReverseDictionaryInfo.addDataToDictionaryMap(ColumnReverseDictionaryInfo.java:101) at org.apache.carbondata.core.cache.dictionary.ColumnReverseDictionaryInfo.addDictionaryChunk(ColumnReverseDictionaryInfo.java:88) at org.apache.carbondata.core.cache.dictionary.DictionaryCacheLoaderImpl.fillDictionaryValuesAndAddToDictionaryChunks(DictionaryCacheLoaderImpl.java:113) at org.apache.carbondata.core.cache.dictionary.DictionaryCacheLoaderImpl.load(DictionaryCacheLoaderImpl.java:81) at org.apache.carbondata.core.cache.dictionary.AbstractDictionaryCache.loadDictionaryData(AbstractDictionaryCache.java:236) at org.apache.carbondata.core.cache.dictionary.AbstractDictionaryCache.checkAndLoadDictionaryData(AbstractDictionaryCache.java:186) at org.apache.carbondata.core.cache.dictionary.ReverseDictionaryCache.getDictionary(ReverseDictionaryCache.java:174) at org.apache.carbondata.core.cache.dictionary.ReverseDictionaryCache.get(ReverseDictionaryCache.java:67) at org.apache.carbondata.core.cache.dictionary.ReverseDictionaryCache.get(ReverseDictionaryCache.java:38) at org.apache.carbondata.processing.newflow.converter.impl.DictionaryFieldConverterImpl.<init>(DictionaryFieldConverterImpl.java:92) at org.apache.carbondata.processing.newflow.converter.impl.FieldEncoderFactory.createFieldEncoder(FieldEncoderFactory.java:77) at org.apache.carbondata.processing.newflow.converter.impl.RowConverterImpl.initialize(RowConverterImpl.java:102) at org.apache.carbondata.processing.newflow.steps.DataConverterProcessorStepImpl.initialize(DataConverterProcessorStepImpl.java:69) at org.apache.carbondata.processing.newflow.steps.SortProcessorStepImpl.initialize(SortProcessorStepImpl.java:57) at org.apache.carbondata.processing.newflow.steps.DataWriterProcessorStepImpl.initialize(DataWriterProcessorStepImpl.java:79) at org.apache.carbondata.processing.newflow.DataLoadExecutor.execute(DataLoadExecutor.java:45) at org.apache.carbondata.spark.rdd.NewDataFrameLoaderRDD$$anon$2.<init>(NewCarbonDataLoadRDD.scala:425) at org.apache.carbondata.spark.rdd.NewDataFrameLoaderRDD.compute(NewCarbonDataLoadRDD.scala:383) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306) at org.apache.spark.rdd.RDD.iterator(RDD.scala:270) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66) at org.apache.spark.scheduler.Task.run(Task.scala:89) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) at java.lang.Thread.run(Thread.java:745)
issueID:CARBONDATA-907
type:Bug
changed files:
texts:The grammar for DELETE SEGMENT FOR DATE in website is not correct
The grammar for DELETE SEGMENT FOR DATE in website is not correct
issueID:CARBONDATA-909
type:Bug
changed files:
texts:Single pass option in dataframe writer
While creating a Carbondata table from dataframe, currently it is not possible to specify single pass load in Spark 1.6. An option is required to specify it as below :df.write.format("carbondata").option("tableName", "test").option("compress","true").option("single_pass","true").mode(SaveMode.Overwrite).save()
issueID:CARBONDATA-91
type:Bug
changed files:
texts:Concurrent query returning empty result
Enviroment:10 node cluster start 20 concurrent querymost of the queries return no data.
issueID:CARBONDATA-911
type:Bug
changed files:
texts:Exception raised while creating table using bucketing example in docs
Exception raised while creating table using bucketing: Column group is not supported for no dictionary columnsSteps to reproduce:1. Create table CREATE TABLE IF NOT EXISTS productSchema.productSalesTable (                                productNumber Int,                                productName String,                                storeCity String,                                storeProvince String,                                productCategory String,                                productBatch String,                                saleQuantity Int,                                revenue Int)   STORED BY 'carbondata'   TBLPROPERTIES ('COLUMN_GROUPS'='(productName,productNumber)',                  'DICTIONARY_EXCLUDE'='productName',                  'DICTIONARY_INCLUDE'='productNumber',                  'NO_INVERTED_INDEX'='productBatch',                  'BUCKETNUMBER'='4',                  'BUCKETCOLUMNS'='productName');2. Expected result :It should create table successfully3. Result on beeline :Error: org.apache.carbondata.spark.exception.MalformedCarbonCommandException: Column group is not supported for no dictionary columns:productname (state=,code=0)
issueID:CARBONDATA-913
type:Bug
changed files:
texts:dead lock problem in unsafe batch parallel read merge sort

issueID:CARBONDATA-914
type:Improvement
changed files:hadoop/src/main/java/org/apache/carbondata/hadoop/CacheAccessClient.java
texts:Clear BTree and Dictionary instances from LRU cache on table drop
After drop table dictionary and BTree instances are not getting cleared from driver memory. Due to this memory will keep growing and after some time GC problems will occur. In real case scenarios usually driver memory is on lower side hence it is more prone to GC problems.
issueID:CARBONDATA-915
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/cache/dictionary/DictionaryColumnUniqueIdentifier.java
texts:Call getAll dictionary from codegen of dictionary decoder to improve dictionary load performance
Currently it gets the dictionary individualy from cache so it is not effective way as it does not load parallel. And also it is not thread safe to just call dictionary instead of getAllCall getAll dictionary from codegen of dictionary decoder to improve dictionary load performance
issueID:CARBONDATA-916
type:Bug
changed files:processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactDataHandlerModel.java
texts:Major compaction is failing
major compaction query on already compacted table throwing exception
issueID:CARBONDATA-917
type:Bug
changed files:integration/hive/src/main/java/org/apache/carbondata/hive/MapredCarbonInputFormat.java
integration/hive/src/main/java/org/apache/carbondata/hive/CarbonHiveRecordReader.java
integration/hive/src/main/java/org/apache/carbondata/hive/CarbonHiveSerDe.java
texts:count(*) doesn&#39;t work
Select query with count fails to render outputSteps to reproduce:1) In Spark Shell :a) Create Table -import org.apache.spark.sql.SparkSessionimport org.apache.spark.sql.CarbonSession._val carbon = SparkSession.builder().config(sc.getConf).getOrCreateCarbonSession("hdfs://localhost:54310/opt/data")scala> carbon.sql(" create table abc(id int, name string) stored by 'carbondata' ").showb) Load Data - scala> carbon.sql(""" load data inpath 'hdfs://localhost:54310/Files/abc.csv' into table abc """ ).show2) In Hive :a) Add Jars - add jar /home/neha/incubator-carbondata/assembly/target/scala-2.11/carbondata_2.11-1.1.0-incubating-SNAPSHOT-shade-hadoop2.7.2.jar;add jar /opt/spark-2.1.0-bin-hadoop2.7/jars/spark-catalyst_2.11-2.1.0.jar;add jar /home/neha/incubator-carbondata/integration/hive/carbondata-hive-1.1.0-incubating-SNAPSHOT.jar;b) Create Table -create table abc(id int,name string);c) Alter location - hive> alter table abc set LOCATION 'hdfs://localhost:54310/opt/data/default/abc' ;d) Set Properties - set hive.mapred.supports.subdirectories=true;set mapreduce.input.fileinputformat.input.dir.recursive=true;d) Alter FileFormat -alter table abc set FILEFORMATINPUTFORMAT "org.apache.carbondata.hive.MapredCarbonInputFormat"OUTPUTFORMAT "org.apache.carbondata.hive.MapredCarbonOutputFormat"SERDE "org.apache.carbondata.hive.CarbonHiveSerDe";e) Query -hive> select count from abc;Expected Output : ResultSet should display the count of the number of rows in the table.Result:Query ID = hduser_20170412181449_85a7db42-42a1-450c-9931-dc7b3b00b412Total jobs = 1Launching Job 1 out of 1Number of reduce tasks determined at compile time: 1In order to change the average load for a reducer (in bytes):  set hive.exec.reducers.bytes.per.reducer=<number>In order to limit the maximum number of reducers:  set hive.exec.reducers.max=<number>In order to set a constant number of reducers:  set mapreduce.job.reduces=<number>Job running in-process (local Hadoop)2017-04-12 18:14:53,949 Stage-1 map = 0%,  reduce = 0%Ended Job = job_local220086106_0001 with errorsError during job, obtaining debugging information...Job Tracking URL: http://localhost:8080/FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.mr.MapRedTaskMapReduce Jobs Launched: Stage-Stage-1:  HDFS Read: 0 HDFS Write: 0 FAILTotal MapReduce CPU Time Spent: 0 msec
issueID:CARBONDATA-918
type:Bug
changed files:
texts:Select query is not working for Complex datatype
Select Query is not working in carbondata for complex datatype:CARBONDATA:0: jdbc:hive2://hadoop-master:10000> create table Array_com (CUST_ID string, YEAR int, MONTH int, AGE int, GENDER string, EDUCATED string, IS_MARRIED string, ARRAY_INT array<int>,ARRAY_STRING array<string>,ARRAY_DATE array<timestamp>,CARD_COUNT int,DEBIT_COUNT int, CREDIT_COUNT int, DEPOSIT double, HQ_DEPOSIT double) STORED BY 'org.apache.carbondata.format' ;---------+ Result  ---------+---------+No rows selected (3.567 seconds)0: jdbc:hive2://hadoop-master:10000>  LOAD DATA INPATH 'hdfs://192.168.2.145:54310/BabuStore/Data/complex/Array.csv' INTO table Array_com  options ('DELIMITER'=',', 'QUOTECHAR'='"', 'FILEHEADER'='CUST_ID,YEAR,MONTH,AGE,GENDER,EDUCATED,IS_MARRIED,ARRAY_INT,ARRAY_STRING,ARRAY_DATE,CARD_COUNT,DEBIT_COUNT,CREDIT_COUNT,DEPOSIT,HQ_DEPOSIT','COMPLEX_DELIMITER_LEVEL_1'='$') ;---------+ Result  ---------+---------+No rows selected (6.541 seconds)0: jdbc:hive2://hadoop-master:10000> select array_int&#91;0&#93;, array_int&#91;0&#93;+ 10 as a  from array_com ;0: jdbc:hive2://192.168.2.126:10000> select * from array_com ;Error: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 254.0 failed 1 times, most recent failure: Lost task 1.0 in stage 254.0 (TID 8631, localhost, executor driver): java.lang.NullPointerException    at org.apache.carbondata.core.datastore.filesystem.AbstractDFSCarbonFile.getLastModifiedTime(AbstractDFSCarbonFile.java:135)    at org.apache.carbondata.core.datastore.filesystem.AbstractDFSCarbonFile.isFileModified(AbstractDFSCarbonFile.java:210)    at org.apache.carbondata.core.cache.dictionary.AbstractDictionaryCache.isDictionaryMetaFileModified(AbstractDictionaryCache.java:119)    at org.apache.carbondata.core.cache.dictionary.AbstractDictionaryCache.checkAndLoadDictionaryData(AbstractDictionaryCache.java:159)    at org.apache.carbondata.core.cache.dictionary.ForwardDictionaryCache.getDictionary(ForwardDictionaryCache.java:195)    at org.apache.carbondata.core.cache.dictionary.ForwardDictionaryCache.get(ForwardDictionaryCache.java:71)    at org.apache.carbondata.core.cache.dictionary.ForwardDictionaryCache.get(ForwardDictionaryCache.java:40)    at org.apache.carbondata.hadoop.readsupport.impl.DictionaryDecodeReadSupport.initialize(DictionaryDecodeReadSupport.java:65)    at org.apache.carbondata.spark.readsupport.SparkRowReadSupportImpl.initialize(SparkRowReadSupportImpl.java:33)    at org.apache.carbondata.hadoop.CarbonRecordReader.initialize(CarbonRecordReader.java:76)    at org.apache.carbondata.spark.rdd.CarbonScanRDD.compute(CarbonScanRDD.scala:204)    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)    at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)    at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)    at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)    at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)    at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)    at org.apache.spark.scheduler.Task.run(Task.scala:99)    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)    at java.lang.Thread.run(Thread.java:745)Driver stacktrace: (state=,code=0)HIVE:create table Array_com_h (CUST_ID string, YEAR int, MONTH int, AGE int, GENDER string, EDUCATED string, IS_MARRIED string, ARRAY_INT array<int>,ARRAY_STRING array<string>,ARRAY_DATE array<timestamp>,CARD_COUNT int,DEBIT_COUNT int, CREDIT_COUNT int, DEPOSIT double, HQ_DEPOSIT double);---------+ Result  ---------+---------+No rows selected (6.541 seconds)load data local inpath '/opt/Carbon/CarbonData/TestData/Data/complex/Array.csv' INTO table Array_com_h;---------+ Result  ---------+---------+No rows selected (6.541 seconds)0: jdbc:hive2://192.168.2.126:10000> select *  from Array_com_h;----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+                                                                                                                                                                                                                                                 CUST_ID                                                                                                                                                                                                                                                   YEAR   MONTH    AGE   GENDER   EDUCATED   IS_MARRIED   ARRAY_INT   ARRAY_STRING   ARRAY_DATE   CARD_COUNT   DEBIT_COUNT   CREDIT_COUNT   DEPOSIT   HQ_DEPOSIT  ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ Cust00000000000000000000,2015,1,20,M,SSC,Y,1234$5678$9101$11121$12357,United Kingdom$England$Basildon$AAID001001$United Kingdom$England$Basildon$AD003001$AAID001001$United Kingdom$England$Basildon$AD003001$United Kingdom$England$Basildon$COUNTY00045,2015-01-01 00:00:00$2014-01-01 00:00:00$2013-01-01 00:00:00$2012-01-01 00:00:00$2011-01-01 00:00:00,21,55,58,337982404.6,989431364.6                                                                                                            NULL   NULL    NULL   NULL     NULL       NULL         NULL        NULL           NULL         NULL         NULL          NULL           NULL      NULL         Cust00000000000000000001,2015,1,30,F,Degree,N,1235$5679$9102$11122$12358,United States$MO$Parkville$AAID001002$United States$MO$Parkville$AD003002$AAID001002$United States$MO$Parkville$AD003002$United States$MO$Parkville$COUNTY00046,2015-01-02 00:00:00$2014-01-02 00:00:00$2013-01-02 00:00:00$2012-01-02 00:00:00$2011-01-02 00:00:00,104,59,50,686815400.5,157442142.4                                                                                                                            NULL   NULL    NULL   NULL     NULL       NULL         NULL        NULL           NULL         NULL         NULL          NULL           NULL      NULL         Cust00000000000000000002,2015,1,40,M,graduation,D,1236$5680$9103$11123$12359,United States$OR$Astoria$AAID001003$United States$OR$Astoria$AD003003$AAID001003$United States$OR$Astoria$AD003003$United States$OR$Astoria$COUNTY00047,2015-01-03 00:00:00$2014-01-03 00:00:00$2013-01-03 00:00:00$2012-01-03 00:00:00$2011-01-03 00:00:00,141,190,145,106733870.5,182602141                                                                                                                                NULL   NULL    NULL   NULL     NULL       NULL         NULL        NULL           NULL         NULL         NULL          NULL           NULL      NULL         Cust00000000000000000003,2015,1,50,F,PG,Y,1237$5681$9104$11124$12360,Australia$Victoria$Echuca$AAID001004$Australia$Victoria$Echuca$AD003004$AAID001004$Australia$Victoria$Echuca$AD003004$Australia$Victoria$Echuca$COUNTY00048,2015-01-04 00:00:00$2014-01-04 00:00:00$2013-01-04 00:00:00$2012-01-04 00:00:00$2011-01-04 00:00:00,162,162,129,702614376.9,499071850.4                                                                                                                                  NULL   NULL    NULL   NULL     NULL       NULL         NULL        NULL           NULL         NULL         NULL          NULL           NULL      NULL         Cust00000000000000000004,2015,1,60,M,MS,N,1238$5682$9105$11125$12361,United States$AL$Cahaba Heights$AAID001005$United States$AL$Cahaba Heights$AD003005$AAID001005$United States$AL$Cahaba Heights$AD003005$United States$AL$Cahaba Heights$COUNTY00049,2015-01-05 00:00:00$2014-01-05 00:00:00$2013-01-05 00:00:00$2012-01-05 00:00:00$2011-01-05 00:00:00,35,139,93,469745206.2,480746358.2
issueID:CARBONDATA-919
type:Bug
changed files:hadoop/src/main/java/org/apache/carbondata/hadoop/CarbonRecordReader.java
hadoop/src/main/java/org/apache/carbondata/hadoop/AbstractRecordReader.java
integration/spark-datasource/src/main/scala/org/apache/carbondata/spark/vectorreader/VectorizedCarbonRecordReader.java
texts:result_size query stats is not giving proper row count if vector reader is enabled.
Incase of vector reader, we return columnarbatch which will have row count as size of the batch, whereas we are incrementing the row count with 1 & the result is printed on the query stats logMoved result_Size calculation into respective reader and logging the results after the task completes in executor.
issueID:CARBONDATA-92
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/datastore/chunk/impl/FixedLengthDimensionDataChunk.java
core/src/main/java/org/apache/carbondata/core/scan/executor/infos/BlockExecutionInfo.java
core/src/main/java/org/apache/carbondata/core/scan/result/impl/NonFilterQueryScannedResult.java
core/src/main/java/org/apache/carbondata/core/scan/executor/QueryExecutorFactory.java
integration/spark-common/src/main/java/org/apache/carbondata/spark/merger/CarbonCompactionExecutor.java
core/src/main/java/org/apache/carbondata/core/scan/result/AbstractScannedResult.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/DimensionColumnDataChunk.java
core/src/main/java/org/apache/carbondata/core/scan/result/BatchResult.java
core/src/main/java/org/apache/carbondata/core/scan/collector/impl/AbstractScannedResultCollector.java
core/src/main/java/org/apache/carbondata/core/datastore/chunk/impl/VariableLengthDimensionDataChunk.java
core/src/main/java/org/apache/carbondata/core/scan/executor/impl/AbstractQueryExecutor.java
core/src/main/java/org/apache/carbondata/core/keygenerator/mdkey/Bits.java
core/src/main/java/org/apache/carbondata/core/scan/complextypes/PrimitiveQueryType.java
core/src/main/java/org/apache/carbondata/core/scan/executor/util/QueryUtil.java
core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
core/src/main/java/org/apache/carbondata/core/keygenerator/KeyGenerator.java
core/src/main/java/org/apache/carbondata/core/scan/result/iterator/RawResultIterator.java
core/src/main/java/org/apache/carbondata/core/scan/result/iterator/DetailQueryResultIterator.java
core/src/main/java/org/apache/carbondata/core/scan/result/impl/FilterQueryScannedResult.java
hadoop/src/main/java/org/apache/carbondata/hadoop/util/CarbonInputFormatUtil.java
core/src/main/java/org/apache/carbondata/core/keygenerator/mdkey/MultiDimKeyVarLengthGenerator.java
core/src/main/java/org/apache/carbondata/core/scan/model/QueryModel.java
core/src/main/java/org/apache/carbondata/core/scan/executor/impl/DetailQueryExecutor.java
core/src/main/java/org/apache/carbondata/core/scan/collector/ScannedResultCollector.java
texts:Remove the unnecessary intermediate conversion of key while scanning.
Remove the unnecessary intermediate conversion of key while scanning.Basically it removes one step in result conversion.It avoids System.arraycopy while converting to result. It avoids the result preparation step.
issueID:CARBONDATA-920
type:Improvement
changed files:
texts:errors while executing create table examples from docs
Examples for creating table in docs throw error while execution(docs/useful-tips-on-carbondata.md)Steps to reproduce:1. run query from examples to create tablecreate table carbondata_table(  Dime_1 String,  HOST String,  MSISDN String,  counter_1 double,  counter_2 double,  BEGIN_TIME bigint,  counter_100 double  )STORED BY 'org.apache.carbondata.format'   TBLPROPERTIES ( 'DICTIONARY_EXCLUDE'='MSISDN,HOST,IMSI',  'DICTIONARY_INCLUDE'='Dime_1,END_TIME,BEGIN_TIME');output on beeline:Error: org.apache.carbondata.spark.exception.MalformedCarbonCommandException: DICTIONARY_EXCLUDE column: imsi does not exist in table. Please check create table statement. (state=,code=0)Expected result :It should create table successfully.
issueID:CARBONDATA-922
type:Bug
changed files:
texts:Spark Context stopped while executing the load query in performance testing.
Spark Context stopped while executing the load query in performance testing.Error Logs:17/04/12 05:39:52 WARN TransportChannelHandler: Exception in connection from /88.99.61.24:43216java.io.IOException: Connection reset by peer at sun.nio.ch.FileDispatcherImpl.read0(Native Method) at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39) at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223) at sun.nio.ch.IOUtil.read(IOUtil.java:192) at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380) at io.netty.buffer.PooledUnsafeDirectByteBuf.setBytes(PooledUnsafeDirectByteBuf.java:221) at io.netty.buffer.AbstractByteBuf.writeBytes(AbstractByteBuf.java:899) at io.netty.channel.socket.nio.NioSocketChannel.doReadBytes(NioSocketChannel.java:275) at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:119) at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:652) at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:575) at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:489) at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:451) at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:140) at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144) at java.lang.Thread.run(Thread.java:745)17/04/12 05:39:52 ERROR TransportResponseHandler: Still have 1 requests outstanding when connection from /88.99.61.24:43216 is closed17/04/12 05:39:52 ERROR YarnSchedulerBackend$YarnSchedulerEndpoint: Sending RequestExecutors(0,0,Map()) to AM was unsuccessfuljava.io.IOException: Connection reset by peer at sun.nio.ch.FileDispatcherImpl.read0(Native Method) at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39) at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223) at sun.nio.ch.IOUtil.read(IOUtil.java:192) at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380) at io.netty.buffer.PooledUnsafeDirectByteBuf.setBytes(PooledUnsafeDirectByteBuf.java:221) at io.netty.buffer.AbstractByteBuf.writeBytes(AbstractByteBuf.java:899) at io.netty.channel.socket.nio.NioSocketChannel.doReadBytes(NioSocketChannel.java:275) at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:119) at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:652) at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:575) at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:489) at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:451) at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:140) at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144) at java.lang.Thread.run(Thread.java:745)17/04/12 05:39:52 INFO SchedulerExtensionServices: Stopping SchedulerExtensionServices(serviceOption=None, services=List(), started=false)17/04/12 05:39:52 ERROR Utils: Uncaught exception in thread Yarn application state monitororg.apache.spark.SparkException: Exception thrown in awaitResult at org.apache.spark.rpc.RpcTimeout$$anonfun$1.applyOrElse(RpcTimeout.scala:77) at org.apache.spark.rpc.RpcTimeout$$anonfun$1.applyOrElse(RpcTimeout.scala:75) at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36) at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:59) at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:59) at scala.PartialFunction$OrElse.apply(PartialFunction.scala:167) at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:83) at org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend.requestTotalExecutors(CoarseGrainedSchedulerBackend.scala:512) at org.apache.spark.scheduler.cluster.YarnSchedulerBackend.stop(YarnSchedulerBackend.scala:93) at org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.stop(YarnClientSchedulerBackend.scala:151) at org.apache.spark.scheduler.TaskSchedulerImpl.stop(TaskSchedulerImpl.scala:467) at org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:1588) at org.apache.spark.SparkContext$$anonfun$stop$8.apply$mcV$sp(SparkContext.scala:1826) at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1283) at org.apache.spark.SparkContext.stop(SparkContext.scala:1825) at org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend$MonitorThread.run(YarnClientSchedulerBackend.scala:108)Caused by: java.io.IOException: Connection reset by peer at sun.nio.ch.FileDispatcherImpl.read0(Native Method) at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39) at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223) at sun.nio.ch.IOUtil.read(IOUtil.java:192) at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380) at io.netty.buffer.PooledUnsafeDirectByteBuf.setBytes(PooledUnsafeDirectByteBuf.java:221) at io.netty.buffer.AbstractByteBuf.writeBytes(AbstractByteBuf.java:899) at io.netty.channel.socket.nio.NioSocketChannel.doReadBytes(NioSocketChannel.java:275) at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:119) at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:652) at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:575) at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:489) at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:451) at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:140) at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144) at java.lang.Thread.run(Thread.java:745)17/04/12 05:39:52 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!17/04/12 05:39:52 INFO HdfsFileLock: main Deleted the lock file hdfs://88.99.61.21:65110/tmp/perfsuite2/carbon.metastore/default/oscon_new_1/meta.lock17/04/12 05:39:52 INFO LoadTable: main Table MetaData Unlocked Successfully after data load%d &#91;%thread&#93; %-5level %logger - %msg%n java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.This stopped SparkContext was created at:org.apache.spark.sql.CarbonSession$CarbonBuilder.getOrCreateCarbonSession(CarbonSession.scala:74)com.huawei.spark.SessionManager.setCarbonSparkSession(SessionManager.java:34)com.huawei.spark.SessionManager.<init>(SessionManager.java:23)com.huawei.utils.Utilities.<init>(Utilities.java:10)com.huawei.utils.Utilities.getInstance(Utilities.java:18)com.huawei.performancesuite.StartDataLoadTest.setUp(StartDataLoadTest.java:43)sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)java.lang.reflect.Method.invoke(Method.java:498)org.junit.internal.runners.MethodRoadie.runBefores(MethodRoadie.java:122)org.junit.internal.runners.MethodRoadie.runBeforesThenTestThenAfters(MethodRoadie.java:86)org.junit.internal.runners.MethodRoadie.runTest(MethodRoadie.java:77)org.junit.internal.runners.MethodRoadie.run(MethodRoadie.java:42)org.junit.internal.runners.JUnit4ClassRunner.invokeTestMethod(JUnit4ClassRunner.java:88)org.junit.internal.runners.JUnit4ClassRunner.runMethods(JUnit4ClassRunner.java:51)org.junit.runners.Parameterized$TestClassRunnerForParameters.run(Parameterized.java:98)org.junit.internal.runners.CompositeRunner.runChildren(CompositeRunner.java:33)org.junit.runners.Parameterized.access$000(Parameterized.java:55)org.junit.runners.Parameterized$1.run(Parameterized.java:131)The currently active SparkContext was created at:org.apache.spark.sql.CarbonSession$CarbonBuilder.getOrCreateCarbonSession(CarbonSession.scala:74)com.huawei.spark.SessionManager.setCarbonSparkSession(SessionManager.java:34)com.huawei.spark.SessionManager.<init>(SessionManager.java:23)com.huawei.utils.Utilities.<init>(Utilities.java:10)com.huawei.utils.Utilities.getInstance(Utilities.java:18)com.huawei.performancesuite.StartDataLoadTest.setUp(StartDataLoadTest.java:43)sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)java.lang.reflect.Method.invoke(Method.java:498)org.junit.internal.runners.MethodRoadie.runBefores(MethodRoadie.java:122)org.junit.internal.runners.MethodRoadie.runBeforesThenTestThenAfters(MethodRoadie.java:86)org.junit.internal.runners.MethodRoadie.runTest(MethodRoadie.java:77)org.junit.internal.runners.MethodRoadie.run(MethodRoadie.java:42)org.junit.internal.runners.JUnit4ClassRunner.invokeTestMethod(JUnit4ClassRunner.java:88)org.junit.internal.runners.JUnit4ClassRunner.runMethods(JUnit4ClassRunner.java:51)org.junit.runners.Parameterized$TestClassRunnerForParameters.run(Parameterized.java:98)org.junit.internal.runners.CompositeRunner.runChildren(CompositeRunner.java:33)org.junit.runners.Parameterized.access$000(Parameterized.java:55)org.junit.runners.Parameterized$1.run(Parameterized.java:131) at org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:100) ~&#91;spark-core_2.11-2.1.0.jar:2.1.0&#93; at org.apache.spark.SparkContext.broadcast(SparkContext.scala:1408) ~&#91;spark-core_2.11-2.1.0.jar:2.1.0&#93; at org.apache.spark.rdd.NewHadoopRDD.<init>(NewHadoopRDD.scala:78) ~&#91;spark-core_2.11-2.1.0.jar:2.1.0&#93; at org.apache.carbondata.spark.util.GlobalDictionaryUtil$.loadDataFrame(GlobalDictionaryUtil.scala:381) ~&#91;carbondata_2.11-1.1.0-incubating-SNAPSHOT-shade-hadoop2.2.0.jar:1.1.0-incubating-SNAPSHOT&#93; at org.apache.carbondata.spark.util.GlobalDictionaryUtil$$anonfun$8.apply(GlobalDictionaryUtil.scala:718) ~&#91;carbondata_2.11-1.1.0-incubating-SNAPSHOT-shade-hadoop2.2.0.jar:1.1.0-incubating-SNAPSHOT&#93; at org.apache.carbondata.spark.util.GlobalDictionaryUtil$$anonfun$8.apply(GlobalDictionaryUtil.scala:718) ~&#91;carbondata_2.11-1.1.0-incubating-SNAPSHOT-shade-hadoop2.2.0.jar:1.1.0-incubating-SNAPSHOT&#93; at scala.Option.getOrElse(Option.scala:121) ~&#91;scala-library-2.11.8.jar:?&#93; at org.apache.carbondata.spark.util.GlobalDictionaryUtil$.generateGlobalDictionary(GlobalDictionaryUtil.scala:718) ~&#91;carbondata_2.11-1.1.0-incubating-SNAPSHOT-shade-hadoop2.2.0.jar:1.1.0-incubating-SNAPSHOT&#93; at org.apache.spark.sql.execution.command.LoadTable.run(carbonTableSchema.scala:558) ~&#91;carbondata_2.11-1.1.0-incubating-SNAPSHOT-shade-hadoop2.2.0.jar:2.1.0&#93; at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58) ~&#91;spark-sql_2.11-2.1.0.jar:2.1.0&#93; at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56) ~&#91;spark-sql_2.11-2.1.0.jar:2.1.0&#93; at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74) ~&#91;spark-sql_2.11-2.1.0.jar:2.1.0&#93; at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114) ~&#91;spark-sql_2.11-2.1.0.jar:2.1.0&#93; at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114) ~&#91;spark-sql_2.11-2.1.0.jar:2.1.0&#93; at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:135) ~&#91;spark-sql_2.11-2.1.0.jar:2.1.0&#93; at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151) ~&#91;spark-core_2.11-2.1.0.jar:2.1.0&#93; at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:132) ~&#91;spark-sql_2.11-2.1.0.jar:2.1.0&#93; at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:113) ~&#91;spark-sql_2.11-2.1.0.jar:2.1.0&#93; at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:87) ~&#91;spark-sql_2.11-2.1.0.jar:2.1.0&#93; at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:87) ~&#91;spark-sql_2.11-2.1.0.jar:2.1.0&#93; at org.apache.spark.sql.Dataset.<init>(Dataset.scala:185) ~&#91;spark-sql_2.11-2.1.0.jar:2.1.0&#93; at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:64) ~&#91;spark-sql_2.11-2.1.0.jar:2.1.0&#93; at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:592) ~&#91;spark-sql_2.11-2.1.0.jar:2.1.0&#93; at com.huawei.spark.SessionManager.sql(SessionManager.java:42) ~&#91;automation.jar:?&#93; at com.huawei.performancesuite.StartDataLoadTest.testDataLoad(StartDataLoadTest.java:64) &#91;automation.jar:?&#93; at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~&#91;?:1.8.0_121&#93; at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~&#91;?:1.8.0_121&#93; at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~&#91;?:1.8.0_121&#93; at java.lang.reflect.Method.invoke(Method.java:498) ~&#91;?:1.8.0_121&#93; at org.junit.internal.runners.TestMethod.invoke(TestMethod.java:59) &#91;automation.jar:?&#93; at org.junit.internal.runners.MethodRoadie.runTestMethod(MethodRoadie.java:98) &#91;automation.jar:?&#93; at org.junit.internal.runners.MethodRoadie$2.run(MethodRoadie.java:79) &#91;automation.jar:?&#93; at org.junit.internal.runners.MethodRoadie.runBeforesThenTestThenAfters(MethodRoadie.java:87) &#91;automation.jar:?&#93; at org.junit.internal.runners.MethodRoadie.runTest(MethodRoadie.java:77) &#91;automation.jar:?&#93; at org.junit.internal.runners.MethodRoadie.run(MethodRoadie.java:42) &#91;automation.jar:?&#93; at org.junit.internal.runners.JUnit4ClassRunner.invokeTestMethod(JUnit4ClassRunner.java:88) &#91;automation.jar:?&#93; at org.junit.internal.runners.JUnit4ClassRunner.runMethods(JUnit4ClassRunner.java:51) &#91;automation.jar:?&#93; at org.junit.runners.Parameterized$TestClassRunnerForParameters.run(Parameterized.java:98) &#91;automation.jar:?&#93; at org.junit.internal.runners.CompositeRunner.runChildren(CompositeRunner.java:33) &#91;automation.jar:?&#93; at org.junit.runners.Parameterized.access$000(Parameterized.java:55) &#91;automation.jar:?&#93; at org.junit.runners.Parameterized$1.run(Parameterized.java:131) &#91;automation.jar:?&#93; at org.junit.internal.runners.ClassRoadie.runUnprotected(ClassRoadie.java:27) &#91;automation.jar:?&#93; at org.junit.internal.runners.ClassRoadie.runProtected(ClassRoadie.java:37) &#91;automation.jar:?&#93; at org.junit.runners.Parameterized.run(Parameterized.java:129) &#91;automation.jar:?&#93; at org.junit.internal.runners.CompositeRunner.runChildren(CompositeRunner.java:33) &#91;automation.jar:?&#93; at org.junit.internal.runners.CompositeRunner.run(CompositeRunner.java:28) &#91;automation.jar:?&#93; at org.junit.runner.JUnitCore.run(JUnitCore.java:130) &#91;automation.jar:?&#93; at org.junit.runner.JUnitCore.run(JUnitCore.java:109) &#91;automation.jar:?&#93; at org.junit.runner.JUnitCore.run(JUnitCore.java:100) &#91;automation.jar:?&#93; at org.junit.runner.JUnitCore.runClasses(JUnitCore.java:60) &#91;automation.jar:?&#93; at com.huawei.performancesuite.StartDataLoadSuite.main(StartDataLoadSuite.java:17) &#91;automation.jar:?&#93; at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~&#91;?:1.8.0_121&#93; at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~&#91;?:1.8.0_121&#93; at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~&#91;?:1.8.0_121&#93; at java.lang.reflect.Method.invoke(Method.java:498) ~&#91;?:1.8.0_121&#93; at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:738) &#91;spark-core_2.11-2.1.0.jar:2.1.0&#93; at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187) &#91;spark-core_2.11-2.1.0.jar:2.1.0&#93; at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212) &#91;spark-core_2.11-2.1.0.jar:2.1.0&#93; at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126) &#91;spark-core_2.11-2.1.0.jar:2.1.0&#93; at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala) &#91;spark-core_2.11-2.1.0.jar:2.1.0&#93;17/04/12 05:39:52 INFO CarbonSparkSqlParser: Parsing command: load data inpath 'hdfs://hacluster/benchmarks/CarbonData/data/datafile_15.csv' into table oscon_new_1 options('DELIMITER'=',', 'QUOTECHAR'='"','FILEHEADER'='ACTIVE_AREA_ID, ACTIVE_CHECK_DY, ACTIVE_CHECK_HOUR, ACTIVE_CHECK_MM, ACTIVE_CHECK_TIME, ACTIVE_CHECK_YR, ACTIVE_CITY, ACTIVE_COUNTRY, ACTIVE_DISTRICT, ACTIVE_EMUI_VERSION, ACTIVE_FIRMWARE_VER, ACTIVE_NETWORK, ACTIVE_OS_VERSION, ACTIVE_PROVINCE, BOM, CHECK_DATE, CHECK_DY, CHECK_HOUR, CHECK_MM, CHECK_YR, CUST_ADDRESS_ID, CUST_AGE, CUST_BIRTH_COUNTRY, CUST_BIRTH_DY, CUST_BIRTH_MM, CUST_BIRTH_YR, CUST_BUY_POTENTIAL, CUST_CITY, CUST_STATE, CUST_COUNTRY, CUST_COUNTY, CUST_EMAIL_ADDR, CUST_LAST_RVW_DATE, CUST_FIRST_NAME, CUST_ID, CUST_JOB_TITLE, CUST_LAST_NAME, CUST_LOGIN, CUST_NICK_NAME, CUST_PRFRD_FLG, CUST_SEX, CUST_STREET_NAME, CUST_STREET_NO, CUST_SUITE_NO, CUST_ZIP, DELIVERY_CITY, DELIVERY_STATE, DELIVERY_COUNTRY, DELIVERY_DISTRICT, DELIVERY_PROVINCE, DEVICE_NAME, INSIDE_NAME, ITM_BRAND, ITM_BRAND_ID, ITM_CATEGORY, ITM_CATEGORY_ID, ITM_CLASS, ITM_CLASS_ID, ITM_COLOR, ITM_CONTAINER, ITM_FORMULATION, ITM_MANAGER_ID, ITM_MANUFACT, ITM_MANUFACT_ID, ITM_ID, ITM_NAME, ITM_REC_END_DATE, ITM_REC_START_DATE, LATEST_AREAID, LATEST_CHECK_DY, LATEST_CHECK_HOUR, LATEST_CHECK_MM, LATEST_CHECK_TIME, LATEST_CHECK_YR, LATEST_CITY, LATEST_COUNTRY, LATEST_DISTRICT, LATEST_EMUI_VERSION, LATEST_FIRMWARE_VER, LATEST_NETWORK, LATEST_OS_VERSION, LATEST_PROVINCE, OL_ORDER_DATE, OL_ORDER_NO, OL_RET_ORDER_NO, OL_RET_DATE, OL_SITE, OL_SITE_DESC, PACKING_DATE, PACKING_DY, PACKING_HOUR, PACKING_LIST_NO, PACKING_MM, PACKING_YR, PRMTION_ID, PRMTION_NAME, PRM_CHANNEL_CAT, PRM_CHANNEL_DEMO, PRM_CHANNEL_DETAILS, PRM_CHANNEL_DMAIL, PRM_CHANNEL_EMAIL, PRM_CHANNEL_EVENT, PRM_CHANNEL_PRESS, PRM_CHANNEL_RADIO, PRM_CHANNEL_TV, PRM_DSCNT_ACTIVE, PRM_END_DATE, PRM_PURPOSE, PRM_START_DATE, PRODUCT_ID, PROD_BAR_CODE, PROD_BRAND_NAME, PRODUCT_NAME, PRODUCT_MODEL, PROD_MODEL_ID, PROD_COLOR, PROD_SHELL_COLOR, PROD_CPU_CLOCK, PROD_IMAGE, PROD_LIVE, PROD_LOC, PROD_LONG_DESC, PROD_RAM, PROD_ROM, PROD_SERIES, PROD_SHORT_DESC, PROD_THUMB, PROD_UNQ_DEVICE_ADDR, PROD_UNQ_MDL_ID, PROD_UPDATE_DATE, PROD_UQ_UUID, SHP_CARRIER, SHP_CODE, SHP_CONTRACT, SHP_MODE_ID, SHP_MODE, STR_ORDER_DATE, STR_ORDER_NO, TRACKING_NO, WH_CITY, WH_COUNTRY, WH_COUNTY, WH_ID, WH_NAME, WH_STATE, WH_STREET_NAME, WH_STREET_NO, WH_STREET_TYPE, WH_SUITE_NO, WH_ZIP, CUST_DEP_COUNT, CUST_VEHICLE_COUNT, CUST_ADDRESS_CNT, CUST_CRNT_CDEMO_CNT, CUST_CRNT_HDEMO_CNT, CUST_CRNT_ADDR_DM, CUST_FIRST_SHIPTO_CNT, CUST_FIRST_SALES_CNT, CUST_GMT_OFFSET, CUST_DEMO_CNT, CUST_INCOME, PROD_UNLIMITED, PROD_OFF_PRICE, PROD_UNITS, TOTAL_PRD_COST, TOTAL_PRD_DISC, PROD_WEIGHT, REG_UNIT_PRICE, EXTENDED_AMT, UNIT_PRICE_DSCNT_PCT, DSCNT_AMT, PROD_STD_CST, TOTAL_TX_AMT, FREIGHT_CHRG, WAITING_PERIOD, DELIVERY_PERIOD, ITM_CRNT_PRICE, ITM_UNITS, ITM_WSLE_CST, ITM_SIZE, PRM_CST, PRM_RESPONSE_TARGET, PRM_ITM_DM, SHP_MODE_CNT, WH_GMT_OFFSET, WH_SQ_FT, STR_ORD_QTY, STR_WSLE_CST, STR_LIST_PRICE, STR_SALES_PRICE, STR_EXT_DSCNT_AMT, STR_EXT_SALES_PRICE, STR_EXT_WSLE_CST, STR_EXT_LIST_PRICE, STR_EXT_TX, STR_COUPON_AMT, STR_NET_PAID, STR_NET_PAID_INC_TX, STR_NET_PRFT, STR_SOLD_YR_CNT, STR_SOLD_MM_CNT, STR_SOLD_ITM_CNT, STR_TOTAL_CUST_CNT, STR_AREA_CNT, STR_DEMO_CNT, STR_OFFER_CNT, STR_PRM_CNT, STR_TICKET_CNT, STR_NET_PRFT_DM_A, STR_NET_PRFT_DM_B, STR_NET_PRFT_DM_C, STR_NET_PRFT_DM_D, STR_NET_PRFT_DM_E, STR_RET_STR_ID, STR_RET_REASON_CNT, STR_RET_TICKET_NO, STR_RTRN_QTY, STR_RTRN_AMT, STR_RTRN_TX, STR_RTRN_AMT_INC_TX, STR_RET_FEE, STR_RTRN_SHIP_CST, STR_RFNDD_CSH, STR_REVERSED_CHRG, STR_STR_CREDIT, STR_RET_NET_LOSS, STR_RTRNED_YR_CNT, STR_RTRN_MM_CNT, STR_RET_ITM_CNT, STR_RET_CUST_CNT, STR_RET_AREA_CNT, STR_RET_OFFER_CNT, STR_RET_PRM_CNT, STR_RET_NET_LOSS_DM_A, STR_RET_NET_LOSS_DM_B, STR_RET_NET_LOSS_DM_C, STR_RET_NET_LOSS_DM_D, OL_ORD_QTY, OL_WSLE_CST, OL_LIST_PRICE, OL_SALES_PRICE, OL_EXT_DSCNT_AMT, OL_EXT_SALES_PRICE, OL_EXT_WSLE_CST, OL_EXT_LIST_PRICE, OL_EXT_TX, OL_COUPON_AMT, OL_EXT_SHIP_CST, OL_NET_PAID, OL_NET_PAID_INC_TX, OL_NET_PAID_INC_SHIP, OL_NET_PAID_INC_SHIP_TX, OL_NET_PRFT, OL_SOLD_YR_CNT, OL_SOLD_MM_CNT, OL_SHIP_DATE_CNT, OL_ITM_CNT, OL_BILL_CUST_CNT, OL_BILL_AREA_CNT, OL_BILL_DEMO_CNT, OL_BILL_OFFER_CNT, OL_SHIP_CUST_CNT, OL_SHIP_AREA_CNT, OL_SHIP_DEMO_CNT, OL_SHIP_OFFER_CNT, OL_WEB_PAGE_CNT, OL_WEB_SITE_CNT, OL_SHIP_MODE_CNT, OL_WH_CNT, OL_PRM_CNT, OL_NET_PRFT_DM_A, OL_NET_PRFT_DM_B, OL_NET_PRFT_DM_C, OL_NET_PRFT_DM_D, OL_RET_RTRN_QTY, OL_RTRN_AMT, OL_RTRN_TX, OL_RTRN_AMT_INC_TX, OL_RET_FEE, OL_RTRN_SHIP_CST, OL_RFNDD_CSH, OL_REVERSED_CHRG, OL_ACCOUNT_CREDIT, OL_RTRNED_YR_CNT, OL_RTRNED_MM_CNT, OL_RTRITM_CNT, OL_RFNDD_CUST_CNT, OL_RFNDD_AREA_CNT, OL_RFNDD_DEMO_CNT, OL_RFNDD_OFFER_CNT, OL_RTRNING_CUST_CNT, OL_RTRNING_AREA_CNT, OL_RTRNING_DEMO_CNT, OL_RTRNING_OFFER_CNT, OL_RTRWEB_PAGE_CNT, OL_REASON_CNT, OL_NET_LOSS, OL_NET_LOSS_DM_A, OL_NET_LOSS_DM_B, OL_NET_LOSS_DM_C','BAD_RECORDS_ACTION'='FORCE','BAD_RECORDS_LOGGER_ENABLE'='FALSE');17/04/12 05:39:52 INFO MemoryStore: MemoryStore cleared17/04/12 05:39:52 INFO BlockManager: BlockManager stopped17/04/12 05:39:52 INFO BlockManagerMaster: BlockManagerMaster stopped17/04/12 05:39:52 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!17/04/12 05:39:52 INFO CarbonLateDecodeRule: main Skip CarbonOptimizer17/04/12 05:39:52 INFO SparkContext: Successfully stopped SparkContext17/04/12 05:39:52 INFO HdfsFileLock: main HDFS lock path:hdfs://88.99.61.21:65110/tmp/perfsuite2/carbon.metastore/default/oscon_new_1/meta.lock17/04/12 05:39:52 INFO LoadTable: main Successfully able to get the table metadata file lock17/04/12 05:39:52 INFO LoadTable: main Initiating Direct Load for the Table : (default.oscon_new_1)17/04/12 05:39:52 INFO GlobalDictionaryUtil$: main Generate global dictionary from source data files!17/04/12 05:39:52 ERROR GlobalDictionaryUtil$: main generate global dictionary failedjava.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.This stopped SparkContext was created at:org.apache.spark.sql.CarbonSession$CarbonBuilder.getOrCreateCarbonSession(CarbonSession.scala:74)com.huawei.spark.SessionManager.setCarbonSparkSession(SessionManager.java:34)com.huawei.spark.SessionManager.<init>(SessionManager.java:23)com.huawei.utils.Utilities.<init>(Utilities.java:10)com.huawei.utils.Utilities.getInstance(Utilities.java:18)com.huawei.performancesuite.StartDataLoadTest.setUp(StartDataLoadTest.java:43)sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)java.lang.reflect.Method.invoke(Method.java:498)org.junit.internal.runners.MethodRoadie.runBefores(MethodRoadie.java:122)org.junit.internal.runners.MethodRoadie.runBeforesThenTestThenAfters(MethodRoadie.java:86)org.junit.internal.runners.MethodRoadie.runTest(MethodRoadie.java:77)org.junit.internal.runners.MethodRoadie.run(MethodRoadie.java:42)org.junit.internal.runners.JUnit4ClassRunner.invokeTestMethod(JUnit4ClassRunner.java:88)org.junit.internal.runners.JUnit4ClassRunner.runMethods(JUnit4ClassRunner.java:51)org.junit.runners.Parameterized$TestClassRunnerForParameters.run(Parameterized.java:98)org.junit.internal.runners.CompositeRunner.runChildren(CompositeRunner.java:33)org.junit.runners.Parameterized.access$000(Parameterized.java:55)org.junit.runners.Parameterized$1.run(Parameterized.java:131)
issueID:CARBONDATA-923
type:Bug
changed files:
texts:InserInto read from one row not working
Reproduce:create table OneRowTable(col1 string, col2 string, col3 int, col4 double) stored by 'carbondata'insert into OneRowTable select '0.1', 'a.b', 1, 1.2Exception:org.apache.spark.sql.AnalysisException: cannot resolve '`0.1`' given input columns: &#91;0.1, a.b, 1, 1.2&#93;;;'Project &#91;&#39;0.1, &#39;a.b&#93;+- Project 0.1 AS 0.1#11, a.b AS a.b#12, 1 AS 1#13, 1.2 AS 1.2#14   +- OneRowRelation$
issueID:CARBONDATA-925
type:Bug
changed files:
texts:CarbonEnv is static & shared among all the Sessions. Cached relation in 1 session is not getting refreshed when another session is adding/dropping column
WIth multiple beeline connects in single thriftserver, CarbonEnv object is static. When user performs alter operation in 1 beeline, latest updated schema information is not reflecting 2nd beeline.Assume following concurrent operations1) Beeline => select query on table1 (CarbonDatasourceRelation is cached)2) spark-sql => add new column on table1 (this will refresh carbonMetastore available in carbonenv)3) Beeline => Select query on table1 (This operation fails because it does not know whether the carbonMetastore is refreshed, hence cached CarbonDatasourceRelation will be returned which will not have newly added columns in schema fields)Corrected this problem by changing CarbonEnv Object to class which will be instantiated for each Session
issueID:CARBONDATA-926
type:Improvement
changed files:processing/src/main/java/org/apache/carbondata/processing/loading/csvinput/CSVInputFormat.java
processing/src/main/java/org/apache/carbondata/processing/loading/model/CarbonLoadModel.java
processing/src/main/java/org/apache/carbondata/processing/loading/CarbonDataLoadConfiguration.java
texts:Set the max column from options in csv parser settings

issueID:CARBONDATA-927
type:Improvement
changed files:
texts:Show segment in data management doc
Minor corrections in docs :  Fix show segment link in data management docs show segment command output to be reformatted
issueID:CARBONDATA-928
type:Improvement
changed files:
texts:Add link to configuration parameters in docs
Add a link to configuration parameters in the main page of doc
issueID:CARBONDATA-929
type:Improvement
changed files:
texts:adding optional field number of blocklet in the carbonindex file
The num_blocklet needed to get the number of Blocklet within a block at the driver /** *  Block index information stored in index file for every block */struct BlockIndex{  1: required i64 num_rows; // Total number of rows in this file  2: required string file_name; // Block file name  3: required i64 offset; // Offset of the footer  4: required carbondata.BlockletIndex block_index; // Blocklet index  5: optional i32 num_blocklet; // total number of Blocklet}
issueID:CARBONDATA-93
type:Bug
changed files:
texts:Task not re-submitted by spark on data load failure
In case of data load failure on any executor, exception thrown is caught and logged as an error but the exception is not re-thrown to the spark therefore spark does not resubmits the task to the available executors again.
issueID:CARBONDATA-930
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/metadata/AbsoluteTableIdentifier.java
texts:Drop table named &#39;is&#39;  throwing exception

issueID:CARBONDATA-931
type:Bug
changed files:
texts:Exception in BigDecimal unsafe store
Problem:a.     NumberFormatException with Big decimal unsafe storeb.     NegativeArraySizeException with Big decimal unsafe storec.     Cast exception with Big decimal unsafe store Solution: In big decimal length is stored as int but while we are converting length to short for last record and it is causing above issue.
issueID:CARBONDATA-932
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/datastore/chunk/store/impl/unsafe/UnsafeVariableLengthDimensionDataChunkStore.java
texts:Variable length filter query is failing with empty data
Problem: Variable length filter query is failing with empty dataSolution: Unsafe variable compare method is failing when record size is zero .
issueID:CARBONDATA-934
type:Bug
changed files:
texts:Cast Filter Expression Pushdown in Carbon
Currently Filter Expression with Casts are not handled in Carbon, therefore from Carbon All rows are scanned and given to spark for evaluating Filters with Cast expressions. Pushdown the Cast Expression to Carbon for filtering out more data and reduce IO.
issueID:CARBONDATA-935
type:Sub-task
changed files:core/src/main/java/org/apache/carbondata/core/metadata/schema/table/CarbonTable.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/PartitionInfo.java
core/src/main/java/org/apache/carbondata/core/util/comparator/SerializableComparator.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/partition/PartitionType.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/TableSchema.java
texts:1. Define PartitionInfo model
modify schema.thrift to define PartitionInfo, add PartitionInfo to TableSchema
issueID:CARBONDATA-936
type:Sub-task
changed files:core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/CarbonTable.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/PartitionInfo.java
core/src/main/java/org/apache/carbondata/core/metadata/converter/ThriftWrapperSchemaConverterImpl.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/table/TableSchema.java
texts:2. Create Table with Partition

issueID:CARBONDATA-937
type:Sub-task
changed files:core/src/main/java/org/apache/carbondata/hadoop/CarbonInputSplit.java
common/src/main/java/org/apache/carbondata/common/exceptions/DeprecatedFeatureException.java
texts:3. Data loading of partition table
use PartitionInfo to generate Partitioner (hash, list, range) use Partitioner to repartition input data file, reuse loadDataFrame flow use partition id to replace task no in carbondata/index file name
issueID:CARBONDATA-938
type:Sub-task
changed files:core/src/main/java/org/apache/carbondata/core/scan/filter/FilterProcessor.java
core/src/main/java/org/apache/carbondata/core/scan/filter/FilterExpressionProcessor.java
core/src/main/java/org/apache/carbondata/core/util/path/CarbonTablePath.java
texts:4. Detail filter query on partition column
use filter(equal,range, in etc.) to get partition id list, use this partition id list to filter BTree.
issueID:CARBONDATA-94
type:Bug
changed files:
texts:Load data failed when the data with header and delimiter is "|"
Load data failed when the data with header and delimiter is "|"Catch exception:ERROR 22-07 02:49:14,460 - &#91;Executor task launch worker-1&#93;&#91;partitionID:default_carbontable1_d0526816-36cd-4b1c-8547-723601e8a1d2&#93; CSV File provided is not proper. Column names in schema and csv header are not same. CSVFile Name : datadelimiter.csvERROR 22-07 02:49:14,464 - &#91;Executor task launch worker-1&#93;&#91;partitionID:default_carbontable1_d0526816-36cd-4b1c-8547-723601e8a1d2&#93; org.carbondata.processing.etl.DataLoadingException: CSV File provided is not proper. Column names in schema and csv header are not same. CSVFile Name : datadelimiter.csv at org.carbondata.processing.csvload.DataGraphExecuter.validateCSV(DataGraphExecuter.java:139) at org.carbondata.processing.csvload.DataGraphExecuter.validateCSVFiles(DataGraphExecuter.java:534) at org.carbondata.processing.csvload.DataGraphExecuter.executeGraph(DataGraphExecuter.java:148) at org.carbondata.spark.load.CarbonLoaderUtil.executeGraph(CarbonLoaderUtil.java:178) at org.carbondata.spark.rdd.CarbonDataLoadRDD$$anon$1.<init>(CarbonDataLoadRDD.scala:196) at org.carbondata.spark.rdd.CarbonDataLoadRDD.compute(CarbonDataLoadRDD.scala:154)
issueID:CARBONDATA-940
type:Sub-task
changed files:core/src/main/java/org/apache/carbondata/core/locks/LockUsage.java
core/src/main/java/org/apache/carbondata/core/scan/wrappers/ByteArrayWrapper.java
processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/SortParameters.java
processing/src/main/java/org/apache/carbondata/processing/loading/steps/CarbonRowDataWriterProcessorStepImpl.java
processing/src/main/java/org/apache/carbondata/processing/loading/steps/DataWriterProcessorStepImpl.java
core/src/main/java/org/apache/carbondata/core/scan/result/iterator/PartitionSpliterRawResultIterator.java
processing/src/main/java/org/apache/carbondata/processing/util/CarbonLoaderUtil.java
processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactDataHandlerModel.java
core/src/main/java/org/apache/carbondata/core/util/path/CarbonTablePath.java
processing/src/main/java/org/apache/carbondata/processing/util/CarbonDataProcessorUtil.java
hadoop/src/main/java/org/apache/carbondata/hadoop/api/CarbonTableInputFormat.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
hadoop/src/main/java/org/apache/carbondata/hadoop/util/CarbonInputFormatUtil.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/impl/ParallelReadMergeSorterImpl.java
core/src/main/java/org/apache/carbondata/core/metadata/schema/PartitionInfo.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/impl/ParallelReadMergeSorterWithBucketingImpl.java
processing/src/main/java/org/apache/carbondata/processing/loading/DataLoadProcessBuilder.java
processing/src/main/java/org/apache/carbondata/processing/merger/CompactionResultSortProcessor.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/impl/UnsafeParallelReadMergeSorterWithBucketingImpl.java
texts:6. Alter table add/split partition

issueID:CARBONDATA-941
type:Sub-task
changed files:
texts:7. Compaction of Partition Table
compaction same partition of segments
issueID:CARBONDATA-942
type:Bug
changed files:
texts:OFF heap sort chunk size property doesnt have validation
property should have min of 1 mb and max of 1024 mb inclusive.
issueID:CARBONDATA-943
type:Bug
changed files:common/src/main/java/org/apache/carbondata/common/exceptions/sql/MalformedCarbonCommandException.java
texts:Failing Mathematical functional in spark 2.1 is not displaying proper error message
Failing Mathematical functional in spark 2.1 is not displaying proper error message
issueID:CARBONDATA-944
type:Improvement
changed files:
texts:Fix wrong log info during drop table in spark-shell
In Spark-shell, use the below script to drop table "t3", the drop operation can be finished successfully , but show the below wrong log message:scala> carbon.sql("drop table t3")17/04/17 23:00:08 AUDIT CarbonDropTableCommand: &#91;AppledeMacBook-Pro.local&#93;&#91;apple&#93;&#91;Thread-1&#93;Deleting table &#91;t3&#93; under database &#91;default&#93;17/04/17 23:00:09 AUDIT CreateTable: &#91;AppledeMacBook-Pro.local&#93;&#91;apple&#93;&#91;Thread-1&#93;Creating Table with Database name &#91;default&#93; and Table name &#91;t3&#93;17/04/17 23:00:09 AUDIT CreateTable: &#91;AppledeMacBook-Pro.local&#93;&#91;apple&#93;&#91;Thread-1&#93;Table creation with Database name &#91;default&#93; and Table name &#91;t3&#93; failed. Table &#91;t3&#93; already exists under database &#91;default&#93;17/04/17 23:00:09 WARN DropTableCommand: org.spark_project.guava.util.concurrent.UncheckedExecutionException: java.lang.RuntimeException: Table &#91;t3&#93; already exists under database &#91;default&#93;org.spark_project.guava.util.concurrent.UncheckedExecutionException: java.lang.RuntimeException: Table &#91;t3&#93; already exists under database &#91;default&#93; at org.spark_project.guava.cache.LocalCache$Segment.get(LocalCache.java:2263) at org.spark_project.guava.cache.LocalCache.get(LocalCache.java:4000) at org.spark_project.guava.cache.LocalCache.getOrLoad(LocalCache.java:4004) at org.spark_project.guava.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4874) at org.spark_project.guava.cache.LocalCache$LocalLoadingCache.getUnchecked(LocalCache.java:4880) at org.spark_project.guava.cache.LocalCache$LocalLoadingCache.apply(LocalCache.java:4898) at org.apache.spark.sql.hive.HiveMetastoreCatalog.lookupRelation(HiveMetastoreCatalog.scala:110) at org.apache.spark.sql.hive.HiveSessionCatalog.lookupRelation(HiveSessionCatalog.scala:69) at org.apache.spark.sql.SparkSession.table(SparkSession.scala:578) at org.apache.spark.sql.SparkSession.table(SparkSession.scala:574) at org.apache.spark.sql.execution.command.DropTableCommand.run(ddl.scala:203) at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58) at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56) at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74) at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114) at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114) at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:135) at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151) at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:132) at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:113) at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:87) at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:87) at org.apache.spark.sql.Dataset.<init>(Dataset.scala:185) at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:64) at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:592) at org.apache.spark.sql.hive.CarbonHiveMetadataUtil$.invalidateAndDropTable(CarbonHiveMetadataUtil.scala:44) at org.apache.spark.sql.hive.CarbonMetastore.dropTable(CarbonMetastore.scala:509) at org.apache.spark.sql.execution.command.CarbonDropTableCommand.run(carbonTableSchema.scala:725) at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58) at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56) at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74) at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114) at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114) at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:135) at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151) at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:132) at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:113) at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:87) at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:87) at org.apache.spark.sql.Dataset.<init>(Dataset.scala:185) at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:64) at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:592) at $line19.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:31) at $line19.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:36) at $line19.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:38) at $line19.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:40) at $line19.$read$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:42) at $line19.$read$$iw$$iw$$iw$$iw$$iw.<init>(<console>:44) at $line19.$read$$iw$$iw$$iw$$iw.<init>(<console>:46) at $line19.$read$$iw$$iw$$iw.<init>(<console>:48) at $line19.$read$$iw$$iw.<init>(<console>:50) at $line19.$read$$iw.<init>(<console>:52) at $line19.$read.<init>(<console>:54) at $line19.$read$.<init>(<console>:58) at $line19.$read$.<clinit>(<console>) at $line19.$eval$.$print$lzycompute(<console>:7) at $line19.$eval$.$print(<console>:6) at $line19.$eval.$print(<console>) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at scala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:786) at scala.tools.nsc.interpreter.IMain$Request.loadAndRun(IMain.scala:1047) at scala.tools.nsc.interpreter.IMain$WrappedRequest$$anonfun$loadAndRunReq$1.apply(IMain.scala:638) at scala.tools.nsc.interpreter.IMain$WrappedRequest$$anonfun$loadAndRunReq$1.apply(IMain.scala:637) at scala.reflect.internal.util.ScalaClassLoader$class.asContext(ScalaClassLoader.scala:31) at scala.reflect.internal.util.AbstractFileClassLoader.asContext(AbstractFileClassLoader.scala:19) at scala.tools.nsc.interpreter.IMain$WrappedRequest.loadAndRunReq(IMain.scala:637) at scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:569) at scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:565) at scala.tools.nsc.interpreter.ILoop.interpretStartingWith(ILoop.scala:807) at scala.tools.nsc.interpreter.ILoop.command(ILoop.scala:681) at scala.tools.nsc.interpreter.ILoop.processLine(ILoop.scala:395) at scala.tools.nsc.interpreter.ILoop.loop(ILoop.scala:415) at scala.tools.nsc.interpreter.ILoop$$anonfun$process$1.apply$mcZ$sp(ILoop.scala:923) at scala.tools.nsc.interpreter.ILoop$$anonfun$process$1.apply(ILoop.scala:909) at scala.tools.nsc.interpreter.ILoop$$anonfun$process$1.apply(ILoop.scala:909) at scala.reflect.internal.util.ScalaClassLoader$.savingContextLoader(ScalaClassLoader.scala:97) at scala.tools.nsc.interpreter.ILoop.process(ILoop.scala:909) at org.apache.spark.repl.Main$.doMain(Main.scala:68) at org.apache.spark.repl.Main$.main(Main.scala:51) at org.apache.spark.repl.Main.main(Main.scala) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:738) at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187) at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212) at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126) at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)Caused by: java.lang.RuntimeException: Table &#91;t3&#93; already exists under database &#91;default&#93; at scala.sys.package$.error(package.scala:27) at org.apache.spark.sql.execution.command.CreateTable.run(carbonTableSchema.scala:160) at org.apache.spark.sql.CarbonSource.createTableIfNotExists(CarbonSource.scala:180) at org.apache.spark.sql.CarbonSource.createRelation(CarbonSource.scala:114) at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:328) at org.apache.spark.sql.hive.HiveMetastoreCatalog$$anon$1.load(HiveMetastoreCatalog.scala:76) at org.apache.spark.sql.hive.HiveMetastoreCatalog$$anon$1.load(HiveMetastoreCatalog.scala:58) at org.spark_project.guava.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599) at org.spark_project.guava.cache.LocalCache$Segment.loadSync(LocalCache.java:2379) at org.spark_project.guava.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342) at org.spark_project.guava.cache.LocalCache$Segment.get(LocalCache.java:2257) ... 91 more17/04/17 23:00:10 AUDIT CarbonDropTableCommand: &#91;AppledeMacBook-Pro.local&#93;&#91;apple&#93;&#91;Thread-1&#93;Deleted table &#91;t3&#93; under database &#91;default&#93;res2: org.apache.spark.sql.DataFrame = []
issueID:CARBONDATA-946
type:Sub-task
changed files:
texts:TUPLEID implicit column support in spark 2.1
Implicit column tupleid, will uniquely identify a tuple (row) in data. So that update and delete can implemented using tupleid.
issueID:CARBONDATA-949
type:Bug
changed files:processing/src/main/java/org/apache/carbondata/processing/merger/CompactionResultSortProcessor.java
texts:Compaction gives NullPointerException after alter table query
After load if new column added to the table and minor compaction has been performed . Then new column contains null value by default or if bad_record exists . And cast should not done with null value.
issueID:CARBONDATA-95
type:Bug
changed files:processing/src/main/java/org/apache/carbondata/processing/datatypes/PrimitiveDataType.java
core/src/main/java/org/apache/carbondata/core/util/DataTypeUtil.java
texts:Columns values with numeric data types are not getting parsed when included in dictionary_include
When a numeric datatype lets say Decimal is defined for a column and the column is included as dictionary_include, then the whatever precision and scale is defined by the user is not taken into consideration and each value is accepted and dictionary is generated for that value.
issueID:CARBONDATA-950
type:Bug
changed files:
texts:selecting table data having a column of "date" type throws exception in hive
selecting data from a hive table containing a column of date datatype is failing to render output.Steps to reproduce:1) In Spark Shell :a) Create Table -import org.apache.spark.sql.SparkSessionimport org.apache.spark.sql.CarbonSession._val carbon = SparkSession.builder().config(sc.getConf).getOrCreateCarbonSession("hdfs://localhost:54310/opt/data")scala> carbon.sql(" create table my_user(id int, name string,dob date) stored by 'carbondata' ").showb) Load Data -scala> carbon.sql(""" load data inpath 'hdfs://localhost:54310/Files/my_user.csv' into table my_user """ ).show2) In Hive :a) Add Jars -add jar /home/neha/incubator-carbondata/assembly/target/scala-2.11/carbondata_2.11-1.1.0-incubating-SNAPSHOT-shade-hadoop2.7.2.jar;add jar /opt/spark-2.1.0-bin-hadoop2.7/jars/spark-catalyst_2.11-2.1.0.jar;add jar /home/neha/incubator-carbondata/integration/hive/carbondata-hive-1.1.0-incubating-SNAPSHOT.jar;b) Create Table -create table my_user(id int, name string,dob date);c) Alter location -hive> alter table my_user set LOCATION 'hdfs://localhost:54310/opt/data/default/my_user' ;d) Set Properties -set hive.mapred.supports.subdirectories=true;set mapreduce.input.fileinputformat.input.dir.recursive=true;d) Alter FileFormat -alter table my_user set FILEFORMATINPUTFORMAT "org.apache.carbondata.hive.MapredCarbonInputFormat"OUTPUTFORMAT "org.apache.carbondata.hive.MapredCarbonOutputFormat"SERDE "org.apache.carbondata.hive.CarbonHiveSerDe";e) Query:select * from my_user;Expected Output: display all the data of table my_user.Actual Output:Failed with exception java.io.IOException:java.lang.ClassCastException: java.lang.Integer cannot be cast to java.lang.Long
issueID:CARBONDATA-953
type:Bug
changed files:processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/SortDataRows.java
hadoop/src/main/java/org/apache/carbondata/hadoop/readsupport/impl/DictionaryDecodeReadSupport.java
core/src/main/java/org/apache/carbondata/core/memory/UnsafeMemoryManager.java
core/src/main/java/org/apache/carbondata/core/util/CarbonProperties.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/UnsafeSortDataRows.java
texts:Add validations to Unsafe dataload. And control the data added to threads
Add validations to Unsafe dataload, now there are no validations of how much chunksize can be configured and how much working thread memory uses. And also there is no control of adding data  to sort threads so it may lead to out of memory.
issueID:CARBONDATA-954
type:Bug
changed files:
texts:The driverExecutorCacheConfTest failed because of interaction between testcases in CacheProviderTest
Problem: The driverExecutorCacheConfTest will fail when run all test cases in CacheProviderTest, while just run driverExecutorCacheConfTest will success.Solution：The driverExecutorCacheConfTest will fail after run the second test case (createCache), because CacheProvider.getInstance() will get the instance which have created caches in second testcase(createCache).So suggest drop all caches after assertion in second testcase(createCache).
issueID:CARBONDATA-955
type:Bug
changed files:
texts:CacheProvider test fails
CacheProvider test fails in core package.
issueID:CARBONDATA-957
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/locks/HdfsFileLock.java
core/src/main/java/org/apache/carbondata/core/locks/LocalFileLock.java
texts:Table not found exception in rename table after lock acquire failure
During rename table if an exception is thrown during acquiring locks then table not exists exception was thrown while reverting the changes.
issueID:CARBONDATA-958
type:Bug
changed files:
texts:Schema modified time not updated in modifiedtime.mdt when dictionary column is updated to no-dictionary column due to high cardinality
Schema modified time not updated in modifiedtime.mdt when dictionary column is updated to no-dictionary column due to high cardinalityDue to this when we do data loading to same table from multiple spark-submit applications, schema status is not synchronized between applications and hence dictionary file gets generated for high cardinality column also
issueID:CARBONDATA-959
type:Bug
changed files:
texts:Unable to query carbon table with complex data type on hive
started the  spark shellcreated carbon table with complex type carbon.sql("create table complextypes(id string,MAC array<string>) stored by 'carbondata' ")carbon.sql("""LOAD DATA INPATH 'hdfs://localhost:54310/newdata.csv' into table complextypes""")start hive shellcreate table complex(id string,mac array<string>);OKalter table complextype set FILEFORMAT    > INPUTFORMAT "org.apache.carbondata.hive.MapredCarbonInputFormat"    > OUTPUTFORMAT "org.apache.carbondata.hive.MapredCarbonOutputFormat"    > SERDE "org.apache.carbondata.hive.CarbonHiveSerDe";alter table complex set LOCATION 'hdfs://localhost:54310/opt/carbonStore/default/complextypes' ;query the data hive> select * from complex;OKFailed with exception java.io.IOException:java.lang.NullPointerExceptionTime taken: 2.05 seconds
issueID:CARBONDATA-96
type:Improvement
changed files:
texts:make zookeeper lock as default if zookeeper url is configured.
make the lock type as zookeeper if zookeeper URL is present in the spark conf.if spark.deploy.zookeeper.url property is set in spark-default.conf then need to take the zookeeper locking.
issueID:CARBONDATA-960
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/util/ByteUtil.java
core/src/main/java/org/apache/carbondata/core/constants/CarbonCommonConstants.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/comparator/UnsafeRowComparator.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/holder/UnsafeInmemoryMergeHolder.java
texts:Unsafe merge sort is not working properly
Unsafe merge sort is not working properly as the sorted data is wrong. When we tried to load 3.5 billion data we found that data is not sorted properly through unsafe sort
issueID:CARBONDATA-962
type:Bug
changed files:
texts:Less than operator(<) doesn&#39;t work properly in presto integration
Less than operator(<) doesn't work properly in presto integrationsteps to reproduce :1. in beelinea) create table:CREATE TABLE uniqdata (CUST_ID int,CUST_NAME String,ACTIVE_EMUI_VERSION string, DOB timestamp, DOJ timestamp, BIGINT_COLUMN1 bigint,BIGINT_COLUMN2 bigint,DECIMAL_COLUMN1 decimal(30,10), DECIMAL_COLUMN2 decimal(36,10),Double_COLUMN1 double, Double_COLUMN2 double,INTEGER_COLUMN1 int) STORED BY 'org.apache.carbondata.format' TBLPROPERTIES ("TABLE_BLOCKSIZE"= "256 MB");b) load data :LOAD DATA INPATH 'hdfs://localhost:54310/2000_UniqData.csv' into table uniqdata OPTIONS('DELIMITER'=',' , 'QUOTECHAR'='"','BAD_RECORDS_ACTION'='FORCE','FILEHEADER'='CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1,Double_COLUMN2,INTEGER_COLUMN1');2. in prestoa) execute the query:select DOB from uniqdata where DOB <cast('1970-06-19 01:00:03'as timestamp) order by DOB;expected result: it should display dob in asc order where dob<’1970-06-19 01:00:03’actual result:          DOB           ------------------------- 1970-01-01 10:00:03.000 (1 row)Query 20170420_110635_00036_4hfqu, FINISHED, 1 nodeSplits: 18 total, 18 done (100.00%)0:00 &#91;1 rows, 1B&#93; &#91;5 rows/s, 5B/s&#93;
issueID:CARBONDATA-963
type:Bug
changed files:processing/src/main/java/org/apache/carbondata/processing/store/writer/v3/CarbonFactDataWriterImplV3.java
processing/src/main/java/org/apache/carbondata/processing/store/CarbonFactDataHandlerColumnar.java
core/src/main/java/org/apache/carbondata/core/memory/UnsafeMemoryAllocator.java
texts:Fixed data mismatch issue and memory leak issue
Problem:Data mismatch issueMemory leak issue and offheapSolution:1. Last page data is not writing in v3 format because of this data mismatch issue is coming ,2. Memory is not getting cleared for last block last blocklet and it is causing OOM
issueID:CARBONDATA-964
type:Bug
changed files:
texts:Add FAQ-How Carbon will behave when execute insert operation in abnormal scenarios?

issueID:CARBONDATA-965
type:Bug
changed files:processing/src/main/java/org/apache/carbondata/processing/util/CarbonLoaderUtil.java
processing/src/main/java/org/apache/carbondata/processing/loading/sort/unsafe/merger/UnsafeSingleThreadFinalSortFilesMerger.java
texts:dataload fail message is not correct when there is no good data to load

issueID:CARBONDATA-966
type:Improvement
changed files:
texts:char and varchar should be supported in ALTER ADD COLUMNS
char and varchar should be supported by ALTER ADD COLUMNS (same way as create table)
issueID:CARBONDATA-967
type:Bug
changed files:
texts:select * with order by and limit for join not working
CREATE TABLE carbon1 (imei string,age int,task bigint,sale decimal(10,3),productdate timestamp,score double)STORED BY 'org.apache.carbondata.format';LOAD DATA INPATH 'hdfs://hacluster/data/carbon1.csv'  INTO TABLE carbon1 options ('DELIMITER'=',', 'QUOTECHAR'='"','BAD_RECORDS_LOGGER_ENABLE'='false','BAD_RECORDS_ACTION'='FORCE',  'FILEHEADER'= ''); CREATE TABLE carbon2 (imei string,age int,task bigint,sale decimal(10,3),productdate timestamp,score double)STORED BY 'org.apache.carbondata.format';LOAD DATA INPATH 'hdfs://hacluster/data/carbon2.csv'  INTO TABLE carbon2 options ('DELIMITER'=',', 'QUOTECHAR'='"','BAD_RECORDS_LOGGER_ENABLE'='false','BAD_RECORDS_ACTION'='FORCE',  'FILEHEADER'= ''); CREATE TABLE carbon3 (imei string,age int,task bigint,sale decimal(10,3),productdate timestamp,score double)STORED BY 'org.apache.carbondata.format';LOAD DATA INPATH 'hdfs://hacluster/data/carbon1.csv'  INTO TABLE carbon3 options ('DELIMITER'=',', 'QUOTECHAR'='"','BAD_RECORDS_LOGGER_ENABLE'='false','BAD_RECORDS_ACTION'='FORCE',  'FILEHEADER'= '');select * from carbon1 a full outer join carbon2 b on substr(a.productdate,1,10)=substr(b.productdate,1,10) order by a.imei limit 100;it is throwing below exceptionERROR TaskSetManager: Task 0 in stage 12.0 failed 1 times; aborting jobException in thread "main" org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 12.0 failed 1 times, most recent failure: Lost task 0.0 in stage 12.0 (TID 211, localhost, executor driver): java.lang.ClassCastException: org.apache.spark.unsafe.types.UTF8String cannot be cast to java.lang.Integer at scala.runtime.BoxesRunTime.unboxToInt(BoxesRunTime.java:101) at org.apache.spark.sql.CarbonDictionaryDecoder$$anonfun$doExecute$1$$anonfun$3$$anon$1$$anonfun$next$1.apply$mcVI$sp(CarbonDictionaryDecoder.scala:112) at org.apache.spark.sql.CarbonDictionaryDecoder$$anonfun$doExecute$1$$anonfun$3$$anon$1$$anonfun$next$1.apply(CarbonDictionaryDecoder.scala:109) at org.apache.spark.sql.CarbonDictionaryDecoder$$anonfun$doExecute$1$$anonfun$3$$anon$1$$anonfun$next$1.apply(CarbonDictionaryDecoder.scala:109) at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59) at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48) at org.apache.spark.sql.CarbonDictionaryDecoder$$anonfun$doExecute$1$$anonfun$3$$anon$1.next(CarbonDictionaryDecoder.scala:109) at org.apache.spark.sql.CarbonDictionaryDecoder$$anonfun$doExecute$1$$anonfun$3$$anon$1.next(CarbonDictionaryDecoder.scala:99) at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:232) at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:225) at
issueID:CARBONDATA-968
type:Bug
changed files:processing/src/main/java/org/apache/carbondata/processing/sort/sortdata/SortParameters.java
processing/src/main/java/org/apache/carbondata/processing/util/CarbonDataProcessorUtil.java
processing/src/main/java/org/apache/carbondata/processing/loading/DataLoadProcessBuilder.java
processing/src/main/java/org/apache/carbondata/processing/util/CarbonLoaderUtil.java
processing/src/main/java/org/apache/carbondata/processing/merger/CompactionResultSortProcessor.java
core/src/main/java/org/apache/carbondata/core/scan/result/vector/MeasureDataVectorProcessor.java
texts:Alter temp store location and decimal data type incorrect result display correction
Below problems need to be addressed through this jira.1. Temp  store location path formation for compaction through sort step is incorrect. Temp store location key being formed is not proper.2. Data type change validations:a. New precision should always be greater than existing precision b. New scale value can be same as old scale value.3. Filter query bug on datatype changed for a decimal column. Steps to reproduce:create table alter_decimal_filter (n1 string, n2 int, n3 decimal(3,2)) stored by 'carbondata';insert into alter_decimal_filter select 'xx',1,1.22;insert into alter_decimal_filter select 'xx',1,1.23;alter table alter_decimal_filter change n3 n3 decimal(8,4);insert into alter_decimal_filter select 'dd',2,111.111;select * from alter_decimal_filter where n3 = 1.22;
issueID:CARBONDATA-969
type:Improvement
changed files:
texts:Don&#39;t persist rdd because it is only use once
In GlobalDictionaryUtil.readAllDictionaryFiles, don't persist rdd because it is only use once.
issueID:CARBONDATA-97
type:Bug
changed files:
texts:Decimal Precision and scale is not getting applied based on schema metadata
In case of decimal columns, the decimal values where getting round-off values and in case of 'IN' Filters the result was not coming because of this issue.In create command the user can set the precision and scale of a decimal column but while querying always the default value is been taken because of which the applied filter values where getting updated resulting in wrong results.
issueID:CARBONDATA-970
type:Bug
changed files:
texts:invalid tasks are getting referred even after files are cleaned from memeory
when we do clean files operation, if the block is not valid, the carbondata files will get deleted in valid segments. but the while getting the splits the, cleaned files are also referred. so we need to check whether the tasks are valid or not before getting the proper splits.
issueID:CARBONDATA-971
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/scan/collector/impl/AbstractScannedResultCollector.java
texts:Select query with where condition is failing
Bigdata Select query with where condition is failing
issueID:CARBONDATA-972
type:Bug
changed files:
texts:Concurrent ADD COLUMN operation fails

issueID:CARBONDATA-974
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
core/src/main/java/org/apache/carbondata/core/util/path/CarbonTablePath.java
texts:Index file loading performance issue in case of large cluter
In case of large cluster index file loading is slow as list file is getting calledCreate a index file path in code instead of list file
issueID:CARBONDATA-975
type:Bug
changed files:
texts:remove unreasonable code
remove the first "encoders.remove("   from the below code in carbonTableSchema.scalaencoders.remove(encoders.remove(Encoding.DICTIONARY))
issueID:CARBONDATA-976
type:Bug
changed files:
texts:Wrong entry getting deleted from schemaEvolution during alter revert

issueID:CARBONDATA-978
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/util/ByteUtil.java
core/src/main/java/org/apache/carbondata/core/scan/filter/executer/RangeValueFilterExecuterImpl.java
texts:Range Filter Evaluation Bug
Problem : Range Expression filter evaluation is unable to retreive correct output.Analysis : Range Filter Expression evaluation is unable to filter out correct values when the filter values are not present in the data. The Binary Search method always returns a value greater than the compared value being passed to it and the value doesn't match in the list.For Less Than and Greater than condition the non macthing return value should be adjusted accordingly.
issueID:CARBONDATA-979
type:Bug
changed files:
texts:Incorrect result displays to user in presto integration as compare to CarbonData.
Incorrect result displays to user in presto integration as compare to CarbonData (As in Carbondata our result set include null values but in presto it exclude those).Steps to reproduce :1. In CarbonData:a) Create table:CREATE TABLE uniqdata (CUST_ID int,CUST_NAME String,ACTIVE_EMUI_VERSION string, DOB timestamp, DOJ timestamp, BIGINT_COLUMN1 bigint,BIGINT_COLUMN2 bigint,DECIMAL_COLUMN1 decimal(30,10), DECIMAL_COLUMN2 decimal(36,10),Double_COLUMN1 double, Double_COLUMN2 double,INTEGER_COLUMN1 int) STORED BY 'org.apache.carbondata.format' TBLPROPERTIES ("TABLE_BLOCKSIZE"= "256 MB"); b) Load data : LOAD DATA INPATH 'hdfs://localhost:54310/2000_UniqData.csv' into table uniqdata OPTIONS('DELIMITER'=',' , 'QUOTECHAR'='"','BAD_RECORDS_ACTION'='FORCE','FILEHEADER'='CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1,Double_COLUMN2,INTEGER_COLUMN1'); 2. In presto a) Execute the query: select CUST_NAME from uniqdata where CUST_NAME !='CUST_NAME_01844' order by CUST_NAME expected result : it should display all cust_name except "cust_name_01844"Actual result: In CarbonData:"| CUST_NAME_01995  | CUST_NAME_01996   CUST_NAME_01997   CUST_NAME_01998   CUST_NAME_01999  ------------------+2,012 rows selected (1.777 seconds)"In presto:"CUST_NAME_01997  CUST_NAME_01998  CUST_NAME_01999 (2000 rows)Query 20170418_105903_00012_disp5, FINISHED, 1 nodeSplits: 18 total, 18 done (100.00%)3:21 &#91;2.01K rows, 1.97KB&#93; &#91;10 rows/s, 10B/s&#93;"
issueID:CARBONDATA-980
type:Bug
changed files:
texts:Result does not displays while using not null operator in presto integration.
Result does not displays while using not null operator in presto integration.Steps to reproduce :1. In CarbonData:a) Create table:CREATE TABLE uniqdata (CUST_ID int,CUST_NAME String,ACTIVE_EMUI_VERSION string, DOB timestamp, DOJ timestamp, BIGINT_COLUMN1 bigint,BIGINT_COLUMN2 bigint,DECIMAL_COLUMN1 decimal(30,10), DECIMAL_COLUMN2 decimal(36,10),Double_COLUMN1 double, Double_COLUMN2 double,INTEGER_COLUMN1 int) STORED BY 'org.apache.carbondata.format' TBLPROPERTIES ("TABLE_BLOCKSIZE"= "256 MB");b) Load data : LOAD DATA INPATH 'hdfs://localhost:54310/2000_UniqData.csv' into table uniqdata OPTIONS('DELIMITER'=',' , 'QUOTECHAR'='"','BAD_RECORDS_ACTION'='FORCE','FILEHEADER'='CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1,Double_COLUMN2,INTEGER_COLUMN1');2. In presto a) Execute the query: select CUST_ID  from uniqdata where CUST_ID IS NOT NULL order by CUST_IDExpected result:it should display all not null values from the table.Actual Result:In CarbonData:"| 10994    | 10995     10996     10997     10998    ----------+ CUST_ID  ----------+ 10999    ----------+2,001 rows selected (0.701 seconds)"In presto:"Query 20170420_073851_00038_hd7jy failed: null"
issueID:CARBONDATA-981
type:Bug
changed files:
texts:Configuration can&#39;t find HIVE_CONNECTION_URL in yarn-client mode

issueID:CARBONDATA-982
type:Bug
changed files:integration/presto/src/main/java/org/apache/carbondata/presto/CarbondataSplitManager.java
texts:Incorrect result displays while using not in clause in presto integration
Incorrect result displays while using not in clause in presto integrationSteps to reproduce :1. In CarbonData:a) Create table:CREATE TABLE uniqdata (CUST_ID int,CUST_NAME String,ACTIVE_EMUI_VERSION string, DOB timestamp, DOJ timestamp, BIGINT_COLUMN1 bigint,BIGINT_COLUMN2 bigint,DECIMAL_COLUMN1 decimal(30,10), DECIMAL_COLUMN2 decimal(36,10),Double_COLUMN1 double, Double_COLUMN2 double,INTEGER_COLUMN1 int) STORED BY 'org.apache.carbondata.format' TBLPROPERTIES ("TABLE_BLOCKSIZE"= "256 MB");b) Load data : LOAD DATA INPATH 'hdfs://localhost:54310/2000_UniqData.csv' into table uniqdata OPTIONS('DELIMITER'=',' , 'QUOTECHAR'='"','BAD_RECORDS_ACTION'='FORCE','FILEHEADER'='CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1,Double_COLUMN2,INTEGER_COLUMN1');2. In presto a) Execute the query: select CUST_ID from uniqdata where CUST_ID not in (9137,10137,14137);Expected result: it should display all the cust_id except 9137 and 10137.Actual Result:In Carbondata: 10995     10996     10997     10998     10999    ----------+2,011 rows selected (0.257 seconds) In presto: CUST_ID ---------(0 rows)Query 20170425_055652_00006_a4za4, FINISHED, 1 nodeSplits: 17 total, 17 done (100.00%)0:00 &#91;0 rows, 0B&#93; &#91;0 rows/s, 0B/s&#93;
issueID:CARBONDATA-983
type:Bug
changed files:
texts:Incorrect result displays while using not equal to (!=) operator in presto integration
Incorrect result displays while using not equal to (!=) operator in presto integration(result set should exclude the provided record but it is present in our   result set)Steps to reproduce :1. In CarbonData:a) Create table:CREATE TABLE uniqdata (CUST_ID int,CUST_NAME String,ACTIVE_EMUI_VERSION string, DOB timestamp, DOJ timestamp, BIGINT_COLUMN1 bigint,BIGINT_COLUMN2 bigint,DECIMAL_COLUMN1 decimal(30,10), DECIMAL_COLUMN2 decimal(36,10),Double_COLUMN1 double, Double_COLUMN2 double,INTEGER_COLUMN1 int) STORED BY 'org.apache.carbondata.format' TBLPROPERTIES ("TABLE_BLOCKSIZE"= "256 MB");b) Load data : LOAD DATA INPATH 'hdfs://localhost:54310/2000_UniqData.csv' into table uniqdata OPTIONS('DELIMITER'=',' , 'QUOTECHAR'='"','BAD_RECORDS_ACTION'='FORCE','FILEHEADER'='CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1,Double_COLUMN2,INTEGER_COLUMN1');2. In presto a) Execute the query: DECIMAL_COLUMN1 from UNIQDATA where DECIMAL_COLUMN1 !=12345678902.1234000000 order by DECIMAL_COLUMN1;b) Actual Result:In Carbondata:-------------------------+     DECIMAL_COLUMN1     -------------------------+ 12345678901.1234000000   12345678901.1234000000   12345678903.1234000000   12345678904.1234000000   12345678905.1234000000  In presto:    DECIMAL_COLUMN1     ------------------------ 12345678901.1234000000  12345678901.1234000000  12345678902.1234000000  12345678903.1234000000  12345678904.1234000000
issueID:CARBONDATA-984
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/util/CarbonMetadataUtil.java
texts:change word from Schenma to Schema
change word from Schenma to Schema
issueID:CARBONDATA-985
type:Bug
changed files:
texts:Remove unnecessary .show method call in test cases

issueID:CARBONDATA-986
type:Bug
changed files:
texts:Add alter table example

issueID:CARBONDATA-987
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/locks/LocalFileLock.java
texts:Can not delete lock file when drop table

issueID:CARBONDATA-989
type:Bug
changed files:core/src/main/java/org/apache/carbondata/core/datastore/impl/FileFactory.java
texts:decompressing error while load &#39;gz&#39; and &#39;bz2&#39; data into table
Run command in spark shell：import org.apache.spark.sql.SparkSessionimport org.apache.spark.sql.CarbonSession._val carbon = SparkSession.builder().config(sc.getConf).getOrCreateCarbonSession("hdfs://nsha/user/ranmx/test/carbon")carbon.sql("CREATE TABLE IF NOT EXISTS test_table(id string, name string, city string, age Int) STORED BY 'carbondata'")carbon.sql("LOAD DATA inpath '/ranmx/test/sh.csv.bz2' INTO TABLE test_table")get error：17/04/26 11:11:26 ERROR LoadTable: mainjava.lang.NullPointerException at org.apache.hadoop.io.compress.bzip2.Bzip2Factory.isNativeBzip2Loaded(Bzip2Factory.java:54) at org.apache.hadoop.io.compress.bzip2.Bzip2Factory.getBzip2DecompressorType(Bzip2Factory.java:120) at org.apache.hadoop.io.compress.BZip2Codec.getDecompressorType(BZip2Codec.java:242) at org.apache.hadoop.io.compress.CodecPool.getDecompressor(CodecPool.java:176) at org.apache.hadoop.io.compress.CompressionCodec$Util.createInputStreamWithCodecPool(CompressionCodec.java:157) at org.apache.hadoop.io.compress.BZip2Codec.createInputStream(BZip2Codec.java:157) at org.apache.carbondata.core.datastore.impl.FileFactory.getDataInputStream(FileFactory.java:139) at org.apache.carbondata.core.datastore.impl.FileFactory.getDataInputStream(FileFactory.java:104) at org.apache.carbondata.core.util.CarbonUtil.readHeader(CarbonUtil.java:1273) at org.apache.carbondata.spark.util.CommonUtil$.getCsvHeaderColumns(CommonUtil.scala:319) at org.apache.spark.sql.execution.command.LoadTable.run(carbonTableSchema.scala:474) ...
issueID:CARBONDATA-99
type:Bug
changed files:core/src/main/java/org/carbondata/scan/executor/util/QueryUtil.java
core/src/main/java/org/carbondata/scan/filter/FilterExpressionProcessor.java
core/src/main/java/org/carbondata/scan/filter/resolver/LogicalFilterResolverImpl.java
texts:Complex type column filters with like and not like not working
For complex type filter queries if query contains filter expression rather than BinaryExpression the system was not able to get the dimensions which are involved in the particular filter expression for executing complex type filter column expressions. because of this reason filter was failingSample Queryselect test3&#91;1&#93; from complex_filter where test4&#91;1&#93; not like'%1%' order by test1select test2&#91;0&#93; from complex_filter  where  test3&#91;0&#93; like '%1234%'
issueID:CARBONDATA-990
type:Bug
changed files:
texts:Installing and Configuring CarbonData instruction wrong
I followed the instruction on the page http://carbondata.incubator.apache.org/installation-guide.html and find an error in Installing and Configuring CarbonData on "Spark on YARN" Cluster part.The value of “spark.driver.extraClassPath” should be “$SPARK_HOME/carbonlib/” rather than “$SPARK_HOME/carbonlib/carbonlib/”.
issueID:CARBONDATA-992
type:Bug
changed files:
texts:Fix error log in Example module
ERROR LOG:17/04/27 09:57:21 ERROR DataLoadExecutor: &#91;Executor task launch worker-0&#93;&#91;partitionID:default_carbon_table_f404f6b7-19b8-491e-b4c6-e580cff48f4b&#93; Data Load is partially success for table carbon_table17/04/27 09:57:33 ERROR LocalFileLock: main Not able to delete the lock file /home/david/Documents/incubator-carbondata/examples/spark2/target/store/default/carbon_table/droptable.lock
issueID:CARBONDATA-993
type:Improvement
changed files:core/src/main/java/org/apache/carbondata/core/datastore/filesystem/AbstractDFSCarbonFile.java
texts:Remove confused variable fs & Fixed spelling mistakes
Remove variable fs, it may not be initialized when create a instance using AbstractDFSCarbonFile(FileStatus fileStatus).Fixed spelling mistakes.
issueID:CARBONDATA-994
type:Improvement
changed files:
texts:Incorrect result displays while using limit in presto integration
Incorrect result displays while using limit in presto integration(in carbondata null values appear first so result contain null values and records but in presto null values appear at last so result contain only records so resultset is different in both)Steps to reproduce :1. In CarbonData:a) Create table:CREATE TABLE uniqdata (CUST_ID int,CUST_NAME String,ACTIVE_EMUI_VERSION string, DOB timestamp, DOJ timestamp, BIGINT_COLUMN1 bigint,BIGINT_COLUMN2 bigint,DECIMAL_COLUMN1 decimal(30,10), DECIMAL_COLUMN2 decimal(36,10),Double_COLUMN1 double, Double_COLUMN2 double,INTEGER_COLUMN1 int) STORED BY 'org.apache.carbondata.format' TBLPROPERTIES ("TABLE_BLOCKSIZE"= "256 MB");b) Load data : LOAD DATA INPATH 'hdfs://localhost:54310/2000_UniqData.csv' into table uniqdata OPTIONS('DELIMITER'=',' , 'QUOTECHAR'='"','BAD_RECORDS_ACTION'='FORCE','FILEHEADER'='CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1,Double_COLUMN2,INTEGER_COLUMN1');2. In presto a) Execute the query:select CUST_ID as a from uniqdata  order by a asc limit 10Expected result:  it should display cust_id as per limitActual result :In CarbonData:"-------+   a   -------+ NULL   NULL   NULL   NULL   NULL   NULL   NULL   NULL   NULL   NULL  -------+10 rows selected (0.22 seconds)"In presto:"  a   ------ 8999  9000  9001  9002  9003  9004  9005  9006  9007  9008 (10 rows)Query 20170420_071958_00026_hd7jy, FINISHED, 1 nodeSplits: 18 total, 18 done (100.00%)0:00 &#91;2.01K rows, 1.97KB&#93; &#91;8.2K rows/s, 8.02KB/s&#93;"
issueID:CARBONDATA-995
type:Bug
changed files:
texts:Incorrect result displays while using variance aggregate function in presto integration
Incorrect result displays while using variance aggregate function in presto integrationSteps to reproduce :1. In CarbonData:a) Create table:CREATE TABLE uniqdata (CUST_ID int,CUST_NAME String,ACTIVE_EMUI_VERSION string, DOB timestamp, DOJ timestamp, BIGINT_COLUMN1 bigint,BIGINT_COLUMN2 bigint,DECIMAL_COLUMN1 decimal(30,10), DECIMAL_COLUMN2 decimal(36,10),Double_COLUMN1 double, Double_COLUMN2 double,INTEGER_COLUMN1 int) STORED BY 'org.apache.carbondata.format' TBLPROPERTIES ("TABLE_BLOCKSIZE"= "256 MB");b) Load data : LOAD DATA INPATH 'hdfs://localhost:54310/2000_UniqData.csv' into table uniqdata OPTIONS('DELIMITER'=',' , 'QUOTECHAR'='"','BAD_RECORDS_ACTION'='FORCE','FILEHEADER'='CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1,Double_COLUMN2,INTEGER_COLUMN1');2. In presto a) Execute the query:select variance(DECIMAL_COLUMN1) as a   from (select DECIMAL_COLUMN1 from UNIQDATA order by DECIMAL_COLUMN1) tActual result :In CarbonData :"--------------------+         a          --------------------+ 333832.4983039884  --------------------+1 row selected (0.695 seconds)"in presto:"         a         ------------------- 333832.3010442859 (1 row)Query 20170420_082837_00062_hd7jy, FINISHED, 1 nodeSplits: 35 total, 35 done (100.00%)0:00 &#91;2.01K rows, 1.97KB&#93; &#91;8.09K rows/s, 7.91KB/s&#93;"Expected result: it should display the same result as showing in CarbonData.
issueID:CARBONDATA-996
type:Bug
changed files:
texts:Incorrect result displays while using var_pop aggregate function in presto integration
Incorrect result displays while using var_pop aggregate function in presto integrationSteps to reproduce :1. In CarbonData:a) Create table:CREATE TABLE uniqdata (CUST_ID int,CUST_NAME String,ACTIVE_EMUI_VERSION string, DOB timestamp, DOJ timestamp, BIGINT_COLUMN1 bigint,BIGINT_COLUMN2 bigint,DECIMAL_COLUMN1 decimal(30,10), DECIMAL_COLUMN2 decimal(36,10),Double_COLUMN1 double, Double_COLUMN2 double,INTEGER_COLUMN1 int) STORED BY 'org.apache.carbondata.format' TBLPROPERTIES ("TABLE_BLOCKSIZE"= "256 MB");b) Load data : LOAD DATA INPATH 'hdfs://localhost:54310/2000_UniqData.csv' into table uniqdata OPTIONS('DELIMITER'=',' , 'QUOTECHAR'='"','BAD_RECORDS_ACTION'='FORCE','FILEHEADER'='CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1,Double_COLUMN2,INTEGER_COLUMN1');2. In presto a) Execute the query:select var_pop(DECIMAL_COLUMN1)  as a from (select DECIMAL_COLUMN1 from UNIQDATA order by DECIMAL_COLUMN1) tActual result:In CarbonData:"---------------------+          a          ---------------------+ 333665.66547125275  ---------------------+1 row selected (0.85 seconds)"In Presto:"         a         ------------------- 333665.4683101308 (1 row)Query 20170420_082535_00061_hd7jy, FINISHED, 1 nodeSplits: 35 total, 35 done (100.00%)0:00 &#91;2.01K rows, 1.97KB&#93; &#91;7.12K rows/s, 6.97KB/s&#93;"Expected result: it should display the same result as showing in CarbonData.
issueID:CARBONDATA-997
type:Bug
changed files:
texts:Correct result does not display in presto integration as compare to CarbonData
Correct result does not display in presto integration as compare to CarbonDataSteps to reproduce :1. In CarbonData:a) Create table:CREATE TABLE uniqdata (CUST_ID int,CUST_NAME String,ACTIVE_EMUI_VERSION string, DOB timestamp, DOJ timestamp, BIGINT_COLUMN1 bigint,BIGINT_COLUMN2 bigint,DECIMAL_COLUMN1 decimal(30,10), DECIMAL_COLUMN2 decimal(36,10),Double_COLUMN1 double, Double_COLUMN2 double,INTEGER_COLUMN1 int) STORED BY 'org.apache.carbondata.format' TBLPROPERTIES ("TABLE_BLOCKSIZE"= "256 MB");b) Load data : LOAD DATA INPATH 'hdfs://localhost:54310/2000_UniqData.csv' into table uniqdata OPTIONS('DELIMITER'=',' , 'QUOTECHAR'='"','BAD_RECORDS_ACTION'='FORCE','FILEHEADER'='CUST_ID,CUST_NAME,ACTIVE_EMUI_VERSION,DOB,DOJ,BIGINT_COLUMN1,BIGINT_COLUMN2,DECIMAL_COLUMN1,DECIMAL_COLUMN2,Double_COLUMN1,Double_COLUMN2,INTEGER_COLUMN1');2. In presto a) Execute the query:select BIGINT_COLUMN1, BIGINT_COLUMN1 from UNIQDATA where DECIMAL_COLUMN1<=BIGINT_COLUMN1 order by BIGINT_COLUMN1Actual result :In CarbonData:"| 123372038849    | 123372038849    | 123372038850     123372038850     123372038851     123372038851     123372038852     123372038852     123372038853     123372038853    --------------------------------2,000 rows selected (1.087 seconds)"In presto:"Query 20170420_091614_00065_hd7jy failed: line 1:100: Column 'bigint_column1' is ambiguousselect BIGINT_COLUMN1, BIGINT_COLUMN1 from UNIQDATA where DECIMAL_COLUMN1<=BIGINT_COLUMN1 order by BIGINT_COLUMN1"Expected result: it should display the same result as showing in CarbonData.
issueID:CARBONDATA-999
type:Bug
changed files:
texts:use carbondata bulket feature，but it doesn&#39;t seem to work?
1.CREATE TABLE shop_test(platFormId int,sellerNick string,companyGuid STRING,companyName STRING) STORED BY 'carbondata' TBLPROPERTIES ('BUCKETNUMBER'='2','BUCKETCOLUMNS'='sellerNick')2. .when loading datathe sorter is type of ParallelReadMergeSorterImpl,not ParallelReadMergeSorterWithBucketingImpl,why configuration.getBucketingInfo is null?What is wrong with that? Can you fix it?3.hadoop dfs -lsr /Opt/CarbonStore/default/shop_testdrwxr-xr-x   - root supergroup          0 2017-04-27 15:37 /Opt/CarbonStore/default/shop_test/Factdrwxr-xr-x   - root supergroup          0 2017-04-27 15:37 /Opt/CarbonStore/default/shop_test/Fact/Part0drwxr-xr-x   - root supergroup          0 2017-04-27 15:37 /Opt/CarbonStore/default/shop_test/Fact/Part0/Segment_0rw-rr-   3 root supergroup        566 2017-04-27 15:37 /Opt/CarbonStore/default/shop_test/Fact/Part0/Segment_0/0_batchno0-0-1493278648826.carbonindexrw-rr-   3 root supergroup        891 2017-04-27 15:37 /Opt/CarbonStore/default/shop_test/Fact/Part0/Segment_0/part-0-0_batchno0-0-1493278648826.carbondata
